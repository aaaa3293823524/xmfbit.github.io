<?xml version="1.0" encoding="utf-8"?>
<search>
  
    
    <entry>
      <title><![CDATA[YOLO 论文阅读]]></title>
      <url>%2F2017%2F02%2F04%2Fyolo-paper%2F</url>
      <content type="text"><![CDATA[YOLO(You Only Look Once)是一个流行的目标检测方法，和Faster RCNN等state of the art方法比起来，主打检测速度快。截止到目前为止（2017年2月初），YOLO已经发布了两个版本，在下文中分别称为YOLO V1和YOLO V2。YOLO V2的代码目前作为Darknet的一部分开源在GitHub。在这篇博客中，记录了阅读YOLO两个版本论文中的重点内容，并着重总结V2版本的改进。 YOLO V1这里不妨把YOLO V1论文“You Only Look Once: Unitied, Real-Time Object Detection”的摘要部分意译如下： 我们提出了一种新的物体检测方法YOLO。之前的目标检测方法大多把分类器重新调整用于检测（这里应该是在说以前的方法大多将检测问题看做分类问题，使用滑动窗提取特征并使用分类器进行分类）。我们将检测问题看做回归，分别给出bounding box的位置和对应的类别概率。对于给定输入图像，只需使用CNN网络计算一次，就同时给出bounding box位置和类别概率。由于整个pipeline都是同一个网络，所以很容易进行端到端的训练。YOLO相当快。base model可以达到45fps，一个更小的model（Fast YOLO），可以达到155fps，同时mAP是其他可以实现实时检测方法的两倍。和目前的State of the art方法相比，YOLO的localization误差较大，但是对于背景有更低的误检（False Positives）。同时，YOLO能够学习到更加泛化的特征，例如艺术品中物体的检测等任务，表现很好。 和以往较为常见的方法，如HoG+SVM的行人检测，DPM，RCNN系方法等不同，YOLO接受image作为输入，直接输出bounding box的位置和对应位置为某一类的概率。我们可以把它和目前的State of the art的Faster RCNN方法对比。Faster RCNN方法需要两个网络RPN和Fast RCNN，其中前者负责接受图像输入，并提出proposal。后续Fast RCNN再给出这些proposal是某一类的概率。也正是因为这种“直来直去”的方法，YOLO才能达到这么快的速度。也真是令人感叹，网络参数够多，数据够多，什么映射关系都能学出来。。。下图就是YOLO的检测系统示意图。在test阶段，经过单个CNN网络前向计算后，再经过非极大值抑制，就可以给出检测结果。 基本思路 网格划分：将输入image划分为$S \times S$个grid cell，如果image中某个object box的中心落在某个grid cell内部，那么这个cell就对检测该object负责（responsible for detection that object）。同时，每个grid cell同时预测$B$个bounding box的位置和一个置信度。这个置信度并不只是该bounding box是待检测目标的概率，而是该bounding box是待检测目标的概率乘上该bounding box和真实位置的IoU的积。通过乘上这个交并比，反映出该bounding box预测位置的精度。如下式所示： \text{confidence} = P(\text{Object})\times \text{IoU}_{\text{pred}}^{\text{truth}} 网络输出：每个bounding box对应于5个输出，分别是$x,y,w,h$和上述提到的置信度。其中，$x,y$代表bounding box的中心离开其所在grid cell边界的偏移。$w,h$代表bounding box真实宽高相对于整幅图像的比例。$x,y,w,h$这几个参数都已经被bounded到了区间$[0,1]$上。除此以外，每个grid cell还产生$C$个条件概率，$P(\text{Class}_i|\text{Object})$。注意，我们不管$B$的大小，每个grid cell只产生一组这样的概率。在test的非极大值抑制阶段，对于每个bounding box，我们应该按照下式衡量该框是否应该予以保留。 \text{confidence}\times P(\text{Class}_i|\text{Object}) = P(\text{Class}_i)\times \text{IoU}_{\text{pred}}^{\text{truth}} 实际参数：在PASCAL VOC进行测试时，使用$S = 7$, $B=2$。由于共有20类，故$C=20$。所以，我们的网络输出大小为$7\times 7 \times 30$ 网络模型结构Inspired by GoogLeNet，但是没有采取inception的结构，simply使用了$1\times 1$的卷积核。base model共有24个卷积层，后面接2个全连接层，如下图所示。 另外，Fast YOLO使用了更小的网络结构（9个卷积层，且filter的数目也少了），其他部分完全一样。 训练同很多人一样，这里作者也是先在ImageNet上做了预训练。使用上图网络结构中的前20个卷积层，后面接一个average-pooling层和全连接层，在ImageNet 1000类数据集上训练了一周，达到了88%的top-5准确率。 由于Ren的论文提到给预训练的模型加上额外的卷积层和全连接层能够提升性能，所以我们加上了剩下的4个卷积层和2个全连接层，权值为随机初始化。同时，我们把网络输入的分辨率从$224\times 224$提升到了$448 \times 448$。 在最后一层，我们使用了线性激活函数；其它层使用了leaky ReLU激活函数。如下所示： f(x)= \begin{cases} x, &\text{if}\ x > 0 \\\\ 0.1x, &\text{otherwise} \end{cases}很重要的问题就是定义很好的loss function，为此，作者提出了以下几点说明： loss的形式采用误差平方和的形式（真是把回归进行到底了。。。） 由于很多的grid cell没有目标物体存在，所以给有目标存在的bounding box和没有目标存在的bounding box设置了不同的比例因子进行平衡。具体来说，\lambda_{\text{coord}} = 5，\lambda_{\text{noobj}} = 0.5 直接使用$w$和$h$，这样大的box的差异在loss中占得比重会和小的box不平衡，所以这里使用$\sqrt{w}$和$\sqrt{h}$。 上文中已经提到，同一个grid cell会提出多个bounding box。在training阶段，我们只想让一个bounding box对应object。所以，我们计算每个bounding box和ground truth的IoU，以此为标准得到最好的那个bounding box，其他的认为no obj。 loss函数的具体形式见下图（实在是不想打这一大串公式。。）。其中，$\mathbb{1}_i$表示是否有目标出现在第$i$个grid cell。 $\mathbb{1}_{i,j}$表示第$i$个grid cell的第$j$个bounding box是否对某个目标负责。 在进行反向传播时，由于loss都是二次形式，所以导数形式都是统一的。下面是Darknet中detection_layer.c中训练部分的代码。在代码中，计算了cost（也就是loss）和delta（也就是反向的导数）， 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121if(state.train)&#123; float avg_iou = 0; float avg_cat = 0; float avg_allcat = 0; float avg_obj = 0; float avg_anyobj = 0; int count = 0; *(l.cost) = 0; int size = l.inputs * l.batch; memset(l.delta, 0, size * sizeof(float)); for (b = 0; b &lt; l.batch; ++b)&#123; int index = b*l.inputs; // for each grid cell for (i = 0; i &lt; locations; ++i) &#123; // locations = S * S = 49 int truth_index = (b*locations + i)*(1+l.coords+l.classes); int is_obj = state.truth[truth_index]; // for each bbox for (j = 0; j &lt; l.n; ++j) &#123; // l.n = B = 2 int p_index = index + locations*l.classes + i*l.n + j; l.delta[p_index] = l.noobject_scale*(0 - l.output[p_index]); // 因为no obj对应的bbox很多，而responsible的只有一个 // 这里统一加上，如果一会判断该bbox responsible for object，再把它减去 *(l.cost) += l.noobject_scale*pow(l.output[p_index], 2); avg_anyobj += l.output[p_index]; &#125; int best_index = -1; float best_iou = 0; float best_rmse = 20; // 该grid cell没有目标，直接返回 if (!is_obj)&#123; continue; &#125; // 否则，找出responsible的bounding box，计算其他几项的loss int class_index = index + i*l.classes; for(j = 0; j &lt; l.classes; ++j) &#123; l.delta[class_index+j] = l.class_scale * (state.truth[truth_index+1+j] - l.output[class_index+j]); *(l.cost) += l.class_scale * pow(state.truth[truth_index+1+j] - l.output[class_index+j], 2); if(state.truth[truth_index + 1 + j]) avg_cat += l.output[class_index+j]; avg_allcat += l.output[class_index+j]; &#125; box truth = float_to_box(state.truth + truth_index + 1 + l.classes); truth.x /= l.side; truth.y /= l.side; // 找到最好的IoU，对应的bbox是responsible的，记录其index for(j = 0; j &lt; l.n; ++j)&#123; int box_index = index + locations*(l.classes + l.n) + (i*l.n + j) * l.coords; box out = float_to_box(l.output + box_index); out.x /= l.side; out.y /= l.side; if (l.sqrt)&#123; out.w = out.w*out.w; out.h = out.h*out.h; &#125; float iou = box_iou(out, truth); //iou = 0; float rmse = box_rmse(out, truth); if(best_iou &gt; 0 || iou &gt; 0)&#123; if(iou &gt; best_iou)&#123; best_iou = iou; best_index = j; &#125; &#125;else&#123; if(rmse &lt; best_rmse)&#123; best_rmse = rmse; best_index = j; &#125; &#125; &#125; if(l.forced)&#123; if(truth.w*truth.h &lt; .1)&#123; best_index = 1; &#125;else&#123; best_index = 0; &#125; &#125; if(l.random &amp;&amp; *(state.net.seen) &lt; 64000)&#123; best_index = rand()%l.n; &#125; int box_index = index + locations*(l.classes + l.n) + (i*l.n + best_index) * l.coords; int tbox_index = truth_index + 1 + l.classes; box out = float_to_box(l.output + box_index); out.x /= l.side; out.y /= l.side; if (l.sqrt) &#123; out.w = out.w*out.w; out.h = out.h*out.h; &#125; float iou = box_iou(out, truth); //printf("%d,", best_index); int p_index = index + locations*l.classes + i*l.n + best_index; *(l.cost) -= l.noobject_scale * pow(l.output[p_index], 2); // 还记得我们曾经统一加过吗？这里需要减去了 *(l.cost) += l.object_scale * pow(1-l.output[p_index], 2); avg_obj += l.output[p_index]; l.delta[p_index] = l.object_scale * (1.-l.output[p_index]); if(l.rescore)&#123; l.delta[p_index] = l.object_scale * (iou - l.output[p_index]); &#125; l.delta[box_index+0] = l.coord_scale*(state.truth[tbox_index + 0] - l.output[box_index + 0]); l.delta[box_index+1] = l.coord_scale*(state.truth[tbox_index + 1] - l.output[box_index + 1]); l.delta[box_index+2] = l.coord_scale*(state.truth[tbox_index + 2] - l.output[box_index + 2]); l.delta[box_index+3] = l.coord_scale*(state.truth[tbox_index + 3] - l.output[box_index + 3]); if(l.sqrt)&#123; l.delta[box_index+2] = l.coord_scale*(sqrt(state.truth[tbox_index + 2]) - l.output[box_index + 2]); l.delta[box_index+3] = l.coord_scale*(sqrt(state.truth[tbox_index + 3]) - l.output[box_index + 3]); &#125; *(l.cost) += pow(1-iou, 2); avg_iou += iou; ++count; &#125; &#125; YOLO V2YOLO V2是原作者在V1基础上做出改进后提出的。为了达到题目中所称的Better，Faster，Stronger的目标，主要改进点如下。当然，具体内容还是要深入论文。 受到Faster RCNN方法的启发，引入了anchor。同时使用了K-Means方法，对anchor数量进行了讨论，在精度和速度之间做出折中； 修改了网络结构，去掉了全连接层，改成了全卷积结构； 引入了WordTree结构，将检测和分类问题做成了一个统一的框架，并充分利用ImageNet和COCO数据集的数据。 下面，还是先把论文的摘要意译如下： 我们引入了YOLO 9000模型，它是实时物体检测的State of the art的工作，能够检测超过9K类目标。首先，我们对以前的工作（YOLO V1）做出了若干改进，使其成为了一种在实时检测方法内在PASCAL VOC和COCO上State of the art的效果。通过一种新颖的多尺度训练犯法（multi-scale training method）， YOLO V2适用于不同大小尺寸输入的image，在精度和效率上达到了很好地trade-off。在67fps的速度下，VOC2007上达到了76.8mAP的精度。40fps时，达到了78.6mAP，已经超过了Faster RCNN with ResNet和SSD的精度，同时比它们更快！最后，我们提出了一种能够同时进行检测任务和分类任务的联合训练方法。使用这种方法，我们在COCO detection dataset和ImageNet classification dataset上同时训练模型。这种方法使得我们能够对那些没有label上detection data的数据做出detection的预测。我们使用ImageNet的detection任务做了验证。YOLO 9000在仅有200类中44类的detection data的情况下，仍然在ImageNet datection任务中取得了19.7mAP的成绩。对于不在COCO中的156类，YOLO9000成绩为16.0mAP。我们使得YOLO9000能够在保证实时的前提下对9K类目标进行检测。 下面，根据论文中提出的各条改进措施做一说明。 改进1：引入BN层（Batch Normalization）Batch Normalization能够加快模型收敛，并提供一定的正则化。作者在每个conv层都加上了了BN层，同时去掉了原来模型中的drop out部分，这带来了2%的性能提升。 改进2：高分辨率分类器（High Resolution Classifier）YOLO V1首先在ImageNet上以$224\times 224$大小图像作为输入进行训练，之后在检测任务中提升到$448\times 448$。这里，作者在训练完224大小的分类网络后，首先调整网络大小为$448\times 448$，然后在ImageNet上进行fine tuning（10个epoch）。也就是得到了一个高分辨率的cls。再把它用detection上训练。这样，能够提升4%。 改进3：引入Anchor BoxYOLO V1中直接在CNN后面街上全连接层，直接回归bounding box的参数。这里引入了Faster RCNN中的anchor box概念，不再直接回归bounding box的参数，而是相对于anchor box的参数。 作者去掉后面的fc层和最后一个max pooling层，以期得到更高分辨率的feature map。同时，shrink网络接受$416\times 416$（而不是448）大小的输入image。这是因为我们想要最后的feature map大小是奇数，这样就能够得到一个center cell（比较大的目标，更有可能占据中间的位置）。由于YOLO conv-pooling的效应是将image downsamplig 32倍，所以最后feature map大小为$416/32 = 13$。 与YOLO V1不同的是，我们不再对同一个grid cell下的bounding box统一产生一个数量为$C$的类别概率，而是对于每一个bounding box都产生对应的$C$类概率。和YOLO V1一样的是，我们仍然产生confidence，意义也完全一样。 使用anchor后，我们的精度accuracy降低了，不过recall上来了。（这也较好理解。原来每个grid cell内部只有2个bounding box，造成recall不高。现在recall高上来了，accuracy会下降一些）。 改进4：Dimension Cluster在引入anchor box后，一个问题就是如何确定anchor的位置和大小？Faster RCNN中是手工选定的，每隔stride设定一个anchor，并根据不同的面积比例和长宽比例产生9个anchor box。在本文中，作者使用了聚类方法对如何选取anchor box做了探究。这点应该是论文中很有新意的地方。 这里对作者使用的方法不再过多赘述，强调以下两点： 作者使用的聚类方法是K-Means； 相似性度量不用欧氏距离，而是用IoU，定义如下：d(\text{box}, \text{centroid}) = 1-\text{IoU}(\text{box}, \text{centroid}) 使用不同的$k$，聚类实验结果如下，作者折中采用了$k = 5$。而且经过实验，发现当取$k=9$时候，已经能够超过Faster RCNN采用的手工固定anchor box的方法。下图右侧图是在COCO和VOC数据集上$k=5$的聚类后结果。这些box可以作为anchor box使用。 改进5：直接位置预测（Direct Location Prediction）我们仍然延续了YOLO V1中的思路，预测box相对于grid cell的位置。使用sigmoid函数作为激活函数，使得最终输出值落在$[0, 1]$这个区间上。 在output的feature map上，对于每个cell（共计$13\times 13$个），给出对应每个bounding box的输出$t_x$, $t_y$, $t_w$, $t_h$。每个cell共计$k=5$个bounding box。如何由这几个参数确定bounding box的真实位置呢？见下图。 设该grid cell距离图像左上角的offset是$(c_x, c_y)$，那么bounding box的位置和宽高计算如下。注意，box的位置是相对于grid cell的，而宽高是相对于anchor box的。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[CS131-立体视觉基础]]></title>
      <url>%2F2017%2F02%2F02%2Fcs131-camera%2F</url>
      <content type="text"><![CDATA[数字图像是现实世界的物体通过光学成像设备在感光材料上的投影。在3D到2D的转换过程中，深度信息丢失。如何从单幅或者多幅图像中恢复出有用的3D信息，需要使用立体视觉的知识进行分析。如下图所示。这篇博客对课程讲义中立体视觉部分做一整理，分别介绍了针孔相机模型和对极集合的基础知识。 针孔相机模型（Pinhole Camera）针孔相机是简化的相机模型。光线沿直线传播。物体反射的光线，通过针孔，在成像面形成倒立的影像。针孔与成像面的距离，称为焦距。一般而言，针孔越小，影像越清晰，但针孔太小，会导致衍射，反而令影像模糊。 投影几何的重要性质在对针孔相机详细说明之前，首先介绍投影几何（Projective Geometry）的几条性质。 在投影变换前后，长度和角度信息丢失，但直线仍然保持是直线。如下图所示。原来垂直和平行的台球桌面的边不再保持原有的角度和相对长度，但是仍然是直线。 另一方面，虽然3D世界中的平行线在投影变换后相交（我们将交点称为Vanishing point），但是3D世界中互相平行的线，在2D平面上的Vanishing point为同一点。而所有平行线的Vanishing point又在同一条直线上。如下图所示（竖直的绿色平行线在投影变换后仍然保持平行，实际上并不想交，但我们这里认为其交于无穷远的某点）。 针孔相机模型如下图所示，为针孔相机成像原理示意图。外部世界的一点$P$发出（或反射）的光线，经过小孔$O$，成像在在像平面$\Pi^\prime$的点$P^\prime$处。通过简单的相似三角形知识，可以得到图上的关系式。 由上图可以知道，3D空间的一点$P$与成像平面上的对应点$P^\prime$坐标之间的数量关系为： \left\{\begin{matrix} x^\prime = fx/z \\ y^\prime = fy/z \end{matrix}\right.可是，由于变量$z$位于分母的位置，也就是说如果直接写成变换矩阵的话，这个矩阵是和变量$z$有关的，我们不想这样，而是想得到一个完全由相机本身确定的变换矩阵。所以，我们借鉴齐次矩阵的思想，将原来的点增加一个维度，转换为齐次坐标。如下图的两个例子所示。 这样，我们可以将3D和2D之间的变换写成下面的形式。当实际计算的时候，我们首先将3D点转换为4维向量（在末尾补1），然后左乘变换矩阵。最后将得到的结果化为末尾元素为1的形式，这样，前面两个元素就代表了变换后2D点的坐标。这样做，使得变换矩阵$M$完全由相机参数（即焦距$f$）决定。 上面的式子只是理想情况下的相机模型，成立的假设条件较强，主要有以下几点假设： 内假设（和相机本身有关） 不同方向上焦距相同； 光学中心在相平面的坐标原点$(0, 0)$ 没有倾斜（no skew） 外假设（和相机位姿有关，和相机本身参数无关） 相机没有旋转（坐标轴与世界坐标系方向重合） 相机没有平移（相机中心与世界坐标系中心重合） 其中，no skew情形我也不知道中文应该如何翻译。这种情况如图所示，由于剪切变形，成像平面的坐标轴并不是严格正交，使得两个轴之间实际存在一个很小的夹角。 下面，按照slide的顺序逐渐将上述假设移去，分析变换矩阵$M$应该如何修正。 理想情况理想情况以上假设全部满足，矩阵$M$如下所示。 光学中心不在像平面的坐标原点假设光学中心位于像平面内坐标为$(u_0, v_0)$的位置。则需要在理想情况计算的坐标基础上加上这个偏移量。所以，矩阵$M$被修正为： 像素非正方形由于透镜在$x$和$y$方向上的焦距不同，使得两个方向上的比例因子不同，不能再使用同一个$f$计算。修正如下： no skew这时候$x$方向上其实有$y$轴坐标的分量，所以修正如下： 相机的旋转和平移相机外部姿态很可能经过了旋转和平移，从而不与世界坐标系完全重合。如图所示。 所以我们需要先对世界坐标系内的3D点做一个齐次变换，将其变换为一个中间坐标系，这个中间坐标系和相机坐标系是重合的。也就是我们的变换矩阵应该是下面这个形式的，其中，$H$是表征旋转和平移的齐次矩阵，$H\in \mathbb{R}^{3\times 4}$。 P^\prime = MHP首先考虑较为简单的平移。所做的修正如下所示，也就是将原来的$\mathbb{0}$矩阵变为了一个平移向量。 进一步考虑旋转时，我们将旋转分解为绕三个坐标轴的三次旋转，单次旋转矩阵如下所示： 将它们按照旋转顺序左乘起来，可以得到修正后的变换形式如下： 最终形式综上所示，变换矩阵的最终形式为： 其中，内参数矩阵中有5个自由度，外参数矩阵中有6个自由度（3平移，3旋转）。 上面的内容总结起来，如下图所示。 对极几何基础概念如下图所示，给出了对极几何中的几个基本概念。两个相机的中心分别位于$O$点和$O^{‘}$点，被观察物体位于$P$点。 极点：$e$和$e^\prime$点分别是$OO^\prime$连线与两个成像平面的交点，也是相机中心在另一台相机成像平面上对应的像。 极平面：点$O$，$O^\prime$，$P$点共同确定的平面（灰色） 极线：极平面与两个成像平面的交线，即$pe$和$p^\prime e^\prime$（蓝色） 基线：两个相机中心的连线（黄色） 极线约束从上图可以看出，如果两台相机已经给定，那么给定物体点在左边相机像平面内的成像点，则右边相机成像平面内的点被唯一确定。如何能够找到这个对应点呢？或者换种说法，在对极几何中，如何描述这种不同相机成像点的约束关系呢？ 如下图所示，从上面的讨论我们已经知道，物点$P$在左右两个相机成像平面的坐标可以使用由相机内参数矩阵和外参数矩阵确定的齐次变换矩阵得到（下图在标注时，右侧相机的点应被标注为$p^\prime$）。为了简化问题，在下面的讨论时，我们假定相机都是理想的针孔相机模型，也就是说，变换矩阵只和外参数矩阵（也即是相机的位姿决定）。再做简化，认为世界坐标系与左侧相机的相机坐标系重合。那么，就得到了图上所示的变换矩阵$M$和$M^\prime$。其中，$R$是左右两个相机之间的旋转矩阵，$T$是$OO^\prime$有向线段，表明相机中心的位移。 （下面的推导参考了博客：计算机视觉基础4——对极几何）。在下面的推导中，我们使用$p^\prime$表示在相机$O^\prime$下的向量$O\prime P$，符号$p$同理。那么，有如下关系成立：$R(p-T) = p^\prime$]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[CS131-描述图像的特征(SIFT)]]></title>
      <url>%2F2017%2F01%2F30%2Fcs131-sift%2F</url>
      <content type="text"><![CDATA[SIFT(尺度不变特征变换，Scale Invariant Feature Transform),最早由Lowe提出，目的是为了解决目标检测（Object Detection）问题中提取图像特征的问题。从名字可以看出，SIFT的优势就在于对尺度变换的不变性，同时SIFT还满足平移变换和旋转变换下的不变性，并对光照变化和3D视角变换有一定的不变性。它的主要步骤如下： scale space上的极值检测。使用DoG在不同的scaleast和image position下找到interest point。 interest point的localization。对上面的interest point进行稳定度检测，并确定其所在的scale和position。 确定方向。通过计算图像的梯度图，确定key point的方向，下一步的feature operation就是在这个方向，scale和position上进行的。 确定key point的描述子。使用图像的局部梯度作为key point的描述子，最终构成SIFT特征向量。 SIFT介绍上讲中介绍的Harris角点方法计算简便，并具有平移不变性和旋转不变性。特征$f$对某种变换$\mathcal{T}$具有不变性，是指在经过变换后原特征保持不变，也就是$f(I) = f(\mathcal{T}(I))$。但是Harris角点不具有尺度变换不变性，如下图所示。当图像被放大后，原图的角点被判定为了边缘点。 而我们想要得到一种对尺度变换保持不变性的特征计算方法。例如图像patch的像素平均亮度，如下图所示。region size缩小为原始的一半后，亮度直方图的形状不变，即平均亮度不变。 而Lowe想到了使用局部极值来作为feature来保证对scale变换的不变性。在算法的具体实现中，他使用DoG来获得局部极值。 Lowe的论文中也提到了SIFT特征的应用。SIFT特征可以产生稠密（dense）的key point，例如一张500x500像素大小的图像，一般可以产生~2000个稳定的SIFT特征。在进行image matching和recognition时，可以将ref image的SIFT特征提前计算出来保存在数据库中，并计算待处理图像的SIFT特征，根据特征向量的欧氏距离进行匹配。 这篇博客主要是Lowe上述论文的读书笔记，按照SIFT特征的计算步骤进行组织。 尺度空间极值的检测方法前人研究已经指出，在一系列合理假设下，高斯核是唯一满足尺度不变性的。所谓的尺度空间，就是指原始图像$I(x,y)$和可变尺度的高斯核$G(x,y,\sigma)$的卷积结果。如下式所示： L(x,y,\sigma) = G(x,y,\sigma)\ast I(x,y)其中，$G(x,y, \sigma) = \frac{1}{2\pi\sigma^2}\exp(-(x^2+y^2)/2\sigma^2)$。不同的$\sigma$代表不同的尺度。 DoG(difference of Gaussian)函数定义为不同尺度的高斯核与图像卷积结果之差，即， D(x,y,\sigma) = L(x,y,k\sigma) - L(x,y,\sigma)如下图所示，输入的图像重复地与高斯核进行卷积得到结果图像$L$（左侧），这样构成了一个octave。相邻的$L$之间作差得到DoG图像（右侧）。当一个octave处理完之后，将当前octave的最后一张高斯卷积结果图像降采样两倍，按照前述方法构造下一个octave。在图中，我们将一个octave划分为$s$个间隔（即$s+1$个图像）时，设$\sigma$最终变成了2倍（即$\sigma$加倍）。那么，显然有相邻两层之间的$k = 2^{1/s}$。不过为了保证首尾两张图像也能够计算差值，我们实际上需要再补上两张（做一个padding），也就是说一个octave内的总图像为$s+3$（下图中的$s=2$）。 为何我们要费力气得到DoG呢？论文中作者给出的解释是：DoG对scale-normalized后的Guassian Laplace函数$\sigma^2\Delta G$提供了足够的近似。其中前面的$\sigma^2$系数正是为了保证尺度不变性。而前人的研究则指出，$\sigma \Delta G$函数的极值，和其他一大票其他function比较时，能够提供最稳定的feature。 对于高斯核函数，有以下性质： \frac{\partial G}{\partial \sigma} = \sigma \Delta G我们将式子左侧的微分变成差分，得到了下式： \sigma\Delta G \approx \frac{G(x,y,k\sigma)-G(x,y,\sigma)}{k\sigma - \sigma}也就是： G(x,y,k\sigma)-G(x,y,\sigma) \approx (k-1)\sigma^2 \Delta G当$k=1$时，上式的近似误差为0（即上面的$s=\infty$，也就是说octave的划分是连续的）。但是当$k$较大时，如$\sqrt{2}$（这时$s=2$，octave划分十分粗糙了），仍然保证了稳定性。同时，由于相邻两层的比例$k$是定值，所以$k-1$这个因子对于检测极值没有影响。我们在计算时，无需除以$k-1$。 构造完了DoG图像金字塔，下面我们的任务就是检测其中的极值。如下图，对于每个点，我们将它与金字塔中相邻的26个点作比较，找到局部极值。 另外，我们将输入图像行列预先使用线性插值的方法resize为原来的2倍，获取更多的key point。 此外，在Lowe的论文中还提到了使用DoG的泰勒展开和海森矩阵进行key point的精细确定方法，并对key point进行过滤。这里也不再赘述了。 128维feature的获取我们需要在每个key point处提取128维的特征向量。这里结合CS131课程作业2的相关练习作出说明。对于每个key point，我们关注它位于图像金字塔中的具体层数以及像素点位置，也就是说通过索引pyramid{scale}(y, x)就可以获取key point。我们遍历key point，为它们赋予一个128维的特征向量。 我们选取point周围16x16大小的区域，称为一个patch。将其分为4x4共计16个cell。这样，每个cell内共有像素点16个。对于每个cell，我们计算它的局部梯度方向直方图，直方图共有8个bin。也就是说每个cell可以得到一个8维的特征向量。将16个cell的特征向量首尾相接，就得到了128维的SIFT特征向量。在下面的代码中，使用变量patch_mag和patch_theta分别代表patch的梯度幅值和角度，它们可以很简单地使用卷积和数学运算得到。 123patch_mag = sqrt(patch_dx.^2 + patch_dy.^2);patch_theta = atan2(patch_dy, patch_dx); % atan2的返回结果在区间[-pi, pi]上。patch_theta = mod(patch_theta, 2*pi); % 这里我们要将其转换为[0, 2pi] 之后，我们需要获取key point的主方向。其定义可见slide，即为key point扩展出来的patch的梯度方向直方图的峰值对应的角度。 所以我们首先应该设计如何构建局部梯度方向直方图。我们只要将[0, 2pi]区间划分为若干个bin，并将patch内的每个点使用其梯度大小向对应的bin内投票即可。如下所示： 12345678910111213141516171819202122232425262728293031323334function [histogram, angles] = ComputeGradientHistogram(num_bins, gradient_magnitudes, gradient_angles)% Compute a gradient histogram using gradient magnitudes and directions.% Each point is assigned to one of num_bins depending on its gradient% direction; the gradient magnitude of that point is added to its bin.%% INPUT% num_bins: The number of bins to which points should be assigned.% gradient_magnitudes, gradient angles:% Two arrays of the same shape where gradient_magnitudes(i) and% gradient_angles(i) give the magnitude and direction of the gradient% for the ith point. gradient_angles ranges from 0 to 2*pi% % OUTPUT% histogram: A 1 x num_bins array containing the gradient histogram. Entry 1 is% the sum of entries in gradient_magnitudes whose corresponding% gradient_angles lie between 0 and angle_step. Similarly, entry 2 is for% angles between angle_step and 2*angle_step. Angle_step is calculated as% 2*pi/num_bins.% angles: A 1 x num_bins array which holds the histogram bin lower bounds.% In other words, histogram(i) contains the sum of the% gradient magnitudes of all points whose gradient directions fall% in the range [angles(i), angles(i + 1)) angle_step = 2 * pi / num_bins; angles = 0 : angle_step : (2*pi-angle_step); histogram = zeros(1, num_bins); num = numel(gradient_angles); for n = 1:num index = floor(gradient_angles(n) / angle_step) + 1; histogram(index) = histogram(index) + gradient_magnitudes(n); end end Lowe论文中推荐的bin数目为36个，计算主方向的函数如下： 12345678910111213141516171819202122232425function direction = ComputeDominantDirection(gradient_magnitudes, gradient_angles)% Computes the dominant gradient direction for the region around a keypoint% given the scale of the keypoint and the gradient magnitudes and gradient% angles of the pixels in the region surrounding the keypoint.%% INPUT% gradient_magnitudes, gradient_angles:% Two arrays of the same shape where gradient_magnitudes(i) and% gradient_angles(i) give the magnitude and direction of the gradient for% the ith point. % Compute a gradient histogram using the weighted gradient magnitudes. % In David Lowe's paper he suggests using 36 bins for this histogram. num_bins = 36; % Step 1: % compute the 36-bin histogram of angles using ComputeGradientHistogram() [histogram, angle_bound] = ComputeGradientHistogram(num_bins, gradient_magnitudes, gradient_angles); % Step 2: % Find the maximum value of the gradient histogram, and set "direction" % to the angle corresponding to the maximum. (To match our solutions, % just use the lower-bound angle of the max histogram bin. (E.g. return % 0 radians if it's bin 1.) [~, max_index] = max(histogram); direction = angle_bound(max_index);end 之后，我们更新patch内各点的梯度方向，计算其与主方向的夹角，作为新的方向。并将梯度进行高斯平滑。 123patch_theta = patch_theta - ComputeDominantDirection(patch_mag, patch_theta);;patch_theta = mod(patch_theta, 2*pi);patch_mag = patch_mag .* fspecial('gaussian', patch_size, patch_size / 2); % patch_size = 16 遍历cell，计算feature如下： 123456789101112131415feature = [];row_iter = 1;for y = 1:num_histograms col_iter = 1; for x = 1:num_histograms cell_mag = patch_mag(row_iter: row_iter + pixelsPerHistogram - 1, ... col_iter: col_iter + pixelsPerHistogram - 1); cell_theta = patch_theta(row_iter: row_iter + pixelsPerHistogram - 1, ... col_iter: col_iter + pixelsPerHistogram - 1); [histogram, ~] = ComputeGradientHistogram(num_angles, cell_mag, cell_theta); feature = [feature, histogram]; col_iter = col_iter + pixelsPerHistogram; end row_iter = row_iter + pixelsPerHistogram;end 最后，对feature做normalization。首先将feature化为单位长度，并将其中太大的分量进行限幅（如0.2的阈值），之后再重新将其转换为单位长度。 这样，就完成了SIFT特征的计算。在Lowe的论文中，有更多对实现细节的讨论，这里只是跟随课程slide和作业走完了算法流程，不再赘述。 应用：图像特征点匹配和Harris角点一样，SIFT特征可以用作两幅图像的特征点匹配，并且具有多种变换不变性的优点。对于两幅图像分别计算得到的特征点SIFT特征向量，可以使用下面简单的方法搜寻匹配点。计算每组点对之间的欧式距离，如果最近的距离比第二近的距离小得多，那么可以认为有一对成功匹配的特征点。其MATLAB代码如下，其中descriptor是两幅图像的SIFT特征向量。阈值默认为取做0.7。 123456789101112131415161718192021222324252627282930313233343536373839function match = SIFTSimpleMatcher(descriptor1, descriptor2, thresh)% SIFTSimpleMatcher% Match one set of SIFT descriptors (descriptor1) to another set of% descriptors (decriptor2). Each descriptor from descriptor1 can at% most be matched to one member of descriptor2, but descriptors from% descriptor2 can be matched more than once.% % Matches are determined as follows:% For each descriptor vector in descriptor1, find the Euclidean distance% between it and each descriptor vector in descriptor2. If the smallest% distance is less than thresh*(the next smallest distance), we say that% the two vectors are a match, and we add the row [d1 index, d2 index] to% the "match" array.% % INPUT:% descriptor1: N1 * 128 matrix, each row is a SIFT descriptor.% descriptor2: N2 * 128 matrix, each row is a SIFT descriptor.% thresh: a given threshold of ratio. Typically 0.7%% OUTPUT:% Match: N * 2 matrix, each row is a match.% For example, Match(k, :) = [i, j] means i-th descriptor in% descriptor1 is matched to j-th descriptor in descriptor2. if ~exist('thresh', 'var'), thresh = 0.7; end match = []; [N1, ~] = size(descriptor1); for i = 1:N1 fea = descriptor1(i, :); err = bsxfun(@minus, fea, descriptor2); dis = sqrt(sum(err.^2, 2)); [sorted_dis, ind] = sort(dis, 1); if sorted_dis(1) &lt; thresh * sorted_dis(2) match = [match; [i, ind(1)]]; end endend 接下来，我们可以使用最小二乘法计算两幅图像之间的仿射变换矩阵（齐次变换矩阵）$H$。其中$H$满足： Hp_{\text{before}} = p_{\text{after}}其中 p = \begin{bmatrix}x \\\\ y \\\\ 1\end{bmatrix}对上式稍作变形，有 p_{\text{before}}^\dagger H^\dagger = p_{\text{after}}\dagger就可以使用标准的最小二乘正则方程进行求解了。代码如下： 1234567891011121314151617181920212223242526272829303132function H = ComputeAffineMatrix( Pt1, Pt2 )%ComputeAffineMatrix% Computes the transformation matrix that transforms a point from% coordinate frame 1 to coordinate frame 2%Input:% Pt1: N * 2 matrix, each row is a point in image 1% (N must be at least 3)% Pt2: N * 2 matrix, each row is the point in image 2 that% matches the same point in image 1 (N should be more than 3)%Output:% H: 3 * 3 affine transformation matrix,% such that H*pt1(i,:) = pt2(i,:) N = size(Pt1,1); if size(Pt1, 1) ~= size(Pt2, 1), error('Dimensions unmatched.'); elseif N&lt;3 error('At least 3 points are required.'); end % Convert the input points to homogeneous coordintes. P1 = [Pt1';ones(1,N)]; P2 = [Pt2';ones(1,N)]; H = P1*P1'\P1*P2'; H = H'; % Sometimes numerical issues cause least-squares to produce a bottom % row which is not exactly [0 0 1], which confuses some of the later % code. So we'll ensure the bottom row is exactly [0 0 1]. H(3,:) = [0 0 1];end 作业中的其他例子也很有趣，这里不再多说了。贴上两张图像拼接的实验结果吧~]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[CS131-描述图像的特征(Harris 角点)]]></title>
      <url>%2F2017%2F01%2F25%2Fcs131-finding-features%2F</url>
      <content type="text"><![CDATA[feature是对图像的描述。比如，图像整体灰度值的均值和方差就可以作为feature。如果我们想在一副图像中检测足球时，可以使用滑动窗方法，逐一检查窗口内的灰度值分布是否和一张给定的典型黑白格子足球相似。可想而知，这种方法的性能一定让人捉急。而在image matching问题中，常常需要将不同视角的同一目标物进行matching，进而计算相机转过的角度。这在SLAM问题中很有意义。如下图所示，同一处景观，在不同摄影师的镜头下，不仅视角不同，而且明暗变化也有很多差别，右侧的图暖色调更浓。这也告诉我们，上面提到的只使用全局图像灰度值来做feature的做法有多么不靠谱。 那么，让我们脱离全局特征，转而将注意力集中在局部特征上。这是因为使用局部特征能够更好地处理图像中的遮挡、变形等情况，而且我们的研究对象常常是图像中的部分区域而不是图像整体。更特殊地，在这一讲中，我们主要探究key point作为local feature的描述，并介绍Harris角点检测方法。在下一节中介绍SIFT特征。 Harris角点角点，即corner，和edge类似，区别在于其在两个方向上都有较为剧烈的灰度变化（而edge只在某一个方向上灰度值变化剧烈）。如图所示。 Harris角点得名于其发明者Harris，是一种常见的角点检测方法。给定观察窗口大小，计算平移后窗口内各个像素差值的加权平方和，如下式。 E(u,v) = \sum_x\sum_yw(x,y)[I(x+u, y+v) - I(x,y)]^2其中，窗口加权函数$w$可以取做门限函数或gaussian函数。如图所示。 使用泰勒级数展开，并忽略非线性项，我们有 I(x+u,y+v) = I(x,y) + I_x(x,y)u+I_y(x,y)v所以上式可以写成（线性二次型写成了矩阵形式）， E(u,v) = \sum_{x,y}w(I_xu+I_yv)^2 = \begin{bmatrix}u&v\end{bmatrix}M\begin{bmatrix}u\\\\v\end{bmatrix}其中， M = w\begin{bmatrix}I_x^2& I_xI_y\\\\I_xI_y&I_y^2\end{bmatrix}当使用门限函数时，权值$w_{i,j} = 1$，则， M = \begin{bmatrix}\sum I_xI_x& \sum I_xI_y\\\\\sum I_xI_y&\sum I_yI_y\end{bmatrix} = \sum \begin{bmatrix}I_x \\\\I_y\end{bmatrix}\begin{bmatrix}I_x &I_y\end{bmatrix}当corner与xy坐标轴对齐时候，如下图所示。由于在黑色观察窗口内，只有上侧和左侧存在边缘，且在上边缘，$I_y$很大，而$I_x=0$，在左侧边缘，$I_x$很大而$I_y = 0$，所以，矩阵 M = \begin{bmatrix}\lambda_1 & 0 \\\\ 0&\lambda_2 \end{bmatrix} 当corner与坐标轴没有对齐时，经过旋转变换就可以将其转换到与坐标轴对齐的角度，而这种旋转操作可以使用矩阵的相似化来表示（其实是二次型的化简，可以使用合同变换，而旋转变换的矩阵是酉矩阵，转置即为矩阵的逆，所以也是相似变换）。也就是说，矩阵$M$相似于某个对角阵。 M = R^{-1}\Sigma R, \text{其中}\Sigma = \begin{bmatrix}\lambda_1&0\\\\0&\lambda_2\end{bmatrix}所以，可以根据下面这张图利用矩阵$M$的特征值来判定角点和边缘点。当两个特征值都较大时为角点；当某个分量近似为0而另一个分量较大时，可以判定为边缘点（因为某个方向的导数为0）；当两个特征值都近似为0时，说明是普通点（flat point）。（课件原图如此，空缺的问号处应分别为$\lambda_1$和$\lambda_2$）。 然而矩阵的特征值计算较为复杂，所以使用下面的方法进行近似计算。 \theta = \det(M)-\alpha\text{trace}(M)^2 = \lambda_1\lambda_2-\alpha(\lambda_1+\lambda_2)^2 为了减弱噪声的影响，常常使用gaussian窗函数。如下式所示： w(x,y) = \exp(-(x^2+y^2)/2\sigma^2)]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[CS131-边缘检测]]></title>
      <url>%2F2017%2F01%2F24%2Fcs131-edge-detection%2F</url>
      <content type="text"><![CDATA[边缘(Edge)在哺乳动物的视觉中有重要意义。在由若干卷积层构成的深度神经网络中，较低层的卷积层就被训练成为对特定形状的边缘做出响应。边缘检测也是计算机视觉和图像处理领域中一个重要的问题。 边缘的产生若仍采取将图像视作某一函数的观点，边缘是指这个函数中的不连续点。边缘检测是后续进行目标检测和形状识别的基础，也能够在立体视觉中恢复视角等。边缘的来源主要有以下几点： 物体表面不平造成灰度值的不连续； 深度值不同造成灰度值不连续； 物体表面颜色的突变造成灰度值不连续 朴素思想利用边缘是图像中不连续点的这一性质，可以通过计算图像的一阶导数，再找到一阶导数的极大值，即认为是边缘点。如下图所示，在白黑交界处，图像一阶导数的值非常大，表明此处灰度值变化剧烈，是边缘。 问题转换为如何求取图像的一阶导数（或梯度）。由于图像是离散的二元函数，所以下文不再区分求导与差分。 在$x$方向上，令$g_x = \frac{\partial f}{\partial x}$；在$y$方向上，令$g_y = \frac{\partial f}{\partial y}$。梯度的大小和方向为 g = \lbrack g_x, g_y\rbrack, \theta = \arctan(g_y/g_x)通过和Sobel算子等做卷积，可以求取两个正交方向上图像的一阶导数，并计算梯度，之后检测梯度的局部极大值就能够找出边缘点了。 只是这种方法很容易受到噪声影响。如图所示，真实的边缘点被湮没在了噪声中。 改进1：先平滑改进措施1，可以首先对图像进行高斯平滑，再按照上面的方法求取边缘点。根据卷积的性质，有： \frac{d}{dx}(f\ast g) = f\ast\frac{d}{dx}g所以我们可以先求取高斯核的一阶导数，再和原始图像直接做一次卷积就可以一举两得了。这样，引出了DoG(Deriative of Gaussian)。$x$方向的DoG如图所示。 进行高斯平滑不可避免会使图像中原本的细节部分模糊，所以需要在克服噪声和引入模糊之间做好折中。 改进2：Canny检测子改进措施2，使用Canny检测子进行检测。Canny检测方法同样基于梯度，其基本原理如下： 使用DoG计算梯度幅值和方向。 非极大值抑制，这个过程需要根据梯度方向做线性插值。如图，沿着点$q$的梯度方向找到了$p$和$r$两个点。这两个点的梯度幅值需要根据其临近的两点做插值得到。 利用梯度方向和边缘线互相垂直这一性质，如图，若已经确定点$p$为边缘点，则向它的梯度方向正交方向上寻找下一个边缘点（点$r$或$s$）。这一步也叫edge linking。 同时，为了提高算法性能，Canny中采用了迟滞阈值的方法，设定low和high两个阈值，来判定某个点是否属于强或弱边缘点。在做edge linking的时候，从强边缘点开始，如果遇到了弱边缘点，则继续，直到某点的梯度幅值甚至比low还要小，则在此停止。 改进3：RANSAC方法有的时候，我们并不是想要找到所有的边缘点，可能只是想找到图像中水平方向的某些边缘。这时候可以考虑采用RANSAC方法。 RANSAC方法的思想在于，认为已有的feature大部分都是好的。这样，每次随机抽取出若干feature，建立model，再在整个feature集合上进行验证。那么由那些好的feature得到的model一定是得分较高的。（世界上还是好人多啊！）这样就剔除了离群点的影响。 以直线拟合为例，在下图中，给出了使用RANSAC方法拟合直线的步骤。如图1所示，由于离群点的存在，如果直接使用最小二乘法进行拟合，拟合结果效果会很不理想。由于确定一条直线需要两个点，所以从点集中选取两个点，并计算拟合直线。并计算点集中的点在这条直线附近的个数，作为对模型好坏的判定，这些点是新的内点。找出最优的那条直线，使用其所有内点再进行拟合，重复上述操作，直至迭代终止。 上述RANSAC方法进行直线拟合的过程可以总结如下： 按照上述思想，我分别使用最小二乘法和RANSAC方法尝试进行直线拟合。在下面的代码中，我首先产生了正常受到一定高斯噪声污染的数据（图中的红色点），这些点的真值都落在直线$y = 2x+1$上。而后，我随机变化了斜率和截距，以期产生一些离群点（图中的蓝色点）。当然，由于随机性，这种方法生成的点有可能仍然是内点。 而后，我分别使用上述两种方法进行拟合。可以从结果图中看出，RANSAC（绿色线）能够有效避免离群点的干扰，获得更好的拟合效果。在某次实验中，两种方法的拟合结果如下：12least square: a = 3.319566, b = -1.446528ransac method: a = 1.899640, b= 1.298608 实验使用的MATLAB代码如下：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162%% generate datax = 0:1:10;y_gt = 2*x+1;y = y_gt + randn(size(y_gt));scatter(x, y, [], [1,0,0]);hold onout_x = 0:1:10;out_y = 5*rand(size(out_x)).*out_x + 4*rand(size(out_x));scatter(out_x, out_y, [], [0,0,1]);X = [x, out_x]';Y = [y, out_y]';X = [X, ones(length(X), 1)];[a, b] = ls_fit(X, Y);plot(x, a*x+b, 'linestyle', '--', 'color', 'r');[ra, rb] = ransac_fit(X, Y, 100, 2, 0.5, 3);plot(x, ra*x+rb, 'linestyle', '-.', 'color', 'g');fprintf('least square: a = %f, b = %f\n',a, b);fprintf('ransac method: a = %f, b= %f\n', ra, rb)function [a, b] = ransac_fit(X, Y, k, n, t ,d)% ransac fit% k -- maximum iteration number% n -- smallest point numer required% t -- threshold to identify a point is fit well% d -- the number of nearby points to assert a model is finedata = [X, Y];N = size(data, 1);best_good_cnt = -1;best_a = 0;best_b = 0;for i = 1:k % sample point idx = randsample(N, n); data_sampled = data(idx, :); % fit with least square [a, b] = ls_fit(data_sampled(:, 1:2), data_sampled(:, 3)); % test model not_sampled = ones(N, 1); not_sampled(idx) = 0; not_sampled_data = data(not_sampled == 1, :); distance = abs(not_sampled_data(:, 1:2) * [a; b] - not_sampled_data(:, 3)) / sqrt(a^2+1); inner_flag = distance &lt; t; good_cnt = sum(inner_flag); if good_cnt &gt;= d &amp;&amp; good_cnt &gt; best_good_cnt best_good_cnt = good_cnt; data_refine = data(find(inner_flag), :); [a, b] = ls_fit(data_refine(:, 1:2), data_refine(:, 3)); best_a = a; best_b = b; end fprintf('iteration %d, best_a = %f, best_b = %f\n', i, best_a, best_b);enda = best_a;b = best_b;endfunction [a, b] = ls_fit(X, Y)% least square fitA = X'*X\X'*Y;a = A(1);b = A(2);end 我们对RANSAC稍作分析，可以大概了解试验次数$k$的确定方法。 仍然使用上述直线拟合的例子。如果所有点中内点所占的比例为$\omega$，每次挑选$n$个点尝试（上述demo代码中取$n=2$）。那么每次挑选的两个点全部是内点的概率为$\omega^n$。当选取的$n$个点全部为内点时，视为有效实验。那么，重复$k$次实验，有效实验次数为0的概率为$(1-\omega^n)^k$。由于底数小于1，所以我们只需尽量增大$k$，就能够降低这种倒霉的概率。下图是不同$n$和$\omega$情况下为了使得实验成功的概率大于0.99所需的$k$的分布。 RANSAC方法的有点在于能够较为鲁棒地估计模型的参数，而且实现简单。缺点在于当离群点比例较大时，为保证实验成功所需的$k$值较大。这时候，可能Hough变换等基于投票的方法更适合用于图像中的直线检测问题。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[CS131-线性滤波器和矩阵的SVD分解]]></title>
      <url>%2F2017%2F01%2F23%2Fcs131-filter-svd%2F</url>
      <content type="text"><![CDATA[数字图像可以看做$\mathbb{R}^2 \rightarrow \mathbb{R}^c$的映射，其中$c$是图像的channel数。使用信号与系统的角度来看，如果单独考察某个channel，可以将图像看做是二维离散系统。 卷积卷积的概念不再详述，利用不同的kernel与原始图像做卷积，就是对图像进行线性滤波的过程。卷积操作时，以某一个点为中心点，最终结果是这个点以及它的邻域点的线性组合，组合系数由kernel决定。一般kernel的大小取成奇数。如下图所示。（图片来自博客《图像卷积与滤波的一些知识点》） 在卷积操作时，常常需要对图像做padding，常用的padding方法有： zero padding，也就是填充0值。 edge replication，也就是复制边缘值进行填充。 mirror extension，也就是将图像看做是周期性的，相当于使用对侧像素值进行填充。 作业1调整图像灰度值为0到255计算相应的k和offset值即可。另外MATLAB中的uint8函数可以将结果削顶与截底为0到255之间。123scale_ratio = 255.0 / (max_val - min_val);offset = -min_val * scale_ratio;fixedimg = scale_ratio * dark + offset; SVD图像压缩使用SVD进行图像压缩，下图是将所有奇异值按照从大到小的顺序排列的大小示意图。可以看到，第一个奇异值比其他高出一个数量级。 MATLAB实现分别使用10，50， 100个分量进行图像压缩，如下图所示。可以看到，k=10时，已经能够复原出原图像的大致轮廓。当k更大时，更多细节被复原出来。 MATLAB代码如下：12345678910111213141516171819202122232425%% read imageim = imread('./flower.bmp');im_gray = double(rgb2gray(im));[u, s, v] = svd(im_gray);%% get sigular valuesigma = diag(s);top_k = sigma(1:10);figureplot(1:length(sigma), sigma, 'r-', 'marker', 's', 'markerfacecolor', 'g');figuresubplot(2, 2, 1);imshow(uint8(im_gray));title('flower.bmp')index = 2;for k = [10, 50, 100] uk = u(:, 1:k); sk = s(1:k, 1:k); vk = v(:, 1:k); im_rec = uk * sk * vk'; subplot(2, 2, index); index = index + 1; imshow(uint8(im_rec)); title(sprintf('k = %d', k));end 图像SVD压缩中的误差分析完全是个人随手推导，不严格的说明： 将矩阵分块。由SVD分解公式$\mathbf{U}\mathbf{\Sigma} \mathbf{V^\dagger} = \mathbf{A}$，把$\mathbf{U}$按列分块，$\mathbf{V^\dagger}$按行分块，有下式成立： \begin{bmatrix} u_1 & u_2 &\vdots &u_n \end{bmatrix} \begin{bmatrix} \sigma_1 & & & \\\\ & \sigma_2& & \\\\ & & \ddots& \\\\ & & &\sigma_m \end{bmatrix} \begin{bmatrix} v_1^\dagger\\\\ v_2^\dagger\\\\ \dots\\\\ v_m^\dagger \end{bmatrix}=\mathbf{A}由于 \begin{bmatrix} u_1 & u_2 &\vdots &u_n \end{bmatrix} \begin{bmatrix} \sigma_1 & & & \\\\ & \sigma_2& & \\\\ & & \ddots& \\\\ & & &\sigma_m \end{bmatrix} = \begin{bmatrix} \sigma_1u_1 & \sigma_2u_2 &\vdots &\sigma_nu_n \end{bmatrix}所以， \mathbf{A} = \sum_{i = 1}^{r}\sigma_iu_iv_i^\dagger上面的式子和式里面只有$r$项，是因为当$k &gt; r$时，$\sigma_k = 0$。 所以\mathbf{A} - \hat{\mathbf{A}} = \sum_{i = k+1}^{r}\sigma_iu_iv_i^\dagger 根绝矩阵范数的性质，我们有， \left\lVert\mathbf{A} - \hat{\mathbf{A}}\right\rVert \le \sum_{i=k+1}^{r}\sigma_i\left\lVert u_i\right\rVert\left\lVert v_i^\dagger\right\rVert由于$u_i$和$v_i$都是标准正交基，所以范数小于1.故， \left\lVert\mathbf{A} - \hat{\mathbf{A}}\right\rVert \le \sum_{i=k+1}^{r}\sigma_i取无穷范数，可以知道对于误差矩阵中的任意元素（也就是压缩重建之后任意位置的像素灰度值之差），都有： e \le \sum_{i=k+1}^{r}\sigma_iSVD与矩阵范数如果某个函数$f$满足以下的性质，就可以作为矩阵的范数。 $f(\mathbf{A}) = \mathbf{0} \Leftrightarrow \mathbf{A} = \mathbf{0}$ $f(c\mathbf{A}) = c f(\mathbf{A}), \forall c \in \mathbb{R}$ $f(\mathbf{A+b}) \le f(\mathbf{A}) + f(\mathbf{B})$ 其中，矩阵的2范数可以定义为 \left\lVert\mathbf{A}\right\rVert_2 = \max{\sqrt{(\mathbf{A}x)^\dagger\mathbf{A}x}}其中，$x$是单位向量。上式的意义在于表明矩阵的2范数是对于所有向量，经过该矩阵线性变换后摸长最大的那个变换后向量的长度。 下面，给出不严格的说明，证明矩阵的2范数数值上等于其最大的奇异值。 对于空间内的任意单位向量$x$，利用矩阵的SVD分解，有（为了书写简单，矩阵不再单独加粗）： (Ax)^\dagger Ax = x^\dagger V \Sigma^\dagger \Sigma V^\dagger x其中，$U^\dagger U = I$，已经被消去了。 进一步化简，我们将$V^\dagger x$看做一个整体，令$\omega = V\dagger x$，那么有， (Ax)^\dagger Ax = (\Sigma \omega)^\dagger \Sigma \omega也就是说，矩阵的2范转换为了$\Sigma \omega$的幅值的最大值。由于$\omega$是酉矩阵和一个单位向量的乘积，所以$\omega$仍然是单位阵。 由于$\Sigma$是对角阵，所以$\omega$与其相乘后，相当于每个分量分别被放大了$\sigma_i$倍。即 \Sigma \omega = \begin{bmatrix} \sigma_1 \omega_1\\\\ \sigma_2 \omega_2\\\\ \cdots\\\\ \sigma_n \omega_n \end{bmatrix}它的幅值平方为 \left\lVert \Sigma \omega \right \rVert ^2 = \sum_{i=1}^{n}\sigma_i^2 \omega_i^2 \le \sigma_{1} \sum_{i=1}^{n}\omega_i^2 = \sigma_1^2当且仅当，$\omega_1 = 1$, $\omega_k = 0, k &gt; 1$时取得等号。 综上所述，矩阵2范数的值等于其最大的奇异值。 矩阵的另一种范数定义方法Frobenius norm定义如下： \left\lVert A \right\rVert_{F} = \sqrt{\sum_{i=1}^{m}\sum_{j=1}^{n}\left\vert a_{i,j}\right\rvert}如果我们两边平方，可以得到，矩阵的F范数实际等于某个矩阵的迹，见下式： \left\lVert A\right \rVert_F^2 = \text{trace}(A^\dagger A)利用矩阵的SVD分解，可以很容易得出，$\text{trace}(A^\dagger A) = \sum_{i=1}^{r}\sigma_i^2$ 说明如下： \text{trace}(A^\dagger A) = \text{trace}(V\Sigma^\dagger\Sigma V^\dagger)由于$V^\dagger = V^{-1}$，而且$\text{trace}(BAB^{-1}) = \text{trace}(A)$，所以， \text{trace}(A^\dagger A) = \text{trace}(\Sigma^\dagger \Sigma) = \sum_{i=1}^{r}\sigma_i^2也就是说，矩阵的F范数等于它的奇异值平方和的平方根。 \left\lVert A\right\rVert_F= \sqrt{\sum_{i=1}^{r}\sigma_i^2}]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[CS131-线代基础]]></title>
      <url>%2F2017%2F01%2F22%2Fcs131-linear-alg%2F</url>
      <content type="text"><![CDATA[CS131课程(Computer Vision: Foundations and Applications)，是斯坦福大学Li Feifei实验室开设的一门计算机视觉入门基础课程，该课程目的在于为刚接触计算机视觉领域的学生提供基本原理和应用介绍。目前2016年冬季课程刚刚结束。CS131博客系列主要是关于本课的slide知识点总结与作业重点问题归纳，作为个人学习本门课程的心得体会和复习材料。 由于是个人项目，所以会比较随意，只对个人感兴趣的内容做一总结。这篇文章是对课前的线代基础做一复习与整理。 向量与矩阵数字图像可以看做二维矩阵，向量是特殊的矩阵，本课程默认的向量都是列向量。slide中给出了一些矩阵行列式和迹的性质，都比较简单，这里不再多说。 矩阵作为线性变换通过线代知识，我们知道，在线性空间中，如果给定一组基，线性变换可以通过对应的矩阵来进行描述。 scale变换对角阵可以用来表示放缩变换。 \begin{bmatrix} s_x & 0\\\\ 0 & s_y \end{bmatrix}\begin{bmatrix} x\\\\ y \end{bmatrix} = \begin{bmatrix} s_xx\\\\ s_yy \end{bmatrix}旋转变换如图所示，逆时针旋转$theta$角度，对应的旋转矩阵为： \mathbf{R} = \begin{bmatrix} \cos\theta &-\sin\theta \\\\ \sin\theta &\cos\theta \end{bmatrix}旋转矩阵是酉矩阵，矩阵内的各列（或者各行）相互正交。满足如下的关系式： \mathbf{R}\mathbf{R^{\dagger}} = \mathbf{I}由于$\det{\mathbf{R}} = \det{\mathbf{R^{\dagger}}}$，所以，对于酉矩阵，$\det{\mathbf{R}} = \pm 1$旋转矩阵是酉矩阵，矩阵内的各列（或者各行）相互正交。满足如下的关系式： \mathbf{R}\mathbf{R^{\dagger}} = \mathbf{I}由于$\det{\mathbf{R}} = \det{\mathbf{R^{\dagger}}}$，所以，对于酉矩阵，$\det{\mathbf{R}} = \pm 1$. 齐次变换(Homogeneous Transform)只用上面的二维矩阵不能表达平移，使用齐次矩阵可以表达放缩，旋转和平移操作。 \mathbf{H} =\begin{bmatrix} a & b & t_x\\\\ c & d & t_y\\\\ 0 & 0 & 1 \end{bmatrix},\mathbf{H}\begin{bmatrix} x\\\\ y\\\\ 1\\\\ \end{bmatrix}=\begin{bmatrix} ax+by+t_x\\\\ cx+dy+t_y\\\\ 1 \end{bmatrix}SVD分解可以将矩阵分成若干个矩阵的乘积，叫做矩阵分解，比如QR分解，满秩分解等。SVD分解，即奇异值分解，也是一种特殊的矩阵分解方法。如下式所示，是将矩阵分解成为三个矩阵的乘积： \mathbf{U}\mathbf{\Sigma}\mathbf{V^\dagger} = \mathbf{A}其中矩阵$\mathbf{A}$大小为$m\times n$，矩阵$\mathbf{U}$是大小为$m\times m$的酉矩阵，$\mathbf{V}$是大小为$n \times n$的酉矩阵，$\mathbf{\Sigma}$是大小为$m \times n$的旋转矩阵，即只有主对角元素不为0. SVD分解在主成分分析中年很有用。由于矩阵$\mathbf{\Sigma}$一般情况下是将奇异值按照从大到小的顺序摆放，所以矩阵$\mathbf{U}$中，前面的若干列被视作主成分，后面的列显得相对不这么重要。可以抛弃后面的列，进行图像压缩。 如下图，是使用前10个分量对原图片进行压缩的效果。 12345678910im = imread('./superman.png');im_gray = rbg2gray(im);[u, s, v] = svd(double(im_gray));k = 10;uk = u(:, 1:k);sigma = diag(s);sk = diag(sigma(1:k));vk = v(:, 1:k);im_k = uk*sk*vk';imshow(uint8(im_k))]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[使用 Visual Studio 编译 GSL 科学计算库]]></title>
      <url>%2F2016%2F12%2F16%2Fgsl-with-vs%2F</url>
      <content type="text"><![CDATA[GSL是一个GNU支持的科学计算库，提供了很丰富的数值计算方法。本文介绍了GSL库在Windows环境下使用VisualStudio进行编译构建的过程。 GSL 的项目主页提供的说明来看，GSL支持如下的科学计算： （下面的这张表格的HTML使用的是No-Cruft Excel to HTML Table Converter生成的） Complex Numbers Roots of Polynomials Special Functions Vectors and Matrices Permutations Sorting BLAS Support Linear Algebra Eigensystems Fast Fourier Transforms Quadrature Random Numbers Quasi-Random Sequences Random Distributions Statistics Histograms N-Tuples Monte Carlo Integration Simulated Annealing Differential Equations Interpolation Numerical Differentiation Chebyshev Approximation Series Acceleration Discrete Hankel Transforms Root-Finding Minimization Least-Squares Fitting Physical Constants IEEE Floating-Point Discrete Wavelet Transforms Basis splines GSL的Linux下的配置很简单，照着它的INSTALL文件一步一步来就可以了。CMAKE大法HAO! 1234./configuremakemake installmake clean 同样的，GSL也可以在Windows环境下配置，下面记录了如何在Windows环境下使用 Visual Studio 和 CMakeGUI 编译测试GSL。 使用CMAKE编译成.SLN文件打开CMAKEGUI，将输入代码路径选为GSL源代码地址，输出路径设为自己想要的输出路径。点击 “Configure“，选择Visual Studio2013为编译器，点击Finish后会进行必要的配置。然后将表格里面的选项都打上勾，再次点击”Configure“，等待完成之后点击”Generate“。完成之后，就可以在输出路径下看到GSL.sln文件了。 使用Visual Studio生成解决方案使用 Visual Studio 打开刚才生成的.SLN文件，分别在Debug和Release模式下生成解决方案，等待完成即可。 当完成后，你应该可以在路径下看到这样一张图，我们主要关注的文件夹是\bin，\gsl，\Debug和\Release。 加入环境变量修改环境变量的Path，将\GSL_Build_Path\bin\Debug加入，这主要是为了\Debug文件夹下面的gsl.dll文件。如果不进行这一步的话，一会虽然可以编译，但是却不能运行。 这里顺便注释一句，当使用第三方库的时候，如果需要动态链接库的支持，其中一种方法就是将DLL文件的路径加入到Path中去。 建立Visual Studio属性表Visual Studio可以通过建立工程属性表的方法来配置工程选项，一个OpenCV的例子可以参见Yuanbo She的这篇博文 Opencv 完美配置攻略 2014 (Win8.1 + Opencv 2.4.8 + VS 2013)。 配置文件中主要是包含文件和静态链接库LIB的路径设置。下面把我的贴出来，只需要根据GSL的生成路径做相应修改即可。注意我的属性表中保留了OpenCV的内容，如果不需要的话，尽可以删掉。上面的博文对这张属性表如何配置讲得很清楚，有问题可以去参考。 12345678910111213141516171819&lt;?xml version="1.0" encoding="utf-8"?&gt;&lt;Project ToolsVersion="4.0" xmlns="http://schemas.microsoft.com/developer/msbuild/2003"&gt; &lt;ImportGroup Label="PropertySheets" /&gt; &lt;PropertyGroup Label="UserMacros" /&gt; &lt;PropertyGroup&gt; &lt;IncludePath&gt;$(OPENCV249)\include;E:\GSLCode\gsl-build\;$(IncludePath)&lt;/IncludePath&gt; &lt;LibraryPath Condition="'$(Platform)'=='Win32'"&gt;$(OPENCV249)\x86\vc12\lib;E:\GSLCode\gsl-build\Debug;$(LibraryPath)&lt;/LibraryPath&gt; &lt;LibraryPath Condition="'$(Platform)'=='X64'"&gt;$(OPENCV249)\x64\vc12\lib;E:\GSLCode\gsl-build\Debug;$(LibraryPath)&lt;/LibraryPath&gt; &lt;/PropertyGroup&gt; &lt;ItemDefinitionGroup&gt; &lt;Link Condition="'$(Configuration)'=='Debug'"&gt; &lt;AdditionalDependencies&gt;opencv_calib3d249d.lib;opencv_contrib249d.lib;opencv_core249d.lib;opencv_features2d249d.lib;opencv_flann249d.lib;opencv_gpu249d.lib;opencv_highgui249d.lib;opencv_imgproc249d.lib;opencv_legacy249d.lib;opencv_ml249d.lib;opencv_nonfree249d.lib;opencv_objdetect249d.lib;opencv_ocl249d.lib;opencv_photo249d.lib;opencv_stitching249d.lib;opencv_superres249d.lib;opencv_ts249d.lib;opencv_video249d.lib;opencv_videostab249d.lib;gsl.lib;gslcblas.lib;%(AdditionalDependencies)&lt;/AdditionalDependencies&gt; &lt;/Link&gt; &lt;Link Condition="'$(Configuration)'=='Release'"&gt; &lt;AdditionalDependencies&gt;opencv_calib3d249.lib;opencv_contrib249.lib;opencv_core249.lib;opencv_features2d249.lib;opencv_flann249.lib;opencv_gpu249.lib;opencv_highgui249.lib;opencv_imgproc249.lib;opencv_legacy249.lib;opencv_ml249.lib;opencv_nonfree249.lib;opencv_objdetect249.lib;opencv_ocl249.lib;opencv_photo249.lib;opencv_stitching249.lib;opencv_superres249.lib;opencv_ts249.lib;opencv_video249.lib;opencv_videostab249.lib;gsl.lib;gslcblas.lib;%(AdditionalDependencies)&lt;/AdditionalDependencies&gt; &lt;/Link&gt; &lt;/ItemDefinitionGroup&gt; &lt;ItemGroup /&gt;&lt;/Project&gt; 在以后建立Visual Studio工程的时候，在属性窗口直接添加现有属性表就可以了！ 测试在项目网站的教程上直接找到一段代码，进行测试，输出贝塞尔函数的值。 123456789#include &lt;stdio.h&gt;#include &lt;gsl/gsl_sf_bessel.h&gt;int main(void)&#123; double x = 5.0; double y = gsl_sf_bessel_J0(x); printf("J0(%g) = %.18e\n", x, y); return 0;&#125; 控制台输出正确：]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Hello World]]></title>
      <url>%2F2016%2F12%2F16%2Fhello-world%2F</url>
      <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment Code highlightHello World! 1234#include &lt;iostream&gt;int main() &#123; std::cout &lt;&lt; "HelloWorld\n";&#125; 1print 'HelloWorld' Latex Support by MathjaxMass-energy equation by Einstein: $E = mc^2$ a linear equation: $$\mathbf{A}\mathbf{v} = \mathbf{y}$$]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Windows环境下使用Doxygen生成注释文档]]></title>
      <url>%2F2016%2F12%2F16%2Fuse-doxygen%2F</url>
      <content type="text"><![CDATA[Doxygen 是一种很好用的代码注释生成工具，然而和很多国外的工具软件一样，在中文环境下，它的使用总是会出现一些问题，也就是中文注释文档出现乱码。经过调试，终于是解决了这个问题。 安装 DoxygenDoxygen 在Windows平台下的安装比较简单，Doxygen的项目主页提供了下载和安装的使用说明，可以下载它们的官方使用手册进行阅读。对于Windows，提供了源代码编译安装和直接安装程序安装两种方式，可以自行选择。 安装成功后，使用命令行命令 1doxygen --help 就可以查看帮助文档，对应参数含义一目了然，降低了入手难度。 使用命令， 1doxygen -g doxygen_filename 就可以在当前目录下建立一个doxygen配置文件，用文本编辑器打开就可以编辑里面的配置选项。 使用命令， 1doxygen doxygen_filename 就可以生成注释文档了。 下面就来说一说对中文的支持。 生成 HTML 格式文档中文之所以乱码，很多时候是由于编码和译码格式不同，所以我们需要先知道自己代码文件的编码方式。我的代码都是建立在Visual Studio上的，可以通过VS的高级保存选项查看自己代码文件的存储编码格式。对于中文版的VS，一般应该是GB2312。 我们打开 Doxygen 的配置文件，将里面的 INPUT_ENCODING 改为我们代码文件的编码格式，这里就改成 GB2312。 这样一来，编译出来的 HTML 页面就不会有中文乱码了。 生成Latex 格式文档生成 Latex 需要本机上安装有 Latex 的编译环境。如果是中文用户，推荐的是CTEX套件，可以到他们的网站上去下载。 可以看到，Doxygen为Latex文件的编译生成了make文件，我们在命令行窗口中执行make命令就可以完成编译，然而这时候会发现编译出错，pdf文档无法生成。 打开生成的refman.latex文档，添加宏包 CJKutf8。然后找到 \begin{document}一行，将其改为 12\begin&#123;document&#125;\begin&#123;CJK&#125;&#123;UTF8&#125;&#123;gbsn&#125; 也就是说为正文提供了CJK环境，这样中文文本就可以正常编译了。 相应的，我们要将结尾的 \end{document)改为：12\end&#123;CJK&#125;\end&#123;document&#125; 这样，运行make命令之后，就可以看到中文的注释文档了。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Python Regular Expressions （Python 正则表达式)]]></title>
      <url>%2F2014%2F07%2F17%2Fpython-reg-exp%2F</url>
      <content type="text"><![CDATA[本文来自于Google Developers中对于Python的介绍。https://developers.google.com/edu/python/regular-expressions。 认识正则表达式Python的正则表达式是使用 re 模块的。 12345match = re.search(pattern,str)if match: print 'found',match.group()else: print 'NOT Found!' 正则表达式的规则基本规则 a, x, 9 都是普通字符 (ordinary characters) . (一个点)可以匹配任何单个字符（除了’\n’） \w（小写的w）可以匹配一个单词里面的字母，数字或下划线 [a-zA-Z0-9_];\W （大写的W）可以匹配非单词里的这些元素 \b 匹配单词与非单词的分界 \s（小写的s）匹配一个 whitespace character，包括 space，newline，return，tab，form(\n\r\t\f)；\S（大写的S）匹配一个非 whitespace character \d 匹配十进制数字 [0-9] ^=start，$=end 用来匹配字符串的开始和结束 \ 是转义字符，用 . 来匹配串里的’.’，等一些基本的例子 12345678910## 在字符串'piiig'中查找'iii'match = re.search(r'iii', 'piiig') # found, match.group() == "iii"match = re.search(r'igs', 'piiig') # not found, match == None## . 匹配除了\n的任意字符match = re.search(r'..g', 'piiig') # found, match.group() == "iig"## \d 匹配0-9的数字字符, \w 匹配单词里的字符match = re.search(r'\d\d\d', 'p123g') # found, match.group() == "123"match = re.search(r'\w\w\w', '@@abcd!!') # found, match.group() == "abc" 重复可以用’+’ ‘*’ ‘?’来匹配0个，1个或多个重复字符。 ‘+’ 用来匹配1个或者多个字符 ‘*’ 用来匹配0个或者多个字符 ‘?’ 用来匹配0个或1个字符 注意，’+’和’*’会匹配尽可能多的字符。 一些重复字符的例子12345678910111213141516## i+ 匹配1个或者多个'i'match = re.search(r'pi+', 'piiig') # found, match.group() == "piii"## 找到字符串中最左边尽可能长的模式。## 注意，并没有匹配到第二个 'i+'match = re.search(r'i+', 'piigiiii') # found, match.group() == "ii"## \s* 匹配0个或1个空白字符 whitespacematch = re.search(r'\d\s*\d\s*\d', 'xx1 2 3xx') # found, match.group() == "1 2 3"match = re.search(r'\d\s*\d\s*\d', 'xx12 3xx') # found, match.group() == "12 3"match = re.search(r'\d\s*\d\s*\d', 'xx123xx') # found, match.group() == "123"## ^ 匹配字符串的第一个字符match = re.search(r'^b\w+', 'foobar') # not found, match == None## 与上例对比match = re.search(r'b\w+', 'foobar') # found, match.group() == "bar" Email考虑一个典型的Email地址：someone@host.com，可以用如下的方式匹配： 12345678910 match = re.search(r'\w+@\w+',str)``` 但是，对于这种Email地址 'xyz alice-b@google.com purple monkey' 则不能奏效。### 使用方括号 ###方括号里面的字符表示一个字符集合。[abc]可以被用来匹配'a'或者'b'或者'c'。\w \s等都可以用在方括号里，除了'.'以外，它只能用来表示字面意义上的‘点’。所以上面的Email规则可以扩充如下：``` py match = re.search('r[\w.-]+@[\w.-]+',str) 你还可以使用’-‘来指定范围，如[a-z]指示的是所有小写字母的集合。所以如果你想构造的字符集合中有’-‘，请把它放到末尾[ab-]。另外，前方加上’^’，用来表示取集合的补集，例如ab表示除了’a’和’b’之外的其他字符。 操作以Email地址为例，如果我们想要分别提取该地址的用户名’someone’和主机名’host.com’该怎么办呢？可以在模式中用圆括号指定。 123456str = 'purple alice-b@google.com monkey dishwasher'match = re.search('([\w.-]+)@([\w.-]+)', str) #用圆括号指定分割if match: print match.group() ## 'alice-b@google.com' (the whole match) print match.group(1) ## 'alice-b' (the username, group 1) print match.group(2) ## 'google.com' (the host, group 2) findall 函数与group函数只找到最左端的一个匹配不同，findall函数找到字符串中所有与模式匹配的串。 12345str = 'purple alice@google.com, blah monkey bob@abc.com blah dishwasher'## findall返回一个包含所有匹配结果的 listemails = re.findall(r'[\w\.-]+@[\w\.-]+', str) ## ['alice@google.com', 'bob@abc.com']for email in emails: print email 在文件中使用findall当然可以读入文件的每一行，然后对每一行的内容调用findall，但是为什么不让这一切自动发生呢？ 12f = open(filename.txt,'r')matches = re.findall(pattern,f.read()) findall 和分组和group的用法相似，也可以指定分组。 12345678str = 'purple alice@google.com, blah monkey bob@abc.com blah dishwasher'## 返回了一个listtuples = re.findall(r'([\w\.-]+)@([\w\.-]+)', str)print tuples ## [('alice', 'google.com'), ('bob', 'abc.com')]## list中的元素是tuplefor tuple in tuples: print tuple[0] ## username print tuple[1] ## host 调试正则表达式异常强大，使用简单的几条规则就可以演变出很多的模式组合。在确定你的模式之前，可能需要很多的调试工作。在一个小的测试集合上测试正则表达式。 其他选项正则表达式还可以设置“选项”。 1match = re.search(pat,str,opt) 这些可选项如下： IGNORECASE 忽视大小写 DOTALL 允许’.’匹配’\n’ MULTILINE 在一个由许多行组成的字符串中，允许’^’和’$’匹配每一行的开始和结束]]></content>
    </entry>

    
  
  
</search>
