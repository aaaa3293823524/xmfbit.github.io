<?xml version="1.0" encoding="utf-8"?>
<search>
  
    
    <entry>
      <title><![CDATA[Residual Net论文阅读 - Deep Residual Learning for Image Recongnition]]></title>
      <url>%2F2017%2F03%2F05%2Fresidualnet-paper%2F</url>
      <content type="text"><![CDATA[Residuel Net是MSRA HeKaiming组的作品，斩获了ImageNet挑战赛的所有项目的第一，并荣获CVPR的best paper，成为state of the ar的网络结构。这篇文章记录了阅读最初论文“Deep Residual Learning for Image Recongnition”的重点。 更深的网络 -&gt; 更好的性能在ImageNet等比赛上，大家已经发现了一个现象，就是更深的网络往往能够获得更好的成绩。从LeNet到AlexNet再到VGG Net和GoogLeNet，网络层次越来越深，然而增加网络深度在实际中遇到了很多的问题。 在序言部分，这篇论文也是首先提出了一个问题：我们只需要不断在现有结构基础上堆叠更多的layer就可以获得更好的网络吗？ Is learning better networks as easy as stacking more layers? 很明显，答案是否定的。一个问题就是梯度的消失（或爆炸）。在bp过程中，过深的网络结构会导致传导到底层的梯度变的很小（或飞升），导致训练失败。这一问题在BN层提出之后得到了一定的解决。 另一个问题是在实验过程中观测到的。通过实验，作者发现，当不断增加网络深度的时候，网络的性能会不再提升。如果再继续添加深度，网络的性能甚至会下降！下面是作者在CIFAR-10上做的实验，使用56层的网络比20层的网络，无论在训练集还是测试集上都落于下风。 个人觉得，这个现象看上去意料之外，情理之中。并不是说56层的网络的学习能力不如20层，而是训练不同深度的神经网络的难度是不同的。作者联想到（这里的想法很好！），如果我们已经有了一个较浅的网络（shadow net），然后我们在其后面接上若干的等同映射（Identity Mapping），那么新得到的更深的网络应该是和前者有相同的表现的。这个思想实验，巧妙地说明了并不是更深的网络变坏了，而是我们现有的方法不能很好地训练更深的网络。 残差单元也是受上面这个思想实验中的Identity Mapping的启发，作者设计了一种残差网络结构，以它为基本单元构建更深的网络，以期解决第二个问题。 使用残差单元时，我们不再让网络去直接学映射$\mathcal{H}(x)$，而是学习映射$\mathcal{F}(x) = \mathcal{H}(x) - x$。作者也简单说了为何使用这种残差结构。这种方法给要学的映射加上了一个等同映射作为参考。同时考虑极端情况，如果最优的结构真的是等同映射的话，那么学习到的$\mathcal{F}(x)$为$0$就好了。这个网络的性能起码是不输于那个浅层网络的。 应用这种残差结构就可以很容易地搭建深层网络了。使用这种技术，作者构建了多达$1000$多层的网络，同时在多项比赛中狂揽桂冠，在实践中证明了它的威力。 残差学习从上面的介绍看出，使用残差结构后，网络不再直接学习最终的映射$\mathcal{H}$，而是这个映射和输入的残差$\mathcal{F}$。这里叫做残差学习（Residual Learning）。 神经网络可以近似任意复杂的函数（作者指出此处存疑，还是作为假设）所以，它不仅可以逼近$\mathcal{H}$，当然也可以逼近残差。这两者虽然都可以通过网络近似，但是训练难度是不同的。 上面途中的残差单元结构可以写成下面的式子，其中的$W_i$就是需要决定残差映射$\mathcal{F}$的参数，也是训练中要优化的东西。这里为了书写简单，省略了偏置项。 y = \mathcal{F(x,\lbrace W_i\rbrace)}+x上面的式子要求$\mathcal{F}(x)$与$x$有相同的维度，如果维度不同的话，可以给输入$x$乘上一个权重参数矩阵$W_s$，做一下维度匹配。当然，即使维度相同，我们也可以乘上一个方阵$W_s$，但是这样一来，一是给网络引入了更多的参数（这样，我们的残差结构打脸效果不就打折扣了？），同时在论文中的实验部分也证明了加入这个矩阵对提升性能没用（Identity Mapping已经够用了）。 同时，在设计残差结构的时候，也不必非要像上面的图那样设计两层，完全可以设计更多（只是别减少得只剩一层了，那样的话$\mathcal{F}$只剩一个线性映射可以学了。。。）。 ImageNet实验和比较由此，我们可以构建残差网络。这里开始，作者通过一系列实验，来证明残差结构的优越性：更少的参数，更深的层数，更优秀的性能。 这里着重介绍作者在ImageNet上的实验结果。 作者首先对比了18层和34层plain网络和残差网络的表现，进一步验证了序言中的结论。采用普通的结构，更深的网络（34层）表现反而不如较浅的网络，而使用残差结构则没有这个问题。从下图左右的对比可以很清楚地看出这个现象。作者同时指出这一现象不大可能是由于梯度消失造成的。 另外一个从实验中观察到的现象指出，对于18层这种较浅的网络，使用残差结构能够加快收敛速度，使得训练更加容易。 同时，对于上面提到的维度不匹配的问题，作者提出了三个解决方案并进行了对比。 方案A使用zero-padding的方法 方案B使用乘上权重矩阵的方法 方案C不止在维度不匹配时乘权重矩阵，而且所有的Identity Mapping都换成这种形式 实验结果表明，模型表现A&lt;B&lt;C。但是性能差距较小。由于C引入了很多额外的参数，所以并不使用这种方法（聚焦主要矛盾）。 Bottleneck结构为了节省训练时间，作者提出了一种新的变形——Bottleneck结构。见下图右侧。首先将两层结构扩展为三层，最前面和最后面都是$1\times 1$的卷积核，来进行channel的变形。通过前面的$1\times 1$卷积核，将channel降下来。和$3\times 3$卷积核作用后，再用最后的$1\times 1$卷积核升上去。 使用这一单元结构，作者构建了50层，101层和152层的深层网络，并最终取得了很好的成绩。 附录在附录中，作者描述了在Pascal VOC和COCO目标检测和定位任务中使用Residual Net的情况。对于目标检测这个任务，后续可以参见MSRA的R-FCN那篇文章。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[toy demo - PyTorch + MNIST]]></title>
      <url>%2F2017%2F03%2F04%2Fpytorch-mnist-example%2F</url>
      <content type="text"><![CDATA[本篇文章介绍了使用PyTorch在MNIST数据集上训练MLP和CNN，并记录自己实现过程中的若干问题。 加载MNIST数据集PyTorch中提供了MNIST，CIFAR，COCO等常用数据集的加载方法。MNIST是torchvision.datasets包中的一个类，负责根据传入的参数加载数据集。如果自己之前没有下载过该数据集，可以将download参数设置为True，会自动下载数据集并解包。如果之前已经下载好了，只需将其路径通过root传入即可。 在加载图像后，我们常常需要对图像进行若干预处理。比如减去RGB通道的均值，或者裁剪或翻转图像实现augmentation等，这些操作可以在torchvision.transforms包中找到对应的操作。在下面的代码中，通过使用transforms.Compose()，我们构造了对数据进行预处理的复合操作序列，ToTensor负责将PIL图像转换为Tensor数据（RGB通道从[0, 255]范围变为[0, 1]）， Normalize负责对图像进行规范化。这里需要注意，虽然MNIST中图像都是灰度图像，通道数均为1，但是仍要传入tuple。 之后，我们通过DataLoader返回一个数据集上的可迭代对象。一会我们通过for循环，就可以遍历数据集了。 1234567891011121314151617181920212223import torchimport torch.nn as nnfrom torch.autograd import Variableimport torchvision.datasets as dsetimport torchvision.transforms as transformsimport torch.nn.functional as Fimport torch.optim as optimtrans = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (1.0,))])train_set = dset.MNIST(root=root, train=True, transform=trans, download=download)test_set = dset.MNIST(root=root, train=False, transform=trans)batch_size = 128kwargs = &#123;'num_workers': 1, 'pin_memory': True&#125;train_loader = torch.utils.data.DataLoader( dataset=train_set, batch_size=batch_size, shuffle=True, **kwargs)test_loader = torch.utils.data.DataLoader( dataset=test_set, batch_size=batch_size, shuffle=False, **kwargs) 网络构建在进行网络构建时，主要通过torch.nn包中的已经实现好的卷积层、池化层等进行搭建。例如下面的代码展示了一个具有一个隐含层的MLP网络。nn.Linear负责构建全连接层，需要提供输入和输出的通道数，也就是y = wx+b中x和y的维度。 1234567891011121314class MLPNet(nn.Module): def __init__(self): super(MLPNet, self).__init__() self.fc1 = nn.Linear(28*28, 500) self.fc2 = nn.Linear(500, 256) self.fc3 = nn.Linear(256, 10) self.ceriation = nn.CrossEntropyLoss() def forward(self, x, target): x = x.view(-1, 28*28) x = F.relu(self.fc1(x)) x = F.relu(self.fc2(x)) x = F.relu(self.fc3(x)) loss = self.ceriation(x, target) return x, loss 由于PyTorch可以实现自动求导，所以我们只需实现forward过程即可。这里由于池化层和非线性变换都没有参数，所以使用了nn.functionals中的对应操作实现。通过看文档，可以发现，一般nn里面的各种层，都会在nn.functionals里面有其对应。例如卷积层的对应实现，如下所示，需要传入卷积核的权重。 1234# With square kernels and equal stridefilters = autograd.Variable(torch.randn(8,4,3,3))inputs = autograd.Variable(torch.randn(1,4,5,5))F.conv2d(inputs, filters, padding=1) 同样地，我们可以实现LeNet的结构如下。 12345678910111213141516171819202122232425262728293031323334353637class MLPNet(nn.Module): def __init__(self): super(MLPNet, self).__init__() self.fc1 = nn.Linear(28*28, 500) self.fc2 = nn.Linear(500, 256) self.fc3 = nn.Linear(256, 10) self.ceriation = nn.CrossEntropyLoss() def forward(self, x, target): x = x.view(-1, 28*28) x = F.relu(self.fc1(x)) x = F.relu(self.fc2(x)) x = F.relu(self.fc3(x)) loss = self.ceriation(x, target) return x, loss def name(self): return 'mlpnet'class LeNet(nn.Module): def __init__(self): super(LeNet, self).__init__() self.conv1 = nn.Conv2d(1, 20, 5, 1) self.conv2 = nn.Conv2d(20, 50, 5, 1) self.fc1 = nn.Linear(4*4*50, 500) self.fc2 = nn.Linear(500, 10) self.ceriation = nn.CrossEntropyLoss() def forward(self, x, target): x = self.conv1(x) x = F.max_pool2d(x, 2, 2) x = F.relu(x) x = self.conv2(x) x = F.max_pool2d(x, 2, 2) x = F.relu(x) x = x.view(-1, 4*4*50) x = self.fc1(x) x = self.fc2(x) loss = self.ceriation(x, target) return x, loss 训练与测试在训练时，我们首先应确定优化方法。这里我们使用带动量的SGD方法。下面代码中的optim.SGD初始化需要接受网络中待优化的Parameter列表（或是迭代器），以及学习率lr，动量momentum。 1optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9) 接下来，我们只需要遍历数据集，同时在每次迭代中清空待优化参数的梯度，前向计算，反向传播以及优化器的迭代求解即可。 123456789101112131415161718192021model = MLPNet().cuda() # 以MLP为例for epoch in xrange(10): # trainning for batch_idx, (x, target) in enumerate(train_loader): optimizer.zero_grad() #每次都要清空上一步中参数的grad，否则会出错的~ x, target = Variable(x.cuda()), Variable(target.cuda()) _, loss = model(x, target) #得到loss loss.backward() #bp optimizer.step() #优化器迭代 if batch_idx % 100 == 0: print '==&gt;&gt;&gt; epoch: &#123;&#125;, batch index: &#123;&#125;, train loss: &#123;:.6f&#125;'.format(epoch, batch_idx, loss.data[0]) # testing correct_cnt, ave_loss = 0, 0 for batch_idx, (x, target) in enumerate(test_loader): x, target = Variable(x.cuda(), volatile=True), Variable(target.cuda(), volatile=True) score, loss = model(x, target) _, pred_label = torch.max(score.data, 1) correct_cnt += (pred_label == target.data).sum() ave_loss += loss.data[0] accuracy = correct_cnt*1.0/len(test_loader)/batch_size ave_loss /= len(test_loader) 当优化完毕后，需要保存模型。这里官方文档给出了推荐的方法，如下所示：123torch.save(model.state_dict(), PATH) #保存网络参数the_model = TheModelClass(*args, **kwargs)the_model.load_state_dict(torch.load(PATH)) #读取网络参数 该博客的完整代码可以见：PyTorch MNIST demo。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[远程登录Jupyter笔记本]]></title>
      <url>%2F2017%2F02%2F26%2Fjupyternotebook-remote-useage%2F</url>
      <content type="text"><![CDATA[Jupyter Notebook可以很方便地记录代码和内容，很适合边写笔记边写示例代码进行学习总结。在本机使用时，只需在相应文件夹下使用jupyter notebook命令即可在浏览器中打开笔记页面，进行编辑。而本篇文章记述了如何在远端登录并使用Jupyter笔记本。这样，就可以利用服务器较强的运算能力来搞事情了。 配置jupter notebook登录远程服务器后，使用如下命令生成配置文件。 1jupyter notebook --generate-config 并对其内容进行修改。我主要修改了两处地方： c.NotebookApp.ip=&#39;*&#39;，即不限制ip访问 c.NotebookApp.password = u&#39;hash_value&#39; 上面的hash_value是由用户给定的密码生成的。可以使用ipython中的命令轻松搞定。 12345from notebook.auth import passwdpasswd()"""这里会要求用户输入密码并确认，之后生成的hash值就是要填写到上面的""" 启动notebook之后，在远程服务器上启动笔记本jupyter notebook。接着，在本地机器上访问远程服务器ip:8888（默认端口为8888，也可以在配置文件中修改），输入密码即可访问远程笔记本。 本篇内容参考自博客远程访问jupyter notebook。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[PyTorch简介]]></title>
      <url>%2F2017%2F02%2F25%2Fpytorch-tutor-01%2F</url>
      <content type="text"><![CDATA[这是一份阅读PyTorch教程的笔记，记录jupyter notebook的关键点。原地址位于GitHub repo。 PyTorch简介PyTorch是一个较新的深度学习框架。从名字可以看出，其和Torch不同之处在于PyTorch使用了Python作为开发语言，所谓“Python first”。一方面，使用者可以将其作为加入了GPU支持的numpy，另一方面，PyTorch也是强大的深度学习框架。 目前有很多深度学习框架，PyTorch主推的功能是动态网络模型。例如在Caffe中，使用者通过编写网络结构的prototxt进行定义，网络结构是不能变化的。而PyTorch中的网络结构不再只能是一成不变的。同时PyTorch实现了多达上百种op的自动求导（AutoGrad）。 TensorsTensor，即numpy中的多维数组。上面已经提到过，PyTorch对其加入了GPU支持。同时，PyTorch中的Tensor可以与numpy中的array很方便地进行互相转换。 通过Tensor(shape)便可以创建所需要大小的tensor。如下所示。 12345x = torch.Tensor(5, 3) # construct a 5x3 matrix, uninitialized# 或者随机填充y = torch.rand(5, 3) # construct a randomly initialized matrix# 使用size方法可以获得tensor的shape信息，torch.Size 可以看做 tuplex.size() # out: torch.Size([5, 3]) PyTorch中已经实现了很多常用的op，如下所示。 1234567891011121314151617# addition: syntax 1x + y # out: [torch.FloatTensor of size 5x3]# addition: syntax 2torch.add(x, y) # 或者使用torch包中的显式的op名称# addition: giving an output tensorresult = torch.Tensor(5, 3) # 预先定义sizetorch.add(x, y, out=result) # 结果被填充到变量result# 对于加法运算，其实没必要这么复杂out = x + y # 无需预先定义size# torch包中带有下划线的op说明是就地进行的，如下所示# addition: in-placey.add_(x) # 将x加到y上# 其他的例子: x.copy_(y), x.t_(). PyTorch中的元素索引方式和numpy相同。 12# standard numpy-like indexing with all bells and whistlesx[:,1] # out: [torch.FloatTensor of size 5] 对于更多的op，可以参见PyTorch的文档页面。 Tensor可以和numpy中的数组进行很方便地转换。并且转换前后并没有发生内存的复制（这里文档里面没有明说？），所以修改其中某一方的值，也会引起另一方的改变。如下所示。 1234567891011121314151617# Tensor 转为 np.arraya = torch.ones(5) # out: [torch.FloatTensor of size 5]# 使用 numpy方法即可实现转换b = a.numpy() # out: array([ 1., 1., 1., 1., 1.], dtype=float32)# 注意！a的值的变化同样引起b的变化a.add_(1)print(a)print(b) # a b的值都变成2# np.array 转为Tensorimport numpy as npa = np.ones(5)# 使用torch.from_numpy即可实现转换b = torch.from_numpy(a) # out: [torch.DoubleTensor of size 5]np.add(a, 1, out=a)print(a)print(b) # a b的值都变为2 PyTorch中使用GPU计算很简单，通过调用.cuda()方法，很容易实现GPU支持。 123456# let us run this cell only if CUDA is availableif torch.cuda.is_available(): print('cuda is avaliable') x = x.cuda() y = y.cuda() x + y # 在GPU上进行计算 Neural Network说完了数据类型Tensor，下一步便是如何实现一个神经网络。首先，对自动求导做一说明。 我们需要关注的是autograd.Variable。这个东西包装了Tensor。一旦你完成了计算，就可以使用.backward()方法自动得到（以该Variable为叶子节点的那个）网络中参数的梯度。Variable有一个名叫data的字段，可以通过它获得被包装起来的那个原始的Tensor数据。同时，使用grad字段，可以获取梯度（也是一个Variable）。 Variable是计算图的节点，同时Function实现了变量之间的变换。它们互相联系，构成了用于计算的无环图。每个Variable有一个creator的字段，表明了它是由哪个Function创建的（除了用户自己显式创建的那些，这时候creator是None）。 当进行反向传播计算梯度时，如果Variable是标量（比如最终的loss是欧氏距离或者交叉熵），那么backward()函数不需要参数。然而如果Variable有不止一个元素的时候，需要为其中的每个元素指明其（由上层传导来的）梯度（也就是一个和Variableshape匹配的Tensor）。看下面的说明代码。 1234567891011121314151617181920212223242526272829from torch.autograd import Variablex = Variable(torch.ones(2, 2), requires_grad = True)x # x 包装了一个2x2的Tensor"""Variable containing: 1 1 1 1[torch.FloatTensor of size 2x2]"""# Variable进行计算# y was created as a result of an operation,# so it has a creatory = x + 2y.creator # out: &lt;torch.autograd._functions.basic_ops.AddConstant at 0x7fa1cc158c08&gt;z = y * y * 3 out = z.mean() # out: Variable containing: 27 [torch.FloatTensor of size 1]# let's backprop nowout.backward() # 其实相当于 out.backward(torch.Tensor([1.0]))# print gradients d(out)/dxx.grad"""Variable containing: 4.5000 4.5000 4.5000 4.5000[torch.FloatTensor of size 2x2]""" 下面的代码就是结果不是标量，而是普通的Tensor的例子。12345678910111213141516171819# 也可以通过Tensor显式地创建Variablex = torch.randn(3)x = Variable(x, requires_grad = True)# 一个更复杂的 op例子y = x * 2while y.data.norm() &lt; 1000: y = y * 2# 计算 dy/dxgradients = torch.FloatTensor([0.1, 1.0, 0.0001])y.backward(gradients)x.grad"""Variable containing: 204.8000 2048.0000 0.2048[torch.FloatTensor of size 3]""" 说完了NN的构成元素Variable，下面可以介绍如何使用PyTorch构建网络了。这部分主要使用了torch.nn包。我们自定义的网络结构是由若干的layer组成的，我们将其设置为 nn.Module的子类，只要使用方法forward(input)就可以返回网络的output。下面的代码展示了如何建立一个包含有conv和max-pooling和fc层的简单CNN网络。 1234567891011121314151617181920212223242526272829303132333435363738394041424344import torch.nn as nn # 以我的理解，貌似有参数的都在nn里面import torch.nn.functional as F # 没有参数的（如pooling和relu）都在functional里面？class Net(nn.Module): def __init__(self): super(Net, self).__init__() self.conv1 = nn.Conv2d(1, 6, 5) # 1 input image channel, 6 output channels, 5x5 square convolution kernel self.conv2 = nn.Conv2d(6, 16, 5) # 一会可以看到，input是1x32x32大小的。经过计算，conv-pooling-conv-pooling后大小为16x5x5。 # 所以fc层的第一个参数是 16x5x5 self.fc1 = nn.Linear(16*5*5, 120) # an affine operation: y = Wx + b self.fc2 = nn.Linear(120, 84) self.fc3 = nn.Linear(84, 10) def forward(self, x): # 你可以发现，构建计算图的过程是在前向计算中完成的，也许这可以让你体会到所谓的动态图结构 # 同时，我们无需实现 backward，这是被自动求导实现的 x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2)) # Max pooling over a (2, 2) window x = F.max_pool2d(F.relu(self.conv2(x)), 2) # If the size is a square you can only specify a single number x = x.view(-1, self.num_flat_features(x)) # 把它拉直 x = F.relu(self.fc1(x)) x = F.relu(self.fc2(x)) x = self.fc3(x) return x def num_flat_features(self, x): size = x.size()[1:] # all dimensions except the batch dimension num_features = 1 for s in size: num_features *= s return num_features# 实例化Net对象net = Net()net # 给出了网络结构"""Net ( (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1)) (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1)) (fc1): Linear (400 -&gt; 120) (fc2): Linear (120 -&gt; 84) (fc3): Linear (84 -&gt; 10))""" 我们可以列出网络中的所有参数。 1234params = list(net.parameters())print(len(params)) # out: 10, 5个权重，5个biasprint(params[0].size()) # conv1's weight out: torch.Size([6, 1, 5, 5])print(params[1].size()) # conv1's bias, out: torch.Size([6]) 给出网络的输入，得到网络的输出。并进行反向传播梯度。 1234input = Variable(torch.randn(1, 1, 32, 32))out = net(input) # 重载了()运算符？net.zero_grad() # bp前，把所有参数的grad buffer清零out.backward(torch.randn(1, 10)) 注意一点，torch.nn只支持mini-batch。所以如果你的输入只有一个样例的时候，使用input.unsqueeze(0)人为给它加上一个维度，让它变成一个4-D的Tensor。 网络训练给定target和网络的output，就可以计算loss函数了。在torch.nn中已经实现好了一些loss函数。 1234567891011output = net(input)target = Variable(torch.range(1, 10)) # a dummy target, for example# 使用平均平方误差，即欧几里得距离criterion = nn.MSELoss()loss = criterion(output, target)loss"""Variable containing: 38.6049[torch.FloatTensor of size 1]""" 网络的整体结构如下所示。 1234input -&gt; conv2d -&gt; relu -&gt; maxpool2d -&gt; conv2d -&gt; relu -&gt; maxpool2d -&gt; view -&gt; linear -&gt; relu -&gt; linear -&gt; relu -&gt; linear -&gt; MSELoss -&gt; loss 我们可以使用previous_functions来获得该节点前面Function的信息。 123456789# For illustration, let us follow a few steps backwardprint(loss.creator) # MSELossprint(loss.creator.previous_functions[0][0]) # Linearprint(loss.creator.previous_functions[0][0].previous_functions[0][0]) # ReLU&quot;&quot;&quot;&lt;torch.nn._functions.thnn.auto.MSELoss object at 0x7fa18011db40&gt;&lt;torch.nn._functions.linear.Linear object at 0x7fa18011da78&gt;&lt;torch.nn._functions.thnn.auto.Threshold object at 0x7fa18011d9b0&gt;&quot;&quot;&quot; 进行反向传播后，让我们查看一下参数的变化。 1234567# now we shall call loss.backward(), and have a look at conv1's bias gradients before and after the backward.net.zero_grad() # zeroes the gradient buffers of all parametersprint('conv1.bias.grad before backward')print(net.conv1.bias.grad)loss.backward()print('conv1.bias.grad after backward')print(net.conv1.bias.grad) 计算梯度后，自然需要更新参数了。简单的方法可以自己手写： 123learning_rate = 0.01for f in net.parameters(): f.data.sub_(f.grad.data * learning_rate) 不过，torch.optim中已经提供了若干优化方法（SGD, Nesterov-SGD, Adam, RMSProp, etc）。如下所示。 123456789import torch.optim as optim# create your optimizeroptimizer = optim.SGD(net.parameters(), lr = 0.01)# in your training loop:optimizer.zero_grad() # zero the gradient buffersoutput = net(input)loss = criterion(output, target)loss.backward()optimizer.step() # Does the update 数据载入由于PyTorch的Python接口和np.array之间的方便转换，所以可以使用其他任何数据读入的方法（例如OpenCV等）。特别地，对于vision的数据，PyTorch提供了torchvision包，可以方便地载入常用的数据集（Imagenet, CIFAR10, MNIST, etc），同时提供了图像的各种变换方法。下面以CIFAR为例子。 123456789101112131415161718192021import torchvisionimport torchvision.transforms as transforms# The output of torchvision datasets are PILImage images of range [0, 1].# We transform them to Tensors of normalized range [-1, 1]# Compose: Composes several transforms together.# see http://pytorch.org/docs/torchvision/transforms.html?highlight=transformstransform=transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)), ]) # torchvision.transforms.Normalize(mean, std)# 读取CIFAR10数据集 trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)# 使用DataLoadertrainloader = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle=True, num_workers=2)# Test集，设置train = Falsetestset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)testloader = torch.utils.data.DataLoader(testset, batch_size=4, shuffle=False, num_workers=2)classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck') 接下来，我们对上面部分的CNN网络进行小修，设置第一个conv层接受3通道的输入。并使用交叉熵定义loss。 1234567891011121314151617181920212223class Net(nn.Module): def __init__(self): super(Net, self).__init__() self.conv1 = nn.Conv2d(3, 6, 5) self.pool = nn.MaxPool2d(2,2) self.conv2 = nn.Conv2d(6, 16, 5) self.fc1 = nn.Linear(16*5*5, 120) self.fc2 = nn.Linear(120, 84) self.fc3 = nn.Linear(84, 10) def forward(self, x): x = self.pool(F.relu(self.conv1(x))) x = self.pool(F.relu(self.conv2(x))) x = x.view(-1, 16*5*5) x = F.relu(self.fc1(x)) x = F.relu(self.fc2(x)) x = self.fc3(x) return xnet = Net()# use a Classification Cross-Entropy losscriterion = nn.CrossEntropyLoss()optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9) 接下来，我们进行模型的训练。我们loop over整个dataset两次，对每个mini-batch进行参数的更新。并且设置每隔2000个mini-batch打印一次loss。 12345678910111213141516171819202122232425for epoch in range(2): # loop over the dataset multiple times running_loss = 0.0 for i, data in enumerate(trainloader, 0): # get the inputs inputs, labels = data # wrap them in Variable inputs, labels = Variable(inputs), Variable(labels) # zero the parameter gradients optimizer.zero_grad() # forward + backward + optimize outputs = net(inputs) loss = criterion(outputs, labels) loss.backward() optimizer.step() # print statistics running_loss += loss.data[0] if i % 2000 == 1999: # print every 2000 mini-batches print('[%d, %5d] loss: %.3f' % (epoch+1, i+1, running_loss / 2000)) running_loss = 0.0print('Finished Training') 我们在测试集上选取一个mini-batch（也就是4张，见上面testloader的定义），进行测试。 1234567891011dataiter = iter(testloader)images, labels = dataiter.next() # 得到image和对应的labeloutputs = net(Variable(images))# the outputs are energies for the 10 classes.# Higher the energy for a class, the more the network# thinks that the image is of the particular class# So, let's get the index of the highest energy_, predicted = torch.max(outputs.data, 1) # 找出分数最高的对应的channel，即为top-1类别print('Predicted: ', ' '.join('%5s'% classes[predicted[j][0]] for j in range(4))) 测试一下整个测试集合上的表现。 12345678910correct = 0total = 0for data in testloader: # 每一个test mini-batch images, labels = data outputs = net(Variable(images)) _, predicted = torch.max(outputs.data, 1) total += labels.size(0) correct += (predicted == labels).sum()print('Accuracy of the network on the 10000 test images: %d %%' % (100 * correct / total)) 对哪一类的预测精度更高呢？ 1234567891011class_correct = list(0. for i in range(10))class_total = list(0. for i in range(10))for data in testloader: images, labels = data outputs = net(Variable(images)) _, predicted = torch.max(outputs.data, 1) c = (predicted == labels).squeeze() for i in range(4): label = labels[i] class_correct[label] += c[i] class_total[label] += 1 上面这些训练和测试都是在CPU上进行的，如何迁移到GPU？很简单，同样用.cuda()方法就行了。 1net.cuda() 不过记得在每次训练测试的迭代中，images和label也要传送到GPU上才可以。 1inputs, labels = Variable(inputs.cuda()), Variable(labels.cuda()) 更多的例子和教程更多的例子更多的教程]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[在Caffe中使用Baidu warpctc实现CTC Loss的计算]]></title>
      <url>%2F2017%2F02%2F22%2Fwarpctc-caffe%2F</url>
      <content type="text"><![CDATA[CTC(Connectionist Temporal Classification) Loss 函数多用于序列有监督学习，优点是不需要对齐输入数据及标签。本文内容并不涉及CTC Loss的原理介绍，而是关于如何在Caffe中移植Baidu美研院实现的warp-ctc，并利用其实现一个LSTM + CTC Loss的验证码识别demo。下面这张图引用自warp-ctc的项目页面。本文介绍内容的相关代码可以参见我的GitHub项目warpctc-caffe 移植warp-ctc本节介绍了如何将warp-ctc的源码在Caffe中进行编译。 首先，我们将warp-ctc的项目代码从GitHub上clone下来。在Caffe的include/caffe和src/caffe下分别创建名为3rdparty的文件夹，将warp-ctc中的头文件和实现文件分别放到对应的文件夹下。之后，我们需要对其代码和配置进行修改，才能在Caffe中顺利编译。 由于warp-ctc中使用了C++11的相关技术，所以需要修改Caffe的Makefile文件，添加C++11支持，可以参见Makefile。 对Caffe的修改就是这么简单，之后我们需要修改warp-ctc中的代码文件。这里的修改多且乱，边改边试着编译，所以可能其中也有不必要的修改。最后的目的就是能够使用GPU进行CTC Loss的计算。 warp-ctc提供了CPU多线程的计算，这里我直接将相应的openmp并行化语句删掉了。 另外，需要将warp-ctc中包含CUDA代码的相关头文件后缀名改为cuh，这样才能够通过编译。否则编译器会给出找不到__host__和__device__等等关键字的错误。 对于详细的修改配置，还请参见GitHub相应的代码文件。 实现CTC Loss计算编译没有问题后，我们可以编写ctc_loss_layer实现CTC Loss的计算。在实现时，注意参考文件ctc.h。这个文件中给出了使用warp-ctc进行CTC Loss计算的全部API接口。 ctc_loss_layer继承自loss_layer，主要是前向和反向计算的实现。由于warp-ctc中只对单精度浮点数float进行支持，所以，对于双精度网络参数，直接将其设置为NOT_IMPLEMENTED，如下所示。 1234567891011template &lt;&gt;void CtcLossLayer&lt;double&gt;::Forward_cpu( const vector&lt;Blob&lt;double&gt;*&gt;&amp; bottom, const vector&lt;Blob&lt;double&gt;*&gt;&amp; top) &#123; NOT_IMPLEMENTED;&#125;template &lt;&gt;void CtcLossLayer&lt;double&gt;::Backward_cpu(const vector&lt;Blob&lt;double&gt;*&gt;&amp; top, const vector&lt;bool&gt;&amp; propagate_down, const vector&lt;Blob&lt;double&gt;*&gt;&amp; bottom) &#123; NOT_IMPLEMENTED;&#125; 使用warp-ctc相关接口进行CTC Loss计算的步骤如下： 设置ctcOptions，指定使用CPU或GPU进行计算，并指定CPU计算的线程数和GPU计算的CUDA stream。 调用get_workspace_size()函数，预先为计算分配空间（分配的内存根据计算平台不同位于CPU或GPU）。 调用compute_ctc_loss()函数，计算loss和gradient。 其中，在第三步中计算gradient时，可以直接将对应blob的cpu/gpu_diff指针传入，作为gradient。 这部分的实现代码分别位于include/caffe/layers和src/caffe/layers/下。 验证码数字识别本部分相关代码位于examples/warpctc文件夹下。实验方案如下。 使用Python中的capycha进行包含0-9数字的验证码图片的产生，图片中数字个数从1到MAX_LEN不等。 使用10作为blank_label，将所有的标签序列在后面补blank_label以达到同样的长度MAX_LEN。 将图像的每一列看做一个time step，网络模型使用image data-&gt;2LSTM-&gt;fc-&gt;CTC Loss，简单粗暴。 模型训练过程中，数据输入使用HDF5格式。 数据产生使用captcha生成验证码图片。这里是一个简单的API demo。默认生成的图片大小为160x60。我们将其长宽缩小一半，使用80x30的彩色图片作为输入。 使用python中的h5py模块生成HDF5格式的数据文件。将全部图片分为两部分，80%作为训练集，20%作为验证集。 LSTM的输入在Caffe中已经有了lstm_layer的实现。lstm_layer要求输入的序列blob为TxNx...，也就是说我们需要 训练结论]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[CS131-MeanShift]]></title>
      <url>%2F2017%2F02%2F12%2Fcs131-mean-shift%2F</url>
      <content type="text"><![CDATA[MeanShift最初由Fukunaga和Hostetler在1975年提出，但是一直到2000左右这篇PAMI的论文Mean Shift: A Robust Approach Toward Feature Space Analysis，将它的原理和收敛性等重新整理阐述，并应用于计算机视觉和图像处理领域之后，才逐渐为人熟知。 MeanShift是一种用来寻找特征空间内模态)的方法。所谓模态（Mode），就是指数据集中最经常出现的数据。例如，连续随机变量概率密度函数的模态就是指函数的极大值。从概率的角度看，我们可以认为数据集（或者特征空间）内的数据点都是从某个概率分布中随机抽取出来的。这样，数据点越密集的地方就说明这里越有可能是密度函数的极大值。MeanShift就是一种能够从离散的抽样点中估计密度函数局部极大值的方法。 核密度估计上面提到的PAMI论文篇幅较长，且数学名词较多，我不是很理解。下面的说明过程主要参考了这篇博客和这篇讲义。 注意下面的推导中$x$是一个$d$维的向量。为了书写简单，没有写成向量的加粗形式。首先，先介绍核函数（Kernel Function）的概念。如果某个$\mathbb{R}^n\rightarrow \mathbb{R}$的函数满足以下条件，就能将其作为核函数。 比如高斯核函数： K(x) = \frac{1}{\sqrt{2\pi\sigma^2}}\exp(-\frac{x^2}{2\sigma^2})$$。 核密度估计是一种用来估计随机变量密度函数的非参数化方法。给定核函数$K$，带宽（bandwidth）参数$h$（就是指观察窗口的大小）。那么密度函数可以使用核函数进行估计，如下面的形式所示。其中，$n$是窗口内的数据点的数量。 $$f(x) = \frac{1}{nh^d}\sum_{i=1}^{n}K(\frac{x-x_i}{h})如果核函数是放射形状且对称的（例如高斯核函数），那么$K(x)$可以写成如下的形式，其中$c_{k,d}$是为了使得核函数满足积分为$1$的正则系数。 K(x) = c_{k,d}k(\Arrowvert x\Arrowvert ^2)mean shift向量那么，原来密度函数的极值点就是使得$f(x)$梯度为$0$的点。求取$f(x)$的梯度，可以得到下式。其中$g(s) = -k^\prime(s)$。 观察后可以发现，乘积第一项连带前面的系数，相当于是使用核函数$G(x) = c_{k,d}g(\Arrowvert x\Arrowvert^2)$得到的点$x$处的密度函数估计值（差了常数倍因子）（所以对于给定的某一点$x = x_0$，这一项是定值），后面一项拿出来，定义为$m_h(x)$，称为mean shift向量。 m_h(x) = \frac{\sum_{i=1}^{n}x_i g(\Arrowvert \frac{x-x_i}{h} \Arrowvert^2)}{\sum_{i=1}^{n}g(\Arrowvert \frac{x-x_i}{h} \Arrowvert^2)}-x所以说，$m_h(x)$的指向和$f(x)$的梯度是相同的。另外，从感性的角度出发。$m_h(x)$中的第一项（分式的那一大坨），相当于是在计算$x$周围窗口大小为$h$的这个领域的均值（$g(s)$是加权的系数）。所以$m_h(x)$实际上指示了局部均值和当前点$x$之间的位移。我们想要找到密度函数的局部极大值，不就应该让局部均值向着点密集的方向移动吗？ 算法流程所以，在给定初始位置$x_0$后，我们首先计算此点处的$m_h$，之后，将$x$沿着$m_h$移动即可。下面是一个简单的例子。数据点服从二维高斯分布，均值为$[1, 2]$。其中，红色菱形指示了迭代过程中mean shift的移动。 123456789101112131415161718192021222324252627282930313233343536373839%% generate datamu = [1 2];Sigma = [1 0; 0 2]; R = chol(Sigma);N = 250;data = repmat(mu, N, 1) + randn(N, 2)*R;figurehold onscatter(data(:, 1), data(:, 2), 50, 'filled');%% meanshiftmu0 = rand(1,2) * 5;mu = mean_shift(mu0, 10, data);function out = gaussian_kernel(x, sigma)% gauss kernel, g(x) = \exp(-x^2/2\sigma^2)out = exp(-x.*x/(2*sigma*sigma));endfunction mu = mean_shift(mu0, h, data)% implementation of meanshift algorithm% mu_&#123;k+1&#125; = meanshift(mu_&#123;k&#125;) + mu_&#123;k&#125; = \frac&#123;\sum_i=1^n xg&#125;&#123;\sum_i=1^n g&#125;mu = mu0;sigma = 1; % parameter for gaussian kernel functionfor iter = 1:20 fprintf('iter = %d, mu = [%f, %f]\n', iter, mu(1), mu(2)); scatter(mu(1), mu(2), 50, [1,0,0], 'd', 'filled'); offset = bsxfun(@minus, mu, data); % offset = x-x_i dis = sum(offset.^2, 2); % dis = ||x-x_i||^2 x = data(dis &lt; h, :); % neighborhood with bandwidth = h g = gaussian_kernel(offset(dis &lt; h), sigma); xg = x.*g; mu_prev = mu; mu = sum(xg, 1) / sum(g, 1); if norm(mu_prev - mu, 2) &lt; 1E-2 break; end plot([mu_prev(1) mu(1)], [mu_prev(2), mu(2)], 'b-.', 'linewidth', 2);endscatter(mu(1), mu(2), 50, [1,0,0], 'd', 'filled');end 同时，我也试验了其他的kernel函数，如神似logistaic形式，效果也是相似的。 K(x) = \frac{1}{e^x+e^{-x}+2}123function out = logistic_kernel(x)out = 1./(exp(x) + exp(-x) + 2);end]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[在Ubuntu14.04构建Caffe]]></title>
      <url>%2F2017%2F02%2F09%2Fbuild-caffe-ubuntu%2F</url>
      <content type="text"><![CDATA[Caffe作为较早的一款深度学习框架，很是流行。然而，由于依赖项众多，而且Jia Yangqing已经毕业，所以留下了不少的坑。这篇博客记录了我在一台操作系统为Ubuntu14.04.3的DELL游匣7559笔记本上编译Caffe的过程，主要是在编译python接口时遇到的import error问题的解决和找不到HDF5链接库的问题。 修改Makefile.config当从github上clone下来Caffe的源码之后，我们首先需要修改Makefile.config文件，自定义配置。下面是我的配置文件，主要修改了CUDNN部分和Anaconda Python部分。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117## Refer to http://caffe.berkeleyvision.org/installation.html# Contributions simplifying and improving our build system are welcome!# cuDNN acceleration switch (uncomment to build with cuDNN).USE_CUDNN := 1 # 这里我们使用cudnn加速# CPU-only switch (uncomment to build without GPU support).# CPU_ONLY := 1# uncomment to disable IO dependencies and corresponding data layers# USE_OPENCV := 0# USE_LEVELDB := 0# USE_LMDB := 0# uncomment to allow MDB_NOLOCK when reading LMDB files (only if necessary)# You should not set this flag if you will be reading LMDBs with any# possibility of simultaneous read and write# ALLOW_LMDB_NOLOCK := 1# Uncomment if you're using OpenCV 3# OPENCV_VERSION := 3# To customize your choice of compiler, uncomment and set the following.# N.B. the default for Linux is g++ and the default for OSX is clang++# CUSTOM_CXX := g++# CUDA directory contains bin/ and lib/ directories that we need.CUDA_DIR := /usr/local/cuda# On Ubuntu 14.04, if cuda tools are installed via# "sudo apt-get install nvidia-cuda-toolkit" then use this instead:# CUDA_DIR := /usr# CUDA architecture setting: going with all of them.# For CUDA &lt; 6.0, comment the *_50 lines for compatibility.# 这里可以去掉sm_20和21，因为实在是已经太老了# 如果保留的话，编译时nvcc会给出警告CUDA_ARCH := -gencode arch=compute_30,code=sm_30 \ -gencode arch=compute_35,code=sm_35 \ -gencode arch=compute_50,code=sm_50 \ -gencode arch=compute_50,code=compute_50# BLAS choice:# atlas for ATLAS (default)# mkl for MKL# open for OpenBlasBLAS := atlas# Custom (MKL/ATLAS/OpenBLAS) include and lib directories.# Leave commented to accept the defaults for your choice of BLAS# (which should work)!# BLAS_INCLUDE := /path/to/your/blas# BLAS_LIB := /path/to/your/blas# Homebrew puts openblas in a directory that is not on the standard search path# BLAS_INCLUDE := $(shell brew --prefix openblas)/include# BLAS_LIB := $(shell brew --prefix openblas)/lib# This is required only if you will compile the matlab interface.# MATLAB directory should contain the mex binary in /bin.# MATLAB_DIR := /usr/local# MATLAB_DIR := /Applications/MATLAB_R2012b.app# NOTE: this is required only if you will compile the python interface.# We need to be able to find Python.h and numpy/arrayobject.h.PYTHON_INCLUDE := /usr/include/python2.7 \ /usr/lib/python2.7/dist-packages/numpy/core/include# Anaconda Python distribution is quite popular. Include path:# Verify anaconda location, sometimes it's in root.# 这里我们使用AnacondaANACONDA_HOME := $(HOME)/anaconda2 PYTHON_INCLUDE := $(ANACONDA_HOME)/include \ $(ANACONDA_HOME)/include/python2.7 \ $(ANACONDA_HOME)/lib/python2.7/site-packages/numpy/core/include# Uncomment to use Python 3 (default is Python 2)# PYTHON_LIBRARIES := boost_python3 python3.5m# PYTHON_INCLUDE := /usr/include/python3.5m \# /usr/lib/python3.5/dist-packages/numpy/core/include# We need to be able to find libpythonX.X.so or .dylib.#PYTHON_LIB := /usr/lib PYTHON_LIB := $(ANACONDA_HOME)/lib# Homebrew installs numpy in a non standard path (keg only)# PYTHON_INCLUDE += $(dir $(shell python -c 'import numpy.core; print(numpy.core.__file__)'))/include# PYTHON_LIB += $(shell brew --prefix numpy)/lib# Uncomment to support layers written in Python (will link against Python libs)WITH_PYTHON_LAYER := 1# Whatever else you find you need goes here.INCLUDE_DIRS := $(PYTHON_INCLUDE) /usr/local/includeLIBRARY_DIRS := $(PYTHON_LIB) /usr/local/lib /usr/lib# If Homebrew is installed at a non standard location (for example your home directory) and you use it for general dependencies# INCLUDE_DIRS += $(shell brew --prefix)/include# LIBRARY_DIRS += $(shell brew --prefix)/lib# NCCL acceleration switch (uncomment to build with NCCL)# https://github.com/NVIDIA/nccl (last tested version: v1.2.3-1+cuda8.0)# USE_NCCL := 1# Uncomment to use `pkg-config` to specify OpenCV library paths.# (Usually not necessary -- OpenCV libraries are normally installed in one of the above $LIBRARY_DIRS.)# USE_PKG_CONFIG := 1# N.B. both build and distribute dirs are cleared on `make clean`BUILD_DIR := buildDISTRIBUTE_DIR := distribute# Uncomment for debugging. Does not work on OSX due to https://github.com/BVLC/caffe/issues/171# DEBUG := 1# The ID of the GPU that 'make runtest' will use to run unit tests.TEST_GPUID := 0# enable pretty build (comment to see full commands)Q ?= @ 对于CUDNN，从Nvidia官方网站上下载后，可不按照官方给定的方法安装。直接将include中的头文件放于/usr/local/cuda-8.0/include下，将lib中的库文件放于/usr/loca/cuda-8.0/lib64文件夹下即可。 构建使用make -j8进行编译，并使用make pycaffe生成python接口。并在.bashrc中添加内容：1export PYTHONPATH=/path_to_caffe/python:$PYTHONPATH 结果在import caffe时出现问题如下：1ImportError: libcudnn.so.5: cannot open shared object file: No such file or directory 解决方法如下，详见GitHub issue讨论。1sudo ldconfig /usr/local/cuda/lib64 然而仍有问题，如下：1ImportError: No module named google.protobuf.internal 解决方法如下，详见G+ caffe-user group的帖子。1pip install protobuf 不过仍然存在的问题是远程SSH登录时，不能在ipython环境下导入caffe，不知为何。 使用make test; make runtest进行测试，结果提示HDF5动态链接库出现问题，怀疑与Anaconda中的HDF5冲突有关。错误信息如下： 1error while loading shared libraries: libhdf5_hl.so.10: cannot open shared object file: No such file or directory 解决方法为手动添加符号链接，详见GitHub讨论帖。 123cd /usr/lib/x86_64-linux-gnusudo ln -s libhdf5.so.7 libhdf5.so.10sudo ln -s libhdf5_hl.so.7 libhdf5_hl.so.10 测试首先，通过make runtest看是否全部test可以通过。其次，可以试运行example下的LeNet训练。1234cd $CAFFE_ROOT./data/mnist/get_mnist.sh./examples/mnist/create_mnist.sh./examples/mnist/train_lenet.sh]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[在DigitalOcean上配置Shadowsocks实现IPV4/IPV6翻墙]]></title>
      <url>%2F2017%2F02%2F08%2Fdigitalocean-shadowsocks%2F</url>
      <content type="text"><![CDATA[身在天朝，GFW是一个很蛋疼的东西。有人说GFW挡住了天朝与世界其他地方的交流，尤其给科研造成了很多不便；也有人认为GFW挡住了美利坚的“坚船利炮”（Google，Facebook，Tweet等），让中国的互联网企业高速发展。这两种思路都很有道理，然而GFW对计算机相关行业人员造成不便倒也是千真万确，让人在无数次404之后，脱口而出F*。也有人打趣，会不会FQ，已经成为了CS入行的第一道关卡，正好用来刷掉一批人。之前我使用GoAgent来FQ，后来又使用了一段时间的免费SS服务，后来机缘巧合从一个印度佬那里挣了一些美元，所以搬到了DigitalOcean上。DO的机器，对我一个学生来说，说实话并不算便宜了，不过好在手里还有一些美元，也在GitHub那里进行了学生认证，算是也可以应付了。 之前我已经在DO的机器上配置过shadowsocks，还顺手给iPad解决了FQ的问题。然而当时没有记录，这次我换了一台机器，机房位于NY，把ss重新配置了一遍，再不做些记录，下次恐怕又要东翻西找。 申请机器在进行GitHub学生认证后，可以使用它发给的一个优惠码注册DO，不过仍然要绑定自己的PayPal账户，先交上五美金。之后，DO会赠送50刀。 申请机器时，直接选择Ubuntu 16.04操作系统，并勾选IPV6 Enable，省去后面的麻烦。 安装ss远程登录后，我们需要安装ss。安装命令很简单。12apt-get install python-pippip install shadowsocks 然而，在安装时，我遇到了一个奇怪的问题，提示我unsupported locale setting，后来搜索得知，是语言配置的问题，见这篇博文，解决办法如下：1export LC_ALL=C 编辑配置文件之后，进入/etc目录，建立一个名叫shadowsocks.json的文件（文件名任意，一会对应即可），文件配置内容如下：123456789&#123;&quot;server&quot;:&quot;::&quot;, &quot;server_port&quot;:8388,&quot;local_address&quot;: &quot;127.0.0.1&quot;,&quot;local_port&quot;: 1080,&quot;password&quot;:&quot;your_password（任写）&quot;,&quot;timeout&quot;:600,&quot;method&quot;:&quot;aes-256-cfb&quot;&#125; 其中第一行写成::即是为了IPV6连接。 编辑启动项，设置自动启动之后，我们编辑启动项配置文件，使得ss服务能够开机自动启动。 编辑/etc/rc.local文件，在exit 0之前添加如下命令。1ssserver -c /etc/shadowsocks.json -d start # 这里的json文件名要相对应 之后，使用reboot命令重启即可。 客户端配置客户端使用ss，编辑服务器，按照json文件中的内容填写即可。注意密码相对应。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[CS131-KMeans聚类]]></title>
      <url>%2F2017%2F02%2F05%2Fcs131-kmeans%2F</url>
      <content type="text"><![CDATA[K-Means聚类是把n个点（可以是样本的一次观察或一个实例）划分到$k$个聚类中，使得每个点都属于离他最近的均值（此即聚类中心）对应的聚类，以之作为聚类的标准，满足最小化聚类中心和属于该中心的数据点之间的平方距离和（Sum of Square Distance，SSD）。 \text{SSD} = \sum_{i=1}^{k}\sum_{x\in c_i}(x-c_i)^2 目标函数K-Means方法实际上需要确定两个参数，$c^\ast$和$\delta^\ast$。其中$c_{i}^\ast$代表各个聚类中心的位置，$\delta_{ij}^\ast$的取值为$\lbrace 0,1\rbrace$，代表点$x_j$是否被分到了第$i$个聚类中心。 那么，目标函数可以写成如下的形式。 c^\ast, \delta^\ast = \arg\min_{c,\delta} \frac{1}{N}\sum_{j=1}^{N}\sum_{i=1}^{k}\delta_{i,j}(c_i-x_j)^2然而这样一来，造成了一种很尴尬的局面。一方面，要想优化$c_i$，需要我们给定每个点所属的类；另一方面，优化$\delta_{i,j}$，需要我们给定聚类中心。由于这两个优化变量之间是纠缠在一起的，K-Means算法如何能够达到收敛就变成了一个讨论“先有鸡还是先有蛋”的问题了。 实际使用中，K-Means的迭代过程，实际是EM算法的一个特例。 算法流程K-Means算法的流程如下所示。 假设我们有$N$个样本点，$\lbrace x_1, \dots, x_N\rbrace, x_i\in\mathbb{R}^D$，并给出聚类数目$k$。 首先，随机选取一系列的聚类中心点$\mu_i, i = 1,\dots, k$。接着，我们按照距离最近的原则为每个数据点指定相应的聚类中心并计算新的数据点均值更新聚类中心。如此这般，直到收敛。 算法细节初始化上面的K-Means方法对初始值很敏感。朴素的初始化方法是直接在数据点中随机抽取$k$个作为聚类初始中心点。不够好的初始值可能造成收敛速度很慢或者聚类失败。下面介绍两种改进方法。 kmeans++方法，尽可能地使初始聚类中心分散开（spread-out）。这种方法首先随机产生一个初始聚类中心点，然后按照概率$P = \omega(x-c_i)^2$选取其他的聚类中心点。其中$\omega$是归一化系数。 多次初始化，保留最好的结果。 K值的选取在上面的讨论中，我们一直把$k$当做已经给定的参数。但是在实际操作中，聚类类别数通常都是未知的。如何确定超参数$k$呢？ 我们可以使用不同的$k$值进行试验，并作出不同试验收敛后目标函数随$k$的变化。如下图所示，分别使用$1$到$4$之间的几个数字作为$k$值，曲线在$k=2$处有一个较为明显的弯曲点（elbow point）。故确定$k$取值为2。 距离的度量目标函数是各个cluster中的点与其中心点的距离均值。其中就涉及到了“距离”的量度。下面是几种较为常用的距离度量方法。 欧几里得距离（最为常用） 余弦距离（向量的夹角） 核函数（Kernel K-Means） 迭代终止条件当判断算法已经收敛时，退出迭代。常用的迭代终止条件如下： 达到了预先给定的最大迭代次数 在将数据点assign到不同聚类中心时，和上轮结果没有变化（说明已经收敛） 目标函数（平均的距离）下降小于阈值 基于K-Means的图像分割图像分割中可以使用K-Means算法。我们首先将一副图像转换为特征空间（Feature Space）。例如使用灰度或者RGB的值作为特征向量，就得到了1D或者3D的特征空间。之后，可以基于Feature Space上各点的相似度，将它们聚类。这样，就完成了对原始图像的分割操作。如下所示。 然而，上述方法有一个缺点。那就是在真实图像中，属于同一个物体的像素点常常在空间上是相关的（Spatial Coherent）。而上述方法没有考虑到这一点。我们可以根据颜色亮度和空间上的距离构建Feature Space，获得更加合理的分割效果。 在2012年PAMI上有一篇文章SLIC Superpixels Compared to State-of-the-art Superpixel Methods介绍了使用K-Means进行超像素分割的相关内容，可以参考。这里不再赘述了。 优点和不足作为非监督学习中的经典算法，K-Means的一大优点便是实现简单，速度较快，最小化条件方差（conditional variance，good represention of data，这里没有具体解释，不太懂）。 它的缺点主要有： 对outlier比较敏感。如下图所示。由于outlier的存在，右边的cluster硬是被拉长了。这方面，K-Medians更有鲁棒性。 每个点只能确定地属于或不属于某一个cluster。可以参考高斯混合模型和Fuzzy K-Means的soft assignment。 在round shape情况下性能比较好（想想圆形和方形各点的欧几里得距离）。而且每个cluster最好数据点数目和分布的密度相近。 如果数据点有非凸形状分布，算法表现糟糕。可以参考谱聚类（Spectral Clustering）或者kernel K-Means。 针对K-Means，也有不少相关改进工作，参考下面这幅图吧。 MATLAB实验下面是我自己写的简单的K-Means demo。首先，在数据产生环节，确定$K = 3$个cluster的中心位置，并随机产生$N$个数据点，并使用scatter函数做出散点图。 代码中的主要部分为my_kmeans函数的实现（为了不与内建的kmeans函数重名，故加上了my前缀）。在此函数内部，首先随机产生$K$个聚类中心，并对数据点进行初始的指定。接着，在迭代过程中，不断计算均值来更新聚类中心和assignment，当达到最大迭代次数或者相邻两次迭代中assignment的值不再变化为止，并计算对应的目标函数$J$的值。 注意到该函数初始确定聚类中心时有一段注释的代码，该段代码用于debug时，指定聚类中心，去除随机性，以验证后续K-Means迭代的正确性。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119%% generate dataK = 3; % number of clusterspos = [-5, 5; 0, 1; 3, 6]; % position of cluster centersN = 20; % number of data pointsR = 3; % radius of clustersdata = zeros(N, 2); % dataclass = zeros(N, 1); % index of clusterfor i = 1:N idx = randi(3, 1); dr = R*rand(); data(i, :) = pos(idx, :) + [dr*cos(rand()*2*pi), dr*sin(rand()*2*pi)]; class(i) = idx;end%% visualization data pointsfigurehold oncolor = [1,0,0; 0,1,0; 0,0,1];for i = 1:K x = data(class == i, 1); y = data(class == i, 2); scatter(x, y, 150, repmat(color(i,:), [length(x), 1]), 'filled');end%% K-Meansbest_J = 1E100;best_idx = 0;for times = 1:5 % 5 times experiments to choose the best result [mu, assignment, J] = my_kmeans(data, K); if best_J &gt; J best_idx = times; best_J = J; end fprintf('%d experiment: J = %f\n', times, J); disp(mu);endfprintf('best: %d experiment: J = %f\n', best_idx, best_J);%% basic functionsfunction J = ssd(X, mu, assignment)% sum of square distance% X -- data, N*D matrix% mu -- centers of clusters, K*D matrix% assignment -- current assignment of data to clustersJ = 0;K = size(mu, 1);for k = 1:K x_k = X(assignment == k, :); mu_k = mu(k, :); err2 = bsxfun(@minus, x_k, mu_k).^2; J = J + sum(err2(:));endJ = J / size(X, 1);endfunction mu = compute_mu(X, assignment, K)mu = zeros(K, size(X, 2));for k = 1:K x_k = X(assignment == k, :); mu(k, :) = mean(x_k, 1);endendfunction assignment = assign(X, mu)% assign data points to clustersN = size(X, 1);assignment = zeros(N, 1);for i = 1:N x = X(i, :); err2 = bsxfun(@minus, x, mu).^2; dis = sum(err2, 2); [~, idx] = min(dis); assignment(i) = idx;endendfunction [mu, assignment, J] = my_kmeans(X, K)N = size(X, 1);assignment = zeros(N, 1);idx = randsample(N, K);mu = X(idx, :);% for i = 1:K% for j = 1:N% if assignment_gt(j) == i% mu(i,:) = X(j,:);% break;% end% end% endfigurehold oncolor = [1,0,0; 0,1,0; 0,0,1];scatter(mu(:,1), mu(:,2), 200, color, 'd');for iter = 1:20 assignment_prev = assignment; assignment = assign(X, mu); if assignment == assignment_prev break; end mu_prev = mu; mu = compute_mu(X, assignment, K); scatter(mu(:, 1), mu(:, 2), 200, color, 'd'); MU = zeros(2*K, 2); MU(1:2:end, :) = mu_prev; MU(2:2:end, :) = mu; mu_x = reshape(MU(:, 1), [], K); mu_y = reshape(MU(:, 2), [], K); plot(mu_x, mu_y, 'k-.');endfor i = 1:K x = X(assignment == i, 1); y = X(assignment == i, 2); scatter(x, y, 150, repmat(color(i,:), [length(x), 1]), 'filled');endJ = ssd(X, mu, assignment);end 在demo中，随机选取初始化的聚类中心，重复进行$5$次实验。下图是产生的原始数据的散点图。用不同的颜色标明了cluster。 下图是某次的实验结果，其中菱形和它们之间的连线指示了聚类中心的变化。注意，颜色只是用来区别不同的cluster，和上图并没有对应关系。可以看到，初始化时绿色标注的cluster的中心（也即是上图中的红色cluster）虽然偏离了很远，但是在迭代过程中也能实现纠正。 再换个大点的数据集来做，效果貌似还不错~ PS这里插一句与本文内容完全无关的一个tip：在Markdown中使用LaTex时，如果在内联形式（也就是行内使用两个美元符号插入公式）时，如果有多个下划线，那么Markdown有时会将下划线之间的部分认为是自己的斜体标记，如下所示：1$c_&#123;i&#125;^\ast$ XXX $\delta_&#123;ij&#125;^\ast$ 它的显示效果为$c{i}^\ast$ XXX $\delta{ij}^\ast$。 这时候，只需在下划线前加入反斜线进行转义即可（虽然略显麻烦）。如下所示：1$c\_&#123;i&#125;^\ast$ XXX $\delta\_&#123;ij&#125;^\ast$ 它的显示效果为$c_{i}^\ast$ XXX $\delta_{ij}^\ast$。 具体分析可以参见博客。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[B站视频“线性代数的本质”观后感]]></title>
      <url>%2F2017%2F02%2F05%2Fvideo-linear-alg-essential-property%2F</url>
      <content type="text"><![CDATA[线性代数对于现代科技的重要性不言而喻，在机器学习领域也是重要的工具。最近，在B站上发现了这样的一个视频：线性代数的本质。这个视频共有十集左右，每集大约10分钟，从线性空间入手，介绍线性变换，并由此介绍矩阵的相关性质，动画做的很棒，翻译得也很好，用了几天时间零散地把几集视频看完，记录一些心得。 从线性空间和线性变换讲起BIT的教学水平，不同的老师参差不齐。所幸的是，大一教授线性代数一课的吴惠彬老师，是一位既有深厚学问又善于教课的老师。我们所用的教材，正是吴老师主编的。和这个视频不同，教材是以线性方程组入手来介绍矩阵，而后引入到线性空间和线性变换。吴老师还曾经告诫大家，线性空间和线性变换一章较为抽象晦涩，不易理解。当然，很多年过去了，我也在研究生阶段继续修了线性代数的深入课程，如今想来，其实线性变换和矩阵的联系更为紧密一些。但是我们的教材之所以不用线性变换为引子，可能正是由于对于大一新生来说，过去十二年所受的基础教育都是操作一些看得见摸得着的数和几何体，一时难以接受线性空间这种东西吧~而也正是从这门课和高数开始，思考问题的方式和初等数学不一样了。我们更多地考察运动和极限，将具有共性的东西抽象出来，研究它们的等价性质。 而抽象这个问题，在本视频中，通过将问题聚焦在二维（偶有提及三维）平面并辅之以动画的形式避开了（所以这个视频也只能算是有趣的不严肃的科普性质）。视频从线性空间入手，首先介绍了向量和基的概念，而后引入出贯穿视频系列始终的线性变换。什么是线性变换呢？从“数”的角度来看，定义在线性空间上，满足叠加性和齐次性就是线性变换。而本视频则从“形”的角度出发（注意，这也是本视频贯穿始终的指导思想：用“形”来思考），给出了下面两个条件： 变换前后原点不动 变换前后平行等距线仍然保持平行等距（但是距离值可能会变）。 线性变换与矩阵的关系视频在阐述线性变换和矩阵关系的时候一带而过，不是很好。下面是我写的一个补充说明。 在由一组基向量$\alpha_i, i = 1,2,\dots,n$张成的线性空间$\mathcal{V}$上，任何一个向量$v$都可以表示为这组基的线性组合，也就是 v = \sum_{i=1}^{n}k_i\alpha_i则线性变换$\mathcal{T}$对$v$作用之后，有， u = \mathcal{T}(v) = \mathcal{T}(\sum_{i=1}^{n}k_i\alpha_i)根据线性变换的叠加性，有， u = \sum_{i=1}^{n}k_i\mathcal{T}(\alpha_i)设$\alpha_i$经过线性变换$\mathcal{T}$作用后，变换为$\beta_i$，那么， u = \sum_{i=1}^{n}k_i\beta_i也就是说， u = \begin{bmatrix}\mathcal{T}(\alpha_1), \mathcal{T}(\alpha_2), \cdots, \mathcal{T}(\alpha_n)\end{bmatrix} \begin{bmatrix}k_1\\\\ k_2\\\\ \vdots\\\\ k_n\end{bmatrix}上式说明，一个抽象的线性变换可以使用一个具体的矩阵来代表。这个矩阵的列，就是线性变换之后的那组基。 举个例子，旋转变换。如果旋转$\frac{\pi}{4}$，那么原来坐标轴方向的单位向量的$i$和$j$分别转到了$(\frac{\sqrt{2}}{2},\frac{\sqrt{2}}{2})$和$(-\frac{\sqrt{2}}{2},\frac{\sqrt{2}}{2})$。所以描述这个线性变换的矩阵为 A = \begin{bmatrix}\frac{\sqrt{2}}{2} & -\frac{\sqrt{2}}{2}\\\\ \frac{\sqrt{2}}{2}& \frac{\sqrt{2}}{2}\end{bmatrix}矩阵$A$的两列分别为变换后的基向量坐标。 矩阵乘法那么矩阵乘法的意义也就有了，那就是对向量做线性变换。例如上面的矩阵$A$与$x = \begin{bmatrix}-1\\ 0\end{bmatrix}$相乘，就是求取向量$-1i+0j$在旋转变换之后的新坐标。这时候，$-1$和$0$就是上面推导过程中的$k_1$和$k_2$，所以， Ax = -1\begin{bmatrix}\frac{\sqrt{2}}{2}\\\\ \frac{\sqrt{2}}{2}\end{bmatrix} + 0\begin{bmatrix}-\frac{\sqrt{2}}{2}\\\\\frac{\sqrt{2}}{2}\end{bmatrix}而矩阵与矩阵相乘呢？只要把右边的矩阵按列分块即可。其实这是矩阵与多个向量相乘的简写形式。 所以，对于任意向量$x$，$Ax$的结果就是组成矩阵$A$的各列向量的线性组合。这个由矩阵$A$的各列张成的空间叫做矩阵的列空间。 而那些使得方程$Ax=0$成立的$x$，张成的空间为矩阵的核空间。 矩阵的秩的意义就是矩阵列空间的维数。 行列式仍然考虑二维矩阵，我们已经知道它表示了二维平面上的线性变换。而矩阵行列式的意义，就表明了变换前后，单位面积的正方形被放缩的比例。如果变换之后，$i$向量跑到了$j$向量的左手边，那么需要加上负号，表明在这个变换过程中，二维平面其实进过了一次翻转。 点积叉积和对偶性这部分的对偶性这个概念比较玄一些，这里就不讲了。。。对于向量的点积，我们已经知道， \langle v, u \rangle = \sum_{i=1}^{n}v_iu_i从几何的角度看，向量的点积是将向量$u$向向量$v$的方向投影（有正负），投影长度乘上$v$的长度得到的。 按照上面矩阵乘法的思路，$u$可以看做代表一个从2维到1维的线性变换（也就是从$\mathbb{R}^2$到$\mathbb{R}$）。而根据上述矩阵变换和矩阵乘法的对应关系可知，变换后的结果就等于$v$的两个分量分别乘上两个基向量（也就是$u$的两个分量），这样我们就得到了向量点积的数值计算方法。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[YOLO 论文阅读]]></title>
      <url>%2F2017%2F02%2F04%2Fyolo-paper%2F</url>
      <content type="text"><![CDATA[YOLO(You Only Look Once)是一个流行的目标检测方法，和Faster RCNN等state of the art方法比起来，主打检测速度快。截止到目前为止（2017年2月初），YOLO已经发布了两个版本，在下文中分别称为YOLO V1和YOLO V2。YOLO V2的代码目前作为Darknet的一部分开源在GitHub。在这篇博客中，记录了阅读YOLO两个版本论文中的重点内容，并着重总结V2版本的改进。 YOLO V1这里不妨把YOLO V1论文“You Only Look Once: Unitied, Real-Time Object Detection”的摘要部分意译如下： 我们提出了一种新的物体检测方法YOLO。之前的目标检测方法大多把分类器重新调整用于检测（这里应该是在说以前的方法大多将检测问题看做分类问题，使用滑动窗提取特征并使用分类器进行分类）。我们将检测问题看做回归，分别给出bounding box的位置和对应的类别概率。对于给定输入图像，只需使用CNN网络计算一次，就同时给出bounding box位置和类别概率。由于整个pipeline都是同一个网络，所以很容易进行端到端的训练。YOLO相当快。base model可以达到45fps，一个更小的model（Fast YOLO），可以达到155fps，同时mAP是其他可以实现实时检测方法的两倍。和目前的State of the art方法相比，YOLO的localization误差较大，但是对于背景有更低的误检（False Positives）。同时，YOLO能够学习到更加泛化的特征，例如艺术品中物体的检测等任务，表现很好。 和以往较为常见的方法，如HoG+SVM的行人检测，DPM，RCNN系方法等不同，YOLO接受image作为输入，直接输出bounding box的位置和对应位置为某一类的概率。我们可以把它和目前的State of the art的Faster RCNN方法对比。Faster RCNN方法需要两个网络RPN和Fast RCNN，其中前者负责接受图像输入，并提出proposal。后续Fast RCNN再给出这些proposal是某一类的概率。也正是因为这种“直来直去”的方法，YOLO才能达到这么快的速度。也真是令人感叹，网络参数够多，数据够多，什么映射关系都能学出来。。。下图就是YOLO的检测系统示意图。在test阶段，经过单个CNN网络前向计算后，再经过非极大值抑制，就可以给出检测结果。 基本思路 网格划分：将输入image划分为$S \times S$个grid cell，如果image中某个object box的中心落在某个grid cell内部，那么这个cell就对检测该object负责（responsible for detection that object）。同时，每个grid cell同时预测$B$个bounding box的位置和一个置信度。这个置信度并不只是该bounding box是待检测目标的概率，而是该bounding box是待检测目标的概率乘上该bounding box和真实位置的IoU的积。通过乘上这个交并比，反映出该bounding box预测位置的精度。如下式所示： \text{confidence} = P(\text{Object})\times \text{IoU}_{\text{pred}}^{\text{truth}} 网络输出：每个bounding box对应于5个输出，分别是$x,y,w,h$和上述提到的置信度。其中，$x,y$代表bounding box的中心离开其所在grid cell边界的偏移。$w,h$代表bounding box真实宽高相对于整幅图像的比例。$x,y,w,h$这几个参数都已经被bounded到了区间$[0,1]$上。除此以外，每个grid cell还产生$C$个条件概率，$P(\text{Class}_i|\text{Object})$。注意，我们不管$B$的大小，每个grid cell只产生一组这样的概率。在test的非极大值抑制阶段，对于每个bounding box，我们应该按照下式衡量该框是否应该予以保留。 \text{confidence}\times P(\text{Class}_i|\text{Object}) = P(\text{Class}_i)\times \text{IoU}_{\text{pred}}^{\text{truth}} 实际参数：在PASCAL VOC进行测试时，使用$S = 7$, $B=2$。由于共有20类，故$C=20$。所以，我们的网络输出大小为$7\times 7 \times 30$ 网络模型结构Inspired by GoogLeNet，但是没有采取inception的结构，simply使用了$1\times 1$的卷积核。base model共有24个卷积层，后面接2个全连接层，如下图所示。 另外，Fast YOLO使用了更小的网络结构（9个卷积层，且filter的数目也少了），其他部分完全一样。 训练同很多人一样，这里作者也是先在ImageNet上做了预训练。使用上图网络结构中的前20个卷积层，后面接一个average-pooling层和全连接层，在ImageNet 1000类数据集上训练了一周，达到了88%的top-5准确率。 由于Ren的论文提到给预训练的模型加上额外的卷积层和全连接层能够提升性能，所以我们加上了剩下的4个卷积层和2个全连接层，权值为随机初始化。同时，我们把网络输入的分辨率从$224\times 224$提升到了$448 \times 448$。 在最后一层，我们使用了线性激活函数；其它层使用了leaky ReLU激活函数。如下所示： f(x)= \begin{cases} x, &\text{if}\ x > 0 \\\\ 0.1x, &\text{otherwise} \end{cases}很重要的问题就是定义很好的loss function，为此，作者提出了以下几点说明： loss的形式采用误差平方和的形式（真是把回归进行到底了。。。） 由于很多的grid cell没有目标物体存在，所以给有目标存在的bounding box和没有目标存在的bounding box设置了不同的比例因子进行平衡。具体来说，\lambda_{\text{coord}} = 5，\lambda_{\text{noobj}} = 0.5 直接使用$w$和$h$，这样大的box的差异在loss中占得比重会和小的box不平衡，所以这里使用$\sqrt{w}$和$\sqrt{h}$。 上文中已经提到，同一个grid cell会提出多个bounding box。在training阶段，我们只想让一个bounding box对应object。所以，我们计算每个bounding box和ground truth的IoU，以此为标准得到最好的那个bounding box，其他的认为no obj。 loss函数的具体形式见下图（实在是不想打这一大串公式。。）。其中，$\mathbb{1}_i$表示是否有目标出现在第$i$个grid cell。 $\mathbb{1}_{i,j}$表示第$i$个grid cell的第$j$个bounding box是否对某个目标负责。 在进行反向传播时，由于loss都是二次形式，所以导数形式都是统一的。下面是Darknet中detection_layer.c中训练部分的代码。在代码中，计算了cost（也就是loss）和delta（也就是反向的导数）， 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121if(state.train)&#123; float avg_iou = 0; float avg_cat = 0; float avg_allcat = 0; float avg_obj = 0; float avg_anyobj = 0; int count = 0; *(l.cost) = 0; int size = l.inputs * l.batch; memset(l.delta, 0, size * sizeof(float)); for (b = 0; b &lt; l.batch; ++b)&#123; int index = b*l.inputs; // for each grid cell for (i = 0; i &lt; locations; ++i) &#123; // locations = S * S = 49 int truth_index = (b*locations + i)*(1+l.coords+l.classes); int is_obj = state.truth[truth_index]; // for each bbox for (j = 0; j &lt; l.n; ++j) &#123; // l.n = B = 2 int p_index = index + locations*l.classes + i*l.n + j; l.delta[p_index] = l.noobject_scale*(0 - l.output[p_index]); // 因为no obj对应的bbox很多，而responsible的只有一个 // 这里统一加上，如果一会判断该bbox responsible for object，再把它减去 *(l.cost) += l.noobject_scale*pow(l.output[p_index], 2); avg_anyobj += l.output[p_index]; &#125; int best_index = -1; float best_iou = 0; float best_rmse = 20; // 该grid cell没有目标，直接返回 if (!is_obj)&#123; continue; &#125; // 否则，找出responsible的bounding box，计算其他几项的loss int class_index = index + i*l.classes; for(j = 0; j &lt; l.classes; ++j) &#123; l.delta[class_index+j] = l.class_scale * (state.truth[truth_index+1+j] - l.output[class_index+j]); *(l.cost) += l.class_scale * pow(state.truth[truth_index+1+j] - l.output[class_index+j], 2); if(state.truth[truth_index + 1 + j]) avg_cat += l.output[class_index+j]; avg_allcat += l.output[class_index+j]; &#125; box truth = float_to_box(state.truth + truth_index + 1 + l.classes); truth.x /= l.side; truth.y /= l.side; // 找到最好的IoU，对应的bbox是responsible的，记录其index for(j = 0; j &lt; l.n; ++j)&#123; int box_index = index + locations*(l.classes + l.n) + (i*l.n + j) * l.coords; box out = float_to_box(l.output + box_index); out.x /= l.side; out.y /= l.side; if (l.sqrt)&#123; out.w = out.w*out.w; out.h = out.h*out.h; &#125; float iou = box_iou(out, truth); //iou = 0; float rmse = box_rmse(out, truth); if(best_iou &gt; 0 || iou &gt; 0)&#123; if(iou &gt; best_iou)&#123; best_iou = iou; best_index = j; &#125; &#125;else&#123; if(rmse &lt; best_rmse)&#123; best_rmse = rmse; best_index = j; &#125; &#125; &#125; if(l.forced)&#123; if(truth.w*truth.h &lt; .1)&#123; best_index = 1; &#125;else&#123; best_index = 0; &#125; &#125; if(l.random &amp;&amp; *(state.net.seen) &lt; 64000)&#123; best_index = rand()%l.n; &#125; int box_index = index + locations*(l.classes + l.n) + (i*l.n + best_index) * l.coords; int tbox_index = truth_index + 1 + l.classes; box out = float_to_box(l.output + box_index); out.x /= l.side; out.y /= l.side; if (l.sqrt) &#123; out.w = out.w*out.w; out.h = out.h*out.h; &#125; float iou = box_iou(out, truth); //printf("%d,", best_index); int p_index = index + locations*l.classes + i*l.n + best_index; *(l.cost) -= l.noobject_scale * pow(l.output[p_index], 2); // 还记得我们曾经统一加过吗？这里需要减去了 *(l.cost) += l.object_scale * pow(1-l.output[p_index], 2); avg_obj += l.output[p_index]; l.delta[p_index] = l.object_scale * (1.-l.output[p_index]); if(l.rescore)&#123; l.delta[p_index] = l.object_scale * (iou - l.output[p_index]); &#125; l.delta[box_index+0] = l.coord_scale*(state.truth[tbox_index + 0] - l.output[box_index + 0]); l.delta[box_index+1] = l.coord_scale*(state.truth[tbox_index + 1] - l.output[box_index + 1]); l.delta[box_index+2] = l.coord_scale*(state.truth[tbox_index + 2] - l.output[box_index + 2]); l.delta[box_index+3] = l.coord_scale*(state.truth[tbox_index + 3] - l.output[box_index + 3]); if(l.sqrt)&#123; l.delta[box_index+2] = l.coord_scale*(sqrt(state.truth[tbox_index + 2]) - l.output[box_index + 2]); l.delta[box_index+3] = l.coord_scale*(sqrt(state.truth[tbox_index + 3]) - l.output[box_index + 3]); &#125; *(l.cost) += pow(1-iou, 2); avg_iou += iou; ++count; &#125; &#125; YOLO V2YOLO V2是原作者在V1基础上做出改进后提出的。为了达到题目中所称的Better，Faster，Stronger的目标，主要改进点如下。当然，具体内容还是要深入论文。 受到Faster RCNN方法的启发，引入了anchor。同时使用了K-Means方法，对anchor数量进行了讨论，在精度和速度之间做出折中； 修改了网络结构，去掉了全连接层，改成了全卷积结构； 引入了WordTree结构，将检测和分类问题做成了一个统一的框架，并充分利用ImageNet和COCO数据集的数据。 下面，还是先把论文的摘要意译如下： 我们引入了YOLO 9000模型，它是实时物体检测的State of the art的工作，能够检测超过9K类目标。首先，我们对以前的工作（YOLO V1）做出了若干改进，使其成为了一种在实时检测方法内在PASCAL VOC和COCO上State of the art的效果。通过一种新颖的多尺度训练犯法（multi-scale training method）， YOLO V2适用于不同大小尺寸输入的image，在精度和效率上达到了很好地trade-off。在67fps的速度下，VOC2007上达到了76.8mAP的精度。40fps时，达到了78.6mAP，已经超过了Faster RCNN with ResNet和SSD的精度，同时比它们更快！最后，我们提出了一种能够同时进行检测任务和分类任务的联合训练方法。使用这种方法，我们在COCO detection dataset和ImageNet classification dataset上同时训练模型。这种方法使得我们能够对那些没有label上detection data的数据做出detection的预测。我们使用ImageNet的detection任务做了验证。YOLO 9000在仅有200类中44类的detection data的情况下，仍然在ImageNet datection任务中取得了19.7mAP的成绩。对于不在COCO中的156类，YOLO9000成绩为16.0mAP。我们使得YOLO9000能够在保证实时的前提下对9K类目标进行检测。 根据论文结构的安排，将从Better，Faster和Stronger三个方面对论文提出的各条改进措施进行介绍。 Better在YOLO V1的基础上，作者提出了不少的改进来进一步提升算法的性能（mAP），主要改进措施包括网络结构的改进（第1，3，5，6条）和Anchor Box的引进（第3，4，5条）以及训练方法（第2，7条）。 改进1：引入BN层（Batch Normalization）Batch Normalization能够加快模型收敛，并提供一定的正则化。作者在每个conv层都加上了了BN层，同时去掉了原来模型中的drop out部分，这带来了2%的性能提升。 改进2：高分辨率分类器（High Resolution Classifier）YOLO V1首先在ImageNet上以$224\times 224$大小图像作为输入进行训练，之后在检测任务中提升到$448\times 448$。这里，作者在训练完224大小的分类网络后，首先调整网络大小为$448\times 448$，然后在ImageNet上进行fine tuning（10个epoch）。也就是得到了一个高分辨率的cls。再把它用detection上训练。这样，能够提升4%。 改进3：引入Anchor BoxYOLO V1中直接在CNN后面街上全连接层，直接回归bounding box的参数。这里引入了Faster RCNN中的anchor box概念，不再直接回归bounding box的参数，而是相对于anchor box的参数。 作者去掉后面的fc层和最后一个max pooling层，以期得到更高分辨率的feature map。同时，shrink网络接受$416\times 416$（而不是448）大小的输入image。这是因为我们想要最后的feature map大小是奇数，这样就能够得到一个center cell（比较大的目标，更有可能占据中间的位置）。由于YOLO conv-pooling的效应是将image downsamplig 32倍，所以最后feature map大小为$416/32 = 13$。 与YOLO V1不同的是，我们不再对同一个grid cell下的bounding box统一产生一个数量为$C$的类别概率，而是对于每一个bounding box都产生对应的$C$类概率。和YOLO V1一样的是，我们仍然产生confidence，意义也完全一样。 使用anchor后，我们的精度accuracy降低了，不过recall上来了。（这也较好理解。原来每个grid cell内部只有2个bounding box，造成recall不高。现在recall高上来了，accuracy会下降一些）。 改进4：Dimension Cluster在引入anchor box后，一个问题就是如何确定anchor的位置和大小？Faster RCNN中是手工选定的，每隔stride设定一个anchor，并根据不同的面积比例和长宽比例产生9个anchor box。在本文中，作者使用了聚类方法对如何选取anchor box做了探究。这点应该是论文中很有新意的地方。 这里对作者使用的方法不再过多赘述，强调以下两点： 作者使用的聚类方法是K-Means； 相似性度量不用欧氏距离，而是用IoU，定义如下：d(\text{box}, \text{centroid}) = 1-\text{IoU}(\text{box}, \text{centroid}) 使用不同的$k$，聚类实验结果如下，作者折中采用了$k = 5$。而且经过实验，发现当取$k=9$时候，已经能够超过Faster RCNN采用的手工固定anchor box的方法。下图右侧图是在COCO和VOC数据集上$k=5$的聚类后结果。这些box可以作为anchor box使用。 改进5：直接位置预测（Direct Location Prediction）我们仍然延续了YOLO V1中的思路，预测box相对于grid cell的位置。使用sigmoid函数作为激活函数，使得最终输出值落在$[0, 1]$这个区间上。 在output的feature map上，对于每个cell（共计$13\times 13$个），给出对应每个bounding box的输出$t_x$, $t_y$, $t_w$, $t_h$。每个cell共计$k=5$个bounding box。如何由这几个参数确定bounding box的真实位置呢？见下图。 设该grid cell距离图像左上角的offset是$(c_x, c_y)$，那么bounding box的位置和宽高计算如下。注意，box的位置是相对于grid cell的，而宽高是相对于anchor box的。 改进6：Fine-Gained Features这个trick是受Faster RCNN和SSD方法中使用多个不同feature map提高算法对不同分辨率目标物体的检测能力的启发，加入了一个pass-through层，直接将倒数第二层的$26\times 26$大小的feature map加进来。 在具体实现时，是将higher resolution（也就是$26\times 26$）的feature map stacking在一起。比如，原大小为$26\times 26 \times 512$的feature map，因为我们要将其变为$13\times 13$大小，所以，将在空间上相近的点移到后面的channel上去，这部分可以参考Darknet中reorg_layer的实现。 使用这一扩展之后的feature map，提高了1%的性能提升。 改进7：多尺度训练（Multi-Scale Training）在实际应用时，输入的图像大小有可能是变化的。我们也将这一点考虑进来。因为我们的网络是全卷积神经网络，只有conv和pooling层，没有全连接层，所以可以适应不同大小的图像输入。所以网络结构上是没问题的。 具体来说，在训练的时候，我们每隔一定的epoch（例如10）就随机改变网络的输入图像大小。由于我们的网络最终降采样的比例是$32$，所以随机生成的图像大小为$32$的倍数，即$\lbrace 320, 352, \dots, 608\rbrace$。 在实际使用中，如果输入图像的分辨率较低，YOLO V2可以在不错的精度下达到很快的检测速度。这可以被用应在计算能力有限的场合（无GPU或者GPU很弱）或多路视频信号的实时处理。如果输入图像的分辨率较高，YOLO V2可以作为state of the art的检测器，并仍能获得不错的检测速度。对于目前流行的检测方法（Faster RCNN，SSD，YOLO）的精度和帧率之间的关系，见下图。可以看到，作者在30fps处画了一条竖线，这是算法能否达到实时处理的分水岭。Faster RCNN败下阵来，而YOLO V2的不同点代表了不同输入图像分辨率下算法的表现。对于详细数据，见图下的表格对比（VOC 2007上进行测试）。 总结在Better这部分的末尾，作者给出了一个表格，指出了主要提升性能的措施。例外是网络结构上改为带Anchor box的全卷积网络结构（提升了recall，但对mAP基本无影响）和使用新的网络（计算量少了~33%）。 Faster这部分的改进为网络结构的变化。包括Faster RCNN在内的很多检测方法都使用VGG-16作为base network。VGG-16精度够高但是计算量较大（对于大小为$224\times 224$的单幅输入图像，卷积层就需要30.69B次浮点运算）。在YOLO V1中，我们的网络结构较为简单，在精度上不如VGG-16（ImageNet测试，88.0% vs 90.0%）。 在YOLO V2中，我们使用了一种新的网络结构Darknet-19（因为base network有19个卷积层）。和VGG相类似，我们的卷积核也都是$3\times 3$大小，同时每次pooling操作后channel数double。另外，在NIN工作基础上，我们在网络最后使用了global average pooling层，同时使用$1\times 1$大小的卷积核来做feature map的压缩（分辨率不变，channel减小，新的元素是原来相应位置不同channel的线性组合），同时使用了Batch Normalization技术。具体的网络结构见下表。Darknet-19计算量大大减小，同时精度超过了VGG-16。 在训练过程中，首先在ImageNet 1K上进行分类器的训练。使用数据增强技术（如随机裁剪、旋转、饱和度变化等）。和上面的讨论相对应，首先使用$224\times 224$大小的图像进行训练，再使用$448\times 448$的图像进行fine tuning，具体训练参数设置可以参见论文和对应的代码。这里不再多说。 然后，我们对检测任务进行训练。对网络结构进行微调，去掉最后的卷积层（因为它是我们当初为了得到1K类的分类置信度加上去的），增加$3$个$3\times 3$大小，channel数为$1024$的卷积层，并每个都跟着一个$1\times 1$大小的卷积层，channel数由我们最终的检测任务需要的输出决定。对于VOC的检测任务来说，我们需要预测$5$个box，每个需要$5$个参数（相对位置，相对大小和置信度），同时$20$个类别的置信度，所以输出共$5\times(5+20)=125$。从YOLO V2的yolo_voc.cfg文件中，我们也可以看到如下的对应结构： 1234567891011121314[convolutional]batch_normalize=1size=3stride=1pad=1filters=1024activation=leaky[convolutional]size=1stride=1pad=1filters=125activation=linear 同时，加上上文提到的pass-through结构。 Stronger未完待续]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[CS131-立体视觉基础]]></title>
      <url>%2F2017%2F02%2F02%2Fcs131-camera%2F</url>
      <content type="text"><![CDATA[数字图像是现实世界的物体通过光学成像设备在感光材料上的投影。在3D到2D的转换过程中，深度信息丢失。如何从单幅或者多幅图像中恢复出有用的3D信息，需要使用立体视觉的知识进行分析。如下图所示。这篇博客对课程讲义中立体视觉部分做一整理，分别介绍了针孔相机模型和对极集合的基础知识。 针孔相机模型（Pinhole Camera）针孔相机是简化的相机模型。光线沿直线传播。物体反射的光线，通过针孔，在成像面形成倒立的影像。针孔与成像面的距离，称为焦距。一般而言，针孔越小，影像越清晰，但针孔太小，会导致衍射，反而令影像模糊。 投影几何的重要性质在对针孔相机详细说明之前，首先介绍投影几何（Projective Geometry）的几条性质。 在投影变换前后，长度和角度信息丢失，但直线仍然保持是直线。如下图所示。原来垂直和平行的台球桌面的边不再保持原有的角度和相对长度，但是仍然是直线。 另一方面，虽然3D世界中的平行线在投影变换后相交（我们将交点称为Vanishing point），但是3D世界中互相平行的线，在2D平面上的Vanishing point为同一点。而所有平行线的Vanishing point又在同一条直线上。如下图所示（竖直的绿色平行线在投影变换后仍然保持平行，实际上并不想交，但我们这里认为其交于无穷远的某点）。 针孔相机模型如下图所示，为针孔相机成像原理示意图。外部世界的一点$P$发出（或反射）的光线，经过小孔$O$，成像在在像平面$\Pi^\prime$的点$P^\prime$处。通过简单的相似三角形知识，可以得到图上的关系式。 由上图可以知道，3D空间的一点$P$与成像平面上的对应点$P^\prime$坐标之间的数量关系为： \left\{\begin{matrix} x^\prime = fx/z \\ y^\prime = fy/z \end{matrix}\right.可是，由于变量$z$位于分母的位置，也就是说如果直接写成变换矩阵的话，这个矩阵是和变量$z$有关的，我们不想这样，而是想得到一个完全由相机本身确定的变换矩阵。所以，我们借鉴齐次矩阵的思想，将原来的点增加一个维度，转换为齐次坐标。如下图的两个例子所示。 这样，我们可以将3D和2D之间的变换写成下面的形式。当实际计算的时候，我们首先将3D点转换为4维向量（在末尾补1），然后左乘变换矩阵。最后将得到的结果化为末尾元素为1的形式，这样，前面两个元素就代表了变换后2D点的坐标。这样做，使得变换矩阵$M$完全由相机参数（即焦距$f$）决定。 上面的式子只是理想情况下的相机模型，成立的假设条件较强，主要有以下几点假设： 内假设（和相机本身有关） 不同方向上焦距相同； 光学中心在相平面的坐标原点$(0, 0)$ 没有倾斜（no skew） 外假设（和相机位姿有关，和相机本身参数无关） 相机没有旋转（坐标轴与世界坐标系方向重合） 相机没有平移（相机中心与世界坐标系中心重合） 其中，no skew情形我也不知道中文应该如何翻译。这种情况如图所示，由于剪切变形，成像平面的坐标轴并不是严格正交，使得两个轴之间实际存在一个很小的夹角。 下面，按照slide的顺序逐渐将上述假设移去，分析变换矩阵$M$应该如何修正。 理想情况理想情况以上假设全部满足，矩阵$M$如下所示。 光学中心不在像平面的坐标原点假设光学中心位于像平面内坐标为$(u_0, v_0)$的位置。则需要在理想情况计算的坐标基础上加上这个偏移量。所以，矩阵$M$被修正为： 像素非正方形由于透镜在$x$和$y$方向上的焦距不同，使得两个方向上的比例因子不同，不能再使用同一个$f$计算。修正如下： no skew这时候$x$方向上其实有$y$轴坐标的分量，所以修正如下： 相机的旋转和平移相机外部姿态很可能经过了旋转和平移，从而不与世界坐标系完全重合。如图所示。 所以我们需要先对世界坐标系内的3D点做一个齐次变换，将其变换为一个中间坐标系，这个中间坐标系和相机坐标系是重合的。也就是我们的变换矩阵应该是下面这个形式的，其中，$H$是表征旋转和平移的齐次矩阵，$H\in \mathbb{R}^{3\times 4}$。 P^\prime = MHP首先考虑较为简单的平移。所做的修正如下所示，也就是将原来的$\mathbb{0}$矩阵变为了一个平移向量。 进一步考虑旋转时，我们将旋转分解为绕三个坐标轴的三次旋转，单次旋转矩阵如下所示： 将它们按照旋转顺序左乘起来，可以得到修正后的变换形式如下： 最终形式综上所示，变换矩阵的最终形式为： 其中，内参数矩阵中有5个自由度，外参数矩阵中有6个自由度（3平移，3旋转）。 上面的内容总结起来，如下图所示。 对极几何基础概念如下图所示，给出了对极几何中的几个基本概念。两个相机的中心分别位于$O$点和$O^{‘}$点，被观察物体位于$P$点。 极点：$e$和$e^\prime$点分别是$OO^\prime$连线与两个成像平面的交点，也是相机中心在另一台相机成像平面上对应的像。 极平面：点$O$，$O^\prime$，$P$点共同确定的平面（灰色） 极线：极平面与两个成像平面的交线，即$pe$和$p^\prime e^\prime$（蓝色） 基线：两个相机中心的连线（黄色） 极线约束从上图可以看出，如果两台相机已经给定，那么给定物体点在左边相机像平面内的成像点，则右边相机成像平面内的点被唯一确定。如何能够找到这个对应点呢？或者换种说法，在对极几何中，如何描述这种不同相机成像点的约束关系呢？ 如下图所示，从上面的讨论我们已经知道，物点$P$在左右两个相机成像平面的坐标可以使用由相机内参数矩阵和外参数矩阵确定的齐次变换矩阵得到（下图在标注时，右侧相机的点应被标注为$p^\prime$）。为了简化问题，在下面的讨论时，我们假定相机都是理想的针孔相机模型，也就是说，变换矩阵只和外参数矩阵（也即是相机的位姿决定）。再做简化，认为世界坐标系与左侧相机的相机坐标系重合。那么，就得到了图上所示的变换矩阵$M$和$M^\prime$。其中，$R$是左右两个相机之间的旋转矩阵，$T$是$OO^\prime$有向线段，表明相机中心的位移。 （下面的推导参考了博客：计算机视觉基础4——对极几何）。在下面的推导中，我们使用$p^\prime$表示在相机$O^\prime$下的向量$O\prime P$，符号$p$同理。那么，有如下关系成立：$R(p-T) = p^\prime$]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[CS131-描述图像的特征(SIFT)]]></title>
      <url>%2F2017%2F01%2F30%2Fcs131-sift%2F</url>
      <content type="text"><![CDATA[SIFT(尺度不变特征变换，Scale Invariant Feature Transform),最早由Lowe提出，目的是为了解决目标检测（Object Detection）问题中提取图像特征的问题。从名字可以看出，SIFT的优势就在于对尺度变换的不变性，同时SIFT还满足平移变换和旋转变换下的不变性，并对光照变化和3D视角变换有一定的不变性。它的主要步骤如下： scale space上的极值检测。使用DoG在不同的scaleast和image position下找到interest point。 interest point的localization。对上面的interest point进行稳定度检测，并确定其所在的scale和position。 确定方向。通过计算图像的梯度图，确定key point的方向，下一步的feature operation就是在这个方向，scale和position上进行的。 确定key point的描述子。使用图像的局部梯度作为key point的描述子，最终构成SIFT特征向量。 SIFT介绍上讲中介绍的Harris角点方法计算简便，并具有平移不变性和旋转不变性。特征$f$对某种变换$\mathcal{T}$具有不变性，是指在经过变换后原特征保持不变，也就是$f(I) = f(\mathcal{T}(I))$。但是Harris角点不具有尺度变换不变性，如下图所示。当图像被放大后，原图的角点被判定为了边缘点。 而我们想要得到一种对尺度变换保持不变性的特征计算方法。例如图像patch的像素平均亮度，如下图所示。region size缩小为原始的一半后，亮度直方图的形状不变，即平均亮度不变。 而Lowe想到了使用局部极值来作为feature来保证对scale变换的不变性。在算法的具体实现中，他使用DoG来获得局部极值。 Lowe的论文中也提到了SIFT特征的应用。SIFT特征可以产生稠密（dense）的key point，例如一张500x500像素大小的图像，一般可以产生~2000个稳定的SIFT特征。在进行image matching和recognition时，可以将ref image的SIFT特征提前计算出来保存在数据库中，并计算待处理图像的SIFT特征，根据特征向量的欧氏距离进行匹配。 这篇博客主要是Lowe上述论文的读书笔记，按照SIFT特征的计算步骤进行组织。 尺度空间极值的检测方法前人研究已经指出，在一系列合理假设下，高斯核是唯一满足尺度不变性的。所谓的尺度空间，就是指原始图像$I(x,y)$和可变尺度的高斯核$G(x,y,\sigma)$的卷积结果。如下式所示： L(x,y,\sigma) = G(x,y,\sigma)\ast I(x,y)其中，$G(x,y, \sigma) = \frac{1}{2\pi\sigma^2}\exp(-(x^2+y^2)/2\sigma^2)$。不同的$\sigma$代表不同的尺度。 DoG(difference of Gaussian)函数定义为不同尺度的高斯核与图像卷积结果之差，即， D(x,y,\sigma) = L(x,y,k\sigma) - L(x,y,\sigma)如下图所示，输入的图像重复地与高斯核进行卷积得到结果图像$L$（左侧），这样构成了一个octave。相邻的$L$之间作差得到DoG图像（右侧）。当一个octave处理完之后，将当前octave的最后一张高斯卷积结果图像降采样两倍，按照前述方法构造下一个octave。在图中，我们将一个octave划分为$s$个间隔（即$s+1$个图像）时，设$\sigma$最终变成了2倍（即$\sigma$加倍）。那么，显然有相邻两层之间的$k = 2^{1/s}$。不过为了保证首尾两张图像也能够计算差值，我们实际上需要再补上两张（做一个padding），也就是说一个octave内的总图像为$s+3$（下图中的$s=2$）。 为何我们要费力气得到DoG呢？论文中作者给出的解释是：DoG对scale-normalized后的Guassian Laplace函数$\sigma^2\Delta G$提供了足够的近似。其中前面的$\sigma^2$系数正是为了保证尺度不变性。而前人的研究则指出，$\sigma \Delta G$函数的极值，和其他一大票其他function比较时，能够提供最稳定的feature。 对于高斯核函数，有以下性质： \frac{\partial G}{\partial \sigma} = \sigma \Delta G我们将式子左侧的微分变成差分，得到了下式： \sigma\Delta G \approx \frac{G(x,y,k\sigma)-G(x,y,\sigma)}{k\sigma - \sigma}也就是： G(x,y,k\sigma)-G(x,y,\sigma) \approx (k-1)\sigma^2 \Delta G当$k=1$时，上式的近似误差为0（即上面的$s=\infty$，也就是说octave的划分是连续的）。但是当$k$较大时，如$\sqrt{2}$（这时$s=2$，octave划分十分粗糙了），仍然保证了稳定性。同时，由于相邻两层的比例$k$是定值，所以$k-1$这个因子对于检测极值没有影响。我们在计算时，无需除以$k-1$。 构造完了DoG图像金字塔，下面我们的任务就是检测其中的极值。如下图，对于每个点，我们将它与金字塔中相邻的26个点作比较，找到局部极值。 另外，我们将输入图像行列预先使用线性插值的方法resize为原来的2倍，获取更多的key point。 此外，在Lowe的论文中还提到了使用DoG的泰勒展开和海森矩阵进行key point的精细确定方法，并对key point进行过滤。这里也不再赘述了。 128维feature的获取我们需要在每个key point处提取128维的特征向量。这里结合CS131课程作业2的相关练习作出说明。对于每个key point，我们关注它位于图像金字塔中的具体层数以及像素点位置，也就是说通过索引pyramid{scale}(y, x)就可以获取key point。我们遍历key point，为它们赋予一个128维的特征向量。 我们选取point周围16x16大小的区域，称为一个patch。将其分为4x4共计16个cell。这样，每个cell内共有像素点16个。对于每个cell，我们计算它的局部梯度方向直方图，直方图共有8个bin。也就是说每个cell可以得到一个8维的特征向量。将16个cell的特征向量首尾相接，就得到了128维的SIFT特征向量。在下面的代码中，使用变量patch_mag和patch_theta分别代表patch的梯度幅值和角度，它们可以很简单地使用卷积和数学运算得到。 123patch_mag = sqrt(patch_dx.^2 + patch_dy.^2);patch_theta = atan2(patch_dy, patch_dx); % atan2的返回结果在区间[-pi, pi]上。patch_theta = mod(patch_theta, 2*pi); % 这里我们要将其转换为[0, 2pi] 之后，我们需要获取key point的主方向。其定义可见slide，即为key point扩展出来的patch的梯度方向直方图的峰值对应的角度。 所以我们首先应该设计如何构建局部梯度方向直方图。我们只要将[0, 2pi]区间划分为若干个bin，并将patch内的每个点使用其梯度大小向对应的bin内投票即可。如下所示： 12345678910111213141516171819202122232425262728293031323334function [histogram, angles] = ComputeGradientHistogram(num_bins, gradient_magnitudes, gradient_angles)% Compute a gradient histogram using gradient magnitudes and directions.% Each point is assigned to one of num_bins depending on its gradient% direction; the gradient magnitude of that point is added to its bin.%% INPUT% num_bins: The number of bins to which points should be assigned.% gradient_magnitudes, gradient angles:% Two arrays of the same shape where gradient_magnitudes(i) and% gradient_angles(i) give the magnitude and direction of the gradient% for the ith point. gradient_angles ranges from 0 to 2*pi% % OUTPUT% histogram: A 1 x num_bins array containing the gradient histogram. Entry 1 is% the sum of entries in gradient_magnitudes whose corresponding% gradient_angles lie between 0 and angle_step. Similarly, entry 2 is for% angles between angle_step and 2*angle_step. Angle_step is calculated as% 2*pi/num_bins.% angles: A 1 x num_bins array which holds the histogram bin lower bounds.% In other words, histogram(i) contains the sum of the% gradient magnitudes of all points whose gradient directions fall% in the range [angles(i), angles(i + 1)) angle_step = 2 * pi / num_bins; angles = 0 : angle_step : (2*pi-angle_step); histogram = zeros(1, num_bins); num = numel(gradient_angles); for n = 1:num index = floor(gradient_angles(n) / angle_step) + 1; histogram(index) = histogram(index) + gradient_magnitudes(n); end end Lowe论文中推荐的bin数目为36个，计算主方向的函数如下： 12345678910111213141516171819202122232425function direction = ComputeDominantDirection(gradient_magnitudes, gradient_angles)% Computes the dominant gradient direction for the region around a keypoint% given the scale of the keypoint and the gradient magnitudes and gradient% angles of the pixels in the region surrounding the keypoint.%% INPUT% gradient_magnitudes, gradient_angles:% Two arrays of the same shape where gradient_magnitudes(i) and% gradient_angles(i) give the magnitude and direction of the gradient for% the ith point. % Compute a gradient histogram using the weighted gradient magnitudes. % In David Lowe's paper he suggests using 36 bins for this histogram. num_bins = 36; % Step 1: % compute the 36-bin histogram of angles using ComputeGradientHistogram() [histogram, angle_bound] = ComputeGradientHistogram(num_bins, gradient_magnitudes, gradient_angles); % Step 2: % Find the maximum value of the gradient histogram, and set "direction" % to the angle corresponding to the maximum. (To match our solutions, % just use the lower-bound angle of the max histogram bin. (E.g. return % 0 radians if it's bin 1.) [~, max_index] = max(histogram); direction = angle_bound(max_index);end 之后，我们更新patch内各点的梯度方向，计算其与主方向的夹角，作为新的方向。并将梯度进行高斯平滑。 123patch_theta = patch_theta - ComputeDominantDirection(patch_mag, patch_theta);;patch_theta = mod(patch_theta, 2*pi);patch_mag = patch_mag .* fspecial('gaussian', patch_size, patch_size / 2); % patch_size = 16 遍历cell，计算feature如下： 123456789101112131415feature = [];row_iter = 1;for y = 1:num_histograms col_iter = 1; for x = 1:num_histograms cell_mag = patch_mag(row_iter: row_iter + pixelsPerHistogram - 1, ... col_iter: col_iter + pixelsPerHistogram - 1); cell_theta = patch_theta(row_iter: row_iter + pixelsPerHistogram - 1, ... col_iter: col_iter + pixelsPerHistogram - 1); [histogram, ~] = ComputeGradientHistogram(num_angles, cell_mag, cell_theta); feature = [feature, histogram]; col_iter = col_iter + pixelsPerHistogram; end row_iter = row_iter + pixelsPerHistogram;end 最后，对feature做normalization。首先将feature化为单位长度，并将其中太大的分量进行限幅（如0.2的阈值），之后再重新将其转换为单位长度。 这样，就完成了SIFT特征的计算。在Lowe的论文中，有更多对实现细节的讨论，这里只是跟随课程slide和作业走完了算法流程，不再赘述。 应用：图像特征点匹配和Harris角点一样，SIFT特征可以用作两幅图像的特征点匹配，并且具有多种变换不变性的优点。对于两幅图像分别计算得到的特征点SIFT特征向量，可以使用下面简单的方法搜寻匹配点。计算每组点对之间的欧式距离，如果最近的距离比第二近的距离小得多，那么可以认为有一对成功匹配的特征点。其MATLAB代码如下，其中descriptor是两幅图像的SIFT特征向量。阈值默认为取做0.7。 123456789101112131415161718192021222324252627282930313233343536373839function match = SIFTSimpleMatcher(descriptor1, descriptor2, thresh)% SIFTSimpleMatcher% Match one set of SIFT descriptors (descriptor1) to another set of% descriptors (decriptor2). Each descriptor from descriptor1 can at% most be matched to one member of descriptor2, but descriptors from% descriptor2 can be matched more than once.% % Matches are determined as follows:% For each descriptor vector in descriptor1, find the Euclidean distance% between it and each descriptor vector in descriptor2. If the smallest% distance is less than thresh*(the next smallest distance), we say that% the two vectors are a match, and we add the row [d1 index, d2 index] to% the "match" array.% % INPUT:% descriptor1: N1 * 128 matrix, each row is a SIFT descriptor.% descriptor2: N2 * 128 matrix, each row is a SIFT descriptor.% thresh: a given threshold of ratio. Typically 0.7%% OUTPUT:% Match: N * 2 matrix, each row is a match.% For example, Match(k, :) = [i, j] means i-th descriptor in% descriptor1 is matched to j-th descriptor in descriptor2. if ~exist('thresh', 'var'), thresh = 0.7; end match = []; [N1, ~] = size(descriptor1); for i = 1:N1 fea = descriptor1(i, :); err = bsxfun(@minus, fea, descriptor2); dis = sqrt(sum(err.^2, 2)); [sorted_dis, ind] = sort(dis, 1); if sorted_dis(1) &lt; thresh * sorted_dis(2) match = [match; [i, ind(1)]]; end endend 接下来，我们可以使用最小二乘法计算两幅图像之间的仿射变换矩阵（齐次变换矩阵）$H$。其中$H$满足： Hp_{\text{before}} = p_{\text{after}}其中 p = \begin{bmatrix}x \\\\ y \\\\ 1\end{bmatrix}对上式稍作变形，有 p_{\text{before}}^\dagger H^\dagger = p_{\text{after}}\dagger就可以使用标准的最小二乘正则方程进行求解了。代码如下： 1234567891011121314151617181920212223242526272829303132function H = ComputeAffineMatrix( Pt1, Pt2 )%ComputeAffineMatrix% Computes the transformation matrix that transforms a point from% coordinate frame 1 to coordinate frame 2%Input:% Pt1: N * 2 matrix, each row is a point in image 1% (N must be at least 3)% Pt2: N * 2 matrix, each row is the point in image 2 that% matches the same point in image 1 (N should be more than 3)%Output:% H: 3 * 3 affine transformation matrix,% such that H*pt1(i,:) = pt2(i,:) N = size(Pt1,1); if size(Pt1, 1) ~= size(Pt2, 1), error('Dimensions unmatched.'); elseif N&lt;3 error('At least 3 points are required.'); end % Convert the input points to homogeneous coordintes. P1 = [Pt1';ones(1,N)]; P2 = [Pt2';ones(1,N)]; H = P1*P1'\P1*P2'; H = H'; % Sometimes numerical issues cause least-squares to produce a bottom % row which is not exactly [0 0 1], which confuses some of the later % code. So we'll ensure the bottom row is exactly [0 0 1]. H(3,:) = [0 0 1];end 作业中的其他例子也很有趣，这里不再多说了。贴上两张图像拼接的实验结果吧~]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[CS131-描述图像的特征(Harris 角点)]]></title>
      <url>%2F2017%2F01%2F25%2Fcs131-finding-features%2F</url>
      <content type="text"><![CDATA[feature是对图像的描述。比如，图像整体灰度值的均值和方差就可以作为feature。如果我们想在一副图像中检测足球时，可以使用滑动窗方法，逐一检查窗口内的灰度值分布是否和一张给定的典型黑白格子足球相似。可想而知，这种方法的性能一定让人捉急。而在image matching问题中，常常需要将不同视角的同一目标物进行matching，进而计算相机转过的角度。这在SLAM问题中很有意义。如下图所示，同一处景观，在不同摄影师的镜头下，不仅视角不同，而且明暗变化也有很多差别，右侧的图暖色调更浓。这也告诉我们，上面提到的只使用全局图像灰度值来做feature的做法有多么不靠谱。 那么，让我们脱离全局特征，转而将注意力集中在局部特征上。这是因为使用局部特征能够更好地处理图像中的遮挡、变形等情况，而且我们的研究对象常常是图像中的部分区域而不是图像整体。更特殊地，在这一讲中，我们主要探究key point作为local feature的描述，并介绍Harris角点检测方法。在下一节中介绍SIFT特征。 Harris角点角点，即corner，和edge类似，区别在于其在两个方向上都有较为剧烈的灰度变化（而edge只在某一个方向上灰度值变化剧烈）。如图所示。 Harris角点得名于其发明者Harris，是一种常见的角点检测方法。给定观察窗口大小，计算平移后窗口内各个像素差值的加权平方和，如下式。 E(u,v) = \sum_x\sum_yw(x,y)[I(x+u, y+v) - I(x,y)]^2其中，窗口加权函数$w$可以取做门限函数或gaussian函数。如图所示。 使用泰勒级数展开，并忽略非线性项，我们有 I(x+u,y+v) = I(x,y) + I_x(x,y)u+I_y(x,y)v所以上式可以写成（线性二次型写成了矩阵形式）， E(u,v) = \sum_{x,y}w(I_xu+I_yv)^2 = \begin{bmatrix}u&v\end{bmatrix}M\begin{bmatrix}u\\\\v\end{bmatrix}其中， M = w\begin{bmatrix}I_x^2& I_xI_y\\\\I_xI_y&I_y^2\end{bmatrix}当使用门限函数时，权值$w_{i,j} = 1$，则， M = \begin{bmatrix}\sum I_xI_x& \sum I_xI_y\\\\\sum I_xI_y&\sum I_yI_y\end{bmatrix} = \sum \begin{bmatrix}I_x \\\\I_y\end{bmatrix}\begin{bmatrix}I_x &I_y\end{bmatrix}当corner与xy坐标轴对齐时候，如下图所示。由于在黑色观察窗口内，只有上侧和左侧存在边缘，且在上边缘，$I_y$很大，而$I_x=0$，在左侧边缘，$I_x$很大而$I_y = 0$，所以，矩阵 M = \begin{bmatrix}\lambda_1 & 0 \\\\ 0&\lambda_2 \end{bmatrix} 当corner与坐标轴没有对齐时，经过旋转变换就可以将其转换到与坐标轴对齐的角度，而这种旋转操作可以使用矩阵的相似化来表示（其实是二次型的化简，可以使用合同变换，而旋转变换的矩阵是酉矩阵，转置即为矩阵的逆，所以也是相似变换）。也就是说，矩阵$M$相似于某个对角阵。 M = R^{-1}\Sigma R, \text{其中}\Sigma = \begin{bmatrix}\lambda_1&0\\\\0&\lambda_2\end{bmatrix}所以，可以根据下面这张图利用矩阵$M$的特征值来判定角点和边缘点。当两个特征值都较大时为角点；当某个分量近似为0而另一个分量较大时，可以判定为边缘点（因为某个方向的导数为0）；当两个特征值都近似为0时，说明是普通点（flat point）。（课件原图如此，空缺的问号处应分别为$\lambda_1$和$\lambda_2$）。 然而矩阵的特征值计算较为复杂，所以使用下面的方法进行近似计算。 \theta = \det(M)-\alpha\text{trace}(M)^2 = \lambda_1\lambda_2-\alpha(\lambda_1+\lambda_2)^2 为了减弱噪声的影响，常常使用gaussian窗函数。如下式所示： w(x,y) = \exp(-(x^2+y^2)/2\sigma^2)]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[CS131-边缘检测]]></title>
      <url>%2F2017%2F01%2F24%2Fcs131-edge-detection%2F</url>
      <content type="text"><![CDATA[边缘(Edge)在哺乳动物的视觉中有重要意义。在由若干卷积层构成的深度神经网络中，较低层的卷积层就被训练成为对特定形状的边缘做出响应。边缘检测也是计算机视觉和图像处理领域中一个重要的问题。 边缘的产生若仍采取将图像视作某一函数的观点，边缘是指这个函数中的不连续点。边缘检测是后续进行目标检测和形状识别的基础，也能够在立体视觉中恢复视角等。边缘的来源主要有以下几点： 物体表面不平造成灰度值的不连续； 深度值不同造成灰度值不连续； 物体表面颜色的突变造成灰度值不连续 朴素思想利用边缘是图像中不连续点的这一性质，可以通过计算图像的一阶导数，再找到一阶导数的极大值，即认为是边缘点。如下图所示，在白黑交界处，图像一阶导数的值非常大，表明此处灰度值变化剧烈，是边缘。 问题转换为如何求取图像的一阶导数（或梯度）。由于图像是离散的二元函数，所以下文不再区分求导与差分。 在$x$方向上，令$g_x = \frac{\partial f}{\partial x}$；在$y$方向上，令$g_y = \frac{\partial f}{\partial y}$。梯度的大小和方向为 g = \lbrack g_x, g_y\rbrack, \theta = \arctan(g_y/g_x)通过和Sobel算子等做卷积，可以求取两个正交方向上图像的一阶导数，并计算梯度，之后检测梯度的局部极大值就能够找出边缘点了。 只是这种方法很容易受到噪声影响。如图所示，真实的边缘点被湮没在了噪声中。 改进1：先平滑改进措施1，可以首先对图像进行高斯平滑，再按照上面的方法求取边缘点。根据卷积的性质，有： \frac{d}{dx}(f\ast g) = f\ast\frac{d}{dx}g所以我们可以先求取高斯核的一阶导数，再和原始图像直接做一次卷积就可以一举两得了。这样，引出了DoG(Deriative of Gaussian)。$x$方向的DoG如图所示。 进行高斯平滑不可避免会使图像中原本的细节部分模糊，所以需要在克服噪声和引入模糊之间做好折中。 改进2：Canny检测子改进措施2，使用Canny检测子进行检测。Canny检测方法同样基于梯度，其基本原理如下： 使用DoG计算梯度幅值和方向。 非极大值抑制，这个过程需要根据梯度方向做线性插值。如图，沿着点$q$的梯度方向找到了$p$和$r$两个点。这两个点的梯度幅值需要根据其临近的两点做插值得到。 利用梯度方向和边缘线互相垂直这一性质，如图，若已经确定点$p$为边缘点，则向它的梯度方向正交方向上寻找下一个边缘点（点$r$或$s$）。这一步也叫edge linking。 同时，为了提高算法性能，Canny中采用了迟滞阈值的方法，设定low和high两个阈值，来判定某个点是否属于强或弱边缘点。在做edge linking的时候，从强边缘点开始，如果遇到了弱边缘点，则继续，直到某点的梯度幅值甚至比low还要小，则在此停止。 改进3：RANSAC方法有的时候，我们并不是想要找到所有的边缘点，可能只是想找到图像中水平方向的某些边缘。这时候可以考虑采用RANSAC方法。 RANSAC方法的思想在于，认为已有的feature大部分都是好的。这样，每次随机抽取出若干feature，建立model，再在整个feature集合上进行验证。那么由那些好的feature得到的model一定是得分较高的。（世界上还是好人多啊！）这样就剔除了离群点的影响。 以直线拟合为例，在下图中，给出了使用RANSAC方法拟合直线的步骤。如图1所示，由于离群点的存在，如果直接使用最小二乘法进行拟合，拟合结果效果会很不理想。由于确定一条直线需要两个点，所以从点集中选取两个点，并计算拟合直线。并计算点集中的点在这条直线附近的个数，作为对模型好坏的判定，这些点是新的内点。找出最优的那条直线，使用其所有内点再进行拟合，重复上述操作，直至迭代终止。 上述RANSAC方法进行直线拟合的过程可以总结如下： 按照上述思想，我分别使用最小二乘法和RANSAC方法尝试进行直线拟合。在下面的代码中，我首先产生了正常受到一定高斯噪声污染的数据（图中的红色点），这些点的真值都落在直线$y = 2x+1$上。而后，我随机变化了斜率和截距，以期产生一些离群点（图中的蓝色点）。当然，由于随机性，这种方法生成的点有可能仍然是内点。 而后，我分别使用上述两种方法进行拟合。可以从结果图中看出，RANSAC（绿色线）能够有效避免离群点的干扰，获得更好的拟合效果。在某次实验中，两种方法的拟合结果如下：12least square: a = 3.319566, b = -1.446528ransac method: a = 1.899640, b= 1.298608 实验使用的MATLAB代码如下：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162%% generate datax = 0:1:10;y_gt = 2*x+1;y = y_gt + randn(size(y_gt));scatter(x, y, [], [1,0,0]);hold onout_x = 0:1:10;out_y = 5*rand(size(out_x)).*out_x + 4*rand(size(out_x));scatter(out_x, out_y, [], [0,0,1]);X = [x, out_x]';Y = [y, out_y]';X = [X, ones(length(X), 1)];[a, b] = ls_fit(X, Y);plot(x, a*x+b, 'linestyle', '--', 'color', 'r');[ra, rb] = ransac_fit(X, Y, 100, 2, 0.5, 3);plot(x, ra*x+rb, 'linestyle', '-.', 'color', 'g');fprintf('least square: a = %f, b = %f\n',a, b);fprintf('ransac method: a = %f, b= %f\n', ra, rb)function [a, b] = ransac_fit(X, Y, k, n, t ,d)% ransac fit% k -- maximum iteration number% n -- smallest point numer required% t -- threshold to identify a point is fit well% d -- the number of nearby points to assert a model is finedata = [X, Y];N = size(data, 1);best_good_cnt = -1;best_a = 0;best_b = 0;for i = 1:k % sample point idx = randsample(N, n); data_sampled = data(idx, :); % fit with least square [a, b] = ls_fit(data_sampled(:, 1:2), data_sampled(:, 3)); % test model not_sampled = ones(N, 1); not_sampled(idx) = 0; not_sampled_data = data(not_sampled == 1, :); distance = abs(not_sampled_data(:, 1:2) * [a; b] - not_sampled_data(:, 3)) / sqrt(a^2+1); inner_flag = distance &lt; t; good_cnt = sum(inner_flag); if good_cnt &gt;= d &amp;&amp; good_cnt &gt; best_good_cnt best_good_cnt = good_cnt; data_refine = data(find(inner_flag), :); [a, b] = ls_fit(data_refine(:, 1:2), data_refine(:, 3)); best_a = a; best_b = b; end fprintf('iteration %d, best_a = %f, best_b = %f\n', i, best_a, best_b);enda = best_a;b = best_b;endfunction [a, b] = ls_fit(X, Y)% least square fitA = X'*X\X'*Y;a = A(1);b = A(2);end 我们对RANSAC稍作分析，可以大概了解试验次数$k$的确定方法。 仍然使用上述直线拟合的例子。如果所有点中内点所占的比例为$\omega$，每次挑选$n$个点尝试（上述demo代码中取$n=2$）。那么每次挑选的两个点全部是内点的概率为$\omega^n$。当选取的$n$个点全部为内点时，视为有效实验。那么，重复$k$次实验，有效实验次数为0的概率为$(1-\omega^n)^k$。由于底数小于1，所以我们只需尽量增大$k$，就能够降低这种倒霉的概率。下图是不同$n$和$\omega$情况下为了使得实验成功的概率大于0.99所需的$k$的分布。 RANSAC方法的有点在于能够较为鲁棒地估计模型的参数，而且实现简单。缺点在于当离群点比例较大时，为保证实验成功所需的$k$值较大。这时候，可能Hough变换等基于投票的方法更适合用于图像中的直线检测问题。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[CS131-线性滤波器和矩阵的SVD分解]]></title>
      <url>%2F2017%2F01%2F23%2Fcs131-filter-svd%2F</url>
      <content type="text"><![CDATA[数字图像可以看做$\mathbb{R}^2 \rightarrow \mathbb{R}^c$的映射，其中$c$是图像的channel数。使用信号与系统的角度来看，如果单独考察某个channel，可以将图像看做是二维离散系统。 卷积卷积的概念不再详述，利用不同的kernel与原始图像做卷积，就是对图像进行线性滤波的过程。卷积操作时，以某一个点为中心点，最终结果是这个点以及它的邻域点的线性组合，组合系数由kernel决定。一般kernel的大小取成奇数。如下图所示。（图片来自博客《图像卷积与滤波的一些知识点》） 在卷积操作时，常常需要对图像做padding，常用的padding方法有： zero padding，也就是填充0值。 edge replication，也就是复制边缘值进行填充。 mirror extension，也就是将图像看做是周期性的，相当于使用对侧像素值进行填充。 作业1调整图像灰度值为0到255计算相应的k和offset值即可。另外MATLAB中的uint8函数可以将结果削顶与截底为0到255之间。123scale_ratio = 255.0 / (max_val - min_val);offset = -min_val * scale_ratio;fixedimg = scale_ratio * dark + offset; SVD图像压缩使用SVD进行图像压缩，下图是将所有奇异值按照从大到小的顺序排列的大小示意图。可以看到，第一个奇异值比其他高出一个数量级。 MATLAB实现分别使用10，50， 100个分量进行图像压缩，如下图所示。可以看到，k=10时，已经能够复原出原图像的大致轮廓。当k更大时，更多细节被复原出来。 MATLAB代码如下：12345678910111213141516171819202122232425%% read imageim = imread('./flower.bmp');im_gray = double(rgb2gray(im));[u, s, v] = svd(im_gray);%% get sigular valuesigma = diag(s);top_k = sigma(1:10);figureplot(1:length(sigma), sigma, 'r-', 'marker', 's', 'markerfacecolor', 'g');figuresubplot(2, 2, 1);imshow(uint8(im_gray));title('flower.bmp')index = 2;for k = [10, 50, 100] uk = u(:, 1:k); sk = s(1:k, 1:k); vk = v(:, 1:k); im_rec = uk * sk * vk'; subplot(2, 2, index); index = index + 1; imshow(uint8(im_rec)); title(sprintf('k = %d', k));end 图像SVD压缩中的误差分析完全是个人随手推导，不严格的说明： 将矩阵分块。由SVD分解公式$\mathbf{U}\mathbf{\Sigma} \mathbf{V^\dagger} = \mathbf{A}$，把$\mathbf{U}$按列分块，$\mathbf{V^\dagger}$按行分块，有下式成立： \begin{bmatrix} u_1 & u_2 &\vdots &u_n \end{bmatrix} \begin{bmatrix} \sigma_1 & & & \\\\ & \sigma_2& & \\\\ & & \ddots& \\\\ & & &\sigma_m \end{bmatrix} \begin{bmatrix} v_1^\dagger\\\\ v_2^\dagger\\\\ \dots\\\\ v_m^\dagger \end{bmatrix}=\mathbf{A}由于 \begin{bmatrix} u_1 & u_2 &\vdots &u_n \end{bmatrix} \begin{bmatrix} \sigma_1 & & & \\\\ & \sigma_2& & \\\\ & & \ddots& \\\\ & & &\sigma_m \end{bmatrix} = \begin{bmatrix} \sigma_1u_1 & \sigma_2u_2 &\vdots &\sigma_nu_n \end{bmatrix}所以， \mathbf{A} = \sum_{i = 1}^{r}\sigma_iu_iv_i^\dagger上面的式子和式里面只有$r$项，是因为当$k &gt; r$时，$\sigma_k = 0$。 所以\mathbf{A} - \hat{\mathbf{A}} = \sum_{i = k+1}^{r}\sigma_iu_iv_i^\dagger 根绝矩阵范数的性质，我们有， \left\lVert\mathbf{A} - \hat{\mathbf{A}}\right\rVert \le \sum_{i=k+1}^{r}\sigma_i\left\lVert u_i\right\rVert\left\lVert v_i^\dagger\right\rVert由于$u_i$和$v_i$都是标准正交基，所以范数小于1.故， \left\lVert\mathbf{A} - \hat{\mathbf{A}}\right\rVert \le \sum_{i=k+1}^{r}\sigma_i取无穷范数，可以知道对于误差矩阵中的任意元素（也就是压缩重建之后任意位置的像素灰度值之差），都有： e \le \sum_{i=k+1}^{r}\sigma_iSVD与矩阵范数如果某个函数$f$满足以下的性质，就可以作为矩阵的范数。 $f(\mathbf{A}) = \mathbf{0} \Leftrightarrow \mathbf{A} = \mathbf{0}$ $f(c\mathbf{A}) = c f(\mathbf{A}), \forall c \in \mathbb{R}$ $f(\mathbf{A+b}) \le f(\mathbf{A}) + f(\mathbf{B})$ 其中，矩阵的2范数可以定义为 \left\lVert\mathbf{A}\right\rVert_2 = \max{\sqrt{(\mathbf{A}x)^\dagger\mathbf{A}x}}其中，$x$是单位向量。上式的意义在于表明矩阵的2范数是对于所有向量，经过该矩阵线性变换后摸长最大的那个变换后向量的长度。 下面，给出不严格的说明，证明矩阵的2范数数值上等于其最大的奇异值。 对于空间内的任意单位向量$x$，利用矩阵的SVD分解，有（为了书写简单，矩阵不再单独加粗）： (Ax)^\dagger Ax = x^\dagger V \Sigma^\dagger \Sigma V^\dagger x其中，$U^\dagger U = I$，已经被消去了。 进一步化简，我们将$V^\dagger x$看做一个整体，令$\omega = V\dagger x$，那么有， (Ax)^\dagger Ax = (\Sigma \omega)^\dagger \Sigma \omega也就是说，矩阵的2范转换为了$\Sigma \omega$的幅值的最大值。由于$\omega$是酉矩阵和一个单位向量的乘积，所以$\omega$仍然是单位阵。 由于$\Sigma$是对角阵，所以$\omega$与其相乘后，相当于每个分量分别被放大了$\sigma_i$倍。即 \Sigma \omega = \begin{bmatrix} \sigma_1 \omega_1\\\\ \sigma_2 \omega_2\\\\ \cdots\\\\ \sigma_n \omega_n \end{bmatrix}它的幅值平方为 \left\lVert \Sigma \omega \right \rVert ^2 = \sum_{i=1}^{n}\sigma_i^2 \omega_i^2 \le \sigma_{1} \sum_{i=1}^{n}\omega_i^2 = \sigma_1^2当且仅当，$\omega_1 = 1$, $\omega_k = 0, k &gt; 1$时取得等号。 综上所述，矩阵2范数的值等于其最大的奇异值。 矩阵的另一种范数定义方法Frobenius norm定义如下： \left\lVert A \right\rVert_{F} = \sqrt{\sum_{i=1}^{m}\sum_{j=1}^{n}\left\vert a_{i,j}\right\rvert}如果我们两边平方，可以得到，矩阵的F范数实际等于某个矩阵的迹，见下式： \left\lVert A\right \rVert_F^2 = \text{trace}(A^\dagger A)利用矩阵的SVD分解，可以很容易得出，$\text{trace}(A^\dagger A) = \sum_{i=1}^{r}\sigma_i^2$ 说明如下： \text{trace}(A^\dagger A) = \text{trace}(V\Sigma^\dagger\Sigma V^\dagger)由于$V^\dagger = V^{-1}$，而且$\text{trace}(BAB^{-1}) = \text{trace}(A)$，所以， \text{trace}(A^\dagger A) = \text{trace}(\Sigma^\dagger \Sigma) = \sum_{i=1}^{r}\sigma_i^2也就是说，矩阵的F范数等于它的奇异值平方和的平方根。 \left\lVert A\right\rVert_F= \sqrt{\sum_{i=1}^{r}\sigma_i^2}]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[CS131-线代基础]]></title>
      <url>%2F2017%2F01%2F22%2Fcs131-linear-alg%2F</url>
      <content type="text"><![CDATA[CS131课程(Computer Vision: Foundations and Applications)，是斯坦福大学Li Feifei实验室开设的一门计算机视觉入门基础课程，该课程目的在于为刚接触计算机视觉领域的学生提供基本原理和应用介绍。目前2016年冬季课程刚刚结束。CS131博客系列主要是关于本课的slide知识点总结与作业重点问题归纳，作为个人学习本门课程的心得体会和复习材料。 由于是个人项目，所以会比较随意，只对个人感兴趣的内容做一总结。这篇文章是对课前线代基础的复习与整理。 向量与矩阵数字图像可以看做二维矩阵，向量是特殊的矩阵，本课程默认的向量都是列向量。slide中给出了一些矩阵行列式和迹的性质，都比较简单，这里不再多说。 矩阵作为线性变换通过线代知识，我们知道，在线性空间中，如果给定一组基，线性变换可以通过对应的矩阵来进行描述。 scale变换对角阵可以用来表示放缩变换。 \begin{bmatrix} s_x & 0\\\\ 0 & s_y \end{bmatrix}\begin{bmatrix} x\\\\ y \end{bmatrix} = \begin{bmatrix} s_xx\\\\ s_yy \end{bmatrix}旋转变换如图所示，逆时针旋转$theta$角度，对应的旋转矩阵为： \mathbf{R} = \begin{bmatrix} \cos\theta &-\sin\theta \\\\ \sin\theta &\cos\theta \end{bmatrix}旋转矩阵是酉矩阵，矩阵内的各列（或者各行）相互正交。满足如下的关系式： \mathbf{R}\mathbf{R^{\dagger}} = \mathbf{I}由于$\det{\mathbf{R}} = \det{\mathbf{R^{\dagger}}}$，所以，对于酉矩阵，$\det{\mathbf{R}} = \pm 1$旋转矩阵是酉矩阵，矩阵内的各列（或者各行）相互正交。满足如下的关系式： \mathbf{R}\mathbf{R^{\dagger}} = \mathbf{I}由于$\det{\mathbf{R}} = \det{\mathbf{R^{\dagger}}}$，所以，对于酉矩阵，$\det{\mathbf{R}} = \pm 1$. 齐次变换(Homogeneous Transform)只用上面的二维矩阵不能表达平移，使用齐次矩阵可以表达放缩，旋转和平移操作。 \mathbf{H} =\begin{bmatrix} a & b & t_x\\\\ c & d & t_y\\\\ 0 & 0 & 1 \end{bmatrix},\mathbf{H}\begin{bmatrix} x\\\\ y\\\\ 1\\\\ \end{bmatrix}=\begin{bmatrix} ax+by+t_x\\\\ cx+dy+t_y\\\\ 1 \end{bmatrix}SVD分解可以将矩阵分成若干个矩阵的乘积，叫做矩阵分解，比如QR分解，满秩分解等。SVD分解，即奇异值分解，也是一种特殊的矩阵分解方法。如下式所示，是将矩阵分解成为三个矩阵的乘积： \mathbf{U}\mathbf{\Sigma}\mathbf{V^\dagger} = \mathbf{A}其中矩阵$\mathbf{A}$大小为$m\times n$，矩阵$\mathbf{U}$是大小为$m\times m$的酉矩阵，$\mathbf{V}$是大小为$n \times n$的酉矩阵，$\mathbf{\Sigma}$是大小为$m \times n$的旋转矩阵，即只有主对角元素不为0. SVD分解在主成分分析中年很有用。由于矩阵$\mathbf{\Sigma}$一般情况下是将奇异值按照从大到小的顺序摆放，所以矩阵$\mathbf{U}$中，前面的若干列被视作主成分，后面的列显得相对不这么重要。可以抛弃后面的列，进行图像压缩。 如下图，是使用前10个分量对原图片进行压缩的效果。 12345678910im = imread('./superman.png');im_gray = rbg2gray(im);[u, s, v] = svd(double(im_gray));k = 10;uk = u(:, 1:k);sigma = diag(s);sk = diag(sigma(1:k));vk = v(:, 1:k);im_k = uk*sk*vk';imshow(uint8(im_k))]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[使用 Visual Studio 编译 GSL 科学计算库]]></title>
      <url>%2F2016%2F12%2F16%2Fgsl-with-vs%2F</url>
      <content type="text"><![CDATA[GSL是一个GNU支持的科学计算库，提供了很丰富的数值计算方法。本文介绍了GSL库在Windows环境下使用VisualStudio进行编译构建的过程。 GSL 的项目主页提供的说明来看，GSL支持如下的科学计算： （下面的这张表格的HTML使用的是No-Cruft Excel to HTML Table Converter生成的） Complex Numbers Roots of Polynomials Special Functions Vectors and Matrices Permutations Sorting BLAS Support Linear Algebra Eigensystems Fast Fourier Transforms Quadrature Random Numbers Quasi-Random Sequences Random Distributions Statistics Histograms N-Tuples Monte Carlo Integration Simulated Annealing Differential Equations Interpolation Numerical Differentiation Chebyshev Approximation Series Acceleration Discrete Hankel Transforms Root-Finding Minimization Least-Squares Fitting Physical Constants IEEE Floating-Point Discrete Wavelet Transforms Basis splines GSL的Linux下的配置很简单，照着它的INSTALL文件一步一步来就可以了。CMAKE大法HAO! 1234./configuremakemake installmake clean 同样的，GSL也可以在Windows环境下配置，下面记录了如何在Windows环境下使用 Visual Studio 和 CMakeGUI 编译测试GSL。 使用CMAKE编译成.SLN文件打开CMAKEGUI，将输入代码路径选为GSL源代码地址，输出路径设为自己想要的输出路径。点击 “Configure“，选择Visual Studio2013为编译器，点击Finish后会进行必要的配置。然后将表格里面的选项都打上勾，再次点击”Configure“，等待完成之后点击”Generate“。完成之后，就可以在输出路径下看到GSL.sln文件了。 使用Visual Studio生成解决方案使用 Visual Studio 打开刚才生成的.SLN文件，分别在Debug和Release模式下生成解决方案，等待完成即可。 当完成后，你应该可以在路径下看到这样一张图，我们主要关注的文件夹是\bin，\gsl，\Debug和\Release。 加入环境变量修改环境变量的Path，将\GSL_Build_Path\bin\Debug加入，这主要是为了\Debug文件夹下面的gsl.dll文件。如果不进行这一步的话，一会虽然可以编译，但是却不能运行。 这里顺便注释一句，当使用第三方库的时候，如果需要动态链接库的支持，其中一种方法就是将DLL文件的路径加入到Path中去。 建立Visual Studio属性表Visual Studio可以通过建立工程属性表的方法来配置工程选项，一个OpenCV的例子可以参见Yuanbo She的这篇博文 Opencv 完美配置攻略 2014 (Win8.1 + Opencv 2.4.8 + VS 2013)。 配置文件中主要是包含文件和静态链接库LIB的路径设置。下面把我的贴出来，只需要根据GSL的生成路径做相应修改即可。注意我的属性表中保留了OpenCV的内容，如果不需要的话，尽可以删掉。上面的博文对这张属性表如何配置讲得很清楚，有问题可以去参考。 12345678910111213141516171819&lt;?xml version="1.0" encoding="utf-8"?&gt;&lt;Project ToolsVersion="4.0" xmlns="http://schemas.microsoft.com/developer/msbuild/2003"&gt; &lt;ImportGroup Label="PropertySheets" /&gt; &lt;PropertyGroup Label="UserMacros" /&gt; &lt;PropertyGroup&gt; &lt;IncludePath&gt;$(OPENCV249)\include;E:\GSLCode\gsl-build\;$(IncludePath)&lt;/IncludePath&gt; &lt;LibraryPath Condition="'$(Platform)'=='Win32'"&gt;$(OPENCV249)\x86\vc12\lib;E:\GSLCode\gsl-build\Debug;$(LibraryPath)&lt;/LibraryPath&gt; &lt;LibraryPath Condition="'$(Platform)'=='X64'"&gt;$(OPENCV249)\x64\vc12\lib;E:\GSLCode\gsl-build\Debug;$(LibraryPath)&lt;/LibraryPath&gt; &lt;/PropertyGroup&gt; &lt;ItemDefinitionGroup&gt; &lt;Link Condition="'$(Configuration)'=='Debug'"&gt; &lt;AdditionalDependencies&gt;opencv_calib3d249d.lib;opencv_contrib249d.lib;opencv_core249d.lib;opencv_features2d249d.lib;opencv_flann249d.lib;opencv_gpu249d.lib;opencv_highgui249d.lib;opencv_imgproc249d.lib;opencv_legacy249d.lib;opencv_ml249d.lib;opencv_nonfree249d.lib;opencv_objdetect249d.lib;opencv_ocl249d.lib;opencv_photo249d.lib;opencv_stitching249d.lib;opencv_superres249d.lib;opencv_ts249d.lib;opencv_video249d.lib;opencv_videostab249d.lib;gsl.lib;gslcblas.lib;%(AdditionalDependencies)&lt;/AdditionalDependencies&gt; &lt;/Link&gt; &lt;Link Condition="'$(Configuration)'=='Release'"&gt; &lt;AdditionalDependencies&gt;opencv_calib3d249.lib;opencv_contrib249.lib;opencv_core249.lib;opencv_features2d249.lib;opencv_flann249.lib;opencv_gpu249.lib;opencv_highgui249.lib;opencv_imgproc249.lib;opencv_legacy249.lib;opencv_ml249.lib;opencv_nonfree249.lib;opencv_objdetect249.lib;opencv_ocl249.lib;opencv_photo249.lib;opencv_stitching249.lib;opencv_superres249.lib;opencv_ts249.lib;opencv_video249.lib;opencv_videostab249.lib;gsl.lib;gslcblas.lib;%(AdditionalDependencies)&lt;/AdditionalDependencies&gt; &lt;/Link&gt; &lt;/ItemDefinitionGroup&gt; &lt;ItemGroup /&gt;&lt;/Project&gt; 在以后建立Visual Studio工程的时候，在属性窗口直接添加现有属性表就可以了！ 测试在项目网站的教程上直接找到一段代码，进行测试，输出贝塞尔函数的值。 123456789#include &lt;stdio.h&gt;#include &lt;gsl/gsl_sf_bessel.h&gt;int main(void)&#123; double x = 5.0; double y = gsl_sf_bessel_J0(x); printf("J0(%g) = %.18e\n", x, y); return 0;&#125; 控制台输出正确：]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Hello World]]></title>
      <url>%2F2016%2F12%2F16%2Fhello-world%2F</url>
      <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment Code highlightHello World! 1234#include &lt;iostream&gt;int main() &#123; std::cout &lt;&lt; "HelloWorld\n";&#125; 1print 'HelloWorld' Latex Support by MathjaxMass-energy equation by Einstein: $E = mc^2$ a linear equation: $$\mathbf{A}\mathbf{v} = \mathbf{y}$$]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Windows环境下使用Doxygen生成注释文档]]></title>
      <url>%2F2016%2F12%2F16%2Fuse-doxygen%2F</url>
      <content type="text"><![CDATA[Doxygen 是一种很好用的代码注释生成工具，然而和很多国外的工具软件一样，在中文环境下，它的使用总是会出现一些问题，也就是中文注释文档出现乱码。经过调试，终于是解决了这个问题。 安装 DoxygenDoxygen 在Windows平台下的安装比较简单，Doxygen的项目主页提供了下载和安装的使用说明，可以下载它们的官方使用手册进行阅读。对于Windows，提供了源代码编译安装和直接安装程序安装两种方式，可以自行选择。 安装成功后，使用命令行命令 1doxygen --help 就可以查看帮助文档，对应参数含义一目了然，降低了入手难度。 使用命令， 1doxygen -g doxygen_filename 就可以在当前目录下建立一个doxygen配置文件，用文本编辑器打开就可以编辑里面的配置选项。 使用命令， 1doxygen doxygen_filename 就可以生成注释文档了。 下面就来说一说对中文的支持。 生成 HTML 格式文档中文之所以乱码，很多时候是由于编码和译码格式不同，所以我们需要先知道自己代码文件的编码方式。我的代码都是建立在Visual Studio上的，可以通过VS的高级保存选项查看自己代码文件的存储编码格式。对于中文版的VS，一般应该是GB2312。 我们打开 Doxygen 的配置文件，将里面的 INPUT_ENCODING 改为我们代码文件的编码格式，这里就改成 GB2312。 这样一来，编译出来的 HTML 页面就不会有中文乱码了。 生成Latex 格式文档生成 Latex 需要本机上安装有 Latex 的编译环境。如果是中文用户，推荐的是CTEX套件，可以到他们的网站上去下载。 可以看到，Doxygen为Latex文件的编译生成了make文件，我们在命令行窗口中执行make命令就可以完成编译，然而这时候会发现编译出错，pdf文档无法生成。 打开生成的refman.latex文档，添加宏包 CJKutf8。然后找到 \begin{document}一行，将其改为 12\begin&#123;document&#125;\begin&#123;CJK&#125;&#123;UTF8&#125;&#123;gbsn&#125; 也就是说为正文提供了CJK环境，这样中文文本就可以正常编译了。 相应的，我们要将结尾的 \end{document)改为：12\end&#123;CJK&#125;\end&#123;document&#125; 这样，运行make命令之后，就可以看到中文的注释文档了。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Python Regular Expressions （Python 正则表达式)]]></title>
      <url>%2F2014%2F07%2F17%2Fpython-reg-exp%2F</url>
      <content type="text"><![CDATA[本文来自于Google Developers中对于Python的介绍。https://developers.google.com/edu/python/regular-expressions。 认识正则表达式Python的正则表达式是使用 re 模块的。 12345match = re.search(pattern,str)if match: print 'found',match.group()else: print 'NOT Found!' 正则表达式的规则基本规则 a, x, 9 都是普通字符 (ordinary characters) . (一个点)可以匹配任何单个字符（除了’\n’） \w（小写的w）可以匹配一个单词里面的字母，数字或下划线 [a-zA-Z0-9_];\W （大写的W）可以匹配非单词里的这些元素 \b 匹配单词与非单词的分界 \s（小写的s）匹配一个 whitespace character，包括 space，newline，return，tab，form(\n\r\t\f)；\S（大写的S）匹配一个非 whitespace character \d 匹配十进制数字 [0-9] ^=start，$=end 用来匹配字符串的开始和结束 \ 是转义字符，用 . 来匹配串里的’.’，等一些基本的例子 12345678910## 在字符串'piiig'中查找'iii'match = re.search(r'iii', 'piiig') # found, match.group() == "iii"match = re.search(r'igs', 'piiig') # not found, match == None## . 匹配除了\n的任意字符match = re.search(r'..g', 'piiig') # found, match.group() == "iig"## \d 匹配0-9的数字字符, \w 匹配单词里的字符match = re.search(r'\d\d\d', 'p123g') # found, match.group() == "123"match = re.search(r'\w\w\w', '@@abcd!!') # found, match.group() == "abc" 重复可以用’+’ ‘*’ ‘?’来匹配0个，1个或多个重复字符。 ‘+’ 用来匹配1个或者多个字符 ‘*’ 用来匹配0个或者多个字符 ‘?’ 用来匹配0个或1个字符 注意，’+’和’*’会匹配尽可能多的字符。 一些重复字符的例子12345678910111213141516## i+ 匹配1个或者多个'i'match = re.search(r'pi+', 'piiig') # found, match.group() == "piii"## 找到字符串中最左边尽可能长的模式。## 注意，并没有匹配到第二个 'i+'match = re.search(r'i+', 'piigiiii') # found, match.group() == "ii"## \s* 匹配0个或1个空白字符 whitespacematch = re.search(r'\d\s*\d\s*\d', 'xx1 2 3xx') # found, match.group() == "1 2 3"match = re.search(r'\d\s*\d\s*\d', 'xx12 3xx') # found, match.group() == "12 3"match = re.search(r'\d\s*\d\s*\d', 'xx123xx') # found, match.group() == "123"## ^ 匹配字符串的第一个字符match = re.search(r'^b\w+', 'foobar') # not found, match == None## 与上例对比match = re.search(r'b\w+', 'foobar') # found, match.group() == "bar" Email考虑一个典型的Email地址：someone@host.com，可以用如下的方式匹配： 12345678910 match = re.search(r'\w+@\w+',str)``` 但是，对于这种Email地址 'xyz alice-b@google.com purple monkey' 则不能奏效。### 使用方括号 ###方括号里面的字符表示一个字符集合。[abc]可以被用来匹配'a'或者'b'或者'c'。\w \s等都可以用在方括号里，除了'.'以外，它只能用来表示字面意义上的‘点’。所以上面的Email规则可以扩充如下：``` py match = re.search('r[\w.-]+@[\w.-]+',str) 你还可以使用’-‘来指定范围，如[a-z]指示的是所有小写字母的集合。所以如果你想构造的字符集合中有’-‘，请把它放到末尾[ab-]。另外，前方加上’^’，用来表示取集合的补集，例如ab表示除了’a’和’b’之外的其他字符。 操作以Email地址为例，如果我们想要分别提取该地址的用户名’someone’和主机名’host.com’该怎么办呢？可以在模式中用圆括号指定。 123456str = 'purple alice-b@google.com monkey dishwasher'match = re.search('([\w.-]+)@([\w.-]+)', str) #用圆括号指定分割if match: print match.group() ## 'alice-b@google.com' (the whole match) print match.group(1) ## 'alice-b' (the username, group 1) print match.group(2) ## 'google.com' (the host, group 2) findall 函数与group函数只找到最左端的一个匹配不同，findall函数找到字符串中所有与模式匹配的串。 12345str = 'purple alice@google.com, blah monkey bob@abc.com blah dishwasher'## findall返回一个包含所有匹配结果的 listemails = re.findall(r'[\w\.-]+@[\w\.-]+', str) ## ['alice@google.com', 'bob@abc.com']for email in emails: print email 在文件中使用findall当然可以读入文件的每一行，然后对每一行的内容调用findall，但是为什么不让这一切自动发生呢？ 12f = open(filename.txt,'r')matches = re.findall(pattern,f.read()) findall 和分组和group的用法相似，也可以指定分组。 12345678str = 'purple alice@google.com, blah monkey bob@abc.com blah dishwasher'## 返回了一个listtuples = re.findall(r'([\w\.-]+)@([\w\.-]+)', str)print tuples ## [('alice', 'google.com'), ('bob', 'abc.com')]## list中的元素是tuplefor tuple in tuples: print tuple[0] ## username print tuple[1] ## host 调试正则表达式异常强大，使用简单的几条规则就可以演变出很多的模式组合。在确定你的模式之前，可能需要很多的调试工作。在一个小的测试集合上测试正则表达式。 其他选项正则表达式还可以设置“选项”。 1match = re.search(pat,str,opt) 这些可选项如下： IGNORECASE 忽视大小写 DOTALL 允许’.’匹配’\n’ MULTILINE 在一个由许多行组成的字符串中，允许’^’和’$’匹配每一行的开始和结束]]></content>
    </entry>

    
  
  
</search>
