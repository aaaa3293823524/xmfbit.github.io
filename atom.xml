<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>来呀，快活呀~</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://xmfbit.github.io/"/>
  <updated>2018-03-24T06:05:29.436Z</updated>
  <id>https://xmfbit.github.io/</id>
  
  <author>
    <name>一个脱离了高级趣味的人</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>论文 - SqueezeNet, AlexNet-level accuracy with 50x fewer parameters and &lt;0.5MB model size</title>
    <link href="https://xmfbit.github.io/2018/03/24/paper-squeezenet/"/>
    <id>https://xmfbit.github.io/2018/03/24/paper-squeezenet/</id>
    <published>2018-03-24T06:02:53.000Z</published>
    <updated>2018-03-24T06:05:29.436Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://arxiv.org/abs/1602.07360" target="_blank" rel="external">SqueezeNet</a>由HanSong等人提出，和AlexNet相比，用少于$50$倍的参数量，在ImageNet上实现了comparable的accuracy。比较本文和HanSoing其他的工作，可以看出，其他工作，如Deep Compression是对已有的网络进行压缩，减小模型size；而SqueezeNet是从网络设计入手，从设计之初就考虑如何使用较少的参数实现较好的性能。可以说是模型压缩的两个不同思路。</p><a id="more"></a><h2 id="模型压缩相关工作"><a href="#模型压缩相关工作" class="headerlink" title="模型压缩相关工作"></a>模型压缩相关工作</h2><p>模型压缩的好处主要有以下几点：</p><ul><li>更好的分布式训练。server之间的通信往往限制了分布式训练的提速比例，较少的网络参数能够降低对server间通信需求。</li><li>云端向终端的部署，需要更低的带宽，例如手机app更新或无人车的软件包更新。</li><li>更易于在FPGA等硬件上部署，因为它们往往都有着非常受限的片上RAM。</li></ul><p>相关工作主要有两个方向，即模型压缩和模型结构自身探索。</p><p>模型压缩方面的工作主要有，使用SVD分解，Deep Compression等。模型结构方面比较有意义的工作是GoogLeNet的Inception module（可在博客内搜索<em>Xception</em>发现Xception的作者是如何受此启发发明Xception结构的）。</p><p>本文的作者从网络设计角度出发，提出了名为SqueezeNet的网络结构，使用比AlexNet少$50$倍的参数，在ImageNet上取得了comparable的结果。此外，还探究了CNN的arch是如何影响model size和最终的accuracy的。主要从两个方面进行了探索，分别是<em>CNN microarch</em>和<em>CNN macroarch</em>。前者意为在更小的粒度上，如每一层的layer怎么设计，来考察；后者是在更为宏观的角度，如一个CNN中的不同layer该如何组织来考察。</p><h2 id="SqueezeNet"><a href="#SqueezeNet" class="headerlink" title="SqueezeNet"></a>SqueezeNet</h2><p>为了简单，下文简称<em>SNet</em>。SNet的基本组成是叫做<em>Fire</em>的module。我们知道，对于一个CONV layer，它的参数数量计算应该是：$K \time K \times M \times N$。其中，$K$是filter的spatial size，$M$和$N$分别是输入feature map和输出activation的channel size。由此，设计SNet时，作者的依据主要是以下几点：</p><ul><li>把$3\times 3$的卷积替换成$1\times 1$，相当于减小上式中的$K$。</li><li>减少$3\times 3$filter对应的输入feature map的channel，相当于减少上式的$M$。</li><li>delayed downsample。使得activation的feature map能够足够大，这样对提高accuracy有益。CNN中的downsample主要是通过CONV layer或pooling layer中stride设置大于$1$得到的，作者指出，应将这种操作尽量后移。</li></ul><blockquote><p>Our intuition is that large activation maps (due to delayed downsampling) can lead to higher classification accuracy, with all else held equal.</p></blockquote><h3 id="Fire-Module"><a href="#Fire-Module" class="headerlink" title="Fire Module"></a>Fire Module</h3><p>Fire Module是SNet的基本组成单元，如下图所示。可以分为两个部分，一个是上面的<em>squeeze</em>部分，是一组$1\times 1$的卷积，用来将输入的channel squeeze到一个较小的值。后面是<em>expand</em>部分，由$1\times 1$和$3\times 3$卷积mix起来。使用$s<em>{1 x 1}$，$e</em>{1x1}$和$e<em>{3x3}$表示squeeze和expand中两种不同卷积的channel数量，令$s</em>{1x1} &lt; e<em>{1x1} + e</em>{3x3}$，用来实现上述策略2.<br><img src="/img/paper-squeezenet-fire-module.png" alt="Fire Module示意"></p><p>下面，对照PyTorch实现的SNet代码看下Fire的实现，注意上面说的CONV后面都接了ReLU。<br><figure class="highlight py"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Fire</span><span class="params">(nn.Module)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, inplanes, squeeze_planes,</span></span></div><div class="line">                 expand1x1_planes, expand3x3_planes):</div><div class="line">        super(Fire, self).__init__()</div><div class="line">        self.inplanes = inplanes</div><div class="line">        <span class="comment">## squeeze 部分</span></div><div class="line">        self.squeeze = nn.Conv2d(inplanes, squeeze_planes, kernel_size=<span class="number">1</span>)</div><div class="line">        self.squeeze_activation = nn.ReLU(inplace=<span class="keyword">True</span>)</div><div class="line">        <span class="comment">## expand 1x1 部分</span></div><div class="line">        self.expand1x1 = nn.Conv2d(squeeze_planes, expand1x1_planes,</div><div class="line">                                   kernel_size=<span class="number">1</span>)</div><div class="line">        self.expand1x1_activation = nn.ReLU(inplace=<span class="keyword">True</span>)</div><div class="line">        <span class="comment">## expand 3x3部分</span></div><div class="line">        self.expand3x3 = nn.Conv2d(squeeze_planes, expand3x3_planes,</div><div class="line">                                   kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</div><div class="line">        self.expand3x3_activation = nn.ReLU(inplace=<span class="keyword">True</span>)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></div><div class="line">        x = self.squeeze_activation(self.squeeze(x))</div><div class="line">        <span class="comment">## 将expand 部分1x1和3x3的cat到一起</span></div><div class="line">        <span class="keyword">return</span> torch.cat([</div><div class="line">            self.expand1x1_activation(self.expand1x1(x)),</div><div class="line">            self.expand3x3_activation(self.expand3x3(x))], <span class="number">1</span>)</div></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1602.07360&quot;&gt;SqueezeNet&lt;/a&gt;由HanSong等人提出，和AlexNet相比，用少于$50$倍的参数量，在ImageNet上实现了comparable的accuracy。比较本文和HanSoing其他的工作，可以看出，其他工作，如Deep Compression是对已有的网络进行压缩，减小模型size；而SqueezeNet是从网络设计入手，从设计之初就考虑如何使用较少的参数实现较好的性能。可以说是模型压缩的两个不同思路。&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>论文 - MobileNets, Efficient Convolutional Neural Networks for Mobile Vision Applications</title>
    <link href="https://xmfbit.github.io/2018/03/23/paper-mobilenet/"/>
    <id>https://xmfbit.github.io/2018/03/23/paper-mobilenet/</id>
    <published>2018-03-23T02:53:43.000Z</published>
    <updated>2018-03-24T06:06:39.578Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://arxiv.org/abs/1704.04861" target="_blank" rel="external">MobileNet</a>是建立在Depthwise Separable Conv基础之上的一个轻量级网络。在本论文中，作者定量计算了使用这一技术带来的计算量节省，提出了MobileNet的结构，同时提出了两个简单的超参数，可以灵活地进行模型性能和inference时间的折中。后续改进的<a href="https://arxiv.org/abs/1801.04381" target="_blank" rel="external">MobileNet v2</a>以后讨论。<br><a id="more"></a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1704.04861&quot;&gt;MobileNet&lt;/a&gt;是建立在Depthwise Separable Conv基础之上的一个轻量级网络。在本论文中，作者定量计算了使用这一技术带来的计算量节省，提出了MobileNet的结构，同时提出了两个简单的超参数，可以灵活地进行模型性能和inference时间的折中。后续改进的&lt;a href=&quot;https://arxiv.org/abs/1801.04381&quot;&gt;MobileNet v2&lt;/a&gt;以后讨论。&lt;br&gt;
    
    </summary>
    
    
      <category term="deep learning" scheme="https://xmfbit.github.io/tags/deep-learning/"/>
    
      <category term="paper" scheme="https://xmfbit.github.io/tags/paper/"/>
    
  </entry>
  
  <entry>
    <title>论文 - Xception, Deep Learning with Depthwise separable Convolution</title>
    <link href="https://xmfbit.github.io/2018/03/22/paper-xception/"/>
    <id>https://xmfbit.github.io/2018/03/22/paper-xception/</id>
    <published>2018-03-22T01:44:38.000Z</published>
    <updated>2018-03-24T06:06:52.599Z</updated>
    
    <content type="html"><![CDATA[<p>在MobileNet, ShuffleMet等轻量级网络中，<strong>depthwise separable conv</strong>是一个很流行的设计。借助<a href="https://arxiv.org/abs/1610.02357" target="_blank" rel="external">Xception: Deep Learning with Depthwise separable Convolution</a>，对这种分解卷积的思路做一个总结。<br><a id="more"></a></p><h2 id="起源"><a href="#起源" class="headerlink" title="起源"></a>起源</h2><p>自从AlexNet以来，DNN的网络设计经过了ZFNet-&gt;VGGNet-&gt;GoogLeNet-&gt;ResNet等几个发展阶段。本文作者的思路正是受GoogLeNet中Inception结构启发。Inception结构是最早有别于VGG等“直筒型”结构的网络module。以Inception V3为例，一个典型的Inception模块长下面这个样子：<br><img src="/img/paper-xception-inception-module.png" alt="一个典型的Inception结构"></p><p>对于一个CONV层来说，它要学习的是一个$3D$的filter，包括两个空间维度（spatial dimension），即width和height；以及一个channel dimension。这个filter和输入在$3$个维度上进行卷积操作，得到最终的输出。可以用伪代码表示如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">// 对于第i个filter</div><div class="line">// 计算输入中心点(x, y)对应的卷积结果</div><div class="line">sum = 0</div><div class="line">for c in 1:C</div><div class="line">  for h in 1:K</div><div class="line">    for w in 1:K</div><div class="line">      sum += in[c, y-K/2+h, x-K/2+w] * filter_i[c, h, w]</div><div class="line">out[i, y, x] = sum</div></pre></td></tr></table></figure></p><p>可以看到，在$3D$卷积中，channel这个维度和spatial的两个维度并无不同。</p><p>在Inception中，卷积操作更加轻量级。输入首先被$1\times 1$的卷积核处理，得到了跨channel的组合(cross-channel correlation)，同时将输入的channel dimension减少了$3\sim 4$倍（一会$4$个支路要做<code>concat</code>操作）。这个结果被后续的$3\times 3$卷积和$5\times 5$卷积核处理，处理方法和普通的卷积一样，见上。</p><p>由此作者想到，Inception能够work证明后面的一条假设就是：卷积的channel相关性和spatial相关性是可以解耦的，我们没必要要把它们一起完成。</p><h2 id="简化Inception，提取主要矛盾"><a href="#简化Inception，提取主要矛盾" class="headerlink" title="简化Inception，提取主要矛盾"></a>简化Inception，提取主要矛盾</h2><p>接着，为了更好地分析问题，作者将Inception结构做了简化，保留了主要结构，去掉了AVE Pooling操作，如下所示。<br><img src="/img/paper-xception-simplified-inception-module.png" alt="简化后的Inception"></p><p>好的，我们现在将底层的$3$个$1\times 1$的卷积核组合起来，其实上面的图和下图是等价的。一个“大的”$1\times 1$的卷积核（channels数目变多），它的输出结果在channel上被分为若干组（group），每组分别和不同的$3\times 3$卷积核做卷积，再将这$3$份输出拼接起来，得到最后的输出。<br><img src="/img/paper-xception-equivalent-inception-module.png" alt="另一种形式"></p><p>那么，如果我们把分组数目继续调大呢？极限情况，我们可以使得group number = channel number，如下所示：<br><img src="/img/paper-xception-extreme-version.png" alt="极限模式"></p><h2 id="Depthwise-Separable-Conv"><a href="#Depthwise-Separable-Conv" class="headerlink" title="Depthwise Separable Conv"></a>Depthwise Separable Conv</h2><p>这种结构和一种名为<strong>depthwise separable conv</strong>的技术很相似，即首先使用group conv在spatial dimension上卷积，然后使用$1\times 1$的卷积核做cross channel的卷积（又叫做<em>pointwise conv</em>）。主要有两点不同：</p><ul><li>操作的顺序。在TensorFlow等框架中，depthwise separable conv的实现是先使用channelwise的filter只在spatial dimension上做卷积，再使用$1\times 1$的卷积核做跨channel的融合。而Inception中先使用$1\times 1$的卷积核。</li><li>非线性变换的缺席。在Inception中，每个conv操作后面都有ReLU的非线性变换，而depthwise separable conv没有。</li></ul><p>第一点不同不太重要，尤其是在深层网络中，这些block都是堆叠在一起的。第二点论文后面通过实验进行了比较。可以看出，去掉中间的非线性激活，能够取得更好的结果。<br><img src="/img/paper-xception-experiment-intermediate-activation.png" alt="非线性激活的影响"></p><h2 id="Xception网络架构"><a href="#Xception网络架构" class="headerlink" title="Xception网络架构"></a>Xception网络架构</h2><p>基于上面的分析，作者认为这样的假设是合理的：cross channel的相关和spatial的相关可以<strong>完全</strong>解耦。</p><blockquote><p>we make the following hypothesis: that the mapping of cross-channels correlations and spatial correlations in the feature maps of convolutional neural networks can be <em>entirely</em> decoupled. </p></blockquote><p>Xception的结构基于ResNet，但是将其中的卷积层换成了depthwise separable conv。如下图所示。整个网络被分为了三个部分：Entry，Middle和Exit。</p><blockquote><p>The Xception architecture: the data first goes through the entry flow, then through the middle flow which is repeated eight times, and finally through the exit flow. Note that all Convolution and SeparableConvolution layers are followed by batch normalization [7] (not included in the diagram). All SeparableConvolution layers use a depth multiplier of 1 (no depth expansion).</p></blockquote><p><img src="/img/paper-xception-arch.png" alt="Xception的网络结构"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在MobileNet, ShuffleMet等轻量级网络中，&lt;strong&gt;depthwise separable conv&lt;/strong&gt;是一个很流行的设计。借助&lt;a href=&quot;https://arxiv.org/abs/1610.02357&quot;&gt;Xception: Deep Learning with Depthwise separable Convolution&lt;/a&gt;，对这种分解卷积的思路做一个总结。&lt;br&gt;
    
    </summary>
    
    
      <category term="deep learning" scheme="https://xmfbit.github.io/tags/deep-learning/"/>
    
      <category term="paper" scheme="https://xmfbit.github.io/tags/paper/"/>
    
  </entry>
  
  <entry>
    <title>CS229 简单的监督学习方法</title>
    <link href="https://xmfbit.github.io/2018/03/21/cs229-supervised-learning/"/>
    <id>https://xmfbit.github.io/2018/03/21/cs229-supervised-learning/</id>
    <published>2018-03-21T03:08:14.000Z</published>
    <updated>2018-03-21T13:25:02.494Z</updated>
    
    <content type="html"><![CDATA[<p>回过头去复习一下基础的监督学习算法，主要包括最小二乘法和logistic回归。<br><a id="more"></a></p><h2 id="最小二乘法"><a href="#最小二乘法" class="headerlink" title="最小二乘法"></a>最小二乘法</h2><p>最小二乘法是一个线性模型，即：</p><script type="math/tex; mode=display">\hat{y} = h_\theta(x) = \sum_{i=1}^{m}\theta_i x_i = \theta^T x</script><p>定义损失函数为Mean Square Error(MSE)，如下所示。其中，不戴帽子的$y$表示给定的ground truth。</p><script type="math/tex; mode=display">J(\theta) = \frac{1}{2}(\hat{y}-y)^2</script><p>那么，最小二乘就是要找到这样的参数$\theta^*$，使得：</p><script type="math/tex; mode=display">\theta^* = \arg\min J(\theta)</script><h3 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h3><p>使用梯度下降方法求解上述优化问题，我们有：</p><script type="math/tex; mode=display">\theta_{i+1} = \theta_{i} - \alpha \nabla_\theta J(\theta)</script><p>求导，有：</p><script type="math/tex; mode=display">\begin{aligned}\nabla_\theta J(\theta) &= \frac{1}{2}\nabla_\theta (\theta^T x - y)^2 \\&= (\theta^T x - y) x\end{aligned}</script><p>由于这里的损失函数是一个凸函数，所以梯度下降方法能够保证到达全局的极值点。</p><p>上面的梯度下降只是对单个样本来做的。实际上，我们可以取整个训练集或者训练集的一部分，计算平均损失函数$J(\theta) = \frac{1}{N}\sum_{i=1}^{N}J_i(\theta)$，做梯度下降，道理是一样的，只不过相差了常数因子$\frac{1}{N}$。</p><h3 id="正则方程"><a href="#正则方程" class="headerlink" title="正则方程"></a>正则方程</h3><p>除了梯度下降方法之外，上述问题还存在着解析解。我们将所有的样本输入$x^{(i)}$作为行向量，构成矩阵$X \in \mathbb{R}^{N\times d}$。其中，$N$为样本总数，$d$为单个样本的特征个数。那么，对于参数$\theta\in\mathbb{R}^{d\times 1}$来说，$X\theta$的第$i$行就可以给出模型对第$i$个样本的预测结果。我们将ground truth排成一个$N\times 1$的矩阵，那么，损失函数可以写作：</p><script type="math/tex; mode=display">J(\theta) = \frac{1}{2N} \Vert X\theta-y \Vert_2^2</script><p>将$\Vert x\Vert_2^2$写作$x^T x$，同时略去常数项，我们有：</p><script type="math/tex; mode=display">\begin{aligned}J &= (X\theta - y)^T (X\theta - y) \\&= \theta^T X^T X\theta - 2\theta^T x^T y +y^T y\end{aligned}</script><p>对其求导，有：</p><script type="math/tex; mode=display">\nabla_\theta J = X^T X\theta - X^T y</script><p><img src="/img/cs229-supervised-learning-least-square-normal-equation.png" alt="具体计算过程贴图"></p><p>这其中，主要用到的矩阵求导性质如下：<br><img src="/img/cs229-supervised-learning-some-useful-matrix-derivatives.png" alt="一些典型求导结果"><br>令导数为$0$，求得极值点处：</p><script type="math/tex; mode=display">\theta^* = (X^TX)^{-1}X^T y</script><h3 id="概率解释"><a href="#概率解释" class="headerlink" title="概率解释"></a>概率解释</h3><p>这里对上述做法给出一个概率论上的解释。首先我们要引入似然函数（likelihood function）的概念。</p><p>似然函数是一个关于模型参数$\theta$的函数，它描述了某个参数$\theta$下，给出输入$x$，得到输出$y$的概率。用具体的公式表示如下：</p><script type="math/tex; mode=display">L(\theta) = \prod_{i=1}^{N}P(y^{(i)}|x^{(i)};\theta)</script><p>假设线性模型的预测结果和ground truth之间的误差服从Gaussian分布，也就是说，</p><script type="math/tex; mode=display">y - \theta^T x  =  \epsilon \sim \mathcal{N}(0, \sigma^2)</script><p>那么上面的似然函数可以写作：</p><script type="math/tex; mode=display">L(\theta) = \prod_{i=1}^{N}\frac{1}{\sqrt{2\pi\sigma}}\exp(\frac{(y^{(i)}-\theta^T x^{(i)})^2}{2\sigma^2})</script><p>如何估计参数$\theta$呢？我们可以认为，参数$\theta$使得出现样本点$(x^{(i)}, y^{(i)})$的概率变大，所以才能被我们观测到。自然，我们需要使得似然函数$L(\theta)$取得极大值，也就是说：</p><script type="math/tex; mode=display">\theta^* = \arg\max L(\theta)</script><p>通过引入$\log(\cdot)$，可以将连乘变成连加，同时不改变函数的单调性。这样，实际上我们操作的是对数似然函数$\log L(\theta)$。有：</p><script type="math/tex; mode=display">\begin{aligned} \mathcal{l} &= \log L(\theta) \\&= \sum_{i=1}^{N}\log \frac{1}{\sqrt{2\pi\sigma^2}} \exp (\frac{(y^{(i)}-\theta^T x^{(i)})^2}{2\sigma^2})\\&= N\log\frac{1}{\sqrt{2\pi\sigma^2}} -\frac{1}{\sigma^2}\frac{1}{2}\sum_{i=1}^{N}(y^{(i)}-\theta^T x^{(i)})^2 \end{aligned}</script><p>略去前面的常数项不管，后面一项正好是最小二乘法的损失函数。要想最大化对数似然函数，也就是要最小化上面的损失函数。</p><p>所以，最小二乘法的损失函数可以由数据集的噪声服从Gaussian分布自然地导出。</p><h3 id="加权最小二乘法"><a href="#加权最小二乘法" class="headerlink" title="加权最小二乘法"></a>加权最小二乘法</h3><p>加权最小二乘法是指对数据集中的数据赋予不同的权重，一个重要的用途是使用权重$w^{(i)} = \exp (-\frac{(x^{(i)}-x)^2}{2\tau^2})$做局部最小二乘。不再多说。</p><h2 id="logistic回归"><a href="#logistic回归" class="headerlink" title="logistic回归"></a>logistic回归</h2><p>虽然叫回归，但是logistic回归解决的问题是分类问题。</p><h3 id="logistic函数"><a href="#logistic函数" class="headerlink" title="logistic函数"></a>logistic函数</h3><p>logistic函数$\sigma(x) = \frac{1}{1+e^{-x}}$，又叫sigmoid函数，将输入$(-\infty, +\infty)$压缩到$(0, 1)$之间。它的形状如下：<br><img src="/img/cs229-supervised-learning-sigmoid.png" alt="sigmoid函数"></p><p>对其求导，发现导数值可以完全不依赖于输入$x$：</p><script type="math/tex; mode=display">\frac{d\sigma(x)} {dx} = \sigma(x)(1-\sigma(x))</script><p>我们将logistic函数的输入取做$x$的feature的线性组合，就得到了假设函数$h_\theta(x) = \sigma(\theta^T x)$。</p><h3 id="logistic回归-1"><a href="#logistic回归-1" class="headerlink" title="logistic回归"></a>logistic回归</h3><p>logistic函数的输出既然是在$(0,1)$上，我们可以将其作为概率。也就是说，我们认为它的输出是样本点属于类别$1$的概率：</p><script type="math/tex; mode=display">\begin{aligned}P(y=1|x) &= h_\theta(x) \\P(y=0|x) &= 1-h_\theta(x) \end{aligned}</script><p>或者我们写的更紧凑些：</p><script type="math/tex; mode=display">P(y|x) = (h_\theta(x))^y (1-h_\theta(x))^(1-y)</script><p>我们仍然使用上述极大似然的估计方法，求取参数$\theta$，为求简练，隐去了上标$(i)$。</p><script type="math/tex; mode=display">\begin{aligned}L(\theta) &= \prod_{i=1}^{N}P(y|x;\theta) \\&=\prod (h_\theta(x))^y (1-h_\theta(x))^(1-y) \end{aligned}</script><p>取对数：</p><script type="math/tex; mode=display">\log L(\theta) = \sum_{i=1}^{N}y\log(h(x)) + (1-y)\log(1-h(x))</script><p>所以，我们的损失函数为$J(\theta) = - [y\log(h(x)) + (1-y)\log(1-h(x))]$。把$h(x)$换成$P$，岂不就是深度学习中常用的交叉损失熵在二分类下的特殊情况？</p><p>回到logistic回归，使用梯度下降，我们可以得到更新参数的策略：</p><script type="math/tex; mode=display">\theta_{i+1} = \theta_i - \alpha (h_\theta(x) - y)x</script><p>啊哈！形式和最小二乘法完全一样。只不过要注意，现在的$h_\theta(x)$已经变成了一个非线性函数。</p><h3 id="感知机"><a href="#感知机" class="headerlink" title="感知机"></a>感知机</h3><p>在上述logistic回归基础上，我们强制将其输出映射到$\lbrace 1, -1\rbrace$。即将$\sigma(x)$换成$g(x)$：</p><script type="math/tex; mode=display">g(x) = \begin{cases} 1, \quad\text{if}\quad x \ge 0\\ 0, \quad\text{if}\quad x < 0\end{cases}</script><p>使用同样的更新方法，我们就得到了感知机模型（perceptron machine）。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;回过头去复习一下基础的监督学习算法，主要包括最小二乘法和logistic回归。&lt;br&gt;
    
    </summary>
    
    
      <category term="公开课" scheme="https://xmfbit.github.io/tags/%E5%85%AC%E5%BC%80%E8%AF%BE/"/>
    
      <category term="cs229" scheme="https://xmfbit.github.io/tags/cs229/"/>
    
  </entry>
  
  <entry>
    <title>Hack PyCaffe</title>
    <link href="https://xmfbit.github.io/2018/03/16/caffe-hack-python-interface/"/>
    <id>https://xmfbit.github.io/2018/03/16/caffe-hack-python-interface/</id>
    <published>2018-03-16T11:01:32.000Z</published>
    <updated>2018-03-16T12:37:52.000Z</updated>
    
    <content type="html"><![CDATA[<p>这篇文章主要是<a href="https://github.com/nitnelave/pycaffe_tutorial/blob/master/04%20Hacking%20the%20Python%20API.ipynb" target="_blank" rel="external">Github: PyCaffe Tutorial</a>中Hack Pycaffe的翻译整理。后续可能会加上一些使用boost和C++为Python接口提供后端的解释。这里主要讨论如何为Pycaffe添加自己想要的功能。至于Pycaffe的使用，留待以后的文章整理。<br><img src="/img/caffe-hack-pycaffe-python-cpp-binding.jpg" alt="Python&amp;&amp;CPP binding"><br><a id="more"></a></p><h2 id="PyCaffe的代码组织结构"><a href="#PyCaffe的代码组织结构" class="headerlink" title="PyCaffe的代码组织结构"></a>PyCaffe的代码组织结构</h2><p>见Caffe的<code>python</code>目录。下面这张图是与PyCaffe相关的代码的分布。其中<code>src</code>和<code>include</code>是Caffe框架的后端C++实现，<code>python</code>目录中是与PyCaffe关系更密切的代码。可以看到，除了<code>_caffe.cpp</code>以外，其他都是纯python代码。<code>_caffe.cpp</code>使用boost提供了C++与python的绑定，而其他python脚本在此层的抽象隔离之上，继续完善了相关功能，提供了更加丰富的API、<br><img src="/img/hack-pycaffe-code-organization.png" alt="代码组织结构"></p><h2 id="添加纯Python功能"><a href="#添加纯Python功能" class="headerlink" title="添加纯Python功能"></a>添加纯Python功能</h2><p>首先，我们介绍如何在C++构建的PyCaffe隔离之上，用纯python实现想要的功能。</p><h3 id="添加的功能和PyCaffe基本平行，不需要改变已有代码"><a href="#添加的功能和PyCaffe基本平行，不需要改变已有代码" class="headerlink" title="添加的功能和PyCaffe基本平行，不需要改变已有代码"></a>添加的功能和PyCaffe基本平行，不需要改变已有代码</h3><p>有的时候想加入的功能和PyCaffe的关系基本是平行的，比如想仿照<code>PyTorch</code>等框架，加入对数据进行预处理的<code>Transformer</code>功能（这个API其实已经在PyCaffe中实现了，这里只是举个例子）。为了实现这个功能，我们可能需要使用<code>numpy</code>和<code>opencv</code>等包装图像的预处理操作，但是和Caffe本身基本没什么关系。在这样的情况下，我们直接编写即可。要注意在<code>python/caffe/__init__.py</code>中import相关的子模块或函数。这个例子可以参考<code>caffe.io</code>的实现（见<code>python/caffe/io.py</code>文件）。</p><h3 id="添加的功能需要Caffe的支持，向已有的类中添加函数"><a href="#添加的功能需要Caffe的支持，向已有的类中添加函数" class="headerlink" title="添加的功能需要Caffe的支持，向已有的类中添加函数"></a>添加的功能需要Caffe的支持，向已有的类中添加函数</h3><p>如果添加的功能需要Caffe的支持，可以在<code>pycaffe.py</code>内添加，详见<code>Net</code>的例子。由于python的灵活性，我们可以参考<code>Net</code>的实现方式，待函数实现完成后，使用<code>&lt;class&gt;.&lt;function&gt; = my_function</code>动态地添加。如下所示，注意<code>_Net_forward</code>函数的第一个参数必须是<code>self</code>。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">_Net_forward</span><span class="params">(self, blobs=None, start=None, end=None, **kwargs)</span>:</span></div><div class="line">    <span class="comment"># do something</span></div><div class="line">Net.forward = _Net_forward</div></pre></td></tr></table></figure><p>与之相似，我们还可以为已经存在的类添加字段。注意，函数用<code>@property</code>装饰，且参数有且只有一个<code>self</code>，</p><figure class="highlight py"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># This function will be called when accessing net.blobs</span></div><div class="line"><span class="meta">@property</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">_Net_blobs</span><span class="params">(self)</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line">    An OrderedDict (bottom to top, i.e., input to output) of network</div><div class="line">    blobs indexed by name</div><div class="line">    """</div><div class="line">    <span class="keyword">if</span> <span class="keyword">not</span> hasattr(self, <span class="string">'_blobs_dict'</span>):</div><div class="line">        self._blobs_dict = OrderedDict(zip(self._blob_names, self._blobs))</div><div class="line">    <span class="keyword">return</span> self._blobs_dict </div><div class="line"></div><div class="line"><span class="comment"># Set the field `blobs` to call _Net_blobs</span></div><div class="line">Net.blobs = _Net_blobs</div></pre></td></tr></table></figure><p>PyCaffe中已经实现的类主要有：<code>Net, SGDSolver, NesterovSolver, AdaGradSolver, RMSPropSolver, AdaDeltaSolver, AdamSolver</code>。</p><h2 id="使用C-添加功能"><a href="#使用C-添加功能" class="headerlink" title="使用C++添加功能"></a>使用C++添加功能</h2><p>当遇到如下情况时，可能需要修改C++代码：</p><ul><li>为了获取更底层的权限控制，如一些私有字段。</li><li>性能考虑。</li></ul><p>这时，你应该去修改<code>python/caffe/_caffe.cpp</code>文件。这个文件使用了boost实现了python与C++的绑定。</p><p>为了添加一个字段，可以在<code>Blob</code>部分添加如下的代码。这样，就会将python中<code>Blob</code>类的<code>num</code>字段绑定到C++的<code>Blob&lt;Dtype&gt;::num()</code>方法上。<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">.add_property(<span class="string">"num"</span>, &amp;Blob&lt;Dtype&gt;::num)</div></pre></td></tr></table></figure></p><p>使用<code>.def</code>可以为python相应的类绑定方法。在下面的代码中，首先实现了<code>Net_Save</code>方法，然后将其绑定到了python中<code>Net</code>类的<code>save</code>方法上。这样，通过python调用<code>net.save(filename)</code>即可。</p><p>注意，当你修改了<code>_caffe,cpp</code>后，记得使用<code>make pycaffe</code>重新生成动态链接库。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"># <span class="function">Declare the function</span></div><div class="line"><span class="keyword">void</span> <span class="title">Net_Save</span><span class="params">(<span class="keyword">const</span> Net&lt;Dtype&gt;&amp; net, <span class="built_in">string</span> filename)</span> &#123;</div><div class="line">    <span class="comment">// ...</span></div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">// ...</span></div><div class="line"></div><div class="line">bp::class_&lt;Net&lt;Dtype&gt;&gt;(<span class="string">"Net"</span>, bp::no_init)</div><div class="line"># Now we can call net.save(file)</div><div class="line">.def(<span class="string">"save"</span>, &amp;Net_Save)</div></pre></td></tr></table></figure><p>当然，上面介绍的这些还很基础，关于boost的python绑定，可以参考官方的文档：<a href="http://www.boost.org/doc/libs/1_58_0/libs/python/doc/tutorial/doc/html/index.html" target="_blank" rel="external">boost: python binding</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这篇文章主要是&lt;a href=&quot;https://github.com/nitnelave/pycaffe_tutorial/blob/master/04%20Hacking%20the%20Python%20API.ipynb&quot;&gt;Github: PyCaffe Tutorial&lt;/a&gt;中Hack Pycaffe的翻译整理。后续可能会加上一些使用boost和C++为Python接口提供后端的解释。这里主要讨论如何为Pycaffe添加自己想要的功能。至于Pycaffe的使用，留待以后的文章整理。&lt;br&gt;&lt;img src=&quot;/img/caffe-hack-pycaffe-python-cpp-binding.jpg&quot; alt=&quot;Python&amp;amp;&amp;amp;CPP binding&quot;&gt;&lt;br&gt;
    
    </summary>
    
    
      <category term="caffe" scheme="https://xmfbit.github.io/tags/caffe/"/>
    
      <category term="python" scheme="https://xmfbit.github.io/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>论文 - Learning both Weights and Connections for Efficient Neural Networks</title>
    <link href="https://xmfbit.github.io/2018/03/14/paper-network-prune-hansong/"/>
    <id>https://xmfbit.github.io/2018/03/14/paper-network-prune-hansong/</id>
    <published>2018-03-14T08:18:53.000Z</published>
    <updated>2018-03-15T06:54:37.459Z</updated>
    
    <content type="html"><![CDATA[<p>Han Song的Deep Compression是模型压缩方面很重要的论文。在Deep Compression中，作者提出了三个步骤来进行模型压缩：剪枝，量化和霍夫曼编码。其中，剪枝对应的方法就是基于本文要总结的这篇论文：<a href="https://arxiv.org/abs/1506.02626" target="_blank" rel="external">Learning both Weights and Connections for Efficient Neural Networks</a>。在这篇论文中，作者介绍了如何在不损失精度的前提下，对深度学习的网络模型进行剪枝，从而达到减小模型大小的目的。<br><img src="/img/paper-pruning-network-demo.png" alt="Pruning的主要过程"><br><a id="more"></a></p><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>DNN虽然能够解决很多以前很难解决的问题，但是一个应用方面的问题就是这些模型通常都太大了。尤其是当运行在手机等移动设备上时，对电源和网络带宽都是负担。对于电源来说，由于模型巨大，所以只能在外部内存DRAM中加载，造成能耗上升。具体数值见下表。所以模型压缩很有必要。本文就是使用剪枝的方法，将模型中不重要的权重设置为$0$，将原来的dense model转变为sparse model，达到压缩的目的。<br><img src="/img/paper-pruning-network-energy-for-different-memory-hieracy.png" alt="操作数地址的不同造成的功耗对比"></p><h3 id="解决什么问题？"><a href="#解决什么问题？" class="headerlink" title="解决什么问题？"></a>解决什么问题？</h3><p>如何在不损失精度的前提下，对DNN进行剪枝（或者说稀疏化），从而压缩模型。</p><h3 id="为什么剪枝是work的？"><a href="#为什么剪枝是work的？" class="headerlink" title="为什么剪枝是work的？"></a>为什么剪枝是work的？</h3><p>为什么能够通过剪枝的方法来压缩模型呢？难道剪掉的那些连接真的不重要到可以去掉吗？论文中，作者指出，DNN模型广泛存在着参数过多的问题，具有很大的冗余（见参考文献NIPS 2013的一篇文章<a href="https://arxiv.org/abs/1306.0543" target="_blank" rel="external">Predicting parameters in deep learning</a>）。</p><blockquote><p>Neural networks are typically over-parameterized, and there is significant redundancy for deep learning models </p></blockquote><p>另外，作者也为自己的剪枝方法找到了生理学上的依据，生理学上发现，对于哺乳动物来说，婴儿期会产生许多的突触连接，在后续的成长过程中，不怎么用的那些突出会退化消失。</p><h3 id="怎么做"><a href="#怎么做" class="headerlink" title="怎么做"></a>怎么做</h3><p>作者的方法分为三个步骤：</p><ul><li>Train Connectivity: 按照正常方法训练初始模型。作者认为该模型中权重的大小表征了其重要程度</li><li>Prune Connection: 将初始模型中那些低于某个阈值的的权重参数置成$0$（即所谓剪枝）</li><li>Re-Train: 重新训练，以期其他未被剪枝的权重能够补偿pruning带来的精度下降</li></ul><p>为了达到一个满意的压缩比例和精度要求，$2$和$3$要重复多次。</p><h2 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h2><p>为了减少网络的冗余，减小模型的size，有以下相关工作：</p><ul><li>定点化。将weight使用8bit定点表示，32bit浮点表示activation。</li><li>低秩近似。使用矩阵分解等方法。</li><li>网络设计上，NIN等使用Global Average Pooling取代FC层，可以大大减少参数量，这种结构已经得到了广泛使用。而FC也并非无用。在Pooling后面再接一个fc层，便于后续做迁移学习transfer learning。</li><li>从优化上下手，使用损失函数的Hessian矩阵，比直接用weight decay更好。</li><li>HashedNet等工作，这里不再详述。</li></ul><h2 id="如何Prune"><a href="#如何Prune" class="headerlink" title="如何Prune"></a>如何Prune</h2><p>主要分为三步，上面 概述 中 怎么做 部分已经简单列出。下面的算法流程摘自作者的博士论文，可能更加详细清楚。<br><img src="/img/paper-pruning-network-algrithem.png" alt="剪枝算法"></p><h3 id="正则项的选择"><a href="#正则项的选择" class="headerlink" title="正则项的选择"></a>正则项的选择</h3><p>L1和L2都可以用来做正则，惩罚模型的复杂度。使用不同的正则方法会对pruning和retraining产生影响。实验发现，采用L2做正则项较好。见下图，可以看到详细的比较结果，分别是with/without retrain下L1和L2正则对精度的影响。还可以看到一个共性的地方，就是当pruning的比例大于某个阈值后，模型的精度会快速下降。</p><p><img src="/img/paper-pruning-network-regularization.png" alt="L1/L2 Regularization"></p><h3 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a>Dropout</h3><p>Dropout是一项防止过拟合的技术。要注意的是，在retraining的时候，我们需要对Dropout ratio做出调整。因为网络中的很多连接都被剪枝剪下来了，所以dropout的比例要变小。下面给出定量的估计。</p><p>对于FC层来说，如果第$i$层的神经元个数是$N_i$，那么该层的连接数$C_i$用乘法原理可以很容易得到：$C_i = N_{i-1}N_i$。也就是说，连接数$C\sim N^2$。而dropout是作用于神经元的（dropout是将$N_i$个神经元输出按照概率dropout掉）。所以，比例$D^2 \sim C$，最后得到：</p><script type="math/tex; mode=display">D_r = D_o \sqrt{\frac{C_{ir}}{C_{io}}}</script><p>其中，下标$r$表示retraining，$o$表示初始模型(original)。</p><h2 id="Local-Pruning"><a href="#Local-Pruning" class="headerlink" title="Local Pruning"></a>Local Pruning</h2><p>在retraining部分，在初始模型基础上继续fine tune较好。为了能够更有效地训练，在训练FC层的时候，可以将CONV的参数固定住。反之亦然。</p><p>另外，不同深度和类型的layer对剪枝的敏感度是不一样的。作者指出，CONV比FC更敏感，第$1$个CONV比后面的要敏感。下图是AlexNet中各个layer剪枝比例和模型精度下降之间的关系。可以印证上面的结论。</p><p><img src="/img/paper-pruning-network-layer-sensitivity.png" alt="CONV和FC的prune和精度下降的关系"></p><h2 id="多次迭代剪枝"><a href="#多次迭代剪枝" class="headerlink" title="多次迭代剪枝"></a>多次迭代剪枝</h2><p>应该迭代地进行多次剪枝 + 重新训练这套组合拳。作者还尝试过根据参数的绝对值依概率进行剪枝，效果不好。<br><img src="/img/paper-pruning-network-iterative-pruning.png" alt="迭代剪枝"></p><h2 id="对神经元进行剪枝"><a href="#对神经元进行剪枝" class="headerlink" title="对神经元进行剪枝"></a>对神经元进行剪枝</h2><p>将神经元之间的connection剪枝后（或者说将权重稀疏化了），那些$0$输入$0$输出的神经元也应该被剪枝了。然后，我们又可以继续以这个神经元出发，剪掉与它相关的connection。这个步骤可以在训练的时候自动发生。因为如果某个神经元已经是dead状态，那么它的梯度也会是$0$。那么只有正则项推着它向$0$的方向。</p><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p>使用Caffe实现，需要加入一个<code>mask</code>来表示剪枝。剪枝的阈值，是该layer的权重标准差乘上某个超参数。这里：<a href="https://github.com/BVLC/caffe/pull/4294/files" target="_blank" rel="external">Add pruning possibilities at inner_product_layer #4294 </a>，有人基于Caffe官方的repo给FC层加上了剪枝。这里：<a href="https://github.com/may0324/DeepCompression-caffe" target="_blank" rel="external">Github: DeepCompression</a>,，有人实现了Deep Compression，可以参考他们的实现思路。</p><p>对于实验结果，论文中比对了LeNet和AlexNet。此外，作者的博士论文中给出了更加详细的实验结果，在更多的流行的模型上取得了不错的压缩比例。直接引用如下，做一个mark：</p><blockquote><p>On the ImageNet dataset, the pruning method reduced the number of parameters of AlexNet by a factor of 9× (61 to 6.7 million), without incurring accuracy loss. Similar experiments with VGG-16 found that the total number of parameters can be reduced by 13× (138 to 10.3 million), again with no loss of accuracy. We also experimented with the more efficient fully-convolutional neural networks: GoogleNet (Inception-V1), SqueezeNet, and ResNet-50, which have zero or very thin fully connected layers. From these experiments we find that they share very similar pruning ratios before the accuracy drops: 70% of the parameters in those fully-convolutional neural networks can be pruned. GoogleNet is pruned from 7 million to 2 million parameters, SqueezeNet from 1.2 million to 0.38 million, and ResNet-50 from 25.5 million to 7.47 million, all with no loss of Top-1 and Top-5 accuracy on Imagenet.</p></blockquote><p><img src="/img/paper-pruning-network-results.png" alt="Results"></p><p>下面 参考资料 部分也给出了作者在GitHub上放出的Deep Compression的结果，可以前去参考。</p><h3 id="学习率的设置"><a href="#学习率的设置" class="headerlink" title="学习率的设置"></a>学习率的设置</h3><p>跑模型跑实验，一个重要的超参数就是学习率$LR$。这里作者也给了一个经验规律。一般在训练初始模型的时候，学习率都是逐渐下降的。刚开始是一个较大的值$LR_1$，最后是一个较小的值$LR_2$。它们之间可能有数量级的差别。作者指出，retraining的学习率应该介于两者之间。可以取做比$LR_1$小$1 \sim 2$个数量级。</p><h3 id="RNN和LSTM"><a href="#RNN和LSTM" class="headerlink" title="RNN和LSTM"></a>RNN和LSTM</h3><p>在博士论文中，作者还是用这一技术对RNN/LSTM在Neural Talk任务上做了剪枝，取得了不错的结果。<br><img src="/img/paper-pruning-network-lstm.png" alt="LSTM"></p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul><li>HanSong的个人主页：<a href="http://stanford.edu/~songhan/" target="_blank" rel="external">Homepage</a></li><li>HanSong的博士论文：<a href="https://purl.stanford.edu/qf934gh3708" target="_blank" rel="external">Efficient Methods and Hardware for Deep Learning</a></li><li>后续的Deep Compression论文：<a href="https://arxiv.org/abs/1510.00149" target="_blank" rel="external">DEEP COMPRESSION- COMPRESSING DEEP NEURAL NETWORKS WITH PRUNING, TRAINED QUANTIZATION AND HUFFMAN CODING</a></li><li>Deep Compression AlexNet: <a href="https://github.com/songhan/Deep-Compression-AlexNet" target="_blank" rel="external">Github: Deep-Compression-AlexNet</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Han Song的Deep Compression是模型压缩方面很重要的论文。在Deep Compression中，作者提出了三个步骤来进行模型压缩：剪枝，量化和霍夫曼编码。其中，剪枝对应的方法就是基于本文要总结的这篇论文：&lt;a href=&quot;https://arxiv.org/abs/1506.02626&quot;&gt;Learning both Weights and Connections for Efficient Neural Networks&lt;/a&gt;。在这篇论文中，作者介绍了如何在不损失精度的前提下，对深度学习的网络模型进行剪枝，从而达到减小模型大小的目的。&lt;br&gt;&lt;img src=&quot;/img/paper-pruning-network-demo.png&quot; alt=&quot;Pruning的主要过程&quot;&gt;&lt;br&gt;
    
    </summary>
    
    
      <category term="deep learning" scheme="https://xmfbit.github.io/tags/deep-learning/"/>
    
      <category term="paper" scheme="https://xmfbit.github.io/tags/paper/"/>
    
  </entry>
  
  <entry>
    <title>Caffe中的Net实现</title>
    <link href="https://xmfbit.github.io/2018/02/28/caffe-net/"/>
    <id>https://xmfbit.github.io/2018/02/28/caffe-net/</id>
    <published>2018-02-28T02:16:43.000Z</published>
    <updated>2018-03-15T06:54:37.449Z</updated>
    
    <content type="html"><![CDATA[<p>Caffe中使用<code>Net</code>实现神经网络，这篇文章对应Caffe代码总结<code>Net</code>的实现。<br><img src="/img/caffe-net-demo.jpg" width="300" height="200" alt="Net示意" align="center"><br><a id="more"></a></p><h2 id="proto中定义的参数"><a href="#proto中定义的参数" class="headerlink" title="proto中定义的参数"></a>proto中定义的参数</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div></pre></td><td class="code"><pre><div class="line">message NetParameter &#123;</div><div class="line">  // net的名字</div><div class="line">  optional string name = 1; // consider giving the network a name</div><div class="line">  // 以下几个都是弃用的参数，为了定义输入的blob（大小）</div><div class="line">  // 下面有使用推荐`InputParameter`进行输入设置的方法</div><div class="line">  // DEPRECATED. See InputParameter. The input blobs to the network.</div><div class="line">  repeated string input = 3;</div><div class="line">  // DEPRECATED. See InputParameter. The shape of the input blobs.</div><div class="line">  repeated BlobShape input_shape = 8;</div><div class="line"></div><div class="line">  // 4D input dimensions -- deprecated.  Use &quot;input_shape&quot; instead.</div><div class="line">  // If specified, for each input blob there should be four</div><div class="line">  // values specifying the num, channels, height and width of the input blob.</div><div class="line">  // Thus, there should be a total of (4 * #input) numbers.</div><div class="line">  repeated int32 input_dim = 4;</div><div class="line">  </div><div class="line">  // Whether the network will force every layer to carry out backward operation.</div><div class="line">  // If set False, then whether to carry out backward is determined</div><div class="line">  // automatically according to the net structure and learning rates.</div><div class="line">  optional bool force_backward = 5 [default = false];</div><div class="line">  // The current &quot;state&quot; of the network, including the phase, level, and stage.</div><div class="line">  // Some layers may be included/excluded depending on this state and the states</div><div class="line">  // specified in the layers&apos; include and exclude fields.</div><div class="line">  optional NetState state = 6;</div><div class="line"></div><div class="line">  // Print debugging information about results while running Net::Forward,</div><div class="line">  // Net::Backward, and Net::Update.</div><div class="line">  optional bool debug_info = 7 [default = false];</div><div class="line"></div><div class="line">  // The layers that make up the net.  Each of their configurations, including</div><div class="line">  // connectivity and behavior, is specified as a LayerParameter.</div><div class="line">  repeated LayerParameter layer = 100;  // ID 100 so layers are printed last.</div><div class="line"></div><div class="line">  // DEPRECATED: use &apos;layer&apos; instead.</div><div class="line">  repeated V1LayerParameter layers = 2;</div><div class="line">&#125;</div></pre></td></tr></table></figure><h3 id="Input的定义"><a href="#Input的定义" class="headerlink" title="Input的定义"></a>Input的定义</h3><p>在<code>train</code>和<code>deploy</code>的时候，输入的定义常常是不同的。在<code>train</code>时，我们需要提供数据$x$和真实值$y$，这样网络的输出$\hat{y} = \mathcal{F}_\theta (x)$与真实值$y$计算损失，bp，更新网络参数$\theta$。</p><p>在<code>deploy</code>时，推荐使用<code>InputLayer</code>定义网络的输入，下面是<code>$CAFFE/models/bvlc_alexnet/deploy.prototxt</code>中的输入定义：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">layer &#123;</div><div class="line">  name: &quot;data&quot;</div><div class="line">  type: &quot;Input&quot;</div><div class="line">  // 该层layer的输出blob名称为data，供后续layer使用</div><div class="line">  top: &quot;data&quot;</div><div class="line">  // 定义输入blob的大小：10 x 3 x 227 x 227</div><div class="line">  // 说明batch size = 10</div><div class="line">  // 输入彩色图像，channel = 3, RGB</div><div class="line">  // 输入image的大小：227 x 227</div><div class="line">  input_param &#123; shape: &#123; dim: 10 dim: 3 dim: 227 dim: 227 &#125; &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p><h2 id="头文件"><a href="#头文件" class="headerlink" title="头文件"></a>头文件</h2><p><code>Net</code>的描述头文件位于<code>$CAFFE/include/caffe/net.hpp</code>中。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Caffe中使用&lt;code&gt;Net&lt;/code&gt;实现神经网络，这篇文章对应Caffe代码总结&lt;code&gt;Net&lt;/code&gt;的实现。&lt;br&gt;&lt;img src=&quot;/img/caffe-net-demo.jpg&quot; width = &quot;300&quot; height = &quot;200&quot; alt=&quot;Net示意&quot; align=center /&gt;&lt;br&gt;
    
    </summary>
    
    
      <category term="caffe" scheme="https://xmfbit.github.io/tags/caffe/"/>
    
  </entry>
  
  <entry>
    <title>捉bug记 - JupyterNotebook中使用pycaffe加载多个模型一直等待的现象</title>
    <link href="https://xmfbit.github.io/2018/02/27/bug-pycaffe-jupyternotebook-awaiting-for-data/"/>
    <id>https://xmfbit.github.io/2018/02/27/bug-pycaffe-jupyternotebook-awaiting-for-data/</id>
    <published>2018-02-27T05:30:25.000Z</published>
    <updated>2018-03-15T06:54:37.447Z</updated>
    
    <content type="html"><![CDATA[<p>JupyteNotebook是个很好的工具，但是在使用pycaffe试图在notebook中同时加载多个caffemodel模型的时候，却出现了无法加载的问题。</p><h2 id="bug重现"><a href="#bug重现" class="headerlink" title="bug重现"></a>bug重现</h2><p>我想在notebook中比较两个使用不同方法训练出来的模型，它们使用了同样的LMDB文件进行训练。加载第一个模型没有问题，但当加载第二个模型时，却一直等待。在StackOverflow上我发现了类似的问题，可以见：<a href="https://stackoverflow.com/questions/37260158/cant-load-2-models-in-pycaffe" target="_blank" rel="external">Can’t load 2 models in pycaffe</a>。</p><h2 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a>解决方法</h2><p>这是由于pycaffe（是否要加上jupyter-notebook？因为不用notebook，以前没有出现过类似问题）不能并发读取同样的LMDB所导致的。但是很遗憾，没有发现太好的解决办法。最后只能是将LMDB重新copy了一份，并修改prototxt文件，使得两个模型分别读取不同的LMDB。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;JupyteNotebook是个很好的工具，但是在使用pycaffe试图在notebook中同时加载多个caffemodel模型的时候，却出现了无法加载的问题。&lt;/p&gt;
&lt;h2 id=&quot;bug重现&quot;&gt;&lt;a href=&quot;#bug重现&quot; class=&quot;headerlink&quot; t
      
    
    </summary>
    
    
      <category term="caffe" scheme="https://xmfbit.github.io/tags/caffe/"/>
    
  </entry>
  
  <entry>
    <title>Caffe中卷积的大致实现思路</title>
    <link href="https://xmfbit.github.io/2018/02/26/conv-in-caffe/"/>
    <id>https://xmfbit.github.io/2018/02/26/conv-in-caffe/</id>
    <published>2018-02-26T07:26:09.000Z</published>
    <updated>2018-03-15T06:54:37.450Z</updated>
    
    <content type="html"><![CDATA[<p>参考资料：知乎：<a href="https://www.zhihu.com/question/28385679" target="_blank" rel="external">在Caffe中如何计算卷积</a>。<br><img src="/img/conv-in-caffe-naive-loop.png" alt="Naive Loop"><br><a id="more"></a></p><p>使用<code>im2col</code>将输入的图像或特征图转换为矩阵，后续就可以使用现成的线代运算优化库，如BLAS中的GEMM，来快速计算。<br><img src="/img/conv-in-caffe-im2col-followed-gemm.png" alt="im2col-&gt;gemm"></p><p>im2col的工作原理如下：每个要和卷积核做卷积的patch被抻成了一个feature vector。不同位置的patch，顺序堆叠起来，<br><img src="/img/conv-in-caffe-im2col-1.png" alt="patches堆起来"></p><p>最后就变成了这样：<br><img src="/img/conv-in-caffe-im2col-2.png" alt="最后的样子"></p><p>同样的，对卷积核也做类似的变换。将单一的卷积核抻成一个行向量，然后把<code>c_out</code>个卷积核顺序排列起来。<br><img src="/img/conv-in-caffe-im2col-3.png" alt="卷积核 to col"></p><p>我们记图像那个矩阵是<code>A</code>，记卷积那个矩阵是<code>F</code>。那么，对于第<code>i</code>个卷积核来说，它现在实际上是<code>F</code>里面的第<code>i</code>个行向量。为了计算它在原来图像上的各个位置的卷积，现在我们需要它和矩阵<code>A</code>中的每行做点积。也就是 <code>F_i * [A_1^T, A_2^T, … A_i^T]</code> （也就是<code>A</code>的转置）。推广到其他的卷积核，就是说，最后的结果是<code>F*A^T</code>.</p><p>我们可以用矩阵维度验证。<code>F</code>的维度是<code>Cout x (C x K x K)</code>. 输入的Feature map matrix的维度是<code>(H x W) x (C x K x K)</code>。那么上述矩阵乘法的结果就是 <code>Cout x (H x W)</code>。正好可以看做输出的三维blob的大小：<code>Cout x H x W</code>。</p><p>这里<a href="https://github.com/Yangqing/caffe/wiki/Convolution-in-Caffe:-a-memo" target="_blank" rel="external">Convolution in Caffe: a memo</a>还有贾扬清对于自己当时在caffe中实现conv的”心路历程“，题图出自此处。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;参考资料：知乎：&lt;a href=&quot;https://www.zhihu.com/question/28385679&quot;&gt;在Caffe中如何计算卷积&lt;/a&gt;。&lt;br&gt;&lt;img src=&quot;/img/conv-in-caffe-naive-loop.png&quot; alt=&quot;Naive Loop&quot;&gt;&lt;br&gt;
    
    </summary>
    
    
      <category term="caffe" scheme="https://xmfbit.github.io/tags/caffe/"/>
    
      <category term="deep learning" scheme="https://xmfbit.github.io/tags/deep-learning/"/>
    
  </entry>
  
  <entry>
    <title>论文 - Learning Structured Sparsity in Deep Neural Networks</title>
    <link href="https://xmfbit.github.io/2018/02/24/paper-ssl-dnn/"/>
    <id>https://xmfbit.github.io/2018/02/24/paper-ssl-dnn/</id>
    <published>2018-02-24T02:21:14.000Z</published>
    <updated>2018-03-15T06:54:37.459Z</updated>
    
    <content type="html"><![CDATA[<p>DNN的稀疏化？用L1正则项不就好了？在很多场合，这种方法的确可行。但是当试图使用FPGA/AISC加速DNN的前向计算时，我们希望DNN的参数能有一些结构化的稀疏性质。这样才能减少不必要的cache missing等问题。在<a href="https://arxiv.org/pdf/1608.03665.pdf" target="_blank" rel="external">这篇文章</a>中，作者提出了一种结构化稀疏的方法，能够在不损失精度的前提下，对深度神经网络进行稀疏化，达到加速的目的。本文作者<a href="http://www.pittnuts.com/" target="_blank" rel="external">温伟</a>，目前是杜克大学Chen Yiran组的博士生，做了很多关于结构化稀疏和DNN加速相关的工作。本文发表在NIPS 2016上。本文的代码已经公开：<a href="https://github.com/wenwei202/caffe/tree/scnn" target="_blank" rel="external">GitHub</a><br><img src="/img/paper-ssldnn.png" alt="SSL的原理示意图"><br><a id="more"></a></p><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>为了满足DNN的计算速度要求，我们提出了Structure Sparisity Learning (SSL)技术来正则化DNN的“结构”（例如CNN filter的参数，filter的形状，包括channel和网络层深）。它可以带来：</p><ul><li>大的DNN —&gt; 紧凑的模型 —&gt; 计算开销节省</li><li>硬件友好的结构化稀疏 —&gt; 便于在专用硬件上加速</li><li>提供了正则化，提高网络泛化能力 —&gt; 提高了精度</li></ul><p>实验结果显示，这种方法可以在CPU/GPU上对AlexNet分别达到平均$5.1$和$3,1$倍的加速。在CIFAR10上训练ResNet，从$20$层减少到$18$层，并提高了精度。</p><h2 id="LASSO"><a href="#LASSO" class="headerlink" title="LASSO"></a>LASSO</h2><p>SSL是基于Group LASSO的，所以正式介绍文章之前，首先简单介绍LASSO和Group LASSO，<br><a href="https://en.wikipedia.org/wiki/Lasso_(statistics" target="_blank" rel="external">LASSO</a>)(least absolute shrinkage and selection operator)是指统计学习中的特征选择方法。以最小二乘法求解线性模型为例，可以加上L1 norm作为正则化约束，见下式，其中$\beta$是模型的参数。具体推导过程可以参见wiki页面。</p><script type="math/tex; mode=display">\min_{\beta \in R^p}\frac{1}{N} \Vert(y-X\beta)\Vert_2^2 + \lambda \Vert \beta \Vert_1</script><p>而Group LASSO就是将参数分组，进行LASSO操作。</p><p>这里只是简单介绍一下LASSO。SSL下面还会详细介绍，不必过多执着于LASSO。</p><h2 id="结构化稀疏"><a href="#结构化稀疏" class="headerlink" title="结构化稀疏"></a>结构化稀疏</h2><p>DNN通常参数很多，计算量很大。为了减少计算开销，目前的研究包括：稀疏化，connection pruning, low rank approximation等。然而前两种方法只能得到随机的稀疏，无规律的内存读取仍然制约了速度。下面这种图是一个例子。我们使用了L1正则进行稀疏。和原模型相比，精度损失了$2$个点。虽然稀疏度比较高，但是实际加速效果却很差。可以看到，在<code>conv3</code>，<code>conv4</code>和<code>conv5</code>中，有的情况下反而加速比是大于$1$的。<br><img src="/img/paper-ssldnn-random-sparity-is-bad.png" alt="随机稀疏的实际加速效果"></p><p>low rank approx利用矩阵分解，将预训练好的模型的参数分解为小矩阵的乘积。这种方法需要较多的迭代次数，同时，网络结构是不能更改的。</p><p>基于下面实验中观察的事实，我们提出了SSL来直接学习结构化稀疏。</p><ul><li>网络中的filter和channel存在冗余</li><li>filter稀疏化为别的形状能够去除不必要的计算</li><li>网络的深度虽然重要，但是并不意味着深层的layer对网络性能一定是好的</li></ul><p>假设第$l$个卷积层的参数是一个$4D$的Tensor，$W^{(l)}\in R^{N_l \times C_l \times M_l \times N_l}$，那么SSL方法可以表示为优化下面这个损失函数：</p><script type="math/tex; mode=display">E(W)=E_D{W} + \lambda R(W) + \lambda_g \sum_{l=1}^{L}R_g(W^{(l)})</script><p>这里，$W$代表DNN中所有权重的集合。$E_D(W)$代表在训练集上的loss。$R$是非结构化的正则项，例如L2 norm。$R_g$是指结构化稀疏的正则项，注意是逐层计算的。对于每一层来说（也就是上述最后一项求和的每一项），group LASSO可以表示为：</p><script type="math/tex; mode=display">R_g(w) = \sum_{g=1}^{G}\Vert w^{(g)} \Vert_g</script><p>其中，$w^{(g)}$是该层权重$W^{(l)}$的一部分，不同的分组可以重叠。$G$是分组的组数。$\Vert \cdot \Vert_g$指的是group LASSO，这里使用的是$\Vert w^{(g)}\Vert_g = \sqrt{\sum_{i=1}^{|w^{(g)}|}(w_i^{(g)})^2}$，也就是$2$范数。</p><h2 id="SSL"><a href="#SSL" class="headerlink" title="SSL"></a>SSL</h2><p>有了上面的损失函数，SSL就取决于如何对weight进行分组。对不同的分组情况分类讨论如下。图示见博客开头的题图。</p><h3 id="惩罚不重要的filter和channel"><a href="#惩罚不重要的filter和channel" class="headerlink" title="惩罚不重要的filter和channel"></a>惩罚不重要的filter和channel</h3><p>假设$W^{(l)}_{n_l,:,:,:}$是第$n$个filter，$W^{(l)}_{:, c_l, :,:}$是所有weight的第$c$个channel。可以通过下面的约束来去除相对不重要的filter和channel。注意，如果第$l$层的weight中某个filter变成了$0$，那么输出的feature map中就有一个全$0$，所以filter和channel的结构化稀疏要放到一起。下面是这种形式下的损失函数。为了简单，后面的讨论中都略去了正常的正则化项$R(W)$。</p><script type="math/tex; mode=display">E(W) = E_D(W) + \lambda_n \sum_{l=1}^{L}(\sum_{n_l=1}^{N_l}\Vert W^{(l)}_{n_l,:,:,:}\Vert_g) + \lambda_c\sum_{l=1}^{L}(\sum_{cl=1}^{C_l}\Vert W^{(l)}_{:,c_l,:,:}\Vert_g)</script><h3 id="任意形状的filter"><a href="#任意形状的filter" class="headerlink" title="任意形状的filter"></a>任意形状的filter</h3><p>所谓任意形状的filter，就是将filter中的一些权重置为$0$。可以使用下面的分组方法：</p><script type="math/tex; mode=display">E(W) = E_D(W) + \lambda_s \sum_{l=1}^{L}(\sum_{c_l=1}^{C_l}\sum_{m_l=1}^{M_l}\sum_{k_l=1}^{K_l})\Vert W^{(l)}_{:,c_l,m_l,k_l} \Vert_g</script><h3 id="网络深度"><a href="#网络深度" class="headerlink" title="网络深度"></a>网络深度</h3><p>损失函数如下：</p><script type="math/tex; mode=display">E(W) = E_D(W) + \lambda_d \sum_{l=1}^{L}\Vert W^{(l)}\Vert_g</script><p>不过要注意的是，某个layer被稀疏掉了，会切断信息的流通。所以受ResNet启发，加上了short-cut结构。即使SSL移去了该layer所有的filter，上层的feature map仍然可以传导到后面。</p><h3 id="两类特殊的稀疏规则"><a href="#两类特殊的稀疏规则" class="headerlink" title="两类特殊的稀疏规则"></a>两类特殊的稀疏规则</h3><p>特意提出下面两种稀疏规则，下面的实验即是基于这两种特殊的稀疏结构。</p><h4 id="2D-filter-sparsity"><a href="#2D-filter-sparsity" class="headerlink" title="2D filter sparsity"></a>2D filter sparsity</h4><p>卷积层中的3D卷积可以看做是2D卷积的组合（做卷积的时候spatial和channel是不相交的）。这种结构化稀疏是将该卷积层中的每个2D的filter，$W^{(l)}_{n_l,c_l,:,:}$，看做一个group，做group LASSO。这相当于是上述filter-wise和channel-wise的组合。</p><h4 id="filter-wise和shape-wise的组合加速GEMM"><a href="#filter-wise和shape-wise的组合加速GEMM" class="headerlink" title="filter-wise和shape-wise的组合加速GEMM"></a>filter-wise和shape-wise的组合加速GEMM</h4><p>在Caffe中，3D的权重tensor是reshape成了一个行向量，然后$N<em>l$个filter的行向量堆叠在一起，就成了一个2D的矩阵。这个矩阵的每一列对应的是$W^{(l)}</em>{:,c_l,m_l,k_l}$，称为shape sparsity。两者组合，矩阵的零行和零列可以被抽去，相当于GEMM的矩阵行列数少了，起到了加速的效果。</p><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p>分别在MNIST，CIFAR10和ImageNet上做了实验，使用公开的模型做baseline，并以此为基础使用SSL训练。</p><h3 id="LeNet-amp-MLP-MNIST"><a href="#LeNet-amp-MLP-MNIST" class="headerlink" title="LeNet&amp;MLP@MNIST"></a>LeNet&amp;MLP@MNIST</h3><p>分别使用Caffe中实现的LeNet和MLP做实验。</p><h4 id="LeNet"><a href="#LeNet" class="headerlink" title="LeNet"></a>LeNet</h4><p>限制SSL为filter-wise和channel-wise稀疏化，来惩罚不重要的filter。下表中，LeNet-1是baseline，2和3是使用不同强度得到的稀疏化结果。可以看到，精度基本没有损失($0.1%$)，但是filter和channel数量都有了较大减少，FLOP大大减少，加速效果比较明显。<br><img src="/img/paper-ssldnn-lenet-penalizing-unimportant-filter-channel.png" alt="实验结果1"></p><p>将网络<code>conv1</code>的filter可视化如下。可以看到，对于LeNet2来说，大多数filter都被稀疏掉了。<br><img src="/img/paper-ssldnn-experiment-on-lenet.png" alt="LeNet的实验结果"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;DNN的稀疏化？用L1正则项不就好了？在很多场合，这种方法的确可行。但是当试图使用FPGA/AISC加速DNN的前向计算时，我们希望DNN的参数能有一些结构化的稀疏性质。这样才能减少不必要的cache missing等问题。在&lt;a href=&quot;https://arxiv.org/pdf/1608.03665.pdf&quot;&gt;这篇文章&lt;/a&gt;中，作者提出了一种结构化稀疏的方法，能够在不损失精度的前提下，对深度神经网络进行稀疏化，达到加速的目的。本文作者&lt;a href=&quot;http://www.pittnuts.com/&quot;&gt;温伟&lt;/a&gt;，目前是杜克大学Chen Yiran组的博士生，做了很多关于结构化稀疏和DNN加速相关的工作。本文发表在NIPS 2016上。本文的代码已经公开：&lt;a href=&quot;https://github.com/wenwei202/caffe/tree/scnn&quot;&gt;GitHub&lt;/a&gt;&lt;br&gt;&lt;img src=&quot;/img/paper-ssldnn.png&quot; alt=&quot;SSL的原理示意图&quot;&gt;&lt;br&gt;
    
    </summary>
    
    
      <category term="deep learning" scheme="https://xmfbit.github.io/tags/deep-learning/"/>
    
      <category term="paper" scheme="https://xmfbit.github.io/tags/paper/"/>
    
  </entry>
  
  <entry>
    <title>捉bug记 - Cannot create Cublas handle. Cublas won&#39;t be available.</title>
    <link href="https://xmfbit.github.io/2018/02/08/bug-pycaffe-using-cublas/"/>
    <id>https://xmfbit.github.io/2018/02/08/bug-pycaffe-using-cublas/</id>
    <published>2018-02-08T06:16:53.000Z</published>
    <updated>2018-03-15T06:54:37.448Z</updated>
    
    <content type="html"><![CDATA[<p>这两天在使用Caffe的时候出现了一个奇怪的bug。当使用C++接口时，完全没有问题；但是当使用python接口时，会出现错误提示如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">common.cpp:114] Cannot create Cublas handle. Cublas won&apos;t be available.</div><div class="line">common.cpp:121] Cannot create Curand generator. Curand won&apos;t be available.</div></pre></td></tr></table></figure></p><a id="more"></a><p>令人疑惑的是，这个python脚本我前段时间已经用过几次了，却没有这样的问题。</p><p>如果在Google上搜索这个问题，很多讨论都是把锅推给了驱动，不过我使用的这台服务器并没有更新过驱动或系统。本来想要试试重启大法，但是上面还有其他人在跑的任务，所以重启不太现实。</p><p>最后找到了这个issue: <a href="https://github.com/BVLC/caffe/issues/440" target="_blank" rel="external">Cannot use Caffe on another GPU when GPU 0 has full memory</a>。联想到我目前使用的服务器上GPU０也正是在跑着一项很吃显存的任务（如下所示），所以赶紧试了一下里面@longjon的方法。<br><img src="/img/bug_pycaffe_nvidia_smi_result.png" alt="nvidia-smi给出的显卡使用信息"></p><p>使用<code>CUDA_VISIBLE_DEVICES</code>变量，指定Caffe能看到的显卡设备。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">CUDA_VISIBLE_DEVICES=2 python my_script.py --gpu_id=0</div></pre></td></tr></table></figure></p><p>果然就可以了！</p><p>这个问题应该出在pycaffe的初始化上。这里不再深究。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这两天在使用Caffe的时候出现了一个奇怪的bug。当使用C++接口时，完全没有问题；但是当使用python接口时，会出现错误提示如下：&lt;br&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;common.cpp:114] Cannot create Cublas handle. Cublas won&amp;apos;t be available.&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;common.cpp:121] Cannot create Curand generator. Curand won&amp;apos;t be available.&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="caffe" scheme="https://xmfbit.github.io/tags/caffe/"/>
    
  </entry>
  
  <entry>
    <title>论文 - Visualizing and Understanding ConvNet</title>
    <link href="https://xmfbit.github.io/2018/02/08/paper-visualize-convnet/"/>
    <id>https://xmfbit.github.io/2018/02/08/paper-visualize-convnet/</id>
    <published>2018-02-08T02:48:21.000Z</published>
    <updated>2018-03-15T06:54:37.460Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://arxiv.org/pdf/1311.2901.pdf" target="_blank" rel="external">Visualizing &amp; Understanding ConvNet</a>这篇文章是比较早期关于CNN调试的文章，作者利用可视化方法，设计了一个超过AlexNet性能的网络结构。</p><p><img src="/img/paper_visconvnet_demo.png" alt="可视化结果"><br><a id="more"></a></p><h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><p>继AlexNet之后，CNN在ImageNet竞赛中得到了广泛应用。AlextNet成功的原因包括以下三点：</p><ul><li>Large data。</li><li>硬件GPU性能。</li><li>一些技巧提升了模型的泛化能力，如Dropout技术。</li></ul><p>不过CNN仍然像一只黑盒子，缺少可解释性。这使得对CNN的调试变得比较困难。我们提出了一种思路，可以找出究竟input中的什么东西对应了激活后的Feature map。</p><p>(对于神经网络的可解释性，可以从基础理论入手，也可以从实践中的经验入手。本文作者实际上就是在探索如何能够更好得使用经验对CNN进行调试。这种方法仍然没有触及到CNN本质的可解释性的东西，不过仍然在工程实践中有很大的意义，相当于将黑盒子变成了灰盒子。从人工取火到炼金术到现代化学，也不是这么一个过程吗？)</p><p>在AlexNet中，每个卷积单元常常由以下几个部分组成：</p><ul><li>卷积层，使用一组学习到的$3D$滤波器与输入（上一层的输出或网络输入的数据）做卷积操作。</li><li>非线性激活层，通常使用<code>ReLU(x) = max(0, x)</code>。</li><li>可选的，池化层，缩小Feature map的尺寸。</li><li>可选的，LRN层（现在已经基本不使用）。</li></ul><h2 id="DeconvNet"><a href="#DeconvNet" class="headerlink" title="DeconvNet"></a>DeconvNet</h2><p>我们使用DeconvNet这项技术，寻找与输出的激活对应的输入模式。这样，我们可以看到，输入中的哪个部分被神经元捕获，产生了较强的激活。</p><p>如图所示，展示了DeconvNet是如何构造的。<br><img src="/img/paer_visconvnet_deconvnet_structure.png" alt="DeconvNet的构造"></p><p>首先，图像被送入卷积网络中，得到输出的feature map。对于输出的某个激活，我们可以将其他激活值置成全$0$，然后顺着deconvNet计算，得到与之对应的输入。具体来说，我们需要对三种不同的layer进行反向操作。</p><h3 id="Uppooling"><a href="#Uppooling" class="headerlink" title="Uppooling"></a>Uppooling</h3><p>在CNN中，max pooling操作是不可逆的（信息丢掉了）。我们可以使用近似操作：记录最大值的位置；在deconvNet中，保留该标记位置处的激活值。如下图所示。右侧为CNN中的max pooling操作。中间switches显示的是最大值的位置（用灰色标出）。在左侧的deconvNet中，激活值对应给到相应的灰色位置。这个操作被称为Uppooing。<br><img src="/img/paper_visconvnet_uppooling.png" alt="Uppooling示意图"></p><h3 id="Rectification"><a href="#Rectification" class="headerlink" title="Rectification"></a>Rectification</h3><p>在CNN中，一般使用relu作为非线性激活。deconvNet中也做同样的处理。</p><h3 id="Filtering"><a href="#Filtering" class="headerlink" title="Filtering"></a>Filtering</h3><p>在CNN中，一组待学习的filter用来与输入的feature map做卷积。得到输出。在deconvNet中，使用deconv操作，输入是整流之后的feature map。</p><p>对于最终输出的activation中的每个值，经过deconv的作用，最终会对应到输入pixel space上的一小块区域，显示了它们对最终输出的贡献。</p><h2 id="CNN的可视化"><a href="#CNN的可视化" class="headerlink" title="CNN的可视化"></a>CNN的可视化</h2><p>要想可视化，先要有训练好的CNN模型。这里用作可视化的模型基于AlexNet，但是去掉了group。另外，为了可视化效果，将layer $1$的filter size从$11\times 11$变成$7\times 7$，步长变成$2$。具体训练过程不再详述。</p><p>训练完之后，我们将ImageNet的validation数据集送入到网络中进行前向计算，</p><p>如下所示，是layer $1$的可视化结果。可以看到，右下方的可视化结果被分成了$9\times 9$的方格，每个方格内又细分成了$9\times 9$的小格子。其中，大格子对应的是$9$个filter，小格子对应的是top 9的激活利用deconvNet反算回去对应的image patch、因为layer 1的filter个数正好也是$9$，所以可能稍显迷惑。<br><img src="/img/paper_visconvnet_layer1_demo.png" alt="layer 1的可视化"></p><h2 id="附录"><a href="#附录" class="headerlink" title="附录"></a>附录</h2><p>这里是关于CNN可视化的一些额外资料：</p><ul><li>Zeiler关于本文的talk：<a href="https://www.youtube.com/watch?v=ghEmQSxT6tw" target="_blank" rel="external">Visualizing and Understanding Deep Neural Networks by Matt Zeiler</a></li><li>斯坦福CS231课程的讲义：<a href="http://cs231n.github.io/understanding-cnn/" target="_blank" rel="external">Visualizing what ConvNets learn</a></li><li>ICML 2015上的另一篇CNN可视化的paper：<a href="https://arxiv.org/pdf/1506.06579.pdf" target="_blank" rel="external">Understanding Neural Networks Through Deep Visualization</a>以及他们的开源工具：<a href="https://github.com/yosinski/deep-visualization-toolbox" target="_blank" rel="external">deep-visualization-toolbox</a></li><li>一篇知乎专栏的文章：<a href="https://zhuanlan.zhihu.com/p/24833574" target="_blank" rel="external">Deep Visualization:可视化并理解CNN</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1311.2901.pdf&quot;&gt;Visualizing &amp;amp; Understanding ConvNet&lt;/a&gt;这篇文章是比较早期关于CNN调试的文章，作者利用可视化方法，设计了一个超过AlexNet性能的网络结构。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/img/paper_visconvnet_demo.png&quot; alt=&quot;可视化结果&quot;&gt;&lt;br&gt;
    
    </summary>
    
    
      <category term="deep learning" scheme="https://xmfbit.github.io/tags/deep-learning/"/>
    
      <category term="paper" scheme="https://xmfbit.github.io/tags/paper/"/>
    
  </entry>
  
  <entry>
    <title>Incremental Network Quantization 论文阅读</title>
    <link href="https://xmfbit.github.io/2018/01/25/inq-paper/"/>
    <id>https://xmfbit.github.io/2018/01/25/inq-paper/</id>
    <published>2018-01-25T07:30:28.000Z</published>
    <updated>2018-03-15T06:54:37.457Z</updated>
    
    <content type="html"><![CDATA[<p>卷积神经网络虽然已经在很多任务上取得了很棒的效果，但是模型大小和运算量限制了它们在移动设备和嵌入式设备上的使用。模型量化压缩等问题自然引起了大家的关注。<a href="https://arxiv.org/abs/1702.03044" target="_blank" rel="external">Incremental Network Quantization</a>这篇文章关注的是如何使用更少的比特数进行模型参数量化以达到压缩模型，减少模型大小同时使得模型精度无损。如下图所示，使用5bit位数，INQ在多个模型上都取得了不逊于原始FP32模型的精度。实验结果还是很有说服力的。作者将其开源在了GitHub上，见<a href="https://github.com/Zhouaojun/Incremental-Network-Quantization" target="_blank" rel="external">Incremental-Network-Quantization</a>。<br><img src="/img/paper-inq-result.png" alt="实验结果"><br><a id="more"></a></p><h2 id="量化方法"><a href="#量化方法" class="headerlink" title="量化方法"></a>量化方法</h2><p>INQ论文中，作者采用的量化方法是将权重量化为$2$的幂次或$0$。具体来说，是将权重$W_l$（表示第$l$层的参数权重）舍入到下面这个有限集合中的元素（在下面的讨论中，我们认为$n_1 &gt; n_2$）：<br><img src="/img/paper-inq-quantize-set.png" alt="权重集合"></p><p>假设用$b$bit表示权重，我们分出$1$位单独表示$0$。</p><p>PS：这里插一句。关于为什么要单独分出$1$位表示$0$，毕竟这样浪费了($2^b$ vs $2^{b-1}+1$)。GitHub上有人发<a href="https://github.com/Zhouaojun/Incremental-Network-Quantization/issues/12" target="_blank" rel="external">issue</a>问，作者也没有正面回复这样做的原因。以我的理解，是方便判定$0$和移位。因为作者将权重都舍入到了$2$的幂次，那肯定是为了后续将乘法变成移位操作。而使用剩下的$b-1$表示，可以方便地读出移位的位数，进行操作。</p><p>这样，剩下的$b-1$位用来表示$2$的幂次。我们需要决定$n_1$和$n_2$。因为它俩决定了表示范围。它们之间的关系为：</p><script type="math/tex; mode=display">(n_1-n_2 + 1) \times 2 = 2^{b-1}</script><p>其中，乘以$2$是考虑到正负对称的表示范围。</p><p>如何确定$n_1$呢（由上式可知，有了$b$和$n_1$，$n_2$就确定了）。作者考虑了待量化权重中的最大值，我们需要设置$n_1$，使其刚好不溢出。所以有：</p><script type="math/tex; mode=display">n_1 = \lfloor \log_2(4s/3) \rfloor</script><p>其中，$s$是权重当中绝对值最大的那个，即$s = \max \vert W_l\vert$。</p><p>之后做最近舍入就可以了。对于小于最小分辨力$2^{n_2}$的那些权重，将其直接截断为$0$。</p><h2 id="训练方法"><a href="#训练方法" class="headerlink" title="训练方法"></a>训练方法</h2><p>量化完成后，网络的精度必然会下降。我们需要对其进行调整，使其精度能够恢复原始模型的水平。为此，作者提出了三个主要步骤，迭代地进行。即 weight partition（权重划分）, group-wise quantization（分组量化） 和re-training（训练）。</p><p>re-training好理解，就是量化之后要继续做finetuning。前面两个名词解释如下：weight partition是指我们不是对整个权重一股脑地做量化，而是将其划分为两个不相交的集合。group-wise quantization是指对其中一个集合中的权重做量化，另一组集合中的权重不变，仍然为FP32。注意，在re-training中，我们只对没有量化的那组参数做参数更新。下面是论文中的表述。</p><blockquote><p>Weight partition is to divide the weights in each layer of a pre-trained full-precision CNN model into two disjoint groups which play comple- mentary roles in our INQ. The weights in the first group are responsible for forming a low-precision base for the original model, thus they are quantized by using Equation (4). The weights in the second group adapt to compensate for the loss in model accuracy, thus they are the ones to be re-trained.</p></blockquote><p>训练步骤可以用下图来表示。在第一个迭代中，将所有的权重划分为黑色和白色两个部分（图$1$）。黑色部分的权重进行量化，白色部分不变（图$2$）。然后，使用SGD更新那些白色部分的权重（图$3$）。在第二次迭代中，我们扩大量化权重的范围，重复进行迭代$1$中的操作。在后面的迭代中，以此类推，只不过要不断调大量化权重的比例，最终使得所有权重都量化为止。<br><img src="/img/paper-inq-algorithm-demo.png" alt="训练图解"></p><h3 id="pruning-inspired-strategy"><a href="#pruning-inspired-strategy" class="headerlink" title="pruning-inspired strategy"></a>pruning-inspired strategy</h3><p>在权重划分步骤，作者指出，随机地将权重量化，不如根据权重的幅值，优先量化那些绝对值比较大的权重。比较结果见下图。<br><img src="/img/paper-inq-different-quantize.png" alt="两种量化方法的比较"></p><p>在代码部分，INQ基于Caffe框架，主要修改的地方集中于<code>blob.cpp</code>和<code>sgd_solver.cpp</code>中。量化部分的代码如下，首先根据要划分的比例计算出两个集合分界点处的权重大小。然后将大于该值的权重进行量化，小于该值的权重保持不变。下面的代码其实有点小问题，<code>data_copy</code>使用完之后没有释放。关于代码中<code>mask</code>的作用，下文介绍。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// blob.cpp</span></div><div class="line"><span class="comment">// INQ  </span></div><div class="line"><span class="keyword">if</span>(is_quantization)</div><div class="line">&#123;</div><div class="line">  Dtype* data_copy=(Dtype*) <span class="built_in">malloc</span>(count_*<span class="keyword">sizeof</span>(Dtype));</div><div class="line">  caffe_copy(count_,data_vec,data_copy);</div><div class="line">  caffe_abs(count_,data_copy,data_copy);</div><div class="line">  <span class="built_in">std</span>::sort(data_copy,data_copy+count_); <span class="comment">//data_copy order from small to large</span></div><div class="line">  </div><div class="line">  <span class="comment">//caculate the n1</span></div><div class="line">  Dtype max_data=data_copy[count_<span class="number">-1</span>];</div><div class="line">  <span class="keyword">int</span> n1=(<span class="keyword">int</span>)<span class="built_in">floor</span>(log2(max_data*<span class="number">4.0</span>/<span class="number">3.0</span>));</div><div class="line">  </div><div class="line">  <span class="comment">//quantizate the top 30% of each layer, change the "partition" until partition=0</span></div><div class="line">  <span class="keyword">int</span> partition=<span class="keyword">int</span>(count_*<span class="number">0.7</span>)<span class="number">-1</span>;</div><div class="line"></div><div class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; (count_); ++i) &#123;</div><div class="line">  </div><div class="line">    <span class="keyword">if</span>(<span class="built_in">std</span>::<span class="built_in">abs</span>(data_vec[i])&gt;=data_copy[partition])</div><div class="line">      &#123;</div><div class="line">        data_vec[i] = weightCluster_zero(data_vec[i],n1);</div><div class="line"> </div><div class="line">        mask_vec[i]=<span class="number">0</span>;</div><div class="line">      &#125;</div><div class="line">  &#125;</div></pre></td></tr></table></figure><h3 id="参数更新"><a href="#参数更新" class="headerlink" title="参数更新"></a>参数更新</h3><p>在re-training中，我们只对未量化的那些参数进行更新。待更新的参数，<code>mask</code>中的值都是$1$，这样和<code>diff</code>相乘仍然不变；不更新的参数，<code>mask</code>中的值都是$0$，和<code>diff</code>乘起来，相当于强制把梯度变成了$0$。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">// sgd_solver.cpp</div><div class="line">caffe_gpu_mul(net_params[param_id]-&gt;count(),net_params[param_id]-&gt;gpu_mask(),net_params[param_id]-&gt;mutable_gpu_diff(),net_params[param_id]-&gt;mutable_gpu_diff());</div></pre></td></tr></table></figure><h2 id="结语"><a href="#结语" class="headerlink" title="结语"></a>结语</h2><p>论文中还有一些其他的小细节，这里不再多说。本文的作者还维护了一个关于模型量化压缩相关的<a href="https://github.com/Zhouaojun/Efficient-Deep-Learning" target="_blank" rel="external">repo</a>，也可以作为参考。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;卷积神经网络虽然已经在很多任务上取得了很棒的效果，但是模型大小和运算量限制了它们在移动设备和嵌入式设备上的使用。模型量化压缩等问题自然引起了大家的关注。&lt;a href=&quot;https://arxiv.org/abs/1702.03044&quot;&gt;Incremental Network Quantization&lt;/a&gt;这篇文章关注的是如何使用更少的比特数进行模型参数量化以达到压缩模型，减少模型大小同时使得模型精度无损。如下图所示，使用5bit位数，INQ在多个模型上都取得了不逊于原始FP32模型的精度。实验结果还是很有说服力的。作者将其开源在了GitHub上，见&lt;a href=&quot;https://github.com/Zhouaojun/Incremental-Network-Quantization&quot;&gt;Incremental-Network-Quantization&lt;/a&gt;。&lt;br&gt;&lt;img src=&quot;/img/paper-inq-result.png&quot; alt=&quot;实验结果&quot;&gt;&lt;br&gt;
    
    </summary>
    
    
      <category term="paper" scheme="https://xmfbit.github.io/tags/paper/"/>
    
      <category term="quantize" scheme="https://xmfbit.github.io/tags/quantize/"/>
    
  </entry>
  
  <entry>
    <title>Caffe 中的 SyncedMem介绍</title>
    <link href="https://xmfbit.github.io/2018/01/12/caffe-syncedmem/"/>
    <id>https://xmfbit.github.io/2018/01/12/caffe-syncedmem/</id>
    <published>2018-01-12T06:05:59.000Z</published>
    <updated>2018-03-15T06:54:37.450Z</updated>
    
    <content type="html"><![CDATA[<p><code>Blob</code>是Caffe中的基本数据结构，类似于TensorFlow和PyTorch中的Tensor。图像读入后作为<code>Blob</code>，开始在各个<code>Layer</code>之间传递，最终得到输出。下面这张图展示了<code>Blob</code>和<code>Layer</code>之间的关系：<br> <img src="/img/caffe_syncedmem_blob_flow.jpg" width="300" height="200" alt="blob的流动" align="center"></p><p>Caffe中的<code>Blob</code>在实现的时候，使用了<code>SyncedMem</code>管理内存，并在内存（Host）和显存（device）之间同步。这篇博客对Caffe中<code>SyncedMem</code>的实现做一总结。<br><a id="more"></a></p><h2 id="SyncedMem的作用"><a href="#SyncedMem的作用" class="headerlink" title="SyncedMem的作用"></a>SyncedMem的作用</h2><p><code>Blob</code>是一个多维的数组，可以位于内存，也可以位于显存（当使用GPU时）。一方面，我们需要对底层的内存进行管理，包括何何时开辟内存空间。另一方面，我们的训练数据常常是首先由硬盘读取到内存中，而训练又经常使用GPU，最终结果的保存或可视化又要求数据重新传回内存，所以涉及到Host和Device内存的同步问题。</p><h2 id="同步的实现思路"><a href="#同步的实现思路" class="headerlink" title="同步的实现思路"></a>同步的实现思路</h2><p>在<code>SyncedMem</code>的实现代码中，作者使用一个枚举量<code>head_</code>来标记当前的状态。如下所示：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// in SyncedMem</span></div><div class="line"><span class="keyword">enum</span> SyncedHead &#123; UNINITIALIZED, HEAD_AT_CPU, HEAD_AT_GPU, SYNCED &#125;;</div><div class="line"><span class="comment">// 使用过Git吗？ 在Git中那个标志着repo最新版本状态的变量就叫 HEAD</span></div><div class="line"><span class="comment">// 这里也是一样，标志着最新的数据位于哪里</span></div><div class="line">SyncedHead head_;</div></pre></td></tr></table></figure><p>这样，利用<code>head_</code>变量，就可以构建一个状态转移图，在不同状态切换时进行必要的同步操作等。<br><img src="/img/caffe_syncedmem_transfer.png" alt="状态转换图"></p><h2 id="具体实现"><a href="#具体实现" class="headerlink" title="具体实现"></a>具体实现</h2><p><code>SyncedMem</code>的类声明如下：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line"> * @brief Manages memory allocation and synchronization between the host (CPU)</div><div class="line"> *        and device (GPU).</div><div class="line"> *</div><div class="line"> * TODO(dox): more thorough description.</div><div class="line"> */</div><div class="line"><span class="keyword">class</span> SyncedMemory &#123;</div><div class="line"> <span class="keyword">public</span>:</div><div class="line">  SyncedMemory();</div><div class="line">  <span class="function"><span class="keyword">explicit</span> <span class="title">SyncedMemory</span><span class="params">(<span class="keyword">size_t</span> size)</span></span>;</div><div class="line">  ~SyncedMemory();</div><div class="line">  <span class="comment">// 获取CPU data指针</span></div><div class="line">  <span class="function"><span class="keyword">const</span> <span class="keyword">void</span>* <span class="title">cpu_data</span><span class="params">()</span></span>;</div><div class="line">  <span class="comment">// 设置CPU data指针</span></div><div class="line">  <span class="function"><span class="keyword">void</span> <span class="title">set_cpu_data</span><span class="params">(<span class="keyword">void</span>* data)</span></span>;</div><div class="line">  <span class="comment">// 获取GPU data指针</span></div><div class="line">  <span class="function"><span class="keyword">const</span> <span class="keyword">void</span>* <span class="title">gpu_data</span><span class="params">()</span></span>;</div><div class="line">  <span class="comment">// 设置GPU data指针</span></div><div class="line">  <span class="function"><span class="keyword">void</span> <span class="title">set_gpu_data</span><span class="params">(<span class="keyword">void</span>* data)</span></span>;</div><div class="line">  <span class="comment">// 获取CPU data指针，并在后续将改变指针所指向内存的值</span></div><div class="line">  <span class="function"><span class="keyword">void</span>* <span class="title">mutable_cpu_data</span><span class="params">()</span></span>;</div><div class="line">  <span class="comment">// 获取GPU data指针，并在后续将改变指针所指向内存的值</span></div><div class="line">  <span class="function"><span class="keyword">void</span>* <span class="title">mutable_gpu_data</span><span class="params">()</span></span>;</div><div class="line">  <span class="comment">// CPU 和 GPU的同步状态：未初始化，在CPU（未同步），在GPU（未同步），已同步</span></div><div class="line">  <span class="keyword">enum</span> SyncedHead &#123; UNINITIALIZED, HEAD_AT_CPU, HEAD_AT_GPU, SYNCED &#125;;</div><div class="line">  <span class="function">SyncedHead <span class="title">head</span><span class="params">()</span> </span>&#123; <span class="keyword">return</span> head_; &#125;</div><div class="line">  <span class="comment">// 内存大小</span></div><div class="line">  <span class="keyword">size_t</span> size() &#123; <span class="keyword">return</span> size_; &#125;</div><div class="line"></div><div class="line"><span class="meta">#<span class="meta-keyword">ifndef</span> CPU_ONLY</span></div><div class="line">  <span class="function"><span class="keyword">void</span> <span class="title">async_gpu_push</span><span class="params">(<span class="keyword">const</span> cudaStream_t&amp; stream)</span></span>;</div><div class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></div><div class="line"></div><div class="line"> <span class="keyword">private</span>:</div><div class="line">  <span class="function"><span class="keyword">void</span> <span class="title">check_device</span><span class="params">()</span></span>;</div><div class="line"></div><div class="line">  <span class="function"><span class="keyword">void</span> <span class="title">to_cpu</span><span class="params">()</span></span>;</div><div class="line">  <span class="function"><span class="keyword">void</span> <span class="title">to_gpu</span><span class="params">()</span></span>;</div><div class="line">  <span class="keyword">void</span>* cpu_ptr_;</div><div class="line">  <span class="keyword">void</span>* gpu_ptr_;</div><div class="line">  <span class="keyword">size_t</span> size_;</div><div class="line">  SyncedHead head_;</div><div class="line">  <span class="keyword">bool</span> own_cpu_data_;</div><div class="line">  <span class="keyword">bool</span> cpu_malloc_use_cuda_;</div><div class="line">  <span class="keyword">bool</span> own_gpu_data_;</div><div class="line">  <span class="comment">// GPU设备编号</span></div><div class="line">  <span class="keyword">int</span> device_;</div><div class="line"></div><div class="line">  DISABLE_COPY_AND_ASSIGN(SyncedMemory);</div><div class="line">&#125;;  <span class="comment">// class SyncedMemory</span></div></pre></td></tr></table></figure><p>我们以<code>to_cpu()</code>为例，看一下如何在不同状态之间切换。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">inline</span> <span class="keyword">void</span> SyncedMemory::to_gpu() &#123;</div><div class="line">  <span class="comment">// 检查设备状态（使用条件编译，只在DEBUG中使能）</span></div><div class="line">  check_device();</div><div class="line"><span class="meta">#<span class="meta-keyword">ifndef</span> CPU_ONLY</span></div><div class="line">  <span class="keyword">switch</span> (head_) &#123;</div><div class="line">  <span class="keyword">case</span> UNINITIALIZED:</div><div class="line">    <span class="comment">// 还没有初始化呢~所以内存啥的还没开</span></div><div class="line">    <span class="comment">// 先在GPU上开块显存吧~</span></div><div class="line">    CUDA_CHECK(cudaMalloc(&amp;gpu_ptr_, size_));</div><div class="line">    caffe_gpu_memset(size_, <span class="number">0</span>, gpu_ptr_);</div><div class="line">    <span class="comment">// 接着，改变状态标志</span></div><div class="line">    head_ = HEAD_AT_GPU;</div><div class="line">    own_gpu_data_ = <span class="literal">true</span>;</div><div class="line">    <span class="keyword">break</span>;</div><div class="line">  <span class="keyword">case</span> HEAD_AT_CPU:</div><div class="line">    <span class="comment">// 数据在CPU上~如果需要，先在显存上开内存</span></div><div class="line">    <span class="keyword">if</span> (gpu_ptr_ == <span class="literal">NULL</span>) &#123;</div><div class="line">      CUDA_CHECK(cudaMalloc(&amp;gpu_ptr_, size_));</div><div class="line">      own_gpu_data_ = <span class="literal">true</span>;</div><div class="line">    &#125;</div><div class="line">    <span class="comment">// 数据拷贝</span></div><div class="line">    caffe_gpu_memcpy(size_, cpu_ptr_, gpu_ptr_);</div><div class="line">    <span class="comment">// 改变状态变量</span></div><div class="line">    head_ = SYNCED;</div><div class="line">    <span class="keyword">break</span>;</div><div class="line">  <span class="comment">// 已经在GPU或者已经同步了，什么都不做</span></div><div class="line">  <span class="keyword">case</span> HEAD_AT_GPU:</div><div class="line">  <span class="keyword">case</span> SYNCED:</div><div class="line">    <span class="keyword">break</span>;</div><div class="line">  &#125;</div><div class="line"><span class="meta">#<span class="meta-keyword">else</span></span></div><div class="line">  <span class="comment">// NO_GPU 是一个宏，打印FATAL ERROR日志信息</span></div><div class="line">  <span class="comment">// 编译选项没有开GPU支持，只能说 无可奉告</span></div><div class="line">  NO_GPU;</div><div class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></div><div class="line">&#125;</div></pre></td></tr></table></figure><p>注意到，除了<code>head_</code>以外，<code>SyncedMemory</code>中还有<code>own_gpu_data_</code>（同样，也有<code>own_cpu_data_</code>）的成员。这个变量是用来标志当前CPU或GPU上有没有分配内存，从而当我们使用<code>set_c/gpu_data</code>或析构函数被调用的时候，能够正确释放内存/显存的。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;code&gt;Blob&lt;/code&gt;是Caffe中的基本数据结构，类似于TensorFlow和PyTorch中的Tensor。图像读入后作为&lt;code&gt;Blob&lt;/code&gt;，开始在各个&lt;code&gt;Layer&lt;/code&gt;之间传递，最终得到输出。下面这张图展示了&lt;code&gt;Blob&lt;/code&gt;和&lt;code&gt;Layer&lt;/code&gt;之间的关系：&lt;br&gt; &lt;img src=&quot;/img/caffe_syncedmem_blob_flow.jpg&quot; width = &quot;300&quot; height = &quot;200&quot; alt=&quot;blob的流动&quot; align=center /&gt;&lt;/p&gt;
&lt;p&gt;Caffe中的&lt;code&gt;Blob&lt;/code&gt;在实现的时候，使用了&lt;code&gt;SyncedMem&lt;/code&gt;管理内存，并在内存（Host）和显存（device）之间同步。这篇博客对Caffe中&lt;code&gt;SyncedMem&lt;/code&gt;的实现做一总结。&lt;br&gt;
    
    </summary>
    
    
      <category term="caffe" scheme="https://xmfbit.github.io/tags/caffe/"/>
    
  </entry>
  
  <entry>
    <title>Caffe中的BatchNorm实现</title>
    <link href="https://xmfbit.github.io/2018/01/08/caffe-batch-norm/"/>
    <id>https://xmfbit.github.io/2018/01/08/caffe-batch-norm/</id>
    <published>2018-01-08T12:12:44.000Z</published>
    <updated>2018-03-15T06:54:37.449Z</updated>
    
    <content type="html"><![CDATA[<p>这篇博客总结了Caffe中BN的实现。<br><a id="more"></a></p><h2 id="BN简介"><a href="#BN简介" class="headerlink" title="BN简介"></a>BN简介</h2><p>由于BN技术已经有很广泛的应用，所以这里只对BN做一个简单的介绍。</p><p>BN是Batch Normalization的简称，来源于Google研究人员的论文：<a href="https://arxiv.org/pdf/1502.03167.pdf" target="_blank" rel="external">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</a>。对于网络的输入层，我们可以采用减去均值除以方差的方法进行归一化，对于网络中间层，BN可以实现类似的功能。</p><p>在BN层中，训练时，会对输入blob各个channel的均值和方差做一统计。在做inference的时候，我们就可以利用均值和方法，对输入$x$做如下的归一化操作。其中，$\epsilon$是为了防止除数是$0$，$i$是channel的index。</p><script type="math/tex; mode=display">\hat{x_i} = \frac{x_i-\mu_i}{\sqrt{Var(x_i)+\epsilon}}</script><p>不过如果只是做如上的操作，会影响模型的表达能力。例如，Identity Map($y = x$)就不能表示了。所以，作者提出还需要在后面添加一个线性变换，如下所示。其中，$\gamma$和$\beta$都是待学习的参数，使用梯度下降进行更新。BN的最终输出就是$y$。</p><script type="math/tex; mode=display">y_i = \gamma \hat{x_i} + \beta</script><p>如下图所示，展示了BN变换的过程。<br><img src="/img/caffe_bn_what_is_bn.jpg" alt="BN变换"></p><p>上面，我们讲的还是inference时候BN变换是什么样子的。那么，训练时候，BN是如何估计样本均值和方差的呢？下面，结合Caffe的代码进行梳理。</p><h2 id="BN-in-Caffe"><a href="#BN-in-Caffe" class="headerlink" title="BN in Caffe"></a>BN in Caffe</h2><p>在BVLC的Caffe实现中，BN层需要和Scale层配合使用。在这里，BN层专门用来做“Normalization”操作（确实是人如其名了），而后续的线性变换层，交给Scale层去做。</p><p>下面的这段代码取自He Kaiming的Residual Net50的<a href="https://github.com/KaimingHe/deep-residual-networks/blob/master/prototxt/ResNet-50-deploy.prototxt#L21" target="_blank" rel="external">模型定义文件</a>。在这里，设置<code>batch_norm_param</code>中<code>use_global_stats</code>为<code>true</code>，是指在inference阶段，我们只使用已经得到的均值和方差统计量，进行归一化处理，而不再更新这两个统计量。后面Scale层设置的<code>bias_term: true</code>是不可省略的。这个选项将其配置为线性变换层。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line">layer &#123;</div><div class="line">bottom: &quot;conv1&quot;</div><div class="line">top: &quot;conv1&quot;</div><div class="line">name: &quot;bn_conv1&quot;</div><div class="line">type: &quot;BatchNorm&quot;</div><div class="line">batch_norm_param &#123;</div><div class="line">use_global_stats: true</div><div class="line">&#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">bottom: &quot;conv1&quot;</div><div class="line">top: &quot;conv1&quot;</div><div class="line">name: &quot;scale_conv1&quot;</div><div class="line">type: &quot;Scale&quot;</div><div class="line">scale_param &#123;</div><div class="line">bias_term: true</div><div class="line">&#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure><p>这就是Caffe中BN层的固定搭配方法。这里只是简单提到，具体参数的意义待我们深入代码可以分析。</p><h2 id="BatchNorm-层的实现"><a href="#BatchNorm-层的实现" class="headerlink" title="BatchNorm 层的实现"></a>BatchNorm 层的实现</h2><p>上面说过，Caffe中的BN层与原始论文稍有不同，只是做了输入的归一化，而后续的线性变换是交由后续的Scale层实现的。</p><h3 id="proto定义的相关参数"><a href="#proto定义的相关参数" class="headerlink" title="proto定义的相关参数"></a>proto定义的相关参数</h3><p>我们首先看一下<code>caffe.proto</code>中关于BN层参数的描述。保留了原始的英文注释，并添加了中文解释。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div></pre></td><td class="code"><pre><div class="line">message BatchNormParameter &#123;</div><div class="line">  // If false, normalization is performed over the current mini-batch</div><div class="line">  // and global statistics are accumulated (but not yet used) by a moving</div><div class="line">  // average.</div><div class="line">  // If true, those accumulated mean and variance values are used for the</div><div class="line">  // normalization.</div><div class="line">  // By default, it is set to false when the network is in the training</div><div class="line">  // phase and true when the network is in the testing phase.</div><div class="line">  // 设置为False的话，更新全局统计量，对当前的mini-batch进行规范化时，不使用全局统计量，而是</div><div class="line">  // 当前batch的均值和方差。</div><div class="line">  // 设置为True，使用全局统计量做规范化。</div><div class="line">  // 后面在BN的实现代码我们会看到，这个变量默认随着当前网络在train或test phase而变化。</div><div class="line">  // 当train时为false，当test时为true。</div><div class="line">  optional bool use_global_stats = 1;</div><div class="line">  </div><div class="line">  // What fraction of the moving average remains each iteration?</div><div class="line">  // Smaller values make the moving average decay faster, giving more</div><div class="line">  // weight to the recent values.</div><div class="line">  // Each iteration updates the moving average @f$S_&#123;t-1&#125;@f$ with the</div><div class="line">  // current mean @f$ Y_t @f$ by</div><div class="line">  // @f$ S_t = (1-\beta)Y_t + \beta \cdot S_&#123;t-1&#125; @f$, where @f$ \beta @f$</div><div class="line">  // is the moving_average_fraction parameter.</div><div class="line">  // BN在统计全局均值和方差信息时，使用的是滑动平均法，也就是</div><div class="line">  // St = (1-beta)*Yt + beta*S_&#123;t-1&#125;</div><div class="line">  // 其中St为当前估计出来的全局统计量（均值或方差），Yt为当前batch的均值或方差</div><div class="line">  // beta是滑动因子。其实这是一种很常见的平滑滤波的方法。</div><div class="line">  optional float moving_average_fraction = 2 [default = .999];</div><div class="line">  </div><div class="line">  // Small value to add to the variance estimate so that we don&apos;t divide by</div><div class="line">  // zero.</div><div class="line">  // 防止除数为0加上去的eps</div><div class="line">  optional float eps = 3 [default = 1e-5];</div><div class="line">&#125;</div></pre></td></tr></table></figure></p><p>OK。现在可以进入BN的代码实现了。阅读大部分代码都没有什么难度，下面主要结合代码讲解<code>use_global_stats</code>变量的作用和均值（方差同理）的计算。由于均值和方差的计算原理相近，所以下面只会详细介绍均值的计算。</p><h3 id="SetUp"><a href="#SetUp" class="headerlink" title="SetUp"></a>SetUp</h3><p>BN层的SetUp代码如下。首先，会根据当前处于train还是test决定是否使用全局的统计量。如果prototxt文件中设置了<code>use_global_stats</code>标志，则会使用用户给定的配置。所以一般在使用BN时，无需对<code>use_global_stats</code>进行配置。</p><p>这里有一个地方容易迷惑。BN中要对样本的均值和方差进行统计，即我们需要两个blob来存储。但是从下面的代码可以看到，BN一共有3个blob作为参数。这里做一解释，主要参考了wiki的<a href="https://wiki2.org/en/Moving_average" target="_blank" rel="external">moving average条目</a>。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="keyword">void</span> BatchNormLayer&lt;Dtype&gt;::LayerSetUp(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,</div><div class="line">      <span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; top) &#123;</div><div class="line">  BatchNormParameter param = <span class="keyword">this</span>-&gt;layer_param_.batch_norm_param();</div><div class="line">  moving_average_fraction_ = param.moving_average_fraction();</div><div class="line">  <span class="comment">// 默认根据当前是否处在TEST模式而决定是否使用全局mean和var</span></div><div class="line">  use_global_stats_ = <span class="keyword">this</span>-&gt;phase_ == TEST;</div><div class="line">  <span class="keyword">if</span> (param.has_use_global_stats())</div><div class="line">    use_global_stats_ = param.use_global_stats();</div><div class="line">  <span class="comment">// 得到channels数量</span></div><div class="line">  <span class="comment">// 为了防止越界，首先检查输入是否为1D</span></div><div class="line">  <span class="keyword">if</span> (bottom[<span class="number">0</span>]-&gt;num_axes() == <span class="number">1</span>)</div><div class="line">    channels_ = <span class="number">1</span>;</div><div class="line">  <span class="keyword">else</span></div><div class="line">    channels_ = bottom[<span class="number">0</span>]-&gt;shape(<span class="number">1</span>);</div><div class="line">  eps_ = param.eps();</div><div class="line">  <span class="keyword">if</span> (<span class="keyword">this</span>-&gt;blobs_.size() &gt; <span class="number">0</span>) &#123;</div><div class="line">    LOG(INFO) &lt;&lt; <span class="string">"Skipping parameter initialization"</span>;</div><div class="line">  &#125; <span class="keyword">else</span> &#123;</div><div class="line">    <span class="comment">// 参数共3个</span></div><div class="line">    <span class="keyword">this</span>-&gt;blobs_.resize(<span class="number">3</span>);</div><div class="line">    <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; sz;</div><div class="line">    sz.push_back(channels_);</div><div class="line">    <span class="comment">// mean 和var都是1D的长度为channels的向量</span></div><div class="line">    <span class="comment">// 因为在规范化过程中，要逐channel进行，即：</span></div><div class="line">    <span class="comment">// for c in range(channels):</span></div><div class="line">    <span class="comment">//     x_hat[c] = (x[c] - mean[c]) / std[c]</span></div><div class="line">    <span class="keyword">this</span>-&gt;blobs_[<span class="number">0</span>].reset(<span class="keyword">new</span> Blob&lt;Dtype&gt;(sz));</div><div class="line">    <span class="keyword">this</span>-&gt;blobs_[<span class="number">1</span>].reset(<span class="keyword">new</span> Blob&lt;Dtype&gt;(sz));</div><div class="line">    <span class="comment">// 这里的解释见下</span></div><div class="line">    sz[<span class="number">0</span>] = <span class="number">1</span>;</div><div class="line">    <span class="keyword">this</span>-&gt;blobs_[<span class="number">2</span>].reset(<span class="keyword">new</span> Blob&lt;Dtype&gt;(sz));</div><div class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">3</span>; ++i) &#123;</div><div class="line">      caffe_set(<span class="keyword">this</span>-&gt;blobs_[i]-&gt;count(), Dtype(<span class="number">0</span>),</div><div class="line">                <span class="keyword">this</span>-&gt;blobs_[i]-&gt;mutable_cpu_data());</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">  <span class="comment">// Mask statistics from optimization by setting local learning rates</span></div><div class="line">  <span class="comment">// for mean, variance, and the bias correction to zero.</span></div><div class="line">  <span class="comment">// mean 和 std在训练的时候是不需要梯度下降来更新的，这里强制把其learning rate</span></div><div class="line">  <span class="comment">// 设置为0</span></div><div class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="keyword">this</span>-&gt;blobs_.size(); ++i) &#123;</div><div class="line">    <span class="keyword">if</span> (<span class="keyword">this</span>-&gt;layer_param_.param_size() == i) &#123;</div><div class="line">      ParamSpec* fixed_param_spec = <span class="keyword">this</span>-&gt;layer_param_.add_param();</div><div class="line">      fixed_param_spec-&gt;set_lr_mult(<span class="number">0.f</span>);</div><div class="line">    &#125; <span class="keyword">else</span> &#123;</div><div class="line">      CHECK_EQ(<span class="keyword">this</span>-&gt;layer_param_.param(i).lr_mult(), <span class="number">0.f</span>)</div><div class="line">          &lt;&lt; <span class="string">"Cannot configure batch normalization statistics as layer "</span></div><div class="line">          &lt;&lt; <span class="string">"parameters."</span>;</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure><p>在求取某个流数据（stream）的平均值的时候，常用的一种方法是滑动平均法，也就是使用系数$\alpha$来做平滑滤波，如下所示：</p><script type="math/tex; mode=display">S_t = \alpha Y_t + (1-\alpha) S_{t-1}</script><p>上面的式子等价于：</p><script type="math/tex; mode=display">S_t = \frac{\text{WeightedSum}_n}{\text{WeightedCount}_n}</script><p>其中，<script type="math/tex">\text{WeightedSum}_n = Y_t + (1-\alpha) \text{WeightedSum}_{n-1}</script></p><script type="math/tex; mode=display">\text{WeightedCount}_n = 1 + (1-\alpha) \text{WeightedCount}_{n-1}</script><p>而Caffe中BN的实现中，<code>blobs_[0]</code>和<code>blobs_[1]</code>中存储的实际是$\text{WeightedSum}_n$，而<code>blos_[2]</code>中存储的是$\text{WeightedCount}_n$。所以，真正的mean和var是两者相除的结果。即：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">mu = blobs_[0] / blobs_[2]</div><div class="line">var = blobs_[1] / blobs_[2]</div></pre></td></tr></table></figure></p><h3 id="Forward"><a href="#Forward" class="headerlink" title="Forward"></a>Forward</h3><p>下面是Forward CPU的代码。主要应该注意当前batch的mean和var的求法。<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="keyword">void</span> BatchNormLayer&lt;Dtype&gt;::Forward_cpu(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,</div><div class="line">    <span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; top) &#123;</div><div class="line">  <span class="keyword">const</span> Dtype* bottom_data = bottom[<span class="number">0</span>]-&gt;cpu_data();</div><div class="line">  Dtype* top_data = top[<span class="number">0</span>]-&gt;mutable_cpu_data();</div><div class="line">  <span class="keyword">int</span> num = bottom[<span class="number">0</span>]-&gt;shape(<span class="number">0</span>);</div><div class="line">  <span class="keyword">int</span> spatial_dim = bottom[<span class="number">0</span>]-&gt;count()/(bottom[<span class="number">0</span>]-&gt;shape(<span class="number">0</span>)*channels_);</div><div class="line"></div><div class="line">  <span class="comment">// 如果不是就地操作，首先将bottom的数据复制到top</span></div><div class="line">  <span class="keyword">if</span> (bottom[<span class="number">0</span>] != top[<span class="number">0</span>]) &#123;</div><div class="line">    caffe_copy(bottom[<span class="number">0</span>]-&gt;count(), bottom_data, top_data);</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="comment">// 如果使用全局统计量，我们需要先计算出真正的mean和var</span></div><div class="line">  <span class="keyword">if</span> (use_global_stats_) &#123;</div><div class="line">    <span class="comment">// use the stored mean/variance estimates.</span></div><div class="line">    <span class="keyword">const</span> Dtype scale_factor = <span class="keyword">this</span>-&gt;blobs_[<span class="number">2</span>]-&gt;cpu_data()[<span class="number">0</span>] == <span class="number">0</span> ?</div><div class="line">        <span class="number">0</span> : <span class="number">1</span> / <span class="keyword">this</span>-&gt;blobs_[<span class="number">2</span>]-&gt;cpu_data()[<span class="number">0</span>];</div><div class="line">    <span class="comment">// mean = blobs[0] / blobs[2]</span></div><div class="line">    caffe_cpu_scale(variance_.count(), scale_factor,</div><div class="line">        <span class="keyword">this</span>-&gt;blobs_[<span class="number">0</span>]-&gt;cpu_data(), mean_.mutable_cpu_data());</div><div class="line">    <span class="comment">// var = blobs[1] / blobs[2]</span></div><div class="line">    caffe_cpu_scale(variance_.count(), scale_factor,</div><div class="line">        <span class="keyword">this</span>-&gt;blobs_[<span class="number">1</span>]-&gt;cpu_data(), variance_.mutable_cpu_data());</div><div class="line">  &#125; <span class="keyword">else</span> &#123;</div><div class="line">    <span class="comment">// 不使用全局统计量时，我们要根据当前batch的mean和var做规范化</span></div><div class="line">    <span class="comment">// compute mean</span></div><div class="line">    <span class="comment">// spatial_sum_multiplier_是全1向量</span></div><div class="line">    <span class="comment">// batch_sum_multiplier_也是全1向量</span></div><div class="line">    <span class="comment">// gemv做矩阵与向量相乘 y = alpha*A*x + beta*y。</span></div><div class="line">    <span class="comment">// 下面式子是将bottom_data这个矩阵与一个全1向量相乘，</span></div><div class="line">    <span class="comment">// 相当于是在统计行和。</span></div><div class="line">    <span class="comment">// 注意第二个参数channels_ * num指矩阵的行数，第三个参数是矩阵的列数</span></div><div class="line">    <span class="comment">// 所以这是在计算每个channel的feature map的和</span></div><div class="line">    <span class="comment">// 结果out[n][c]是指输入第n个sample的第c个channel的和</span></div><div class="line">    <span class="comment">// 同时，传入了 1. / (num * spatial_dim) 作为因子乘到结果上面，作用见下面</span></div><div class="line">    caffe_cpu_gemv&lt;Dtype&gt;(CblasNoTrans, channels_ * num, spatial_dim,</div><div class="line">        <span class="number">1.</span> / (num * spatial_dim), bottom_data,</div><div class="line">        spatial_sum_multiplier_.cpu_data(), <span class="number">0.</span>,</div><div class="line">        num_by_chans_.mutable_cpu_data());</div><div class="line">    <span class="comment">// 道理和上面相同，注意下面通过传入CblasTrans，指定了矩阵要转置。所以是在求列和</span></div><div class="line">    <span class="comment">// 这样，就求出了各个channel的和。</span></div><div class="line">    <span class="comment">// 上面不是已经除了 num * spatial_dim 吗？这就是求和元素的总数量</span></div><div class="line">    <span class="comment">// 到此，我们就完成了对当前batch的平均值的求解</span></div><div class="line">    caffe_cpu_gemv&lt;Dtype&gt;(CblasTrans, num, channels_, <span class="number">1.</span>,</div><div class="line">        num_by_chans_.cpu_data(), batch_sum_multiplier_.cpu_data(), <span class="number">0.</span>,</div><div class="line">        mean_.mutable_cpu_data());</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="comment">// subtract mean</span></div><div class="line">  <span class="comment">// gemm是在做矩阵与矩阵相乘 C = alpha*A*B + beta*C</span></div><div class="line">  <span class="comment">// 下面这个是在做broadcasting subtraction</span></div><div class="line">  caffe_cpu_gemm&lt;Dtype&gt;(CblasNoTrans, CblasNoTrans, num, channels_, <span class="number">1</span>, <span class="number">1</span>,</div><div class="line">      batch_sum_multiplier_.cpu_data(), mean_.cpu_data(), <span class="number">0.</span>,</div><div class="line">      num_by_chans_.mutable_cpu_data());</div><div class="line">  caffe_cpu_gemm&lt;Dtype&gt;(CblasNoTrans, CblasNoTrans, channels_ * num,</div><div class="line">      spatial_dim, <span class="number">1</span>, <span class="number">-1</span>, num_by_chans_.cpu_data(),</div><div class="line">      spatial_sum_multiplier_.cpu_data(), <span class="number">1.</span>, top_data);</div><div class="line"></div><div class="line">  <span class="comment">// 计算当前的var</span></div><div class="line">  <span class="keyword">if</span> (!use_global_stats_) &#123;</div><div class="line">    <span class="comment">// compute variance using var(X) = E((X-EX)^2)</span></div><div class="line">    caffe_sqr&lt;Dtype&gt;(top[<span class="number">0</span>]-&gt;count(), top_data,</div><div class="line">                     temp_.mutable_cpu_data());  <span class="comment">// (X-EX)^2</span></div><div class="line">    caffe_cpu_gemv&lt;Dtype&gt;(CblasNoTrans, channels_ * num, spatial_dim,</div><div class="line">        <span class="number">1.</span> / (num * spatial_dim), temp_.cpu_data(),</div><div class="line">        spatial_sum_multiplier_.cpu_data(), <span class="number">0.</span>,</div><div class="line">        num_by_chans_.mutable_cpu_data());</div><div class="line">    caffe_cpu_gemv&lt;Dtype&gt;(CblasTrans, num, channels_, <span class="number">1.</span>,</div><div class="line">        num_by_chans_.cpu_data(), batch_sum_multiplier_.cpu_data(), <span class="number">0.</span>,</div><div class="line">        variance_.mutable_cpu_data());  <span class="comment">// E((X_EX)^2)</span></div><div class="line"></div><div class="line">    <span class="comment">// compute and save moving average</span></div><div class="line">    <span class="comment">// 做滑动平均，更新全局统计量，这里可以参见上面的式子</span></div><div class="line">    <span class="keyword">this</span>-&gt;blobs_[<span class="number">2</span>]-&gt;mutable_cpu_data()[<span class="number">0</span>] *= moving_average_fraction_;</div><div class="line">    <span class="keyword">this</span>-&gt;blobs_[<span class="number">2</span>]-&gt;mutable_cpu_data()[<span class="number">0</span>] += <span class="number">1</span>;</div><div class="line">    caffe_cpu_axpby(mean_.count(), Dtype(<span class="number">1</span>), mean_.cpu_data(),</div><div class="line">        moving_average_fraction_, <span class="keyword">this</span>-&gt;blobs_[<span class="number">0</span>]-&gt;mutable_cpu_data());</div><div class="line">    <span class="keyword">int</span> m = bottom[<span class="number">0</span>]-&gt;count()/channels_;</div><div class="line">    Dtype bias_correction_factor = m &gt; <span class="number">1</span> ? Dtype(m)/(m<span class="number">-1</span>) : <span class="number">1</span>;</div><div class="line">    caffe_cpu_axpby(variance_.count(), bias_correction_factor,</div><div class="line">        variance_.cpu_data(), moving_average_fraction_,</div><div class="line">        <span class="keyword">this</span>-&gt;blobs_[<span class="number">1</span>]-&gt;mutable_cpu_data());</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="comment">// normalize variance</span></div><div class="line">  caffe_add_scalar(variance_.count(), eps_, variance_.mutable_cpu_data());</div><div class="line">  caffe_sqrt(variance_.count(), variance_.cpu_data(),</div><div class="line">             variance_.mutable_cpu_data());</div><div class="line"></div><div class="line">  <span class="comment">// replicate variance to input size</span></div><div class="line">  <span class="comment">// 同样是在做broadcasting</span></div><div class="line">  caffe_cpu_gemm&lt;Dtype&gt;(CblasNoTrans, CblasNoTrans, num, channels_, <span class="number">1</span>, <span class="number">1</span>,</div><div class="line">      batch_sum_multiplier_.cpu_data(), variance_.cpu_data(), <span class="number">0.</span>,</div><div class="line">      num_by_chans_.mutable_cpu_data());</div><div class="line">  caffe_cpu_gemm&lt;Dtype&gt;(CblasNoTrans, CblasNoTrans, channels_ * num,</div><div class="line">      spatial_dim, <span class="number">1</span>, <span class="number">1.</span>, num_by_chans_.cpu_data(),</div><div class="line">      spatial_sum_multiplier_.cpu_data(), <span class="number">0.</span>, temp_.mutable_cpu_data());</div><div class="line">  caffe_div(temp_.count(), top_data, temp_.cpu_data(), top_data);</div><div class="line">  <span class="comment">// TODO(cdoersch): The caching is only needed because later in-place layers</span></div><div class="line">  <span class="comment">//                 might clobber the data.  Can we skip this if they won't?</span></div><div class="line">  caffe_copy(x_norm_.count(), top_data,</div><div class="line">      x_norm_.mutable_cpu_data());</div><div class="line">&#125;</div></pre></td></tr></table></figure></p><p>由上面的计算过程不难得出，当经过很多轮迭代之后，<code>blobs_[2]</code>的值会趋于稳定。下面我们使用$m_t$来表示第$t$轮迭代后的<code>blobs_[2]</code>的值，也就是$\text{WeightedCount}_n$，使用$\alpha$表示<code>moving_average_fraction_</code>，那么我们有：</p><script type="math/tex; mode=display">m_t = 1 + \alpha m_{t-1}</script><p>可以求取$m_t$的通项后令$t=\infty$，可以得到，$m_{\infty}=\frac{1}{1-\alpha}$。</p><h3 id="Backward"><a href="#Backward" class="headerlink" title="Backward"></a>Backward</h3><p>在做BP的时候，我们需要分情况讨论。</p><ul><li>当<code>use_global_stats == true</code>的时候，BN所做的操作是一个线性变换<script type="math/tex; mode=display">BN(x) = \frac{x-\mu}{\sqrt{Var}}</script>所以<script type="math/tex; mode=display">\frac{\partial L}{\partial x} = \frac{1}{\sqrt{Var}}\frac{\partial L}{\partial y}</script></li></ul><p>对应的代码如下。其中，<code>temp_</code>是broadcasting之后的输入<code>x</code>的标准差（见上面<code>Forward</code>部分的代码最后），做逐元素的除法即可。<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">if</span> (use_global_stats_) &#123;</div><div class="line">  caffe_div(temp_.count(), top_diff, temp_.cpu_data(), bottom_diff);</div><div class="line">  <span class="keyword">return</span>;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p><ul><li>当<code>use_global_stats == false</code>的时候，BN所做操作虽然也是上述线性变换。但是注意，现在式子里面的$\mu$和$Var(x)$都是当前batch计算出来的，也就是它们都是输入<code>x</code>的函数。所以就麻烦了不少。这里我并没有推导，而是看了<a href="https://kevinzakka.github.io/2016/09/14/batch_normalization/" target="_blank" rel="external">这篇博客</a>，里面有详细的推导过程，写的很易懂。我将最后的结果贴在下面，对计算过程感兴趣的可以去原文章查看。<br><img src="/img/caffe_bn_bp_of_bn.jpg" alt="BP的推导结果"></li></ul><p>我们使用$y$来代替上面的$\hat{x_i}$，并且上下同时除以$m$，就可以得到Caffe BN代码中所给的BP式子：</p><script type="math/tex; mode=display">\frac{\partial f}{\partial x_i} = \frac{\frac{\partial f}{\partial y}-E[\frac{\partial f}{\partial y}]-yE[\frac{\partial f}{\partial y}y]}{\sqrt{\sigma^2+\epsilon}}</script><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// if Y = (X-mean(X))/(sqrt(var(X)+eps)), then</span></div><div class="line"><span class="comment">//</span></div><div class="line"><span class="comment">// dE(Y)/dX =</span></div><div class="line"><span class="comment">//   (dE/dY - mean(dE/dY) - mean(dE/dY \cdot Y) \cdot Y)</span></div><div class="line"><span class="comment">//     ./ sqrt(var(X) + eps)</span></div><div class="line"><span class="comment">//</span></div><div class="line"><span class="comment">// where \cdot and ./ are hadamard product and elementwise division,</span></div><div class="line"><span class="comment">// respectively, dE/dY is the top diff, and mean/var/sum are all computed</span></div><div class="line"><span class="comment">// along all dimensions except the channels dimension.  In the above</span></div><div class="line"><span class="comment">// equation, the operations allow for expansion (i.e. broadcast) along all</span></div><div class="line"><span class="comment">// dimensions except the channels dimension where required.</span></div></pre></td></tr></table></figure><p>下面的代码部分就是实现上面这个式子的内容，注释很详细，要解决的一个比较棘手的问题就是broadcasting，这个有兴趣可以看一下。对Caffe中BN的介绍就到这里。下面介绍与BN经常成对出现的Scale层。</p><h2 id="Scale层的实现"><a href="#Scale层的实现" class="headerlink" title="Scale层的实现"></a>Scale层的实现</h2><p>Caffe中将后续的线性变换使用单独的Scale层实现。Caffe中的Scale可以根据需要配置成不同的模式：</p><ul><li>当输入blob为两个时，计算输入blob的逐元素乘的结果（维度不相同时，第二个blob可以做broadcasting）。</li><li>当输入blob为一个时，计算输入blob与一个可学习参数<code>gamma</code>的按元素相乘结果。</li><li>当设置<code>bias_term: true</code>时，添加一个偏置项。</li></ul><p>用于BN的线性变换的计算方法很直接，这里不再多说了。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这篇博客总结了Caffe中BN的实现。&lt;br&gt;
    
    </summary>
    
    
      <category term="caffe" scheme="https://xmfbit.github.io/tags/caffe/"/>
    
  </entry>
  
  <entry>
    <title>shell编程</title>
    <link href="https://xmfbit.github.io/2017/11/10/shell-programming/"/>
    <id>https://xmfbit.github.io/2017/11/10/shell-programming/</id>
    <published>2017-11-10T05:06:30.000Z</published>
    <updated>2018-03-15T06:54:37.462Z</updated>
    
    <content type="html"><![CDATA[<p>介绍基本的shell编程方法，参考的教程是<a href="http://www.freeos.com/guides/lsst/" target="_blank" rel="external">Linux Shell Scripting Tutorial, A Beginner’s handbook</a>。<br><img src="/img/shell-programming-bash-logo.png" alt="Bash Logo"><br><a id="more"></a></p><h2 id="变量"><a href="#变量" class="headerlink" title="变量"></a>变量</h2><p>变量是代码的基本组成元素。可以认为shell中的变量类型都是字符串。</p><p>shell中的变量可以分为两类：系统变量和用户自定义变量。下面分别进行介绍。</p><p>在代码中使用变量值的时候，需要在前面加上<code>$</code>。<code>echo</code>命令可以在控制台打印相应输出。所以使用<code>echo $var</code>就可以输出变量<code>var</code>的值。</p><h3 id="系统变量"><a href="#系统变量" class="headerlink" title="系统变量"></a>系统变量</h3><p>系统变量是指Linux中自带的一些变量。例如<code>HOME</code>,<code>PATH</code>等。其中<code>PATH</code>又叫环境变量。更多的系统变量见下表：<br><img src="/img/shell-programming-system-variables.jpg" alt="系统变量列表"></p><h3 id="用户定义的变量"><a href="#用户定义的变量" class="headerlink" title="用户定义的变量"></a>用户定义的变量</h3><p>用户自定义变量是用户命名并赋值的变量。使用下面的方法定义：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 注意不要在等号两边插入空格</span></div><div class="line">name=value</div><div class="line"><span class="comment"># 如 n=10</span></div></pre></td></tr></table></figure><h3 id="局部变量和全局变量"><a href="#局部变量和全局变量" class="headerlink" title="局部变量和全局变量"></a>局部变量和全局变量</h3><p>局部变量是指在当前代码块内可见的变量，使用<code>local</code>声明。例如下面的代码，将依次输出：111, 222, 111.<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#! /bin/sh</span></div><div class="line">num=111 <span class="comment"># 全局变量</span></div><div class="line"><span class="function"><span class="title">func1</span></span>()</div><div class="line">&#123;</div><div class="line">  <span class="built_in">local</span> num=222 <span class="comment"># 局部变量</span></div><div class="line">  <span class="built_in">echo</span> <span class="variable">$num</span></div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="built_in">echo</span> <span class="string">"before---<span class="variable">$num</span>"</span></div><div class="line">func1</div><div class="line"><span class="built_in">echo</span> <span class="string">"after---<span class="variable">$num</span>"</span></div></pre></td></tr></table></figure></p><h3 id="变量之间的运算"><a href="#变量之间的运算" class="headerlink" title="变量之间的运算"></a>变量之间的运算</h3><p>使用<code>expr</code>可以进行变量之间的运算，如下所示：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 注意要在操作符两边空余空格</span></div><div class="line">expr 1 + 3</div><div class="line"><span class="comment"># 由于*是特殊字符，所以乘法要使用转义</span></div><div class="line">expr 10 \* 2</div></pre></td></tr></table></figure><h3 id="和””"><a href="#和””" class="headerlink" title="``和””"></a>``和””</h3><p>使用``（也就是TAB键上面的那个）包起来的部分，是可执行的命令。而使用””（引号）包起来的部分，是字符串。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">a=`expr 10 \* 3`</div><div class="line"><span class="comment"># output: 3</span></div><div class="line"><span class="built_in">echo</span> <span class="variable">$a</span></div><div class="line"><span class="comment"># output: a</span></div><div class="line"><span class="built_in">echo</span> a</div><div class="line"><span class="comment"># output: expr 10 \* 3</span></div><div class="line">a=<span class="string">"expr 10 \* 3"</span></div><div class="line"><span class="built_in">echo</span> <span class="variable">$a</span></div></pre></td></tr></table></figure><p>另外，使用””（双引号）括起来的字符串会发生变量替换，而用’’（单引号）括起来的字符串则不会。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">a=1</div><div class="line"><span class="built_in">echo</span> <span class="string">"<span class="variable">$a</span>"</span>  <span class="comment"># 输出 1</span></div><div class="line"><span class="built_in">echo</span> <span class="string">'$a'</span>  <span class="comment"># 输出 $a</span></div></pre></td></tr></table></figure><h3 id="读取输入"><a href="#读取输入" class="headerlink" title="读取输入"></a>读取输入</h3><p>使用<code>read var1, var2, ...</code>的方式从键盘的输入读取变量的值。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># input a=1</span></div><div class="line"><span class="built_in">read</span> a</div><div class="line"><span class="comment"># ouptut: 2</span></div><div class="line"><span class="built_in">echo</span> `expr <span class="variable">$a</span> + 1`</div></pre></td></tr></table></figure><h2 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h2><h3 id="命令的返回值"><a href="#命令的返回值" class="headerlink" title="命令的返回值"></a>命令的返回值</h3><p>当bash命令成功执行后，返回给系统的返回值为<code>0</code>；否则为非零。可以据此判断上步操作的状态。使用<code>$?</code>可以取出上一步执行的返回值。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 将echo 错输为ecoh</span></div><div class="line">ecoh <span class="string">"hello"</span></div><div class="line"><span class="comment"># output: 非零(127)</span></div><div class="line"><span class="built_in">echo</span> $?</div><div class="line"><span class="comment"># output: 0</span></div><div class="line"><span class="built_in">echo</span> $?</div></pre></td></tr></table></figure><h3 id="通配符"><a href="#通配符" class="headerlink" title="通配符"></a>通配符</h3><p>通配符是指<code>*</code>,<code>?</code>和<code>[...]</code>这三类。</p><p><code>*</code>可以匹配任意多的字符，<code>?</code>用来匹配一个字符。<code>[...]</code>用来匹配括号内的字符。见下表。<br><img src="/img/shell-programming-wild-cards.jpg" alt="通配符"></p><p><code>[...]</code>表示法还有如下变形：</p><ul><li>使用<code>-</code>用来指示范围。如<code>[a-z]</code>，表示<code>a</code>到<code>z</code>间任意一个字符。</li><li>使用<code>^</code>或<code>!</code>表示取反。如<code>[!a-p]</code>表示除了<code>a</code>到<code>p</code>间字符的其他字符。</li></ul><h3 id="输入输出重定向"><a href="#输入输出重定向" class="headerlink" title="输入输出重定向"></a>输入输出重定向</h3><p>重定向是指改变命令的输出位置。使用<code>&gt;</code>进行输出重定向。使用<code>&lt;</code>进行输入重定向。例如，<code>ls -l &gt; a.txt</code>，将本目录下的文件信息输出到文本文件<code>a.txt</code>中，而不再输出到终端。</p><p>此外，<code>&gt;&gt;</code>同样是输出重定向。但是它会在文件末尾追加写入，不会覆盖文件的原有内容。</p><p>搭配使用<code>&lt;</code>和<code>&gt;</code>可以做文件处理。例如，<code>tr group1 group2</code>命令可以将<code>group1</code>中的字符变换为<code>group2</code>中对应位置的字符。使用如下命令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">tr <span class="string">"[a-z]"</span> <span class="string">"A-Z"</span> &lt; ori.txt &gt; out.txt</div></pre></td></tr></table></figure><p>可以将<code>ori.txt</code>中的小写字母转换为大写字母输出到<code>out.txt</code>中。</p><h3 id="管道（pipeline）"><a href="#管道（pipeline）" class="headerlink" title="管道（pipeline）"></a>管道（pipeline）</h3><p>管道<code>|</code>可以将第一个程序的输出作为第二个程序的输入。例如：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">cat ori.txt | tr <span class="string">"[a-z]"</span> <span class="string">"A-Z"</span></div></pre></td></tr></table></figure><p>会将<code>ori.txt</code>中的小写字母转换为大写，并在终端输出。</p><h3 id="过滤器（Filter）"><a href="#过滤器（Filter）" class="headerlink" title="过滤器（Filter）"></a>过滤器（Filter）</h3><p>Filter是指那些输入和输出都是控制台的命令。通过Filter和输入输出重定向，可以很方便地对文件内容进行整理。例如：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sort &lt; names.txt | uniq &gt; u_names.txt</div></pre></td></tr></table></figure><p><code>uniq</code>命令可以实现去重，但是需要首先对输入数据进行排序。上面的Filter可以将输入文件<code>names.txt</code>中的行文本去重后输出到<code>u_names.txt</code>中去。</p><h2 id="控制流"><a href="#控制流" class="headerlink" title="控制流"></a>控制流</h2><h3 id="if-条件控制"><a href="#if-条件控制" class="headerlink" title="if 条件控制"></a>if 条件控制</h3><p>在bash中使用<code>if</code>条件控制的语法和MATLAB等很像，要在末尾加上类似<code>end</code>的指示符，如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">if</span> condition</div><div class="line"><span class="keyword">then</span> </div><div class="line">XXX</div><div class="line"><span class="keyword">fi</span></div></pre></td></tr></table></figure><p>或者加上<code>else</code>，使用如下的形式：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">if</span> condition</div><div class="line"><span class="keyword">then</span></div><div class="line">    <span class="keyword">do</span> something</div><div class="line"><span class="keyword">elif</span> condition</div><div class="line"><span class="keyword">then</span></div><div class="line">    <span class="keyword">do</span> something</div><div class="line"><span class="keyword">else</span></div><div class="line">    <span class="keyword">do</span> something</div><div class="line"><span class="keyword">fi</span></div></pre></td></tr></table></figure></p><p>那么，如何做逻辑运算呢？需要借助<code>test</code>关键字。</p><p>对于整数来说，我们可以使用<code>if test op1 oprator op2</code>的方式，判断操作数<code>op1</code>和<code>op2</code>的大小关系。其中，<code>operator</code>可以是<code>-gt</code>，<code>-eq</code>等。</p><p>或者另一种写法：<code>if [ op1 operator op2 ]</code>，但是注意后者<code>[]</code>与操作数之间有空格。<br>如下表所示（点击可放大）：</p><p><img src="/img/shell-programming-if-operators.jpg" alt="比较整数的逻辑运算"></p><p>对于字符串，支持的逻辑判断如下：<br><img src="/img/bash-programming-comparing-string.jpg" alt="比较字符串的逻辑运算"></p><p>举个例子，我们想判断输入的值是否为1或2，可以使用如下的脚本。注意<code>[]</code>的两边一定要加空格。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#! /bin/bash</span></div><div class="line">a=1</div><div class="line"><span class="keyword">if</span> [ <span class="variable">$1</span>=<span class="variable">$a</span> ]</div><div class="line"><span class="keyword">then</span></div><div class="line">    <span class="built_in">echo</span> <span class="string">"you input 1"</span></div><div class="line"><span class="keyword">elif</span> [ <span class="variable">$1</span>=2 ]</div><div class="line"><span class="keyword">then</span></div><div class="line">    <span class="built_in">echo</span> <span class="string">"you input 2"</span></div><div class="line"><span class="keyword">else</span></div><div class="line">    <span class="built_in">echo</span> <span class="string">"you input <span class="variable">$1</span>"</span></div><div class="line"><span class="keyword">fi</span></div></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;介绍基本的shell编程方法，参考的教程是&lt;a href=&quot;http://www.freeos.com/guides/lsst/&quot;&gt;Linux Shell Scripting Tutorial, A Beginner’s handbook&lt;/a&gt;。&lt;br&gt;&lt;img src=&quot;/img/shell-programming-bash-logo.png&quot; alt=&quot;Bash Logo&quot;&gt;&lt;br&gt;
    
    </summary>
    
    
      <category term="linux" scheme="https://xmfbit.github.io/tags/linux/"/>
    
      <category term="shell" scheme="https://xmfbit.github.io/tags/shell/"/>
    
  </entry>
  
  <entry>
    <title>过秦论</title>
    <link href="https://xmfbit.github.io/2017/10/27/fuck-gfw/"/>
    <id>https://xmfbit.github.io/2017/10/27/fuck-gfw/</id>
    <published>2017-10-27T01:56:00.000Z</published>
    <updated>2018-03-15T06:54:37.456Z</updated>
    
    <content type="html"><![CDATA[<p>秦以耕战立国，起于西北，从边陲弱小诸侯，借以天时地利，用商鞅、李斯，举法家大旗。而后逐鹿中原，坑赵括，灌大梁，掳六国贵族，结束战乱，建立大一统帝国。然而，强秦却二世而亡。“始皇之心，自以为关中之固，金城千里，子孙帝王万世之业也。”封建王朝的皇帝总觉得自己的国祚能够绵延不绝，历史却一次次地打他们的脸。<br><img src="/img/just-a-joke.png" alt="just a joke"></p><a id="more"></a><p>　　秦孝公据崤函之固，拥雍州之地，君臣固守以窥周室，有席卷天下，包举宇内，囊括四海之意，并吞八荒之心。当是时也，商君佐之，内立法度，务耕织，修守战之具；外连衡而斗诸侯。于是秦人拱手而取西河之外。 </p><p>　　孝公既没，惠文、武、昭襄蒙故业，因遗策，南取汉中，西举巴、蜀，东割膏腴之地，北收要害之郡。诸侯恐惧，会盟而谋弱秦，不爱珍器重宝肥饶之地，以致天下之士，合从缔交，相与为一。当此之时，齐有孟尝，赵有平原，楚有春申，魏有信陵。此四君者，皆明智而忠信，宽厚而爱人，尊贤而重士，约从离衡，兼韩、魏、燕、楚、齐、赵、宋、卫、中山之众。于是六国之士，有宁越、徐尚、苏秦、杜赫之属为之谋，齐明、周最、陈轸、召滑、楼缓、翟景、苏厉、乐毅之徒通其意，吴起、孙膑、带佗、倪良、王廖、田忌、廉颇、赵奢之伦制其兵。尝以十倍之地，百万之众，叩关而攻秦。秦人开关延敌，九国之师，逡巡而不敢进。秦无亡矢遗镞之费，而天下诸侯已困矣。于是从散约败，争割地而赂秦。秦有余力而制其弊，追亡逐北，伏尸百万，流血漂橹。因利乘便，宰割天下，分裂山河。强国请服，弱国入朝。 </p><p>　　延及孝文王、庄襄王，享国之日浅，国家无事。及至始皇，奋六世之余烈，振长策而御宇内，吞二周而亡诸侯，履至尊而制六合，执敲扑而鞭笞天下，威振四海。南取百越之地，以为桂林、象郡；百越之君，俯首系颈，委命下吏。乃使蒙恬北筑长城而守藩篱，却匈奴七百余里。胡人不敢南下而牧马，士不敢弯弓而报怨。于是废先王之道，焚百家之言，以愚黔首；隳名城，杀豪杰，收天下之兵，聚之咸阳，销锋镝，铸以为金人十二，以弱天下之民。然后践华为城，因河为池，据亿丈之城，临不测之渊，以为固。良将劲弩守要害之处，信臣精卒陈利兵而谁何。天下已定，始皇之心，自以为关中之固，金城千里，子孙帝王万世之业也。</p><p>　　始皇既没，余威震于殊俗。然陈涉瓮牖绳枢之子，氓隶之人，而迁徙之徒也；才能不及中人，非有仲尼、墨翟之贤，陶朱、猗顿之富；蹑足行伍之间，而倔起阡陌之中，率疲弊之卒，将数百之众，转而攻秦，斩木为兵，揭竿为旗，天下云集响应，赢粮而景从。山东豪俊遂并起而亡秦族矣。 </p><p>　　且夫天下非小弱也，雍州之地，崤函之固，自若也。陈涉之位，非尊于齐、楚、燕、赵、韩、魏、宋、卫、中山之君也；锄櫌棘矜，非铦于钩戟长铩也；谪戍之众，非抗于九国之师也；深谋远虑，行军用兵之道，非及向时之士也。然而成败异变，功业相反，何也？试使山东之国与陈涉度长絜大，比权量力，则不可同年而语矣。然秦以区区之地，致万乘之势，序八州而朝同列，百有余年矣；然后以六合为家，崤函为宫；一夫作难而七庙隳，身死人手，为天下笑者，何也？仁义不施而攻守之势异也。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;秦以耕战立国，起于西北，从边陲弱小诸侯，借以天时地利，用商鞅、李斯，举法家大旗。而后逐鹿中原，坑赵括，灌大梁，掳六国贵族，结束战乱，建立大一统帝国。然而，强秦却二世而亡。“始皇之心，自以为关中之固，金城千里，子孙帝王万世之业也。”封建王朝的皇帝总觉得自己的国祚能够绵延不绝，历史却一次次地打他们的脸。&lt;br&gt;&lt;img src=&quot;/img/just-a-joke.png&quot; alt=&quot;just a joke&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="扯淡" scheme="https://xmfbit.github.io/tags/%E6%89%AF%E6%B7%A1/"/>
    
  </entry>
  
  <entry>
    <title>Ubuntu/Mac 工具软件列表</title>
    <link href="https://xmfbit.github.io/2017/10/22/useful-tools-list/"/>
    <id>https://xmfbit.github.io/2017/10/22/useful-tools-list/</id>
    <published>2017-10-22T12:12:09.000Z</published>
    <updated>2018-03-15T06:54:37.463Z</updated>
    
    <content type="html"><![CDATA[<p>工作环境大部分在Ubuntu和MacOS上进行，这里是一个我觉得在这两个平台上很有用的工具的整理列表，大部分都可以在两个系统上找到。这里并不会给出具体安装方式，因为最好的文档总是软件的官方Document或者GitHub的README。<br><a id="more"></a></p><h2 id="zsh和Oh-my-zsh"><a href="#zsh和Oh-my-zsh" class="headerlink" title="zsh和Oh-my-zsh"></a>zsh和Oh-my-zsh</h2><p>如果经常在终端敲命令而且还在用系统自带的Bash？可以考虑试一下zsh替代bash，并使用<a href="https://github.com/robbyrussell/oh-my-zsh" target="_blank" rel="external">oh-my-zsh</a>武装zsh。</p><p>关于oh-my-zsh的帖子网上已经有很多，不过我还并没有用到太多的功能。oh-my-zsh中可以配置插件，不过我只是使用了<code>colored-man-pages</code>。顾名思义，它可以将使用<code>man</code>查询时的页面彩色输出。如下所示。<br><img src="/img/useful_tools_colored_man_pages.jpg" alt="彩色的cp man页面"></p><h2 id="autojump"><a href="#autojump" class="headerlink" title="autojump"></a>autojump</h2><p>使用<a href="https://github.com/wting/autojump" target="_blank" rel="external">autojump</a>，可以很方便地在已经访问过的文件夹间快速跳转。甚至都不需要输入目标文件夹的全名，支持自动联想。</p><p>除了自动跳转功能，我还将其作为终端到文件资源管理器(Mac: Finder)的跳转功能。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"># 跳转到path并使用文件资源管理器打开</div><div class="line">jo path</div></pre></td></tr></table></figure></p><h2 id="tldr"><a href="#tldr" class="headerlink" title="tldr"></a>tldr</h2><p><a href="https://github.com/tldr-pages/tldr" target="_blank" rel="external">tldr</a> (too long don’t read)是一款能够给出bash命令常用功能的工具。在Linux系统中，很多命令都有一长串参数。这其中很多都是不常用的。而我们使用时，常常是使用某几个常见的功能选项。tldr就能够给出命令的简要描述和例子。</p><p>例如，使用其查询<code>tar</code>的常用方法：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line">tldr tar</div><div class="line"><span class="comment"># output</span></div><div class="line">tar</div><div class="line"></div><div class="line">Archiving utility.</div><div class="line">Often combined with a compression method, such as gzip or bzip.</div><div class="line"></div><div class="line">- Create an archive from files:</div><div class="line">    tar cf target.tar file1 file2 file3</div><div class="line"></div><div class="line">- Create a gzipped archive:</div><div class="line">    tar czf target.tar.gz file1 file2 file3</div><div class="line"></div><div class="line">- Extract an archive <span class="keyword">in</span> a target folder:</div><div class="line">    tar xf source.tar -C folder</div><div class="line"></div><div class="line">- Extract a gzipped archive <span class="keyword">in</span> the current directory:</div><div class="line">    tar xzf source.tar.gz</div><div class="line"></div><div class="line">- Extract a bzipped archive <span class="keyword">in</span> the current directory:</div><div class="line">    tar xjf source.tar.bz2</div><div class="line"></div><div class="line">- Create a compressed archive, using archive suffix to determine the compression program:</div><div class="line">    tar caf target.tar.xz file1 file2 file3</div><div class="line"></div><div class="line">- List the contents of a tar file:</div><div class="line">    tar tvf source.tar</div></pre></td></tr></table></figure><p>tldr支持多种语言，我使用了python包安装。但是不知为何，tldr在我这里总显示奇怪的背景颜色，看上去很别扭。所以我实际使用的是<a href="https://github.com/lord63/tldr.py" target="_blank" rel="external">tldr-py</a>。</p><h2 id="tmux"><a href="#tmux" class="headerlink" title="tmux"></a>tmux</h2><p>用SSH登录到服务器上时，如果网络连接不稳定或是自己的主机意外断电，会造成正在跑的代码死掉。因为进程是依附于SSH的会话Session的。tmux是一个终端的“分线器”，可以很方便地将正在进行的终端会话detach掉，使其转入后台运行。正是有这一特点，所以我们可以在SSH会话时，新建tmux会话，在其中跑一些耗时很长的代码，而不必担心SSH掉线。当然，也可以将tmux作为一款终端多任务的管理软件，方便地在多个任务中进行跳转。不过这个功能，我更加常用的是下面的guake。</p><p>虽然Ubuntu14.04可以通过<code>apt-get</code>的方式安装tmux，不过为了能够使用一款好用的配置<a href="https://github.com/gpakosz/.tmux" target="_blank" rel="external">oh-my-tmux</a>（要求tmux&gt;=2.1），还是推荐去GitHub上自己编译安装<a href="https://github.com/tmux/tmux" target="_blank" rel="external">tmux</a>。</p><h2 id="guake"><a href="#guake" class="headerlink" title="guake"></a>guake</h2><p><a href="https://github.com/Guake/guake" target="_blank" rel="external">guake</a>是一款Ubuntu上可以方便呼出终端的应用（按下F12，终端将以全屏的方式铺满桌面，F11可以切换全屏或半屏）。</p><h2 id="Dash-Zeal"><a href="#Dash-Zeal" class="headerlink" title="Dash/Zeal"></a>Dash/Zeal</h2><p>Dash是Mac上一款用于查询API文档的软件。在Ubuntu或Windows上，我们可以使用Zeal这个替代软件。Zeal和Dash基本上无缝衔接，但是却是免费的（Mac上的软件真是好贵。。。）。之前我已经写过一篇<a href="https://xmfbit.github.io/2017/08/26/doc2dash-usage/">博客</a>，介绍如何自己制作文档导入Zeal中。</p><h2 id="sshfs"><a href="#sshfs" class="headerlink" title="sshfs"></a>sshfs</h2><p>使用sshfs可以在本地机器上挂载远程服务器某个文件夹。这样，操作本地的该文件夹就相当于操作远程服务器上的该文件夹（小心使用<code>rm</code>）。</p><h2 id="Alfred-Mutate"><a href="#Alfred-Mutate" class="headerlink" title="Alfred/Mutate"></a>Alfred/Mutate</h2><p>Alfred是Mac上一款非常好用的软件，就像蝙蝠侠身边的老管家一样，可以帮你自动化处理很多事情。除了原生<br>功能，还可以自己编写脚本实现扩展。例如查询豆瓣电影，查询ip，计算器等。鉴于这款软件的大名，这里不再多说。</p><p><a href="https://github.com/qdore/Mutate" target="_blank" rel="external">Mutate</a>是Ubuntu上的一款替代软件。同时，它也提供了方便的扩展接口，只需要按照模板编写python/shell代码，可以很方便地将自己的自动化处理功能加入软件中。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;工作环境大部分在Ubuntu和MacOS上进行，这里是一个我觉得在这两个平台上很有用的工具的整理列表，大部分都可以在两个系统上找到。这里并不会给出具体安装方式，因为最好的文档总是软件的官方Document或者GitHub的README。&lt;br&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>学一点PyQT</title>
    <link href="https://xmfbit.github.io/2017/08/29/learn-pyqt/"/>
    <id>https://xmfbit.github.io/2017/08/29/learn-pyqt/</id>
    <published>2017-08-29T06:07:55.000Z</published>
    <updated>2018-03-15T06:54:37.458Z</updated>
    
    <content type="html"><![CDATA[<p>Qt是一个流行的GUI框架，支持C++/Python。这篇文章是我在这两天通过PyQT制作一个串口通信并画图的小程序的时候，阅读<a href="http://zetcode.com/gui/pyqt5/introduction/" target="_blank" rel="external">PyQT5的一篇教程</a>时候的记录。<br><a id="more"></a></p><h2 id="主要模块"><a href="#主要模块" class="headerlink" title="主要模块"></a>主要模块</h2><p>PyQt5中的主要三个模块如下：</p><ul><li><code>QtCore</code>: 和GUI无关的核心功能：文件，时间，多线程等</li><li><code>QtGui</code>：和GUI相关的的东西：事件处理，2D图形，字体和文本等</li><li><code>QtWidget</code>：GUI中的相关组件，例如按钮，窗口等。</li></ul><p>其他模块还有<code>QtBluetooth</code>，<code>QtNetwork</code>等，都是比较专用的模块，用到再说。</p><h2 id="HelloWorld"><a href="#HelloWorld" class="headerlink" title="HelloWorld"></a>HelloWorld</h2><p>这里首先给出一段简单的程序，可以在桌面上显示一个窗口。<br><figure class="highlight py"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> sys</div><div class="line"><span class="keyword">from</span> PyQt5.QtWidgets <span class="keyword">import</span> QApplication, QWidget</div><div class="line"></div><div class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</div><div class="line">    app = QApplication(sys.argv)</div><div class="line">    w = QWidget()</div><div class="line">    w.resize(<span class="number">250</span>, <span class="number">150</span>)</div><div class="line">    w.move(<span class="number">300</span>, <span class="number">300</span>)</div><div class="line">    w.setWindowTitle(<span class="string">'Simple'</span>)</div><div class="line">    w.show()</div><div class="line"></div><div class="line">    sys.exit(app.exec_())</div></pre></td></tr></table></figure></p><p>下面介绍上面代码的含义：<br><figure class="highlight py"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">app = QApplication(sys.argv)</div></pre></td></tr></table></figure></p><p>每个Qt5应用必须首先创建一个application，后面会用到。<br><figure class="highlight py"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">w = QWidget()</div><div class="line">w.resize(<span class="number">250</span>, <span class="number">150</span>)</div><div class="line">w.move(<span class="number">300</span>, <span class="number">300</span>)</div><div class="line">w.setWindowTitle(<span class="string">'Simple'</span>)</div><div class="line">w.show()</div></pre></td></tr></table></figure></p><p><code>QtWidget</code>是所有组件的父类，我们创建了一个<code>Widget</code>。没有任何parent widget的Widget会作为窗口出现。接下来，调用其成员函数实现调整大小等功能。最后使用<code>show()</code>将其显示出来。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sys.exit(app.exec_())</div></pre></td></tr></table></figure><p>进入application的主循环，等待事件的触发。当退出程序（也许是通过Ctrl+C实现的）或者关闭窗口（点击关闭）后，主循环退出。</p><h2 id="添加一个按钮"><a href="#添加一个按钮" class="headerlink" title="添加一个按钮"></a>添加一个按钮</h2><p>下面，我们为窗口添加按钮，并为其添加事件响应动作。</p><p>参考文档可知，按钮<code>QPushButton</code>存在这样的构造函数：<br><figure class="highlight py"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">__init__ (self, QWidget parent = <span class="keyword">None</span>)</div></pre></td></tr></table></figure></p><p>下面的代码在初始化<code>QPushButton</code>实例<code>btn</code>时，将<code>self</code>作为参数传入，指定了其parent。另外，在指定按钮大小的时候，使用了<code>sizeHint()</code>方法自适应调节其大小。</p><p>同时，为按钮关联了点击动作。Qt中的事件响应机制通过信号和槽实现。点击事件一旦发生，信号<code>clicked</code>会被释放。然后槽相对的处理函数被调用。所谓的槽可以使PyQt提供的slot，或者是任何Python的可调用对象（函数或者实现了<code>__call__()</code>方法的对象）。</p><p>我们调用了现成的处理函数，来达到关闭窗口的目的。使用<code>instance()</code>可以得到当前application实例，调用其<code>quit()</code>方法即是退出当前应用，自然窗口就被关闭了。<br><figure class="highlight py"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> sys</div><div class="line"><span class="keyword">from</span> PyQt5.QtWidgets <span class="keyword">import</span> QApplication, QWidget, QPushButton, QToolTip</div><div class="line"><span class="keyword">from</span> PyQt5.QtCore <span class="keyword">import</span> QCoreApplication</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyWindow</span><span class="params">(QWidget)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></div><div class="line">        super(MyWindow, self).__init__()</div><div class="line">        self._init_ui()</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_init_ui</span><span class="params">(self)</span>:</span></div><div class="line">        btn = QPushButton(<span class="string">'quit'</span>, self)</div><div class="line">        btn.clicked.connect(QCoreApplication.instance().quit)</div><div class="line">        btn.setToolTip(<span class="string">'This is a &lt;b&gt;QPushButton&lt;/b&gt; widget'</span>)</div><div class="line">        btn.move(<span class="number">50</span>, <span class="number">50</span>)</div><div class="line">        btn.resize(btn.sizeHint())</div><div class="line"></div><div class="line">        self.setGeometry(<span class="number">300</span>, <span class="number">300</span>, <span class="number">300</span>, <span class="number">200</span>)</div><div class="line">        self.setWindowTitle(<span class="string">'Window with Button'</span>)</div><div class="line">        self.show()</div><div class="line"></div><div class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</div><div class="line">    app = QApplication(sys.argv)</div><div class="line">    window = MyWindow()</div><div class="line">    sys.exit(app.exec_())</div></pre></td></tr></table></figure></p><h2 id="使用Event处理事件"><a href="#使用Event处理事件" class="headerlink" title="使用Event处理事件"></a>使用Event处理事件</h2><p>除了上述的信号和槽的处理方式，也可以使用Event相关的类进行处理。下面的代码在关闭窗口时弹出对话框确认是否关闭。根据用户做出的选择，调用<code>event.accept()</code>或<code>ignore()</code>完成对事件的处理。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> sys</div><div class="line"><span class="keyword">from</span> PyQt5.QtWidgets <span class="keyword">import</span> QWidget, QMessageBox, QApplication</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyWindow</span><span class="params">(QWidget)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></div><div class="line">        super(MyWindow, self).__init__()</div><div class="line">        self._init_ui()</div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_init_ui</span><span class="params">(self)</span>:</span></div><div class="line">        self.setGeometry(<span class="number">300</span>, <span class="number">300</span>, <span class="number">300</span>, <span class="number">200</span>)</div><div class="line">        self.show()</div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">closeEvent</span><span class="params">(self, ev)</span>:</span></div><div class="line">        reply = QMessageBox.question(self, <span class="string">'Message'</span>, <span class="string">'Are you sure?'</span>,</div><div class="line">                    QMessageBox.Yes | QMessageBox.No, QMessageBox.No)</div><div class="line">        <span class="keyword">if</span> reply == QMessageBox.Yes:</div><div class="line">            ev.accept()</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            ev.ignore()</div><div class="line"></div><div class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</div><div class="line">    app = QApplication(sys.argv)</div><div class="line">    win = MyWindow()</div><div class="line">    sys.exit(app.exec_())</div></pre></td></tr></table></figure><h2 id="使用Layout组织Widget"><a href="#使用Layout组织Widget" class="headerlink" title="使用Layout组织Widget"></a>使用Layout组织Widget</h2><p>组织Widget的方式可以通过绝对位置调整，但是更推荐使用<code>Layout</code>组织。</p><p>绝对位置是通过指定像素多少来确定widget的大小和位置。这样的话，有以下几个缺点：</p><ul><li>不同平台可能显示效果不统一；</li><li>当parent resize的时候，widget大小和位置并不会自动调整</li><li>编码太麻烦，牵一发而动全身</li></ul><p>下面介绍几种常见的<code>Layout</code>类。</p><h3 id="Box-Layout"><a href="#Box-Layout" class="headerlink" title="Box Layout"></a>Box Layout</h3><p>有<code>QVBoxLayout</code>和<code>QHBoxLayout</code>，用来将widget水平或者竖直排列起来。下面的代码通过这两个layout将按钮放置在窗口的右下角。关键的地方在于使用<code>addSkretch()</code>方法将一个<code>QSpacerItem</code>实例对象插入到了layout中，占据了相应位置。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> sys</div><div class="line"><span class="keyword">from</span> PyQt5.QtWidgets <span class="keyword">import</span> (QWidget, QPushButton,</div><div class="line">    QHBoxLayout, QVBoxLayout, QApplication)</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyWindow</span><span class="params">(QWidget)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></div><div class="line">        super(MyWindow, self).__init__()</div><div class="line">        self._init_ui()</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_init_ui</span><span class="params">(self)</span>:</span></div><div class="line">        okButton = QPushButton(<span class="string">"OK"</span>)</div><div class="line">        cancelButton = QPushButton(<span class="string">"Cancel"</span>)</div><div class="line"></div><div class="line">        hbox = QHBoxLayout()</div><div class="line">        hbox.addStretch(<span class="number">1</span>)</div><div class="line">        hbox.addWidget(okButton)</div><div class="line">        hbox.addWidget(cancelButton)</div><div class="line"></div><div class="line">        vbox = QVBoxLayout()</div><div class="line">        vbox.addStretch(<span class="number">1</span>)</div><div class="line">        vbox.addLayout(hbox)</div><div class="line"></div><div class="line">        self.setLayout(vbox)    </div><div class="line"></div><div class="line">        self.setGeometry(<span class="number">300</span>, <span class="number">300</span>, <span class="number">300</span>, <span class="number">150</span>)</div><div class="line">        self.setWindowTitle(<span class="string">'Buttons'</span>)    </div><div class="line">        self.show()</div><div class="line"></div><div class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</div><div class="line">    app = QApplication(sys.argv)</div><div class="line">    win = MyWindow()</div><div class="line">    sys.exit(app.exec_())</div></pre></td></tr></table></figure><h3 id="Grid-Layout"><a href="#Grid-Layout" class="headerlink" title="Grid Layout"></a>Grid Layout</h3><p><code>QGridLayout</code>将空间划分为行列的grid。在向其中添加item的时候，要指定位置。如下，将5行4列的grid设置为计算器的面板模式。<br><figure class="highlight py"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> sys</div><div class="line"><span class="keyword">from</span> PyQt5.QtWidgets <span class="keyword">import</span> (QWidget, QGridLayout,</div><div class="line">    QPushButton, QApplication)</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyWindow</span><span class="params">(QWidget)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></div><div class="line">        super(MyWindow, self).__init__()</div><div class="line">        self._init_ui()</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_init_ui</span><span class="params">(self)</span>:</span></div><div class="line">        grid = QGridLayout()</div><div class="line">        self.setLayout(grid)</div><div class="line">        names = [<span class="string">'Cls'</span>, <span class="string">'Bck'</span>, <span class="string">''</span>, <span class="string">'Close'</span>,</div><div class="line">                 <span class="string">'7'</span>, <span class="string">'8'</span>, <span class="string">'9'</span>, <span class="string">'/'</span>,</div><div class="line">                <span class="string">'4'</span>, <span class="string">'5'</span>, <span class="string">'6'</span>, <span class="string">'*'</span>,</div><div class="line">                 <span class="string">'1'</span>, <span class="string">'2'</span>, <span class="string">'3'</span>, <span class="string">'-'</span>,</div><div class="line">                <span class="string">'0'</span>, <span class="string">'.'</span>, <span class="string">'='</span>, <span class="string">'+'</span>]</div><div class="line">        positions = [(i,j) <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">5</span>) <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">4</span>)]</div><div class="line">        <span class="keyword">for</span> position, name <span class="keyword">in</span> zip(positions, names):</div><div class="line">            <span class="keyword">if</span> name == <span class="string">''</span>:</div><div class="line">                <span class="keyword">continue</span></div><div class="line">            button = QPushButton(name)</div><div class="line">            grid.addWidget(button, *position)</div><div class="line"></div><div class="line">        self.move(<span class="number">300</span>, <span class="number">150</span>)</div><div class="line">        self.setWindowTitle(<span class="string">'Calculator'</span>)</div><div class="line">        self.show()</div><div class="line"></div><div class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</div><div class="line"></div><div class="line">    app = QApplication(sys.argv)</div><div class="line">    win = MyWindow()</div><div class="line">    sys.exit(app.exec_())</div></pre></td></tr></table></figure></p><p>另外，我们还可以通过<code>setSpacing()</code>方法设置每个单元格之间的间隔。如果某个widget需要占据多个单元格，可以在<code>addWidget()</code>方法中指定要扩展的行列数。</p><h2 id="事件驱动"><a href="#事件驱动" class="headerlink" title="事件驱动"></a>事件驱动</h2><p>PyQt提供了两种事件驱动的处理方式：</p><ul><li>使用<code>event</code>句柄。事件可能是由于UI交互或者定时器等引起，由接收对象进行处理。</li><li>信号和槽。某个widge交互时，释放相应信号，被槽对应的函数捕获进行处理。</li></ul><p>信号和槽可以见上面使用按钮关闭窗口的例子，关键在于调用信号的<code>connect()</code>函数将其绑定到某个槽上。Python中的可调用对象都可以作为槽。</p><p>而使用event句柄处理时，需要重写override原来的处理函数，见上面使用其在关闭窗口时进行弹窗确认的例子。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Qt是一个流行的GUI框架，支持C++/Python。这篇文章是我在这两天通过PyQT制作一个串口通信并画图的小程序的时候，阅读&lt;a href=&quot;http://zetcode.com/gui/pyqt5/introduction/&quot;&gt;PyQT5的一篇教程&lt;/a&gt;时候的记录。&lt;br&gt;
    
    </summary>
    
    
      <category term="python" scheme="https://xmfbit.github.io/tags/python/"/>
    
      <category term="qt" scheme="https://xmfbit.github.io/tags/qt/"/>
    
  </entry>
  
  <entry>
    <title>doc2dash——制作自己的dash文档</title>
    <link href="https://xmfbit.github.io/2017/08/26/doc2dash-usage/"/>
    <id>https://xmfbit.github.io/2017/08/26/doc2dash-usage/</id>
    <published>2017-08-26T11:32:00.000Z</published>
    <updated>2018-03-15T06:54:37.453Z</updated>
    
    <content type="html"><![CDATA[<p>Dash是Mac上一款超棒的应用，提供了诸如C/C++/Python甚至是OpenCV/Vim等软件包或工具软件的参考文档。只要使用App的“Download Docsets”功能，就能轻松下载相应文档。使用的时候只需在Dash的搜索框内输入相应关键词，Dash会在所有文档中进行搜索，给出相关内容的列表。点击我们要寻找的条款，就能够直接在本地阅读文档。在Ubuntu/Windows平台上，Dash也有对应的替代品，例如<a href="https://zealdocs.org" target="_blank" rel="external">zeal</a>就是一款Windows/Linux平台通用的Dash替代软件。</p><p>这样强大的软件，如果只能使用官方提供的文档岂不是有些大材小用？<a href="https://doc2dash.readthedocs.io/en/stable/" target="_blank" rel="external">doc2dash</a>就是一款能够自动生成Dash兼容docset文件的工具。例如，可以使用它为PyTorch生成本地化的docset文件，并导入Dash/zeal中，在本地进行搜索阅读。这不是美滋滋？</p><p>本文章是基于doc2dash的官方介绍，对其使用进行的总结。<br><img src="/img/doc2dash_pytorch_example.jpg" alt="Demo"></p><a id="more"></a><h2 id="安装doc2dash"><a href="#安装doc2dash" class="headerlink" title="安装doc2dash"></a>安装doc2dash</h2><p>doc2dash是基于Python开发的。按照官方网站介绍，为了避免Python包的冲突，最好使用虚拟环境进行安装。我的机器上安装有Anaconda环境，所以首先使用<code>conda create</code>命令新建用于doc2dash的虚拟环境。<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">conda create -n doc2dash</div></pre></td></tr></table></figure></p><p>接下来，激活虚拟环境，并使用<code>pip install</code>命令安装。<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="built_in">source</span> activate doc2dash</div><div class="line">pip install doc2dash</div></pre></td></tr></table></figure></p><p>doc2dash支持的输出格式可以通过sphinx或者pydoctor。其中前者更加常用。下面以PyTorch项目的文档生成为例，介绍doc2dash的具体用法。</p><h2 id="生成PyTorch文档"><a href="#生成PyTorch文档" class="headerlink" title="生成PyTorch文档"></a>生成PyTorch文档</h2><p>doc2dash使用sphinx生成相应的文档。在上述安装doc2dash的过程中，应该已经安装了sphinx包。不过我们还需要手动安装，以便处理rst文档。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">pip install sphinx_rtd_theme</div></pre></td></tr></table></figure><p>进入PyTorch的文档目录<code>docs/</code>，PyTorch已经为我们提供了Makefile，调用sphinx包进行文档处理，可以选择<code>make html</code>命令生成相应的HTML文档，生成的位置为<code>build/html</code>。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"># in directory $PYTORCH/docs, run</div><div class="line">make html</div></pre></td></tr></table></figure><p>接下来，就可以使用doc2dash来继续sphinx的工作，生成Dash可用的文档文件了~使用<code>-n</code>指定生成的文件名称，后面跟source文件夹路径即可。</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># $PYTORCH/docs/build/html即为生成的HTML目录</span></div><div class="line">doc2dash -n pytorch <span class="variable">$PYTORCH</span>/docs/build/html</div></pre></td></tr></table></figure><p>之后，把生成的<code>pytorch.docset</code>导入到Dash中即可。如下图所示，点击“+”找到文件添加即可。<br><img src="/img/doc2dash_how_to_add_docset.jpg" alt="添加docset"></p><h2 id="在Ubuntu上安装zeal"><a href="#在Ubuntu上安装zeal" class="headerlink" title="在Ubuntu上安装zeal"></a>在Ubuntu上安装zeal</h2><p>zeal是Dash在非Mac平台上的替代软件。在Ubuntu上可以使用如下方式轻松安装（见<a href="https://zealdocs.org/download.html#linux" target="_blank" rel="external">官方网站介绍</a>）。</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">sudo add-apt-repository ppa:zeal-developers/ppa</div><div class="line">sudo apt-get update</div><div class="line">sudo apt-get install zeal</div></pre></td></tr></table></figure><p>安装后，可以使用<code>Tool/Docsets</code>下载相应的公开文档。如果想要添加自己生成的文档，只需要将生成的docset文件放到软件的文档库中即可，默认位置应在<code>$HOME/.local/share/Zeal/Zeal/docsets</code>。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Dash是Mac上一款超棒的应用，提供了诸如C/C++/Python甚至是OpenCV/Vim等软件包或工具软件的参考文档。只要使用App的“Download Docsets”功能，就能轻松下载相应文档。使用的时候只需在Dash的搜索框内输入相应关键词，Dash会在所有文档中进行搜索，给出相关内容的列表。点击我们要寻找的条款，就能够直接在本地阅读文档。在Ubuntu/Windows平台上，Dash也有对应的替代品，例如&lt;a href=&quot;https://zealdocs.org&quot;&gt;zeal&lt;/a&gt;就是一款Windows/Linux平台通用的Dash替代软件。&lt;/p&gt;
&lt;p&gt;这样强大的软件，如果只能使用官方提供的文档岂不是有些大材小用？&lt;a href=&quot;https://doc2dash.readthedocs.io/en/stable/&quot;&gt;doc2dash&lt;/a&gt;就是一款能够自动生成Dash兼容docset文件的工具。例如，可以使用它为PyTorch生成本地化的docset文件，并导入Dash/zeal中，在本地进行搜索阅读。这不是美滋滋？&lt;/p&gt;
&lt;p&gt;本文章是基于doc2dash的官方介绍，对其使用进行的总结。&lt;br&gt;&lt;img src=&quot;/img/doc2dash_pytorch_example.jpg&quot; alt=&quot;Demo&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="tool" scheme="https://xmfbit.github.io/tags/tool/"/>
    
  </entry>
  
</feed>
