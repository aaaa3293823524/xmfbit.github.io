<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>来呀，快活呀~</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://xmfbit.github.io/"/>
  <updated>2019-03-09T04:58:35.441Z</updated>
  <id>https://xmfbit.github.io/</id>
  
  <author>
    <name>一个脱离了高级趣味的人</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>YOLO Caffe模型转换BN的坑</title>
    <link href="https://xmfbit.github.io/2019/03/09/darknet-caffe-converter/"/>
    <id>https://xmfbit.github.io/2019/03/09/darknet-caffe-converter/</id>
    <published>2019-03-09T04:51:18.000Z</published>
    <updated>2019-03-09T04:58:35.441Z</updated>
    
    <content type="html"><![CDATA[<p>YOLO虽好，但是Darknet框架实在是小众，有必要在Inference阶段将其转换为其他框架，以便后续统一部署和管理。Caffe作为小巧灵活的老资格框架，使用灵活，方便魔改，所以尝试将Darknet训练的YOLO模型转换为Caffe。这里简单记录下YOLO V3 原始Darknet模型转换为Caffe模型过程中的一个坑。</p><a id="more"></a><h1 id="Darknet中BN的计算"><a href="#Darknet中BN的计算" class="headerlink" title="Darknet中BN的计算"></a>Darknet中BN的计算</h1><p>以CPU代码为例，在Darknet中，BN做normalization的操作如下，<a href="https://github.com/pjreddie/darknet/blob/master/src/blas.c#L147" target="_blank" rel="external">normalize_cpu</a></p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">void</span> <span class="title">normalize_cpu</span><span class="params">(<span class="keyword">float</span> *x, <span class="keyword">float</span> *mean, <span class="keyword">float</span> *variance, <span class="keyword">int</span> batch, <span class="keyword">int</span> filters, <span class="keyword">int</span> spatial)</span></span></div><div class="line">&#123;</div><div class="line">    <span class="keyword">int</span> b, f, i;</div><div class="line">    <span class="keyword">for</span>(b = <span class="number">0</span>; b &lt; batch; ++b)&#123;</div><div class="line">        <span class="keyword">for</span>(f = <span class="number">0</span>; f &lt; filters; ++f)&#123;</div><div class="line">            <span class="keyword">for</span>(i = <span class="number">0</span>; i &lt; spatial; ++i)&#123;</div><div class="line">                <span class="keyword">int</span> index = b*filters*spatial + f*spatial + i;</div><div class="line">                x[index] = (x[index] - mean[f])/(<span class="built_in">sqrt</span>(variance[f]) + <span class="number">.000001</span>f);</div><div class="line">            &#125;</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure><p>可以看到，Darknet中的BN计算如下：</p><script type="math/tex; mode=display">\hat{x} = \frac{x - \mu}{\sqrt{\sigma^2} + \epsilon}</script><p>而且，$\epsilon$参数是固定的，为$1\times 10^{-6}$。</p><h1 id="问题和解决"><a href="#问题和解决" class="headerlink" title="问题和解决"></a>问题和解决</h1><p>然而，在Caffe（以及大部分其他框架）中，$\epsilon$的位置是在根号里面的，也就是：</p><script type="math/tex; mode=display">\hat{x} = \frac{x - \mu}{\sqrt{\sigma^2 + \epsilon}}</script><p>另外，查看<code>caffe.proto</code>可以知道，Caffe默认的$\epsilon$值为$1\times 10^{-5}$。</p><p>所以，在转换为caffe prototxt时，需要设置<code>batch_norm_param</code>如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">batch_norm_param &#123;</div><div class="line">  use_global_stats: true</div><div class="line">  eps: 1e-06</div><div class="line">&#125;</div></pre></td></tr></table></figure><p>另外，需要重新求解$\sigma^2$，按照layer输出要相等的等量关系，可以求得：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">convert_running_var</span><span class="params">(var, eps=DARKNET_EPS)</span>:</span></div><div class="line">    <span class="keyword">return</span> np.square(np.sqrt(var) + eps) - eps</div></pre></td></tr></table></figure><p>这里调整之后，转换后的Caffe模型和原始Darknet模型的输出误差已经是$1\times 10^{-7}$量级，可以认为转换成功。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;YOLO虽好，但是Darknet框架实在是小众，有必要在Inference阶段将其转换为其他框架，以便后续统一部署和管理。Caffe作为小巧灵活的老资格框架，使用灵活，方便魔改，所以尝试将Darknet训练的YOLO模型转换为Caffe。这里简单记录下YOLO V3 原始Darknet模型转换为Caffe模型过程中的一个坑。&lt;/p&gt;
    
    </summary>
    
    
      <category term="caffe" scheme="https://xmfbit.github.io/tags/caffe/"/>
    
      <category term="deep learning" scheme="https://xmfbit.github.io/tags/deep-learning/"/>
    
  </entry>
  
  <entry>
    <title>MacOS Mojave更新之后一定要做这几件事！</title>
    <link href="https://xmfbit.github.io/2018/10/27/mac-update-mojave/"/>
    <id>https://xmfbit.github.io/2018/10/27/mac-update-mojave/</id>
    <published>2018-10-27T06:57:12.000Z</published>
    <updated>2018-10-27T07:16:52.401Z</updated>
    
    <content type="html"><![CDATA[<p>很奇怪，对于手机上的APP，我一般能不升级就不升级；但是对于PC上的软件或操作系统更新，则是能升级就升级。。在将手中的MacOS更新到最新版本Mojave后，发现了一些需要手动调节的问题，记录在这里，原谅我标题党的画风。。。<br><a id="more"></a></p><h2 id="Git等工具"><a href="#Git等工具" class="headerlink" title="Git等工具"></a>Git等工具</h2><p>试图使用<code>git</code>是出现了如下错误：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">git clone xx.git</div><div class="line">xcrun: error: invalid active developer path (/Library/Developer/CommandLineTools), missing xcrun at: /Library/Developer/CommandLineTools/usr/bin/xcrun</div></pre></td></tr></table></figure><p>解决办法参考<a href="https://apple.stackexchange.com/questions/254380/macos-mojave-invalid-active-developer-path" target="_blank" rel="external">macOS Mojave: invalid active developer path</a>中的最高赞回答：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">xcode-select --install</div></pre></td></tr></table></figure></p><h2 id="osxfuse"><a href="#osxfuse" class="headerlink" title="osxfuse"></a>osxfuse</h2><p>参考Github讨论帖<a href="https://github.com/osxfuse/osxfuse/issues/542" target="_blank" rel="external">osxfuse not compatible with MacOS Mojave</a>，从官网下载最新的3.8.2版本安装即可。</p><h2 id="VSCode等编辑器字体变“瘦”"><a href="#VSCode等编辑器字体变“瘦”" class="headerlink" title="VSCode等编辑器字体变“瘦”"></a>VSCode等编辑器字体变“瘦”</h2><p>更新之后，发现VSCode编辑器中的字体变得“很瘦”，不美观。执行下面的命令，并重启机器，应该可以恢复。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">defaults write -g CGFontRenderingFontSmoothingDisabled -bool NO</div></pre></td></tr></table></figure></p><h2 id="Mos-Caffine-IINA-等APP"><a href="#Mos-Caffine-IINA-等APP" class="headerlink" title="Mos Caffine IINA 等APP"></a>Mos Caffine IINA 等APP</h2><p>Mos可以平滑Mac上外接鼠标的滚动，并调整鼠标滚动方向和Windows相同。更新后发现Mos失灵。这应该是和新版本中更强的权限管理有关，解决办法是在”安全隐私设置” -&gt; “辅助功能”中，先把Mos的勾勾去掉，然后重新勾选。Caffine同样的操作。</p><p>IINA是一款Mac上的播放器软件，是我在Mac上的默认播放器。更新后点击媒体文件，发现只是弹出IINA软件的界面，却没有自动播放。解决办法是在媒体文件上右键，在打开方式中重新选择IINA，并勾选默认打开方式选项。</p><p>更新新系统后，遇到的坑暂时就这么多。希望能够帮助到需要的人。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;很奇怪，对于手机上的APP，我一般能不升级就不升级；但是对于PC上的软件或操作系统更新，则是能升级就升级。。在将手中的MacOS更新到最新版本Mojave后，发现了一些需要手动调节的问题，记录在这里，原谅我标题党的画风。。。&lt;br&gt;
    
    </summary>
    
    
      <category term="tool" scheme="https://xmfbit.github.io/tags/tool/"/>
    
  </entry>
  
  <entry>
    <title>论文 - Rethinking The Value of Network Pruning</title>
    <link href="https://xmfbit.github.io/2018/10/22/paper-rethinking-the-value-of-network-pruning/"/>
    <id>https://xmfbit.github.io/2018/10/22/paper-rethinking-the-value-of-network-pruning/</id>
    <published>2018-10-22T14:25:42.000Z</published>
    <updated>2018-10-27T07:16:52.407Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://openreview.net/forum?id=rJlnB3C5Ym" target="_blank" rel="external">这篇文章</a>是ICLR 2019的投稿文章，最近也引发了大家的注意。在我的博客中，已经对此做过简单的介绍，请参考<a href="https://xmfbit.github.io/2018/10/03/paper-summary-model-pruning/">论文总结 - 模型剪枝 Model Pruning</a>。</p><p>这篇文章的主要观点在于想纠正人们之前的认识误区。当然这个认识误区和DL的发展是密不可分的。DL中最先提出的AlexNet是一个很大的模型。后面的研究者虽然也在不断发明新的网络结构（如inception，Global Pooling，ResNet等）来获得参数更少更强大的模型，但模型的size总还是很大。既然研究社区是从这样的“大”模型出发的，那当面对工程上需要小模型以便在手机等移动设备上使用时，很自然的一条路就是去除大模型中已有的参数从而得到小模型。也是很自然的，我们需要保留大模型中“有用的”那些参数，让小模型以此为基础进行fine tune，补偿因为去除参数而导致的模型性能下降。</p><p>然而，自然的想法就是合理的么？这篇文章对此提出了质疑。这篇论文的主要思路已经在上面贴出的博文链接中说过了。这篇文章主要是结合作者开源的代码对论文进行梳理：<a href="https://github.com/Eric-mingjie/rethinking-network-pruning" target="_blank" rel="external">Eric-mingjie/rethinking-network-pruning</a>。</p><a id="more"></a><h2 id="FLOP的计算"><a href="#FLOP的计算" class="headerlink" title="FLOP的计算"></a>FLOP的计算</h2><p>代码中有关于PyTorch模型的FLOPs的计算，见<a href="https://github.com/Eric-mingjie/rethinking-network-pruning/blob/master/imagenet/l1-norm-pruning/compute_flops.py" target="_blank" rel="external">compute_flops.py</a>。可以很方便地应用到自己的代码中。</p><h2 id="ThiNet的实现"><a href="#ThiNet的实现" class="headerlink" title="ThiNet的实现"></a>ThiNet的实现</h2><h2 id="实验比较"><a href="#实验比较" class="headerlink" title="实验比较"></a>实验比较</h2><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>几个仍然有疑问的地方：</p><ol><li><p>作者已经证明在ImageNet/CIFAR等样本分布均衡的数据集上的结论，如果样本分布不均衡呢？有三种思路有待验证：</p><ul><li>prune模型需要从大模型处继承权重，然后直接在不均衡数据集上训练即可；</li><li>prune模型不需要从大模型处继承权重， 但是需要先在ImageNet数据集上训练，然后再在不均衡数据集上训练；</li><li>prune模型直接在不均衡数据集上训练（以我的经验，这种思路应该是不work的）</li></ul></li><li><p>prune前的大模型权重不重要，结构重要，这是本文的结论之一。自动搜索树的prune算法可以看做是模型结构搜索，但是大模型给出了搜索空间的一个很好的初始点。这个初始点是否是任务无关的？也就是说，对A任务有效的小模型，是否在B任务上也是很work的？</p></li><li><p>现在的网络搜索中应用了强化学习/遗传算法等方法，这些方法怎么能够和prune结合？ECCV 2018中HanSong和He Yihui发表了AMC方法。</p></li></ol><p>总之，作者用自己辛勤的实验，给我们指出了一个”可能的”（毕竟文章还没被接收）误区，但是仍然有很多乌云漂浮在上面，需要更多的实验。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;https://openreview.net/forum?id=rJlnB3C5Ym&quot;&gt;这篇文章&lt;/a&gt;是ICLR 2019的投稿文章，最近也引发了大家的注意。在我的博客中，已经对此做过简单的介绍，请参考&lt;a href=&quot;https://xmfbit.github.io/2018/10/03/paper-summary-model-pruning/&quot;&gt;论文总结 - 模型剪枝 Model Pruning&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;这篇文章的主要观点在于想纠正人们之前的认识误区。当然这个认识误区和DL的发展是密不可分的。DL中最先提出的AlexNet是一个很大的模型。后面的研究者虽然也在不断发明新的网络结构（如inception，Global Pooling，ResNet等）来获得参数更少更强大的模型，但模型的size总还是很大。既然研究社区是从这样的“大”模型出发的，那当面对工程上需要小模型以便在手机等移动设备上使用时，很自然的一条路就是去除大模型中已有的参数从而得到小模型。也是很自然的，我们需要保留大模型中“有用的”那些参数，让小模型以此为基础进行fine tune，补偿因为去除参数而导致的模型性能下降。&lt;/p&gt;
&lt;p&gt;然而，自然的想法就是合理的么？这篇文章对此提出了质疑。这篇论文的主要思路已经在上面贴出的博文链接中说过了。这篇文章主要是结合作者开源的代码对论文进行梳理：&lt;a href=&quot;https://github.com/Eric-mingjie/rethinking-network-pruning&quot;&gt;Eric-mingjie/rethinking-network-pruning&lt;/a&gt;。&lt;/p&gt;
    
    </summary>
    
    
      <category term="paper" scheme="https://xmfbit.github.io/tags/paper/"/>
    
      <category term="deep learning" scheme="https://xmfbit.github.io/tags/deep-learning/"/>
    
      <category term="model compression" scheme="https://xmfbit.github.io/tags/model-compression/"/>
    
  </entry>
  
  <entry>
    <title>论文总结 - 模型剪枝 Model Pruning</title>
    <link href="https://xmfbit.github.io/2018/10/03/paper-summary-model-pruning/"/>
    <id>https://xmfbit.github.io/2018/10/03/paper-summary-model-pruning/</id>
    <published>2018-10-03T08:31:07.000Z</published>
    <updated>2018-10-27T07:16:52.410Z</updated>
    
    <content type="html"><![CDATA[<p>模型剪枝是常用的模型压缩方法之一。这篇是最近看的模型剪枝相关论文的总结。</p><p><img src="/img/paper-summary-model-pruning-joke.jpg" alt="剪枝的学问"></p><a id="more"></a><h2 id="Deep-Compression-Han-Song"><a href="#Deep-Compression-Han-Song" class="headerlink" title="Deep Compression, Han Song"></a>Deep Compression, Han Song</h2><p>抛去LeCun等人在90年代初的几篇论文，HanSong是这个领域的先行者。发表了一系列关于模型压缩的论文。其中NIPS 2015上的这篇<a href="https://arxiv.org/abs/1506.02626" target="_blank" rel="external">Learning both weights and connections for efficient neural network</a>着重讨论了对模型进行剪枝的方法。这篇论文之前我已经写过了<a href="https://xmfbit.github.io/2018/03/14/paper-network-prune-hansong/">阅读总结</a>，比较详细。</p><p>概括来说，作者提出的主要观点包括，L1 norm作为neuron是否重要的metric，train -&gt; pruning -&gt; retrain三阶段方法以及iteratively pruning。需要注意的是，作者的方法只能得到非结构化的稀疏，对于作者的专用硬件EIE可能会很有帮助。但是如果想要在通用GPU或CPU上用这种方法做加速，是不太现实的。</p><h2 id="SSL，WenWei"><a href="#SSL，WenWei" class="headerlink" title="SSL，WenWei"></a>SSL，WenWei</h2><p>既然非结构化稀疏对现有的通用GPU/CPU不友好，那么可以考虑构造结构化的稀疏。将Conv中的某个filter或filter的某个方形区域甚至是某个layer直接去掉，应该是可以获得加速效果的。WenWei<a href="https://arxiv.org/abs/1608.03665" target="_blank" rel="external">论文Learning Structured Sparsity in Deep Neural Networks</a>发表在NIPS 2016上，介绍了如何使用LASSO，给损失函数加入相应的惩罚，进行结构化稀疏。这篇论文之前也已经写过博客，可以参考<a href="https://xmfbit.github.io/2018/02/24/paper-ssl-dnn/">博客文章</a>。</p><p>概括来说，作者引入LASSO正则惩罚项，通过不同的具体形式，构造了对不同结构化稀疏的损失函数。</p><h2 id="L1-norm-Filter-Pruning，Li-Hao"><a href="#L1-norm-Filter-Pruning，Li-Hao" class="headerlink" title="L1-norm Filter Pruning，Li Hao"></a>L1-norm Filter Pruning，Li Hao</h2><p>在通用GPU/CPU上，加速效果最好的还是整个Filter直接去掉。作者发表在ICLR 2017上的<a href="https://arxiv.org/abs/1608.08710" target="_blank" rel="external">论文Pruning Filters for Efficient ConvNets</a>提出了一种简单的对卷积层的filter进行剪枝的方法。</p><p>这篇论文真的很简单。。。主要观点就是通过Filter的L1 norm来判断这个filter是否重要。人为设定剪枝比例后，将该层不重要的那些filter直接去掉，并进行fine tune。在确定剪枝比例的时候，假定每个layer都是互相独立的，分别对其在不同剪枝比例下进行剪枝，并评估模型在验证集上的表现，做sensitivity分析，然后确定合理的剪枝比例。在实现的时候要注意，第$i$个layer中的第$j$个filter被去除，会导致其输出的feature map中的第$j$个channel缺失，所以要相应调整后续的BN层和Conv层的对应channel上的参数。</p><p>另外，实现起来还有一些细节，这些可以参见原始论文。提一点，在对ResNet这种有旁路结构的网络进行剪枝时，每个block中的最后一个conv不太好处理。因为它的输出要与旁路做加和运算。如果channel数量不匹配，是没法做的。作者在这里的处理方法是，听identity那一路的。如果那一路确定了剪枝后剩余的index是多少，那么$\mathcal{F}(x)$那一路的最后那个conv也这样剪枝。</p><p>这里给出一张在ImageNet上做sensitivity analysis的图表。需要对每个待剪枝的layer进行类似的分析。</p><p><img src="/img/paper-model-pruning-filter-pruning-sensitivity-results.png" alt="sensitivity分析"></p><h2 id="Automated-Gradual-Pruning-Gupta"><a href="#Automated-Gradual-Pruning-Gupta" class="headerlink" title="Automated Gradual Pruning, Gupta"></a>Automated Gradual Pruning, Gupta</h2><p>这篇文章发表在NIPS 2017的一个关于移动设备的workshop上，名字很有意思（这些人起名字为什么都这么熟练啊）：<a href="https://arxiv.org/abs/1710.01878" target="_blank" rel="external">To prune, or not to prune: exploring the efficacy of pruning for model compression</a>。TensorFlow的repo中已经有了对应的实现（亲儿子。。）：<a href="https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/model_pruning" target="_blank" rel="external">Model pruning: Training tensorflow models to have masked connections</a>。哈姆雷特不能回答的问题，作者的答案则是Yes。</p><p><img src="/img/paper-model-pruning-why-so-baixue.jpg" alt="为什么你们起名字这么熟练啊"></p><p>这篇文章主要有两个贡献。一是比较了large模型经过prune之后得到的large-sparse模型和相似memory footprint但是compact-small模型的性能，得出结论：对于很多网络结构（CNN，stacked LSTM, seq-to-seq LSTM）等，都是前者更好。具体的数据参考论文。</p><p>二是提出了一个渐进的自动调节的pruning策略。首先，作者也着眼于非结构化稀疏。同时和上面几篇文章一样，作者也使用绝对值大小作为衡量importance的标准，作者提出，sparsity可以按照下式自动调节：</p><script type="math/tex; mode=display">s_t = s_f + (s_i-s_f)(1-\frac{t-t_0}{n\Delta t})^3 \quad \text{for}\quad t \in \{t_0, t_0+\Delta t,\dots,t_0+n\Delta t\}</script><p>其中，$s_i$是初始剪枝比例，一般为$0$。$s_f$为最终的剪枝比例，开始剪枝的迭代次数为$t_0$，剪枝间隔为$\Delta t$，共进行$n$次。</p><h2 id="Net-Sliming-Liu-Zhuang-amp-Huang-Gao"><a href="#Net-Sliming-Liu-Zhuang-amp-Huang-Gao" class="headerlink" title="Net Sliming, Liu Zhuang &amp; Huang Gao"></a>Net Sliming, Liu Zhuang &amp; Huang Gao</h2><p>这篇文章<a href="https://arxiv.org/abs/1708.06519" target="_blank" rel="external">Learning Efficient Convolutional Networks through Network Slimming</a>发表在ICCV 2017，利用CNN网络中的必备组件——BN层中的gamma参数，实现端到端地学习剪枝参数，决定某个layer中该去除掉哪些channel。作者中有DenseNet的作者——姚班学生刘壮和康奈尔大学博士后黄高。代码已经开源：<a href="https://github.com/liuzhuang13/slimming" target="_blank" rel="external">liuzhuang13/slimming</a>。</p><p>作者的主要贡献是提出可以使用BN层的gamma参数，标志其前面的conv输出的feature map的某个channel是否重要，相应地，也是conv参数中的那个filter是否重要。</p><p>首先，需要给BN的gamma参数加上L1 正则惩罚训练模型，新的损失函数变为$L= \sum_{(x,y)}l(f(x, W), y) + \lambda \sum_{\gamma \in \Gamma}g(\gamma)$。</p><p>接着将该网络中的所有gamma进行排序，根据人为给出的剪枝比例，去掉那些gamma很小的channel，也就是对应的filter。最后进行finetune。这个过程可以反复多次，得到更好的效果。如下所示：<br><img src="/img/paper-model-pruning-net-sliming-procedure.png" alt="Net Sliming的大致流程"></p><p>还是上面遇到过的问题，如果处理ResNet或者DenseNet Feature map会多路输出的问题。这里作者提出使用一个”channel selection layer”，统一对该feature map的输出进行处理，只选择没有被mask掉的那些channel输出。具体实现可以参见开源代码<a href="https://github.com/Eric-mingjie/network-slimming/blob/master/models/channel_selection.py#L6" target="_blank" rel="external">channel selection layer</a>：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">channel_selection</span><span class="params">(nn.Module)</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line">    Select channels from the output of BatchNorm2d layer. It should be put directly after BatchNorm2d layer.</div><div class="line">    The output shape of this layer is determined by the number of 1 in `self.indexes`.</div><div class="line">    """</div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, num_channels)</span>:</span></div><div class="line">        <span class="string">"""</span></div><div class="line">        Initialize the `indexes` with all one vector with the length same as the number of channels.</div><div class="line">        During pruning, the places in `indexes` which correpond to the channels to be pruned will be set to 0.</div><div class="line">        """</div><div class="line">        super(channel_selection, self).__init__()</div><div class="line">        self.indexes = nn.Parameter(torch.ones(num_channels))</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, input_tensor)</span>:</span></div><div class="line">        <span class="string">"""</span></div><div class="line">        Parameter</div><div class="line">        ---------</div><div class="line">        input_tensor: (N,C,H,W). It should be the output of BatchNorm2d layer.</div><div class="line">        """</div><div class="line">        selected_index = np.squeeze(np.argwhere(self.indexes.data.cpu().numpy()))</div><div class="line">        <span class="keyword">if</span> selected_index.size == <span class="number">1</span>:</div><div class="line">            selected_index = np.resize(selected_index, (<span class="number">1</span>,))</div><div class="line">        output = input_tensor[:, selected_index, :, :]</div><div class="line">        <span class="keyword">return</span> output</div></pre></td></tr></table></figure><p>略微解释一下：在开始加入L1正则，惩罚gamma的时候，相当于identity变换；当确定剪枝参数后，相应index会被置为$0$，被mask掉，这样输出就没有这个channel了。后面的几路都可以用这个共同的输出。</p><h2 id="AutoPruner-Wu-Jianxin"><a href="#AutoPruner-Wu-Jianxin" class="headerlink" title="AutoPruner, Wu Jianxin"></a>AutoPruner, Wu Jianxin</h2><p>这篇文章<a href="https://arxiv.org/abs/1805.08941" target="_blank" rel="external">AutoPruner: An End-to-End Trainable Filter Pruning Method for Efficient Deep Model Inference</a>是南大Wu Jianxin组新进发的文章，还没有投稿到任何学术会议或期刊，只是挂在了Arvix上，应该是还不够完善。他们还有一篇文章ThiNet：<a href="https://arxiv.org/abs/1707.06342" target="_blank" rel="external">ThiNet: A Filter Level Pruning Method for Deep Neural Network Compression</a>发表在ICCV 2017上。</p><p>这篇文章的主要贡献是提出了一种端到端的模型剪枝方法，如下图所示。为第$i$个Conv输出加上一个旁路，输入为其输出的Feature map，依次经过Batch-wise Pooling -&gt; FC -&gt; scaled sigmoid的变换，按channel输出取值在$[0,1]$范围的向量作为mask，与Feature map做积，mask掉相应的channel。通过学习FC的参数，就可以得到适当的mask，判断该剪掉第$i$个Conv的哪个filter。其中，scaled sigmoid变换是指$y = \sigma(\alpha x)$。通过训练过程中不断调大$\alpha$，就可以控制sigmoid的“硬度”，最终实现$0-1$门的效果。<br><img src="/img/paper-summary-autopruner-arch.png" alt="AutoPruner框图"></p><p>构造损失函数$\mathcal{L} = \mathcal{L}_{\text{cross-entropy}} + \lambda \Vert \frac{\Vert v \Vert_1}{C} - r \Vert_2^2$。其中，$v$是sigmoid输出的mask，$C$为输出的channel数量，$r$为目标稀疏度。</p><p>不过在具体的细节上，作者表示要注意的东西很多。主要是FC层的初始化和几个超参数的处理。作者在论文中提出了相应想法：</p><ul><li>FC层初始化权重为$0$均值，方差为$10\sqrt{\frac{2}{n}}$的高斯分布，其中$n = C\times H \times W$。</li><li>上述$\alpha$的控制，如何增长$\alpha$。作者设计了一套if-else的规则。</li><li>上述损失函数中的比例$\lambda$，作者使用了$\lambda = 100 \vert r_b - r\vert$的自适应调节方法。</li></ul><p><img src="/img/paper-summary-model-compression-autopruner-alg.png" alt="AutoPruner Alg"></p><h2 id="Rethinking-Net-Pruning-匿名"><a href="#Rethinking-Net-Pruning-匿名" class="headerlink" title="Rethinking Net Pruning, 匿名"></a>Rethinking Net Pruning, 匿名</h2><p>这篇文章<a href="https://openreview.net/pdf?id=rJlnB3C5Ym" target="_blank" rel="external">Rethinking the Value of Network Pruning</a>有意思了。严格说来，它还在ICLR 2019的匿名评审阶段，并没有被接收。不过这篇文章的炮口已经瞄准了之前提出的好几个model pruning方法，对它们的结果提出了质疑。上面的链接中，也有被diss的方法之一的作者He Yihui和本文作者的交流。</p><p>之前的剪枝算法大多考虑两个问题：</p><ol><li>怎么求得一个高效的剪枝模型结构，如何确定剪枝方式和剪枝比例：在哪里剪，剪多少</li><li>剪枝模型的参数求取：如何保留原始模型中重要的weight，对进行补偿，使得accuracy等性能指标回复到原始模型</li></ol><p>而本文的作者check了六种SOA的工作，发现：在剪枝算法得到的模型上进行finetune，只比相同结构，但是使用random初始化权重的网络performance好了一点点，甚至有的时候还不如。作者的结论是：</p><ol><li>训练一个over parameter的model对最终得到一个efficient的小模型不是必要的</li><li>为了得到剪枝后的小模型，求取大模型中的important参数其实并不打紧</li><li>剪枝得到的结构，相比求得的weight，更重要。所以不如将剪枝算法看做是网络结构搜索的一种特例。</li></ol><p>作者立了两个论点来打：</p><ol><li>要先训练一个over-parameter的大模型，然后在其基础上剪枝。因为大模型有更强大的表达能力。</li><li>剪枝之后的网络结构和权重都很重要，是剪枝模型finetune的基础。</li></ol><p>作者试图通过实验证明，很多剪枝方法并没有他们声称的那么有效，很多时候，无需剪枝之后的权重，而是直接随机初始化并训练，就能达到这些论文中的剪枝方法的效果。当然，这些论文并不是一无是处。作者提出，是剪枝之后的结构更重要。这些剪枝方法可以看做是网络结构的搜索。</p><p>论文的其他部分就是对几种现有方法的实验和diss。我还没有细看，如果后续这篇论文得到了接收，再做总结吧~夹带一些私货，基于几篇论文的实现经验和在真实数据集上的测试，这篇文章的看法我是同意的。</p><p>更新：这篇文章的作者原来正是Net Sliming的作者Liu Zhuang和Huang Gao，那实验和结论应该是很有保障的。最近这篇文章确实也引起了大家的注意，值得好好看一看。</p><h2 id="其他论文等资源"><a href="#其他论文等资源" class="headerlink" title="其他论文等资源"></a>其他论文等资源</h2><ul><li><a href="https://nervanasystems.github.io/distiller/index.html" target="_blank" rel="external">Distiller</a>：一个使用PyTorch实现的剪枝工具包</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;模型剪枝是常用的模型压缩方法之一。这篇是最近看的模型剪枝相关论文的总结。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/img/paper-summary-model-pruning-joke.jpg&quot; alt=&quot;剪枝的学问&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="paper" scheme="https://xmfbit.github.io/tags/paper/"/>
    
      <category term="deep learning" scheme="https://xmfbit.github.io/tags/deep-learning/"/>
    
      <category term="model compression" scheme="https://xmfbit.github.io/tags/model-compression/"/>
    
  </entry>
  
  <entry>
    <title>VIM安装YouCompleteMe和Jedi进行自动补全</title>
    <link href="https://xmfbit.github.io/2018/10/02/vim-you-complete-me/"/>
    <id>https://xmfbit.github.io/2018/10/02/vim-you-complete-me/</id>
    <published>2018-10-02T14:30:54.000Z</published>
    <updated>2018-10-27T07:16:52.421Z</updated>
    
    <content type="html"><![CDATA[<p>这篇主要记录自己尝试编译Anaconda + VIM并安装Jedi和YouCompleteMe自动补全插件的过程。踩了一些坑，不过最后还是装上了。给VIM装上了Dracula主题，有点小清新的感觉~</p><p><img src="/img/vim-config-demo.png" alt="我的VIM"></p><a id="more"></a><h2 id="使用Jedi和YouCompleteMe配置Vim"><a href="#使用Jedi和YouCompleteMe配置Vim" class="headerlink" title="使用Jedi和YouCompleteMe配置Vim"></a>使用Jedi和YouCompleteMe配置Vim</h2><p>在远程开发机上调试代码时，我的习惯是大型项目使用sshfs将其镜像到本地，然后使用VSCode打开编辑。VSCode中有终端可以方便的ssh到远端开发机，我将”CTRL+`”配置成了编辑器和终端之间的切换快捷键。加上vim插件，就可以实现不用鼠标，不离开当前编辑环境进行代码编写和调试了。</p><p>然而，如果是想在开发机上写一段小的代码，上述方法就显得太麻烦了。</p><h2 id="编译Vim"><a href="#编译Vim" class="headerlink" title="编译Vim"></a>编译Vim</h2><p>编译Vim，注意我们要设定其安装目录为anaconda下的bin目录：</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">./configure --with-features=huge --enable-multibyte --enable-pythoninterp=yes --with-python-config-dir=/path/to/anaconda/bin/python-config --enable-gui=gtk2 --prefix=/path/to/anaconda</div></pre></td></tr></table></figure><p>编译并安装：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">make -j4 VIMRUNTIMEDIR=/path/to/anaconda/share/vim/vim81</div><div class="line">make install</div></pre></td></tr></table></figure></p><p>安装后，可以查看vim的version进行确认。安装没有问题，会提示刚才编译的版本信息。<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">vim --version</div></pre></td></tr></table></figure></p><p>使用Vundle管理插件，这个没有什么问题，直接按照README提示即可，见：<a href="https://github.com/VundleVim/Vundle.vim" target="_blank" rel="external">Vundle@Github</a>。</p><p>使用Vundle进行插件管理，只需要以下面的形式指明插件目录或Github仓库名称，进入vim后，在Normal状态，输入<code>:PluginInstall</code>即可。</p><h2 id="Jedi"><a href="#Jedi" class="headerlink" title="Jedi"></a>Jedi</h2><p>首先需要安装jedi的python包：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">pip install jedi</div></pre></td></tr></table></figure></p><p>使用Vbudle安装<a href="https://github.com/davidhalter/jedi-vim" target="_blank" rel="external">jedi-vim</a>，并在<code>.vimrc</code>中添加以下内容。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">let g:jedi#force_py_version=2.7</div></pre></td></tr></table></figure></p><h2 id="YouCompleteMe"><a href="#YouCompleteMe" class="headerlink" title="YouCompleteMe"></a>YouCompleteMe</h2><p>使用Vundle安装<a href="https://github.com/Valloric/YouCompleteMe#ubuntu-linux-x64" target="_blank" rel="external">YouCompleteMe</a>。</p><p>之后，进入目录<code>.vim/bundle/YouCompleteMe</code>，执行<code>./install.py</code>。如果需要C++支持，执行<code>./install.py --clang-completer</code>。</p><p>但是，其中遇到了问题，找不到Python.h文件。使用<code>locate Python.h</code>，明确该文件确实存在，且其位于<code>/path/to/anaconda/include/python2.7</code>后，手动修改CMakeLists.txt，指定该文件目录位置即可。</p><p>修改这个：<br><code>.vim/bundle/YouCompleteMe/third_party/ycmd/cpp/CMakeLists.txt</code><br>和<br><code>.vim/bundle/YouCompleteMe/third_party/ycmd/cpp/ycm/CMakeLists.txt</code>，向其中添加：</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="built_in">set</span>( CMAKE_CXX_FLAGS <span class="string">"<span class="variable">$&#123;CMAKE_CXX_FLAGS&#125;</span> -I/path/to/anaconda/include/python2.7"</span> )</div></pre></td></tr></table></figure><p>强行指定头文件包含目录。</p><h2 id="括号自动补全"><a href="#括号自动补全" class="headerlink" title="括号自动补全"></a>括号自动补全</h2><p>虽然SO上有人指出可以直接通过设置<code>.vimrc</code>的方法实现，不过还是直接用现成的插件吧。推荐使用<a href="https://github.com/jiangmiao/auto-pairs" target="_blank" rel="external">jiangmiao/auto-pairs</a>。可以按照README的说明进行安装。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这篇主要记录自己尝试编译Anaconda + VIM并安装Jedi和YouCompleteMe自动补全插件的过程。踩了一些坑，不过最后还是装上了。给VIM装上了Dracula主题，有点小清新的感觉~&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/img/vim-config-demo.png&quot; alt=&quot;我的VIM&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="vim" scheme="https://xmfbit.github.io/tags/vim/"/>
    
      <category term="tools" scheme="https://xmfbit.github.io/tags/tools/"/>
    
  </entry>
  
  <entry>
    <title>MXNet fit介绍</title>
    <link href="https://xmfbit.github.io/2018/10/02/mxnet-fit-usage/"/>
    <id>https://xmfbit.github.io/2018/10/02/mxnet-fit-usage/</id>
    <published>2018-10-02T14:11:15.000Z</published>
    <updated>2018-10-27T07:16:52.402Z</updated>
    
    <content type="html"><![CDATA[<p>在MXNet中，<code>Module</code>提供了训练模型的方便接口。使用<code>symbol</code>将计算图建好之后，用<code>Module</code>包装一下，就可以通过<code>fit()</code>方法对其进行训练。当然，官方提供的接口一般只适合用来训练分类任务，如果是其他任务（如detection, segmentation等），单纯使用<code>fit()</code>接口就不太合适。这里把<code>fit()</code>代码梳理一下，也是为了后续方便在其基础上实现扩展，更好地用在自己的任务。</p><p>其实如果看开源代码数量的话，MXNet已经显得式微，远不如TensorFlow，PyTorch也早已经后来居上。不过据了解，很多公司内部都有基于MXNet自研的框架或平台工具。下面这张图来自LinkedIn上的一个<a href="https://www.slideshare.net/beam2d/differences-of-deep-learning-frameworks" target="_blank" rel="external">Slide分享</a>，姑且把它贴在下面，算是当前流行框架的一个比较（应该可以把Torch换成PyTorch）。</p><p><img src="/img/differences-of-deep-learning-frameworks-22-638.jpg" alt="Differences of Deep Learning Frameworks"></p><a id="more"></a><h2 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h2><p>首先，需要将数据绑定到计算图上，并初始化模型的参数，并初始化求解器。这些是求解模型必不可少的。</p><p>其次，还会建立训练的metric，方便我们掌握训练进程和当前模型在训练任务的表现。</p><p>这些是在为后续迭代进行梯度下降更新做准备。</p><h2 id="迭代更新"><a href="#迭代更新" class="headerlink" title="迭代更新"></a>迭代更新</h2><p>使用SGD进行训练的时候，我们需要不停地从数据迭代器中获取包含data和label的batch，并将其feed到网络模型中。进行forward computing后进行bp，获得梯度，并根据具体的优化方法（SGD, SGD with momentum, RMSprop等）进行参数更新。</p><p>这部分可以抽成：<br><figure class="highlight py"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># in an epoch</span></div><div class="line"><span class="keyword">while</span> <span class="keyword">not</span> end_epoch:</div><div class="line">    batch = next(train_iter)</div><div class="line">    m.forward_backward(batch)</div><div class="line">    m.update()</div><div class="line">    <span class="keyword">try</span>:</div><div class="line">        next_batch = next(data_iter)</div><div class="line">        m.prepare(next_batch)</div><div class="line">    <span class="keyword">except</span> StopIteration:</div><div class="line">        end_epoch = <span class="keyword">True</span></div></pre></td></tr></table></figure></p><h2 id="metric"><a href="#metric" class="headerlink" title="metric"></a>metric</h2><p>在训练的时候，观察输出的各种metric是必不可少的。我们对训练过程的把握就是通过metric给出的信息。通常在分类任务中常用到的metric有Accuracy，TopK-Accuracy以及交叉熵损失等，这些已经在MXNet中有了现成的实现。而在<code>fit</code>中，调用了<code>m.update_metric(eval_metric, data_batch.label)</code>实现。这里的<code>eval_metric</code>就是我们指定的metric，而<code>label</code>是batch提供的label。注意，在MXNet中，label一般都是以<code>list</code>的形式给出（对应于多任务学习），也就是说这里的label是<code>list of NDArray</code>。当自己魔改的时候要注意。</p><h2 id="logging"><a href="#logging" class="headerlink" title="logging"></a>logging</h2><p>计算了eval_metric等信息，我们需要将其在屏幕上打印出来。MXNet中可以通过callback实现。另外，保存模型checkpoint这样的功能也是通过callback实现的。一种常用的场景是每过若干个batch，做一次logging，打印当前的metric信息，如交叉熵损失降到多少了，准确率提高到多少了等。MXNet会将以下信息打包成<code>BatchEndParam</code>类型（其实是一个自定义的<code>namedtuple</code>）的变量，包括当前epoch，当前迭代次数，评估的metric。如果你需要更多的信息或者更自由的logging监控，也可以参考代码自己实现。</p><p>我们以常用的<code>Speedometer</code>看一下如何使用这些信息，其功能如下，将训练的速度和metric打印出来。</p><blockquote><p>Logs training speed and evaluation metrics periodically</p></blockquote><p>PS:这里有个隐藏的坑。MXNet中的<code>Speedometer</code>每回调一次，会把<code>metric</code>的内容清除。这在训练的时候当然没问题。但是如果是在validation上跑，就会有问题了。这样最终得到的只是最后一个回调周期那些batch的metric，而不是整个验证集上的。如果在<code>fit</code>方法中传入了<code>eval_batch_end_callback</code>参数就要注意这个问题了。解决办法一是在<code>Speedometer</code>实例初始化时传入<code>auto_reset=False</code>，另一种干脆就不要加这个参数，默认为<code>None</code>好了。同样的问题也发生在调用<code>Module.score()</code>方法来获取模型在验证集上metric的时候。</p><p>可以在<code>Speedometer</code>代码中寻找下面这几行，会更清楚：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">if</span> param.eval_metric <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</div><div class="line">    name_value = param.eval_metric.get_name_value()</div><div class="line">    <span class="keyword">if</span> self.auto_reset:</div><div class="line">        param.eval_metric.reset()</div></pre></td></tr></table></figure><h2 id="在验证集上测试"><a href="#在验证集上测试" class="headerlink" title="在验证集上测试"></a>在验证集上测试</h2><p>当在训练集上跑过一个epoch后，如果提供了验证集的迭代器，会在验证集上对模型进行测试。这里，MXNet直接封装了<code>score()</code>方法。在<code>score</code>中，基本流程和<code>fit()</code>相同，只是我们只需要forward computing即可。</p><h2 id="附"><a href="#附" class="headerlink" title="附"></a>附</h2><p>用了一段时间的MXNet，给我的最大的感觉是MXNet就像一个写计算图的前端，提供了很方便的python接口生成静态图，以及很多“可插拔”的插件（虽然可能不是很全，更像是一份guide而不是拿来即用的tool），如上文中的metric等，使其更适合做成流程化的基础DL平台，供给更上层方便地配置使用。缺点就是隐藏了比较多的实现细节（当然，你完全可以从代码中自己学习，比如从<code>fit()</code>代码了解神经网络的大致训练流程）。至于MXNet宣扬的诸如速度快，图优化，省计算资源等优点，因为我没有过数据对比，就不说了。</p><p>缺点就是写图的时候有时不太灵活（可能也是我写的看的还比较少），即使是和TensorFlow这种同为静态图的DL框架比。另外，貌似MXNet中很多东西都没有跟上最新的论文等，比如Cosine的learning rate decay就没有。Model Zoo也比较少(gluon可能会好一点，Gluon-CV和Gluon-NLP貌似是在搞一些论文复现的工作)。对开发来讲，很多东西都需要阅读代码才能知道是怎么回事，只是读文档的话容易踩坑。</p><p>说到这里，感觉MXNet的python训练接口（包括module，optimizer，metric等）更像是一份example代码，是在教你怎么去用MXNet，而不像一个灵活地强大的工具箱。当然，很多东西不能得兼，希望MXNet越来越好。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在MXNet中，&lt;code&gt;Module&lt;/code&gt;提供了训练模型的方便接口。使用&lt;code&gt;symbol&lt;/code&gt;将计算图建好之后，用&lt;code&gt;Module&lt;/code&gt;包装一下，就可以通过&lt;code&gt;fit()&lt;/code&gt;方法对其进行训练。当然，官方提供的接口一般只适合用来训练分类任务，如果是其他任务（如detection, segmentation等），单纯使用&lt;code&gt;fit()&lt;/code&gt;接口就不太合适。这里把&lt;code&gt;fit()&lt;/code&gt;代码梳理一下，也是为了后续方便在其基础上实现扩展，更好地用在自己的任务。&lt;/p&gt;
&lt;p&gt;其实如果看开源代码数量的话，MXNet已经显得式微，远不如TensorFlow，PyTorch也早已经后来居上。不过据了解，很多公司内部都有基于MXNet自研的框架或平台工具。下面这张图来自LinkedIn上的一个&lt;a href=&quot;https://www.slideshare.net/beam2d/differences-of-deep-learning-frameworks&quot;&gt;Slide分享&lt;/a&gt;，姑且把它贴在下面，算是当前流行框架的一个比较（应该可以把Torch换成PyTorch）。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/img/differences-of-deep-learning-frameworks-22-638.jpg&quot; alt=&quot;Differences of Deep Learning Frameworks&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="mxnet" scheme="https://xmfbit.github.io/tags/mxnet/"/>
    
  </entry>
  
  <entry>
    <title>论文 - Like What You Like - Knowledge Distill via Neuron Selectivity Transfer</title>
    <link href="https://xmfbit.github.io/2018/10/02/paper-knowledge-transfer-neural-selectivity-transfer/"/>
    <id>https://xmfbit.github.io/2018/10/02/paper-knowledge-transfer-neural-selectivity-transfer/</id>
    <published>2018-10-02T13:32:05.000Z</published>
    <updated>2018-10-27T07:16:52.405Z</updated>
    
    <content type="html"><![CDATA[<p>好长时间没有写博客了，国庆假期把最近看的东西整理一下。<a href="https://arxiv.org/abs/1707.01219" target="_blank" rel="external">Like What You Like: Knowledge Distill via Neuron Selectivity Transfer</a>这篇文章是图森的工作，在Knowledge Distilling基础上做出了改进Neural Selectivity Transfer，使用KD + NST方法能够取得SOTA的结果。PS：DL领域的论文名字真的是百花齐放。。。Like what you like。。。感受一下。。。</p><p><img src="/img/paper-nst-kt-like-what-you-like.gif" alt="jump if you jump"></p><p>另外，这篇论文的作者Wang Naiyan大神和Huang Zehao在今年的ECCV 2018上还有一篇论文发表，同样是模型压缩，但是使用了剪枝方法，有兴趣可以关注一下：<a href="https://arxiv.org/abs/1707.01213" target="_blank" rel="external">Data-driven sparse structure selection for deep neural networks</a>。</p><p>另另外，其实这两篇文章挂在Arxiv的时间很接近，<a href="https://www.zhihu.com/question/62068158" target="_blank" rel="external">知乎的讨论帖：如何评价图森科技连发的三篇关于深度模型压缩的文章？</a>有相关回答，可以看一下。DL/CV方法论文实在太多了，感觉Naiyan大神和图森的工作还是很值得信赖的，值得去follow。</p><a id="more"></a><h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><p>KD的一个痛点在于其只适用于softmax分类问题。这样，对于Detection。Segmentation等一系列问题，没有办法应用。另一个问题在于当分类类别数较少时，KD效果不理想。这个问题比较好理解，假设我们面对一个二分类问题，那么我们并不关心类间的similarity，而是尽可能把两类分开即可。同时，这篇文章的实验部分也验证了这个猜想：当分类问题分类类别数较多时，使用KD能够取得best的结果。</p><p>作者联想到，我们是否可以把CNN中某个中间层输出的feature map利用起来呢？Student输出的feature map要和Teacher的相似，相当于是Student学习到了Teacher提取特征的能力。在CNN中，每个filter都是在和一个feature map上的patch做卷积得到输出，很多个filter都做卷积运算，就得到了feature（map）。另外，当filter和patch有相似的结构时，得到的激活比较大。举个例子，如果filter是Sobel算子，那么当输入image是边缘的时候，得到的响应是最大的。filter学习出来的是输入中的某些模式。当模式匹配上时，激活。这里也可以参考一些对CNN中filter做可视化的研究。</p><p>顺着上面的思路，有人提出了Attention Transfer的方法，可以参见这篇文章：<a href="https://arxiv.org/abs/1612.03928" target="_blank" rel="external">Improving the Performance of Convolutional Neural Networks via Attention Transfer</a>。而在NST这篇文章中，作者引入了新的损失函数，用于衡量Student和Teacher对相同输入的激活Feature map的不同，可以说除了下面要介绍的数学概念以外，没有什么难理解的地方。整个训练的网络结构如下所示：<br><img src="/img/paper-nst-student-and-teacher.png" alt="NST知识蒸馏的整体框图结构"></p><h2 id="Maximum-Mean-Discrepancy"><a href="#Maximum-Mean-Discrepancy" class="headerlink" title="Maximum Mean Discrepancy"></a>Maximum Mean Discrepancy</h2><p>MMD 是用来衡量sampled data之间分布差异的距离量度。如果有两个不同的分布$p$和$q$，以及从两个分布中采样得到的Data set$\mathcal{X}$和$\mathcal{Y}$。那么MMD距离如下：</p><script type="math/tex; mode=display">\mathcal{L}(\mathcal{X}, \mathcal{Y}) = \Vert \frac{1}{N}\sum_{i=1}^{N}\phi(x^i) - \frac{1}{M}\sum_{j=1}^{M}\phi(y^j) \Vert_2^2</script><p>其中，$\phi$表示某个mapping function。变形之后（内积打开括号），可以得到：</p><script type="math/tex; mode=display">\mathcal{L}(\mathcal{X}, \mathcal{Y}) = \frac{1}{N^2}\sum_{i=1}^{N}\sum_{j=1}^{N}k(x^i, x^j) + \frac{1}{M^2}\sum_{i=1}^{M}\sum_{j=1}^{M}k(y^i, y^j) - \frac{2}{MN}\sum_{i=1}^{N}\sum_{j=1}^{M}k(x^i, y^j)</script><p>其中，$k$是某个kernel function，$k(x, y) = \phi(x)^{T}\phi(y)$。</p><p>我们可以使用MMD来衡量Student模型和Teacher模型中间输出的激活feature map的相似程度。通过优化这个损失函数，使得S的输出分布接近T。通过引入MMD，将NST loss定义如下，下标$S$表示Student的输出，$T$表示Teacher的输出。第一项$\mathcal{H}$是指由样本类别标签计算的CrossEntropy Loss。第二项即为上述的MMD Loss。</p><script type="math/tex; mode=display">\mathcal{L} = \mathcal{H}(y, p_S) + \frac{\lambda}{2}\mathcal{L}_{MMD}(F_T, F_S)</script><p>注意，为了确保后一项有意义，需要保证$F_T$和$F_S$有相同长度。具体来说，对于网络中间输出的feature map，我们将每个channel上的$HW$维的feature vector作为分布$\mathcal{X}$的一个采样。按照作者的设定，我们需要保证S和T对应的feature map在spatial dimension上必须一样大。如果不一样，可以使用插值方法进行扩展。</p><p>为了不受相对幅值大小的影响，需要对feature vector做normalization。</p><p>对于kernal的选择，作者提出了三种可行方案：线性，多项式和高斯核。在后续通过实验对比了它们的性能。</p><h2 id="和其他方法的关联"><a href="#和其他方法的关联" class="headerlink" title="和其他方法的关联"></a>和其他方法的关联</h2><p>如果使用线性核函数，也就是$\phi$是一个identity mapping，那么MMD就成了直接比较两个样本分布质心的距离。这时候，和上文提到的AT方法的一种形式是类似的。（这个我觉得有点强行扯关系。。。）</p><p>如果使用二次多项式核函数，可以得到，$\mathcal{L}_{MMD}(F_T, F_S) = \Vert G_T - G_S\Vert_F^2$。其中，$G \in \mathbb{R}^{HW\times HW}$为Gram矩阵，其中的元素$g_{ij} = (f^i)^Tf^j$。</p><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p>作者在CIFAR10，ImageNet等数据集上进行了实验。Student均使用Inception-BN网络，Teacher分别使用了ResNet-1001和ResNet-101。一些具体的参数设置参考论文即可。</p><p>下面是CIFAR10上的结果。可以看到，单一方法下，CIFAR10分类NST效果最好，CIFAR100分类KD最好。组合方法中，KD+NST最好。<br><img src="/img/paper-nst-cifar10-results.png" alt="CIFAR10 结果"></p><p>下面是ImageNet上的结果。KD+NST的组合仍然是效果最好的。<br><img src="/img/paper-nst-imagenet-results.png" alt="ImageNet结果"></p><p>作者还对NST前后，Student和Teacher的输出Feature map做了聚类，发现NST确实能够使得S的输出去接近T的输出分布。如下图所示：<br><img src="/img/paper-nst-visulization-teacher-student-feature-map.png" alt="NST减小了T和S的激活feature map的distance"></p><p>此外，作者还实验了在Detection任务上的表现。在PASCAL VOC2007数据集上基于Faster RCNN方法进行了实验。backbone网络仍然是Inception BN，从<code>4b</code>layer获取feature map，此时stide为16。</p><p><img src="/img/paper-nst-pascal-voc-results.png" alt="PASCAL VOC结果"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;好长时间没有写博客了，国庆假期把最近看的东西整理一下。&lt;a href=&quot;https://arxiv.org/abs/1707.01219&quot;&gt;Like What You Like: Knowledge Distill via Neuron Selectivity Transfer&lt;/a&gt;这篇文章是图森的工作，在Knowledge Distilling基础上做出了改进Neural Selectivity Transfer，使用KD + NST方法能够取得SOTA的结果。PS：DL领域的论文名字真的是百花齐放。。。Like what you like。。。感受一下。。。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/img/paper-nst-kt-like-what-you-like.gif&quot; alt=&quot;jump if you jump&quot;&gt;&lt;/p&gt;
&lt;p&gt;另外，这篇论文的作者Wang Naiyan大神和Huang Zehao在今年的ECCV 2018上还有一篇论文发表，同样是模型压缩，但是使用了剪枝方法，有兴趣可以关注一下：&lt;a href=&quot;https://arxiv.org/abs/1707.01213&quot;&gt;Data-driven sparse structure selection for deep neural networks&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;另另外，其实这两篇文章挂在Arxiv的时间很接近，&lt;a href=&quot;https://www.zhihu.com/question/62068158&quot;&gt;知乎的讨论帖：如何评价图森科技连发的三篇关于深度模型压缩的文章？&lt;/a&gt;有相关回答，可以看一下。DL/CV方法论文实在太多了，感觉Naiyan大神和图森的工作还是很值得信赖的，值得去follow。&lt;/p&gt;
    
    </summary>
    
    
      <category term="paper" scheme="https://xmfbit.github.io/tags/paper/"/>
    
      <category term="deep learning" scheme="https://xmfbit.github.io/tags/deep-learning/"/>
    
      <category term="model compression" scheme="https://xmfbit.github.io/tags/model-compression/"/>
    
  </entry>
  
  <entry>
    <title>论文 - Distilling the Knowledge in a Neural Network</title>
    <link href="https://xmfbit.github.io/2018/06/07/knowledge-distilling/"/>
    <id>https://xmfbit.github.io/2018/06/07/knowledge-distilling/</id>
    <published>2018-06-07T13:56:12.000Z</published>
    <updated>2018-10-27T07:16:52.401Z</updated>
    
    <content type="html"><![CDATA[<p>知识蒸馏（Knowledge Distilling）是模型压缩的一种方法，是指利用已经训练的一个较复杂的Teacher模型，指导一个较轻量的Student模型训练，从而在减小模型大小和计算资源的同时，尽量保持原Teacher模型的准确率的方法。这种方法受到大家的注意，主要是由于Hinton的论文<a href="https://arxiv.org/abs/1503.02531" target="_blank" rel="external">Distilling the Knowledge in a Neural Network</a>。这篇博客做一总结。后续还会有KD方法的改进相关论文的心得介绍。</p><a id="more"></a><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>这里我将Wang Naiyang在知乎相关问题的<a href="https://www.zhihu.com/question/50519680/answer/136363665" target="_blank" rel="external">回答</a>粘贴如下，将KD方法的motivation讲的很清楚。图森也发了论文对KD进行了改进，下篇笔记总结。</p><blockquote><p>Knowledge Distill是一种简单弥补分类问题监督信号不足的办法。传统的分类问题，模型的目标是将输入的特征映射到输出空间的一个点上，例如在著名的Imagenet比赛中，就是要将所有可能的输入图片映射到输出空间的1000个点上。这么做的话这1000个点中的每一个点是一个one hot编码的类别信息。这样一个label能提供的监督信息只有log(class)这么多bit。然而在KD中，我们可以使用teacher model对于每个样本输出一个连续的label分布，这样可以利用的监督信息就远比one hot的多了。另外一个角度的理解，大家可以想象如果只有label这样的一个目标的话，那么这个模型的目标就是把训练样本中每一类的样本强制映射到同一个点上，这样其实对于训练很有帮助的类内variance和类间distance就损失掉了。然而使用teacher model的输出可以恢复出这方面的信息。具体的举例就像是paper中讲的， 猫和狗的距离比猫和桌子要近，同时如果一个动物确实长得像猫又像狗，那么它是可以给两类都提供监督。综上所述，KD的核心思想在于”打散”原来压缩到了一个点的监督信息，让student模型的输出尽量match teacher模型的输出分布。其实要达到这个目标其实不一定使用teacher model，在数据标注或者采集的时候本身保留的不确定信息也可以帮助模型的训练。</p></blockquote><h2 id="蒸馏"><a href="#蒸馏" class="headerlink" title="蒸馏"></a>蒸馏</h2><p>这篇论文很好阅读。论文中实现蒸馏是靠soften softmax prob实现的。在分类任务中，常常使用交叉熵作为损失函数，使用one-hot编码的标注好的类别标签${1,2,\dots,K}$作为target，如下所示：</p><script type="math/tex; mode=display">\mathcal{L} = -\sum_{i=1}^{K}t_i\log p_i</script><p>作者指出，粗暴地使用one-hot编码丢失了类间和类内关于相似性的额外信息。举个例子，在手写数字识别时，$2$和$3$就长得很像。但是使用上述方法，完全没有考虑到这种相似性。对于已经训练好的模型，当识别数字$2$时，很有可能它给出的概率是：数字$2$为$0.99$，数字$3$为$10^{-2}$，数字$7$为$10^{-4}$。如何能够利用训练好的Teacher模型给出的这种信息呢？</p><p>可以使用带温度的softmax函数。对于softmax的输入（下文统一称为logit），我们按照下式给出输出：</p><script type="math/tex; mode=display">q_i = \frac{\exp(z_i/T)}{\sum_j \exp(z_j/T)}</script><p>其中，当$T = 1$时，就是普通的softmax变换。这里令$T &gt; 1$，就得到了软化的softmax。（这个很好理解，除以一个比$1$大的数，相当于被squash了，线性的sqush被指数放大，差距就不会这么大了）。OK，有了这个东西，我们将Teacher网络和Student的最后充当分类器的那个全连接层的输出都做这个处理。</p><p>对Teacher网络的logit如此处理，得到的就是soft target。相比于one-hot的ground truth或softmax的prob输出，这个软化之后的target能够提供更多的类别间和类内信息。<br>可以对待训练的Student网络也如此处理，这样就得到了另外一个“交叉熵”损失：</p><script type="math/tex; mode=display">\mathcal{L}_{soft}=-\sum_{i=1}^{K}p_i\log q_i</script><p>其中，$p_i$为Teacher模型给出的soft target，$q_i$为Student模型给出的soft output。作者发现，最好的方式是做一个multi task learning，将上面这个损失函数和真正的交叉熵损失加权相加。相应地，我们将其称为hard target。</p><script type="math/tex; mode=display">\mathcal{L} = \mathcal{L}_{hard} + \lambda \mathcal{L}_{soft}</script><p>其中，$\mathcal{L}_{hard}$是分类问题中经典的交叉熵损失。由于做softened softmax计算时，需要除以$T$，导致soft target关联的梯度幅值被缩小了$T^2$倍，所以有必要在$\lambda$中预先考虑到$T^2$这个因子。</p><p>PS:这里有一篇地平线烫叔关于多任务中loss函数设计的回答：<a href="https://www.zhihu.com/question/268105631/answer/335246543" target="_blank" rel="external">神经网络中，设计loss function有哪些技巧? - Alan Huang的回答 - 知乎</a>。</p><h2 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h2><p>这里给出一个开源的MXNet的实现:<a href="https://github.com/TuSimple/neuron-selectivity-transfer/blob/master/symbol/transfer.py#L4" target="_blank" rel="external">kd loss by mxnet</a>。MXNet中的<code>SoftmaxOutput</code>不仅能直接支持one-hot编码类型的array作为label输入，甚至label的<code>dtype</code>也可以不是整型！</p><figure class="highlight py"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">kd</span><span class="params">(student_hard_logits, teacher_hard_logits, temperature, weight_lambda, prefix)</span>:</span></div><div class="line">    student_soft_logits = student_hard_logits / temperature</div><div class="line">    teacher_soft_logits = teacher_hard_logits / temperature</div><div class="line">    teacher_soft_labels = mx.symbol.SoftmaxActivation(teacher_soft_logits,</div><div class="line">        name=<span class="string">"teacher%s_soft_labels"</span> % prefix)</div><div class="line">    kd_loss = mx.symbol.SoftmaxOutput(data=student_soft_logits, label=teacher_soft_labels,</div><div class="line">                                      grad_scale=weight_lambda, name=<span class="string">"%skd_loss"</span> % prefix)</div><div class="line">    <span class="keyword">return</span> kd_loss</div></pre></td></tr></table></figure><h2 id="matching-logit是特例"><a href="#matching-logit是特例" class="headerlink" title="matching logit是特例"></a>matching logit是特例</h2><p>（这部分没什么用，练习推导了一下交叉熵损失的梯度计算）</p><p>在Hinton之前，有学者提出可以匹配Teacher和Student输出的logit，Hinton指出这是本文方法在一定假设下的近似。为了和论文中的符号相同，下面我们使用$C$表示soft target带来的loss，Teacher和Student第$i$个神经元输出的logit分别为$v_i$和$z_i$，输出的softened softmax分别为$p_i$和$q_i$。那么我们有：</p><script type="math/tex; mode=display">C = -\sum_{j=1}^{C}p_j \log q_j</script><p>而且，</p><script type="math/tex; mode=display">p_i = \frac{\exp(v_i/T)}{\sum_j \exp(v_j/T)}</script><script type="math/tex; mode=display">q_i = \frac{\exp(z_i/T)}{\sum_j \exp(z_j/T)}</script><p>让我们暂时忽略$T$（最后我们乘上$\frac{1}{T}$即可），我们有：</p><script type="math/tex; mode=display">\frac{\partial C}{\partial z_i} = -\sum_{j=1}^{K}p_j\frac{1}{q_j}\frac{\partial q_j}{\partial z_i}</script><p>分情况讨论，当$i = j$时，有：</p><script type="math/tex; mode=display">\frac{\partial q_j}{\partial z_i} = q_i (1-q_i)</script><p>当$i \neq j$时，有：</p><script type="math/tex; mode=display">\begin{aligned}\frac{\partial q_j}{\partial z_i} &= \frac{-e^{z_i}e^{z_j}}{(\sum_k e^{z_k})^2}  \\&=-q_iq_j\end{aligned}</script><p>这样，我们有：</p><script type="math/tex; mode=display">\begin{aligned} \frac{\partial C}{\partial z_i} &= - p_i\frac{1}{q_i}q_i(1-q_i) + \sum_{j=1, j\neq i}^{K}p_j\frac{1}{q_j}q_iq_j  \\&= -p_i + p_iq_i + \sum_{j=1, j\neq i}^K p_jq_i \\&= q_i -p_i\end{aligned}</script><p>当然，其实上面的推导过程只不过是重复了一遍one-hot编码的交叉熵损失的计算。</p><p>这样，如果我们假设logit是零均值的，也就是说$\sum_j z_j = \sum_j v_j = 0$，那么有：</p><script type="math/tex; mode=display">\frac{\partial C}{\partial z_i} \sim \frac{1}{NT^2}(z_i - v_i)</script><p>所以说，MSE下进行logit的匹配，是本文方法的一个特例。</p><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p>作者使用了MNIST进行图片分类的实验，一个有趣的地方在于（和论文前半部分举的$2$和$3$识别的例子呼应），作者在数据集中有意地去除了标签为$3$的样本。没有KD的student网络不能识别测试时候提供的$3$，有KD的student网络能够识别一些$3$（虽然它从来没有在训练样本中出现过！）。后面，作者在语音识别和一个Google内部的很大的图像分类数据集（JFT dataset）上做了实验，</p><h2 id="附"><a href="#附" class="headerlink" title="附"></a>附</h2><ul><li>知乎上关于soft target的讨论，有Wang Naiyan和Zhou Bolei的分析：<a href="https://www.zhihu.com/question/50519680" target="_blank" rel="external">如何理解soft target这一做法？</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;知识蒸馏（Knowledge Distilling）是模型压缩的一种方法，是指利用已经训练的一个较复杂的Teacher模型，指导一个较轻量的Student模型训练，从而在减小模型大小和计算资源的同时，尽量保持原Teacher模型的准确率的方法。这种方法受到大家的注意，主要是由于Hinton的论文&lt;a href=&quot;https://arxiv.org/abs/1503.02531&quot;&gt;Distilling the Knowledge in a Neural Network&lt;/a&gt;。这篇博客做一总结。后续还会有KD方法的改进相关论文的心得介绍。&lt;/p&gt;
    
    </summary>
    
    
      <category term="paper" scheme="https://xmfbit.github.io/tags/paper/"/>
    
      <category term="deep learning" scheme="https://xmfbit.github.io/tags/deep-learning/"/>
    
      <category term="model compression" scheme="https://xmfbit.github.io/tags/model-compression/"/>
    
  </entry>
  
  <entry>
    <title>（译）PyTorch 0.4.0 Migration Guide</title>
    <link href="https://xmfbit.github.io/2018/04/27/pytorch-040-migration-guide/"/>
    <id>https://xmfbit.github.io/2018/04/27/pytorch-040-migration-guide/</id>
    <published>2018-04-27T02:49:31.000Z</published>
    <updated>2018-10-27T07:16:52.414Z</updated>
    
    <content type="html"><![CDATA[<p>PyTorch在前两天官方发布了0.4.0版本。这个版本与之前相比，API发生了较大的变化，所以官方也出了一个<a href="http://pytorch.org/2018/04/22/0_4_0-migration-guide.html" target="_blank" rel="external">转换指导</a>，这篇博客是这篇指导的中文翻译版。归结起来，对我们代码影响最大的地方主要有：</p><ul><li><code>Tensor</code>和<code>Variable</code>合并，<code>autograd</code>的机制有所不同，变得更简单，使用<code>requires_grad</code>和上下文相关环境管理。</li><li>Numpy风格的<code>Tensor</code>构建。</li><li>提出了<code>device</code>，更简单地在cpu和gpu中移动数据。</li></ul><a id="more"></a><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>在0.4.0版本中，PyTorch引入了许多令人兴奋的新特性和bug fixes。为了方便以前版本的使用者转换到新的版本，我们编写了此指导，主要包括以下几个重要的方面：</p><ul><li><code>Tensors</code> 和 <code>Variables</code> 已经merge到一起了</li><li>支持0维的Tensor（即标量scalar）</li><li>弃用了 <code>volatile</code> 标志</li><li><code>dtypes</code>, <code>devices</code>, 和 Numpy 风格的 Tensor构造函数</li><li>（更好地编写）设备无关代码</li></ul><p>下面分条介绍。</p><h2 id="Tensor-和-Variable-合并"><a href="#Tensor-和-Variable-合并" class="headerlink" title="Tensor 和 Variable 合并"></a><code>Tensor</code> 和 <code>Variable</code> 合并</h2><p>在PyTorch以前的版本中，<code>Tensor</code>类似于<code>numpy</code>中的<code>ndarray</code>，只是对多维数组的抽象。为了能够使用自动求导机制，必须使用<code>Variable</code>对其进行包装。而现在，这两个东西已经完全合并成一个了，以前<code>Variable</code>的使用情境都可以使用<code>Tensor</code>。所以以前训练的时候总要额外写的warpping语句用不到了。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">for</span> data, target <span class="keyword">in</span> data_loader:</div><div class="line">    <span class="comment">## 用不到了</span></div><div class="line">    data, target = Variable(data), Variable(target)</div><div class="line">    loss = criterion(model(data), target)</div></pre></td></tr></table></figure><h3 id="Tensor的类型type"><a href="#Tensor的类型type" class="headerlink" title="Tensor的类型type()"></a><code>Tensor</code>的类型<code>type()</code></h3><p>以前我们可以使用<code>type()</code>获取<code>Tensor</code>的data type（FloatTensor，LongTensor等）。现在需要使用<code>x.type()</code>获取类型或<code>isinstance()</code>判别类型。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="meta">&gt;&gt;&gt; </span>x = torch.DoubleTensor([<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>])</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>print(type(x))  <span class="comment"># 曾经会给出 torch.DoubleTensor</span></div><div class="line"><span class="string">"&lt;class 'torch.Tensor'&gt;"</span></div><div class="line"><span class="meta">&gt;&gt;&gt; </span>print(x.type())  <span class="comment"># OK: 'torch.DoubleTensor'</span></div><div class="line"><span class="string">'torch.DoubleTensor'</span></div><div class="line"><span class="meta">&gt;&gt;&gt; </span>print(isinstance(x, torch.DoubleTensor))  <span class="comment"># OK: True</span></div><div class="line"><span class="keyword">True</span></div></pre></td></tr></table></figure><h3 id="autograd现在如何追踪计算图的历史"><a href="#autograd现在如何追踪计算图的历史" class="headerlink" title="autograd现在如何追踪计算图的历史"></a><code>autograd</code>现在如何追踪计算图的历史</h3><p><code>Tensor</code>和<code>Variable</code>的合并，简化了计算图的构建，具体规则见本条和以下几条说明。</p><p><code>requires_grad</code>, 这个<code>autograd</code>中的核心标志量,现在成了<code>Tensor</code>的属性。之前的<code>Variable</code>使用规则可以同样应用于<code>Tensor</code>，<code>autograd</code>自动跟踪那些至少有一个input的<code>requires_grad==True</code>的计算节点构成的图。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div></pre></td><td class="code"><pre><div class="line"><span class="meta">&gt;&gt;&gt; </span>x = torch.ones(<span class="number">1</span>)  <span class="comment">## 默认requires_grad = False</span></div><div class="line"><span class="meta">&gt;&gt;&gt; </span>x.requires_grad</div><div class="line"><span class="keyword">False</span></div><div class="line"><span class="meta">&gt;&gt;&gt; </span>y = torch.ones(<span class="number">1</span>)  <span class="comment">## 同样，y的requires_grad标志也是False</span></div><div class="line"><span class="meta">&gt;&gt;&gt; </span>z = x + y</div><div class="line"><span class="meta">&gt;&gt;&gt; </span><span class="comment">## 所有的输入节点都不要求梯度，所以z的requires_grad也是False</span></div><div class="line"><span class="meta">&gt;&gt;&gt; </span>z.requires_grad</div><div class="line"><span class="keyword">False</span></div><div class="line"><span class="meta">&gt;&gt;&gt; </span><span class="comment">## 所以如果试图对z做梯度反传，会抛出Error</span></div><div class="line"><span class="meta">&gt;&gt;&gt; </span>z.backward()</div><div class="line">RuntimeError: element <span class="number">0</span> of tensors does <span class="keyword">not</span> require grad <span class="keyword">and</span> does <span class="keyword">not</span> have a grad_fn</div><div class="line">&gt;&gt;&gt;</div><div class="line"><span class="meta">&gt;&gt;&gt; </span><span class="comment">## 通过手动指定的方式创建 requires_grad=True 的Tensor</span></div><div class="line"><span class="meta">&gt;&gt;&gt; </span>w = torch.ones(<span class="number">1</span>, requires_grad=<span class="keyword">True</span>)</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>w.requires_grad</div><div class="line"><span class="keyword">True</span></div><div class="line"><span class="meta">&gt;&gt;&gt; </span><span class="comment">## 把它和之前requires_grad=False的节点相加，得到输出</span></div><div class="line"><span class="meta">&gt;&gt;&gt; </span>total = w + z</div><div class="line"><span class="meta">&gt;&gt;&gt; </span><span class="comment">## 由于w需要梯度，所以total也需要</span></div><div class="line"><span class="meta">&gt;&gt;&gt; </span>total.requires_grad</div><div class="line"><span class="keyword">True</span></div><div class="line"><span class="meta">&gt;&gt;&gt; </span><span class="comment">## 可以做bp</span></div><div class="line"><span class="meta">&gt;&gt;&gt; </span>total.backward()</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>w.grad</div><div class="line">tensor([ <span class="number">1.</span>])</div><div class="line"><span class="meta">&gt;&gt;&gt; </span><span class="comment">## 不用有时间浪费在求取 x y z的梯度上，因为它们没有 require grad，它们的grad == None</span></div><div class="line"><span class="meta">&gt;&gt;&gt; </span>z.grad == x.grad == y.grad == <span class="keyword">None</span></div><div class="line"><span class="keyword">True</span></div></pre></td></tr></table></figure><h3 id="操作-requires-grad-标志"><a href="#操作-requires-grad-标志" class="headerlink" title="操作 requires_grad 标志"></a>操作 <code>requires_grad</code> 标志</h3><p>除了直接设置这个属性，你可以使用<code>my_tensor.requires_grad_()</code>就地修改这个标志（还记得吗，以<code>_</code>结尾的方法名表示in-place的操作）。或者就在构造的时候传入此参数。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="meta">&gt;&gt;&gt; </span>existing_tensor.requires_grad_()</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>existing_tensor.requires_grad</div><div class="line"><span class="keyword">True</span></div><div class="line"><span class="meta">&gt;&gt;&gt; </span>my_tensor = torch.zeros(<span class="number">3</span>, <span class="number">4</span>, requires_grad=<span class="keyword">True</span>)</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>my_tensor.requires_grad</div><div class="line"><span class="keyword">True</span></div></pre></td></tr></table></figure><h3 id="data怎么办？What-about-data"><a href="#data怎么办？What-about-data" class="headerlink" title=".data怎么办？What about .data?"></a><code>.data</code>怎么办？What about .data?</h3><p>原来版本中，对于某个<code>Variable</code>，我们可以通过<code>x.data</code>的方式获取其包装的<code>Tensor</code>。现在两者已经merge到了一起，如果你调用<code>y = x.data</code>仍然和以前相似，<code>y</code>现在会共享<code>x</code>的data，并与<code>x</code>的计算历史无关，且其<code>requires_grad</code>标志为<code>False</code>。</p><p>然而，<code>.data</code>有的时候可能会成为代码中不安全的一个点。对<code>x.data</code>的任何带动都不会被<code>aotograd</code>跟踪。所以，当做反传的时候，计算的梯度可能会不对，一种更安全的替代方法是调用<code>x.detach()</code>，仍然会返回一个共享<code>x</code>data的Tensor，且<code>requires_grad=False</code>，但是当<code>x</code>需要bp的时候，</p><p>However, .data can be unsafe in some cases. Any changes on x.data wouldn’t be tracked by autograd, and the computed gradients would be incorrect if x is needed in a backward pass. A safer alternative is to use x.detach(), which also returns a Tensor that shares data with requires_grad=False, but will have its in-place changes reported by autograd if x is needed in backward.</p><h2 id="支持0维-scalar-的Tensor"><a href="#支持0维-scalar-的Tensor" class="headerlink" title="支持0维(scalar)的Tensor"></a>支持0维(scalar)的Tensor</h2><p>原来的版本中，对Tensor vector（1D Tensor）做索引得到的结果是一个python number，但是对一个Variable vector来说，得到的就是一个<code>size(1,)</code>的vector!对于reduction function（如<code>torch.sum</code>，<code>torch.max</code>）也有这样的问题。</p><p>所以我们引入了scalar（0D Tensor）。它可以使用<code>torch.tensor()</code> 函数来创建，现在你可以这样做：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line"><span class="meta">&gt;&gt;&gt; </span>torch.tensor(<span class="number">3.1416</span>)         <span class="comment"># 直接创建scalar</span></div><div class="line">tensor(<span class="number">3.1416</span>)</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>torch.tensor(<span class="number">3.1416</span>).size()  <span class="comment"># scalar 是 0D</span></div><div class="line">torch.Size([])</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>torch.tensor([<span class="number">3</span>]).size()     <span class="comment"># 和1D对比</span></div><div class="line">torch.Size([<span class="number">1</span>])</div><div class="line">&gt;&gt;&gt;</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>vector = torch.arange(<span class="number">2</span>, <span class="number">6</span>)  <span class="comment"># 1D的vector</span></div><div class="line"><span class="meta">&gt;&gt;&gt; </span>vector</div><div class="line">tensor([ <span class="number">2.</span>,  <span class="number">3.</span>,  <span class="number">4.</span>,  <span class="number">5.</span>])</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>vector.size()</div><div class="line">torch.Size([<span class="number">4</span>])</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>vector[<span class="number">3</span>]                    <span class="comment"># 对1D的vector做indexing，得到的是scalar</span></div><div class="line">tensor(<span class="number">5.</span>)</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>vector[<span class="number">3</span>].item()             <span class="comment"># 使用.item()获取python number</span></div><div class="line"><span class="number">5.0</span></div><div class="line"><span class="meta">&gt;&gt;&gt; </span>mysum = torch.tensor([<span class="number">2</span>, <span class="number">3</span>]).sum()</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>mysum</div><div class="line">tensor(<span class="number">5</span>)</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>mysum.size()</div><div class="line">torch.Size([])</div></pre></td></tr></table></figure><h3 id="累积losses"><a href="#累积losses" class="headerlink" title="累积losses"></a>累积losses</h3><p>我们在训练的时候，经常有这样的用法：<code>total_loss += loss.data[0]</code>。<code>loss</code>通常都是由损失函数计算出来的一个标量，也就是包装了<code>(1,)</code>大小<code>Tensor</code>的<code>Variable</code>。在新的版本中，<code>loss</code>则变成了0D的scalar。对一个scalar做indexing是没有意义的，应该使用<code>loss.item()</code>获取python number。</p><p>注意，如果你在做累加的时候没有转换为python number，你的程序可能会出现不必要的内存占用。因为<code>autograd</code>会记录调用过程，以便做反向传播。所以，你现在应该写成 <code>total_loss += loss.item()</code>。</p><h2 id="弃用volatile标志"><a href="#弃用volatile标志" class="headerlink" title="弃用volatile标志"></a>弃用<code>volatile</code>标志</h2><p><code>volatile</code> 标志被弃用了，现在没有任何效果。以前的版本中，一个设置<code>volatile=True</code>的<code>Variable</code> 表明其不会被<code>autograd</code>追踪。现在，被替换成了一个更灵活的上下文管理器，如<code>torch.no_grad()</code>，<code>torch.set_grad_enable(grad_mode)</code>等。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line"><span class="meta">&gt;&gt;&gt; </span>x = torch.zeros(<span class="number">1</span>, requires_grad=<span class="keyword">True</span>)</div><div class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">with</span> torch.no_grad():    <span class="comment"># 使用 torch,no_grad()构建不需要track的上下文环境</span></div><div class="line"><span class="meta">... </span>    y = x * <span class="number">2</span></div><div class="line"><span class="meta">&gt;&gt;&gt; </span>y.requires_grad</div><div class="line"><span class="keyword">False</span></div><div class="line">&gt;&gt;&gt;</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>is_train = <span class="keyword">False</span></div><div class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">with</span> torch.set_grad_enabled(is_train):   <span class="comment"># 在inference的时候，设置不要track</span></div><div class="line"><span class="meta">... </span>    y = x * <span class="number">2</span></div><div class="line"><span class="meta">&gt;&gt;&gt; </span>y.requires_grad</div><div class="line"><span class="keyword">False</span></div><div class="line"><span class="meta">&gt;&gt;&gt; </span>torch.set_grad_enabled(<span class="keyword">True</span>)  <span class="comment"># 当然也可以不用with构建上下文环境，而单独这样用</span></div><div class="line"><span class="meta">&gt;&gt;&gt; </span>y = x * <span class="number">2</span></div><div class="line"><span class="meta">&gt;&gt;&gt; </span>y.requires_grad</div><div class="line"><span class="keyword">True</span></div><div class="line"><span class="meta">&gt;&gt;&gt; </span>torch.set_grad_enabled(<span class="keyword">False</span>)</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>y = x * <span class="number">2</span></div><div class="line"><span class="meta">&gt;&gt;&gt; </span>y.requires_grad</div><div class="line"><span class="keyword">False</span></div></pre></td></tr></table></figure><h2 id="dtypes-devices-和NumPy风格的构建函数"><a href="#dtypes-devices-和NumPy风格的构建函数" class="headerlink" title="dtypes, devices 和NumPy风格的构建函数"></a><code>dtypes</code>, <code>devices</code> 和NumPy风格的构建函数</h2><p>以前的版本中，我们需要以”tensor type”的形式给出对data type（如<code>float</code>或<code>double</code>），device type（如cpu或gpu）以及layout（dense或sparse）的限定。例如，<code>torch.cuda.sparse.DoubleTensor</code>用来构造一个data type是<code>double</code>，在GPU上以及sparse的tensor。</p><p>现在我们引入了<code>torch.dtype</code>，<code>torch.device</code>和<code>torch.layout</code>来更好地使用Numpy风格的构建函数。</p><h3 id="torch-dtype"><a href="#torch-dtype" class="headerlink" title="torch.dtype"></a><code>torch.dtype</code></h3><p>下面是可用的 <code>torch.dtypes</code> (data types) 和它们对应的tensor types。可以使用<code>x.dtype</code>获取。</p><table>   <tr>      <td>data type</td>      <td>torch.dtype</td>      <td>Tensor types</td>   </tr>   <tr>      <td>32-bit floating point</td>      <td>torch.float32 or torch.float</td>      <td>torch.*.FloatTensor</td>   </tr>   <tr>      <td>64-bit floating point</td>      <td>torch.float64 or torch.double</td>      <td>torch.*.DoubleTensor</td>   </tr>   <tr>      <td>16-bit floating point</td>      <td>torch.float16 or torch.half</td>      <td>torch.*.HalfTensor</td>   </tr>   <tr>      <td>8-bit integer (unsigned)</td>      <td>torch.uint8</td>      <td>torch.*.ByteTensor</td>   </tr>   <tr>      <td>8-bit integer (signed)</td>      <td>torch.int8</td>      <td>torch.*.CharTensor</td>   </tr>   <tr>      <td>16-bit integer (signed)</td>      <td>torch.int16 or torch.short</td>      <td>torch.*.ShortTensor</td>   </tr>   <tr>      <td>32-bit integer (signed)</td>      <td>torch.int32 or torch.int</td>      <td>torch.*.IntTensor</td>   </tr>   <tr>      <td>64-bit integer (signed)</td>      <td>torch.int64 or torch.long</td>      <td>torch.*.LongTensor</td>   </tr></table><h3 id="torch-device"><a href="#torch-device" class="headerlink" title="torch.device"></a><code>torch.device</code></h3><p><code>torch.device</code>包含了device type（如cpu或cuda）和可能的设备id。使用<code>torch.device(&#39;{device_type}&#39;)</code>或<code>torch.device(&#39;{device_type}:{device_ordinal}&#39;)</code>的方式来初始化。 </p><p>如果没有指定<code>device ordinal</code>，那么默认是当前的device。例如，<code>torch.device(&#39;cuda&#39;)</code>相当于<code>torch.device(&#39;cuda:X&#39;)</code>，其中，<code>X</code>是<code>torch.cuda.current_device()</code>的返回结果。</p><p>使用<code>x.device</code>来获取。</p><h3 id="torch-layout"><a href="#torch-layout" class="headerlink" title="torch.layout"></a><code>torch.layout</code></h3><p><code>torch.layout</code>代表了<code>Tensor</code>的data layout。 目前支持的是<code>torch.strided</code> (dense，也是默认的) 和 <code>torch.sparse_coo</code> (COOG格式的稀疏tensor)。</p><p>使用<code>x.layout</code>来获取。</p><h3 id="创建Tensor（Numpy风格）"><a href="#创建Tensor（Numpy风格）" class="headerlink" title="创建Tensor（Numpy风格）"></a>创建<code>Tensor</code>（Numpy风格）</h3><p>你可以使用<code>dtype</code>，<code>device</code>，<code>layout</code>和<code>requires_grad</code>更好地控制<code>Tensor</code>的创建。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="meta">&gt;&gt;&gt; </span>device = torch.device(<span class="string">"cuda:1"</span>) </div><div class="line"><span class="meta">&gt;&gt;&gt; </span>x = torch.randn(<span class="number">3</span>, <span class="number">3</span>, dtype=torch.float64, device=device)</div><div class="line">tensor([[<span class="number">-0.6344</span>,  <span class="number">0.8562</span>, <span class="number">-1.2758</span>],</div><div class="line">        [ <span class="number">0.8414</span>,  <span class="number">1.7962</span>,  <span class="number">1.0589</span>],</div><div class="line">        [<span class="number">-0.1369</span>, <span class="number">-1.0462</span>, <span class="number">-0.4373</span>]], dtype=torch.float64, device=<span class="string">'cuda:1'</span>)</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>x.requires_grad  <span class="comment"># default is False</span></div><div class="line"><span class="keyword">False</span></div><div class="line"><span class="meta">&gt;&gt;&gt; </span>x = torch.zeros(<span class="number">3</span>, requires_grad=<span class="keyword">True</span>)</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>x.requires_grad</div><div class="line"><span class="keyword">True</span></div></pre></td></tr></table></figure><h3 id="torch-tensor-data"><a href="#torch-tensor-data" class="headerlink" title="torch.tensor(data, ...)"></a><code>torch.tensor(data, ...)</code></h3><p><code>torch.tensor</code>是新加入的<code>Tesnor</code>构建函数。它接受一个”array-like”的参数，并将其value copy到一个新的<code>Tensor</code>中。可以将它看做<code>numpy.array</code>的等价物。不同于<code>torch.*Tensor</code>方法，你可以创建0D的Tensor（也就是scalar）。此外，如果<code>dtype</code>参数没有给出，它会自动推断。推荐使用这个函数从已有的data，如Python List创建<code>Tensor</code>。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="meta">&gt;&gt;&gt; </span>cuda = torch.device(<span class="string">"cuda"</span>)</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>torch.tensor([[<span class="number">1</span>], [<span class="number">2</span>], [<span class="number">3</span>]], dtype=torch.half, device=cuda)</div><div class="line">tensor([[ <span class="number">1</span>],</div><div class="line">        [ <span class="number">2</span>],</div><div class="line">        [ <span class="number">3</span>]], device=<span class="string">'cuda:0'</span>)</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>torch.tensor(<span class="number">1</span>)               <span class="comment"># scalar</span></div><div class="line">tensor(<span class="number">1</span>)</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>torch.tensor([<span class="number">1</span>, <span class="number">2.3</span>]).dtype  <span class="comment"># type inferece</span></div><div class="line">torch.float32</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>torch.tensor([<span class="number">1</span>, <span class="number">2</span>]).dtype    <span class="comment"># type inferece</span></div><div class="line">torch.int64</div></pre></td></tr></table></figure><p>我们还加了更多的<code>Tensor</code>创建方法。其中有一些<code>torch.*_like</code>，<code>tensor.new_*</code>这样的形式。</p><ul><li><p><code>torch.*_like</code>的参数是一个input tensor， 它返回一个相同属性的tensor，除非有特殊指定。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="meta">&gt;&gt;&gt; </span>x = torch.randn(<span class="number">3</span>, dtype=torch.float64)</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>torch.zeros_like(x)</div><div class="line">tensor([ <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>], dtype=torch.float64)</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>torch.zeros_like(x, dtype=torch.int)</div><div class="line">tensor([ <span class="number">0</span>,  <span class="number">0</span>,  <span class="number">0</span>], dtype=torch.int32)</div></pre></td></tr></table></figure></li><li><p><code>tensor.new_*</code>类似，不过它通常需要接受一个指定shape的参数。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="meta">&gt;&gt;&gt; </span>x = torch.randn(<span class="number">3</span>, dtype=torch.float64)</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>x.new_ones(<span class="number">2</span>)</div><div class="line">tensor([ <span class="number">1.</span>,  <span class="number">1.</span>], dtype=torch.float64)</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>x.new_ones(<span class="number">4</span>, dtype=torch.int)</div><div class="line">tensor([ <span class="number">1</span>,  <span class="number">1</span>,  <span class="number">1</span>,  <span class="number">1</span>], dtype=torch.int32)</div></pre></td></tr></table></figure></li></ul><p>为了指定shape参数，你可以使用<code>tuple</code>，如<code>torch.zeros((2, 3))</code>（Numpy风格）或者可变数量参数<code>torch.zeros(2, 3)</code>（以前的版本只支持这种）。</p><table>   <tr>      <td>Name</td>      <td>Returned Tensor</td>      <td>torch.*_likevariant</td>      <td>tensor.new_*variant</td>   </tr>   <tr>      <td>torch.empty</td>      <td>unintialized memory</td>      <td>✔</td>      <td>✔</td>   </tr>   <tr>      <td>torch.zeros</td>      <td>all zeros</td>      <td>✔</td>      <td>✔</td>   </tr>   <tr>      <td>torch.ones</td>      <td>all ones</td>      <td>✔</td>      <td>✔</td>   </tr>   <tr>      <td>torch.full</td>      <td>filled with a given value</td>      <td>✔</td>      <td>✔</td>   </tr>   <tr>      <td>torch.rand</td>      <td>i.i.d. continuous Uniform[0, 1)</td>      <td>✔</td>      <td></td>   </tr>   <tr>      <td>torch.randn</td>      <td>i.i.d. Normal(0, 1)</td>      <td>✔</td>      <td></td>   </tr>   <tr>      <td>torch.randint</td>      <td>i.i.d. discrete Uniform in given range</td>      <td>✔</td>      <td></td>   </tr>   <tr>      <td>torch.randperm</td>      <td>random permutation of {0, 1, ..., n - 1}</td>      <td></td>      <td></td>   </tr>   <tr>      <td>torch.tensor</td>      <td>copied from existing data (list, NumPy ndarray, etc.)</td>      <td></td>      <td>✔</td>   </tr>   <tr>      <td>torch.from_numpy*</td>      <td>from NumPy ndarray (sharing storage without copying)</td>      <td></td>      <td></td>   </tr>   <tr>      <td>torch.arange, torch.range and torch.linspace</td>      <td>uniformly spaced values in a given range</td>      <td></td>      <td></td>   </tr>   <tr>      <td>torch.logspace</td>      <td>logarithmically spaced values in a given range</td>      <td></td>      <td></td>   </tr>   <tr>      <td>torch.eye</td>      <td>identity matrix</td>      <td></td>      <td></td>   </tr></table><p>注：<code>torch.from_numpy</code>只接受NumPy <code>ndarray</code>作为输入参数。</p><h2 id="书写设备无关代码（device-agnostic-code）"><a href="#书写设备无关代码（device-agnostic-code）" class="headerlink" title="书写设备无关代码（device-agnostic code）"></a>书写设备无关代码（device-agnostic code）</h2><p>以前版本很难写设备无关代码。我们使用两种方法使其变得简单：</p><ul><li><code>Tensor</code>的<code>device</code>属性可以给出其<code>torch.device</code>（<code>get_device</code>只能获取CUDA tensor）</li><li>使用<code>x.to()</code>方法，可以很容易将<code>Tensor</code>或者<code>Module</code>在devices间移动（而不用调用<code>x.cpu()</code>或者<code>x.cuda()</code>。</li></ul><p>推荐使用下面的模式：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 在脚本开始的地方，指定device</span></div><div class="line">device = torch.device(<span class="string">"cuda:0"</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">"cpu"</span>)</div><div class="line"></div><div class="line"><span class="comment">## 一些代码</span></div><div class="line"></div><div class="line"><span class="comment"># 当你想创建新的Tensor或者Module时候，使用下面的方法</span></div><div class="line"><span class="comment"># 如果已经在相应的device上了，将不会发生copy</span></div><div class="line">input = data.to(device)</div><div class="line">model = MyModule(...).to(device)</div></pre></td></tr></table></figure><h2 id="在nn-Module中对于submodule，parameter和buffer名字新的约束"><a href="#在nn-Module中对于submodule，parameter和buffer名字新的约束" class="headerlink" title="在nn.Module中对于submodule，parameter和buffer名字新的约束"></a>在<code>nn.Module</code>中对于submodule，parameter和buffer名字新的约束</h2><p>当使用<code>module.add_module(name, value)</code>, <code>module.add_parameter(name, value)</code> 或者 <code>module.add_buffer(name, value)</code>时候不要使用空字符串或者包含<code>.</code>的字符串，可能会导致<code>state_dict</code>中的数据丢失。如果你在load这样的<code>state_dict</code>，注意打补丁，并且应该更新代码，规避这个问题。</p><h2 id="一个具体的例子"><a href="#一个具体的例子" class="headerlink" title="一个具体的例子"></a>一个具体的例子</h2><p>下面是一个code snippet，展示了从0.3.1跨越到0.4.0的不同。</p><h3 id="0-3-1-version"><a href="#0-3-1-version" class="headerlink" title="0.3.1 version"></a>0.3.1 version</h3><figure class="highlight py"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line">model = MyRNN()</div><div class="line"><span class="keyword">if</span> use_cuda:</div><div class="line">    model = model.cuda()</div><div class="line"></div><div class="line"><span class="comment"># train</span></div><div class="line">total_loss = <span class="number">0</span></div><div class="line"><span class="keyword">for</span> input, target <span class="keyword">in</span> train_loader:</div><div class="line">    input, target = Variable(input), Variable(target)</div><div class="line">    hidden = Variable(torch.zeros(*h_shape))  <span class="comment"># init hidden</span></div><div class="line">    <span class="keyword">if</span> use_cuda:</div><div class="line">        input, target, hidden = input.cuda(), target.cuda(), hidden.cuda()</div><div class="line">    ...  <span class="comment"># get loss and optimize</span></div><div class="line">    total_loss += loss.data[<span class="number">0</span>]</div><div class="line"></div><div class="line"><span class="comment"># evaluate</span></div><div class="line"><span class="keyword">for</span> input, target <span class="keyword">in</span> test_loader:</div><div class="line">    input = Variable(input, volatile=<span class="keyword">True</span>)</div><div class="line">    <span class="keyword">if</span> use_cuda:</div><div class="line">        ...</div><div class="line">    ...</div></pre></td></tr></table></figure><h3 id="0-4-0-version"><a href="#0-4-0-version" class="headerlink" title="0.4.0 version"></a>0.4.0 version</h3><figure class="highlight py"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># torch.device object used throughout this script</span></div><div class="line">device = torch.device(<span class="string">"cuda"</span> <span class="keyword">if</span> use_cuda <span class="keyword">else</span> <span class="string">"cpu"</span>)</div><div class="line"></div><div class="line">model = MyRNN().to(device)</div><div class="line"></div><div class="line"><span class="comment"># train</span></div><div class="line">total_loss = <span class="number">0</span></div><div class="line"><span class="keyword">for</span> input, target <span class="keyword">in</span> train_loader:</div><div class="line">    input, target = input.to(device), target.to(device)</div><div class="line">    hidden = input.new_zeros(*h_shape)  <span class="comment"># has the same device &amp; dtype as `input`</span></div><div class="line">    ...  <span class="comment"># get loss and optimize</span></div><div class="line">    total_loss += loss.item()           <span class="comment"># get Python number from 1-element Tensor</span></div><div class="line"></div><div class="line"><span class="comment"># evaluate</span></div><div class="line"><span class="keyword">with</span> torch.no_grad():                   <span class="comment"># operations inside don't track history</span></div><div class="line">    <span class="keyword">for</span> input, target <span class="keyword">in</span> test_loader:</div><div class="line">        ...</div></pre></td></tr></table></figure><h2 id="附"><a href="#附" class="headerlink" title="附"></a>附</h2><ul><li><a href="https://github.com/pytorch/pytorch/releases/tag/v0.4.0" target="_blank" rel="external">Release Note</a></li><li><a href="http://pytorch.org/docs/stable/index.html" target="_blank" rel="external">Documentation</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;PyTorch在前两天官方发布了0.4.0版本。这个版本与之前相比，API发生了较大的变化，所以官方也出了一个&lt;a href=&quot;http://pytorch.org/2018/04/22/0_4_0-migration-guide.html&quot;&gt;转换指导&lt;/a&gt;，这篇博客是这篇指导的中文翻译版。归结起来，对我们代码影响最大的地方主要有：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;Tensor&lt;/code&gt;和&lt;code&gt;Variable&lt;/code&gt;合并，&lt;code&gt;autograd&lt;/code&gt;的机制有所不同，变得更简单，使用&lt;code&gt;requires_grad&lt;/code&gt;和上下文相关环境管理。&lt;/li&gt;
&lt;li&gt;Numpy风格的&lt;code&gt;Tensor&lt;/code&gt;构建。&lt;/li&gt;
&lt;li&gt;提出了&lt;code&gt;device&lt;/code&gt;，更简单地在cpu和gpu中移动数据。&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
      <category term="pytorch" scheme="https://xmfbit.github.io/tags/pytorch/"/>
    
  </entry>
  
  <entry>
    <title>JupyterNotebook设置Python环境</title>
    <link href="https://xmfbit.github.io/2018/04/09/set-env-in-jupyternotebook/"/>
    <id>https://xmfbit.github.io/2018/04/09/set-env-in-jupyternotebook/</id>
    <published>2018-04-09T05:44:04.000Z</published>
    <updated>2018-10-27T07:16:52.417Z</updated>
    
    <content type="html"><![CDATA[<p>使用Python时，常遇到的一个问题就是Python和库的版本不同。Anaconda的env算是解决这个问题的一个好用的方法。但是，在使用Jupyter Notebook的时候，我却发现加载的仍然是默认的Python Kernel。这篇博客记录了如何在Jupyter Notebook中也能够设置相应的虚拟环境。<br><a id="more"></a></p><h2 id="conda的虚拟环境"><a href="#conda的虚拟环境" class="headerlink" title="conda的虚拟环境"></a>conda的虚拟环境</h2><p>在Anaconda中，我们可以使用<code>conda create -n your_env_name python=your_python_version</code>的方法创建虚拟环境，并使用<code>source activate your_env_name</code>方式激活该虚拟环境，并在其中安装与默认（主）python环境不同的软件包等。</p><p>当激活该虚拟环境时，ipython下是可以正常加载的。但是打开Jupyter Notebook，会发现其加载的仍然是默认的Python kernel，而我们需要在notebook中也能使用新添加的虚拟环境。</p><h2 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a>解决方法</h2><p>解决方法见这个帖子：<a href="https://stackoverflow.com/questions/39604271/conda-environments-not-showing-up-in-jupyter-notebook" target="_blank" rel="external">Conda environments not showing up in Jupyter Notebook</a>.</p><p>首先，安装<code>nb_conda_kernels</code>包：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">conda install nb_conda_kernels</div></pre></td></tr></table></figure></p><p>然后，打开Notebook，点击<code>New</code>，会出现当前所有安装的虚拟环境以供选择，如下所示。<br><img src="/img/set-env-in-notebook-choose-kernel.png" alt="选择特定的kernel加载"></p><p>如果是已经编辑过的notebook，只需要打开该笔记本，在菜单栏中选择<code>Kernel -&gt; choose kernel -&gt; your env kernel</code>即可。<br><img src="/img/set-env-in-notebook-change-kernel.png" alt="改变当前notebook的kernel"></p><p>关于<code>nb_conda_kernels</code>的详细信息，可以参考其GitHub页面：<a href="https://github.com/Anaconda-Platform/nb_conda_kernels" target="_blank" rel="external">nb_conda_kernels</a>。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;使用Python时，常遇到的一个问题就是Python和库的版本不同。Anaconda的env算是解决这个问题的一个好用的方法。但是，在使用Jupyter Notebook的时候，我却发现加载的仍然是默认的Python Kernel。这篇博客记录了如何在Jupyter Notebook中也能够设置相应的虚拟环境。&lt;br&gt;
    
    </summary>
    
    
      <category term="python" scheme="https://xmfbit.github.io/tags/python/"/>
    
      <category term="tool" scheme="https://xmfbit.github.io/tags/tool/"/>
    
  </entry>
  
  <entry>
    <title>数值优化之牛顿方法</title>
    <link href="https://xmfbit.github.io/2018/04/03/newton-method/"/>
    <id>https://xmfbit.github.io/2018/04/03/newton-method/</id>
    <published>2018-04-03T13:43:16.000Z</published>
    <updated>2018-10-27T07:16:52.403Z</updated>
    
    <content type="html"><![CDATA[<p>简要介绍一下优化方法中的牛顿方法（Newton’s Method）。下面的动图demo来源于<a href="https://zh.wikipedia.org/wiki/%E7%89%9B%E9%A1%BF%E6%B3%95" target="_blank" rel="external">Wiki页面</a>。<br><img src="/img/newton-method-demo.gif" width="400" height="300" alt="牛顿法动图" align="center"><br><a id="more"></a></p><h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>牛顿方法，是一种用来求解方程$f(x) = 0$的根的方法。从题图可以看出它是如何使用的。</p><p>首先，需要给定根的初始值$x_0$。接下来，在函数曲线上找到其所对应的点$(x_0, f(x_0))$，并过该点做切线交$x$轴于一点$x_1$。从$x_1$出发，重复上述操作，直至收敛。</p><p>根据图上的几何关系和导数的几何意义，有：</p><script type="math/tex; mode=display">x_{n+1} = x_n - \frac{f(x_n)}{f^\prime(x_n)}</script><h2 id="优化上的应用"><a href="#优化上的应用" class="headerlink" title="优化上的应用"></a>优化上的应用</h2><p>做优化的时候，我们常常需要的是求解某个损失函数$L$的极值。在极值点处，函数的导数为$0$。所以这个问题被转换为了求解$L$的导数的零点。我们有</p><script type="math/tex; mode=display">\theta_{n+1} = \theta_n - \frac{L^\prime(\theta_n)}{L^{\prime\prime}(\theta_n)}</script><h2 id="推广到向量形式"><a href="#推广到向量形式" class="headerlink" title="推广到向量形式"></a>推广到向量形式</h2><p>机器学习中的优化问题常常是在高维空间进行，可以将其推广到向量形式：</p><script type="math/tex; mode=display">\theta_{n+1} = \theta_n - H^{-1}\nabla_\theta L(\theta_n)</script><p>其中，$H$表示海森矩阵，是一个$n\times n$的矩阵，其中元素为：</p><script type="math/tex; mode=display">H_{ij} = \frac{\partial^2 L}{\partial \theta_i \partial \theta_j}</script><p>特别地，当海森矩阵为正定时，此时的极值为极小值（可以使用二阶的泰勒展开式证明）。</p><p>PS:忘了什么是正定矩阵了吗？想想二次型的概念，对于$\forall x$不为$0$向量，都有$x^THx &gt; 0$。</p><h2 id="优缺点"><a href="#优缺点" class="headerlink" title="优缺点"></a>优缺点</h2><p>牛顿方法的收敛速度较SGD为快（二阶收敛），但是会涉及到求解一个$n\times n$的海森矩阵的逆，所以虽然需要的迭代次数更少，但反而可能比较耗时（$n$的大小）。</p><h2 id="L-BFGS"><a href="#L-BFGS" class="headerlink" title="L-BFGS"></a>L-BFGS</h2><p>由于牛顿方法中需要计算海森矩阵的逆，所以很多时候并不实用。大家就想出了一些近似计算$H^{-1}$的方法，如L-BFGS等。</p><p><em>推导过程待续。。。</em></p><p>L-BFGS的资料网上还是比较多的，这里有一个PyTorch中L-BFGS方法的实现：<a href="https://github.com/pytorch/pytorch/blob/master/torch/optim/lbfgs.py" target="_blank" rel="external">optim.lbfgs</a>。</p><p>这里有一篇不错的文章<a href="http://www.hankcs.com/ml/l-bfgs.html" target="_blank" rel="external">数值优化：理解L-BFGS算法</a>，本博客写作过程参考很多。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;简要介绍一下优化方法中的牛顿方法（Newton’s Method）。下面的动图demo来源于&lt;a href=&quot;https://zh.wikipedia.org/wiki/%E7%89%9B%E9%A1%BF%E6%B3%95&quot;&gt;Wiki页面&lt;/a&gt;。&lt;br&gt;&lt;img src=&quot;/img/newton-method-demo.gif&quot; width = &quot;400&quot; height = &quot;300&quot; alt=&quot;牛顿法动图&quot; align=center /&gt;&lt;br&gt;
    
    </summary>
    
    
      <category term="math" scheme="https://xmfbit.github.io/tags/math/"/>
    
  </entry>
  
  <entry>
    <title>论文 - Feature Pyramid Networks for Object Detection (FPN)</title>
    <link href="https://xmfbit.github.io/2018/04/02/paper-fpn/"/>
    <id>https://xmfbit.github.io/2018/04/02/paper-fpn/</id>
    <published>2018-04-02T02:12:03.000Z</published>
    <updated>2018-10-27T07:16:52.404Z</updated>
    
    <content type="html"><![CDATA[<p>图像金字塔或特征金字塔是传统CV方法中常用的技巧，例如求取<a href="https://xmfbit.github.io/2017/01/30/cs131-sift/">SIFT特征</a>就用到了DoG图像金字塔。但是在Deep Learning统治下的CV detection下，这种方法变得无人问津。一个重要的问题就是计算量巨大。而本文提出了一种仅用少量额外消耗建立特征金字塔的方法，提高了detector的性能。<br><a id="more"></a></p><h2 id="Pyramid-or-not-It’s-a-question"><a href="#Pyramid-or-not-It’s-a-question" class="headerlink" title="Pyramid or not? It’s a question."></a>Pyramid or not? It’s a question.</h2><p>在DL席卷CV之前，特征大多需要研究人员手工设计，如SIFT/Harr/HoG等。人们在使用这些特征的时候发现，往往需要使用图像金字塔，在multi scale下进行检测，才能得到不错的结果。然而，使用CNN时，由于其本身具有的一定的尺度不变性，大家常常是只在单一scale下（也就是原始图像作为输入），就可以达到不错的结果。不过很多时候参加COCO等竞赛的队伍还是会在TEST的时候使用这项技术，能够取得更好的成绩。但是这样会造成计算时间的巨大开销，TRAIN和TEST的不一致。TRAIN中引入金字塔，内存就会吃紧。所以主流的Fast/Faster RCNN并没有使用金字塔。</p><p>换个角度，我们知道在CNN中，输入会逐层处理，经过Conv/Pooling的操作后，不同深度的layer产生的feature map的spatial dimension是不一样的，这就是作者在摘要中提到的“inherent multi-scale pyramidal hierarchy of deep CNN”。不过，还有一个问题，就是深层和浅层的feature map虽然构成了一个feature pyramid，但是它们的语义并不对等：深层layer的feature map有更抽象的语义信息，而浅层feature map有较高的resolution，但是语义信息还是too yong too simple。</p><p>SSD做过这方面的探索。但是它采用的方法是从浅层layer引出，又加了一些layer，导致无法reuse high resolution的feature map。我们发现，浅层的high resolution feature map对检测小目标很有用处。</p><p>那我们想要怎样呢？</p><ul><li>高层的low resolution，strong semantic info特征如何和浅层的high resolution，weak semantic info自然地结合？</li><li>不引入过多的额外计算，最好也只需要用single scale的原始输入。</li></ul><p>用一张图总结一下。下图中蓝色的轮廓线框起来的就是不同layer输出的feature map。蓝色线越粗，代表其语义信息越强。在（a）中，是将图像做成金字塔，分别跑一个NN来做，这样计算量极大。（b）中是目前Faster RCNN等采用的方法，只在single scale上做。（c）中是直接将各个layer输出的层级feature map自然地看做feature pyramid来做。（d）是本文的方法，不同层级的feature map做了merge，能够使得每个level的语义信息都比较强（注意看蓝色线的粗细）。<br><img src="/img/paper-fpn-different-pyramids.png" alt="不同金字塔方法"></p><p>我们使用这种名为FPN的技术，不用什么工程上的小花招，就打败了目前COCO上的最好结果。不止detection，FPN也能用在图像分割上（当然，现在我们知道，MaskRCNN中的关键技术之一就是FPN）。</p><h2 id="FPN"><a href="#FPN" class="headerlink" title="FPN"></a>FPN</h2><p>有人可能会想，其实前面的网络有人也做过不同深度layer的merge啊，通过skip connection就可以了。作者指出，那种方法仍然是只能在最终的single scale的output feature map上做，而我们的方法是在all level上完成，如下图所示。<br><img src="/img/paper-fpn-different-with-related-work.png" alt="我们才是真正的金字塔"></p><h3 id="Bottom-up-pathway"><a href="#Bottom-up-pathway" class="headerlink" title="Bottom-up pathway"></a>Bottom-up pathway</h3><p> Bottom-up pathway指的是网络的前向计算部分，会产生一系列scale相差2x的feature map。当然，在这些downsample中间，还会有layer的输出spatial dimension是一致的。那些连续的有着相同spatial dimension输出的layer是一个stage。这样，我们就完成了传统金字塔方法和CNN网络的名词的对应。</p><p> 以ResNet为例，我们用每个stage中最后一个residual block的输出作为构建金字塔的feature map，也就是<code>C2~C5</code>。它们的步长分别是$4, 8, 16, 32$。我们没用<code>conv1</code>。</p><h3 id="Top-down-pathway和lateral-connection"><a href="#Top-down-pathway和lateral-connection" class="headerlink" title="Top-down pathway和lateral connection"></a>Top-down pathway和lateral connection</h3><p>Top-down pathway是指将深层的有更强语义信息的feature经过upsampling变成higher resolution的过程。然后再与bottom-up得到的feature经过lateral connection（侧边连接）进行增强。</p><p>下面这张图展示了做lateral connection的过程。注意在图中，越深的layer位于图的上部。我们以框出来放大的那部分举例子。从更深的层输出的feature经过<code>2x up</code>处理（spatial dimension一致了），从左面来的浅层的feature经过<code>1x1 conv</code>处理（channel dimension一致了），再进行element-wise的相加，得到了该stage最后用于prediction的feature（其实还要经过一个<code>3x3 conv</code>的处理，见下引文）。<br><img src="/img/paper-fpn-lateral-connection.png" alt="lateral connection"></p><p>一些细节，直接引用：</p><blockquote><p>To start the iteration, we simply attach a 1x1 convolutional layer on C5 to produce the coarsest resolution map. Finally, we append a 3x3 convolution on each merged map to generate the final feature map, which is to reduce the aliasing effect of upsampling.</p></blockquote><p>此外，由于金字塔上的所有feature共享classifier和regressor，要求它们的channel dimension必须一致。本文固定使用$256$。而且这些外的conv layer没有使用非线性激活。</p><p>这里给出一个基于PyTorch的FPN的第三方实现<a href="https://github.com/kuangliu/pytorch-fpn/blob/master/fpn.py" target="_blank" rel="external">kuangliu/pytorch-fpn</a>，可以对照论文捋一遍。<br><figure class="highlight py"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div></pre></td><td class="code"><pre><div class="line"><span class="comment">## ResNet的block</span></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Bottleneck</span><span class="params">(nn.Module)</span>:</span></div><div class="line">    expansion = <span class="number">4</span></div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, in_planes, planes, stride=<span class="number">1</span>)</span>:</span></div><div class="line">        super(Bottleneck, self).__init__()</div><div class="line">        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=<span class="number">1</span>, bias=<span class="keyword">False</span>)</div><div class="line">        self.bn1 = nn.BatchNorm2d(planes)</div><div class="line">        self.conv2 = nn.Conv2d(planes, planes, kernel_size=<span class="number">3</span>, stride=stride, padding=<span class="number">1</span>, bias=<span class="keyword">False</span>)</div><div class="line">        self.bn2 = nn.BatchNorm2d(planes)</div><div class="line">        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=<span class="number">1</span>, bias=<span class="keyword">False</span>)</div><div class="line">        self.bn3 = nn.BatchNorm2d(self.expansion*planes)</div><div class="line"></div><div class="line">        self.shortcut = nn.Sequential()</div><div class="line">        <span class="keyword">if</span> stride != <span class="number">1</span> <span class="keyword">or</span> in_planes != self.expansion*planes:</div><div class="line">            self.shortcut = nn.Sequential(</div><div class="line">                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=<span class="number">1</span>, stride=stride, bias=<span class="keyword">False</span>),</div><div class="line">                nn.BatchNorm2d(self.expansion*planes)</div><div class="line">            )</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></div><div class="line">        out = F.relu(self.bn1(self.conv1(x)))</div><div class="line">        out = F.relu(self.bn2(self.conv2(out)))</div><div class="line">        out = self.bn3(self.conv3(out))</div><div class="line">        out += self.shortcut(x)</div><div class="line">        out = F.relu(out)</div><div class="line">        <span class="keyword">return</span> out</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">FPN</span><span class="params">(nn.Module)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, block, num_blocks)</span>:</span></div><div class="line">        super(FPN, self).__init__()</div><div class="line">        self.in_planes = <span class="number">64</span></div><div class="line"></div><div class="line">        self.conv1 = nn.Conv2d(<span class="number">3</span>, <span class="number">64</span>, kernel_size=<span class="number">7</span>, stride=<span class="number">2</span>, padding=<span class="number">3</span>, bias=<span class="keyword">False</span>)</div><div class="line">        self.bn1 = nn.BatchNorm2d(<span class="number">64</span>)</div><div class="line"></div><div class="line">        <span class="comment"># Bottom-up layers, backbone of the network</span></div><div class="line">        self.layer1 = self._make_layer(block,  <span class="number">64</span>, num_blocks[<span class="number">0</span>], stride=<span class="number">1</span>)</div><div class="line">        self.layer2 = self._make_layer(block, <span class="number">128</span>, num_blocks[<span class="number">1</span>], stride=<span class="number">2</span>)</div><div class="line">        self.layer3 = self._make_layer(block, <span class="number">256</span>, num_blocks[<span class="number">2</span>], stride=<span class="number">2</span>)</div><div class="line">        self.layer4 = self._make_layer(block, <span class="number">512</span>, num_blocks[<span class="number">3</span>], stride=<span class="number">2</span>)</div><div class="line"></div><div class="line">        <span class="comment"># Top layer</span></div><div class="line">        <span class="comment"># 我们需要在C5后面接一个1x1, 256 conv，得到金字塔最顶端的feature</span></div><div class="line">        self.toplayer = nn.Conv2d(<span class="number">2048</span>, <span class="number">256</span>, kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>, padding=<span class="number">0</span>)  <span class="comment"># Reduce channels</span></div><div class="line"></div><div class="line">        <span class="comment"># Smooth layers</span></div><div class="line">        <span class="comment"># 这个是上面引文中提到的抗aliasing的3x3卷积</span></div><div class="line">        self.smooth1 = nn.Conv2d(<span class="number">256</span>, <span class="number">256</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>)</div><div class="line">        self.smooth2 = nn.Conv2d(<span class="number">256</span>, <span class="number">256</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>)</div><div class="line">        self.smooth3 = nn.Conv2d(<span class="number">256</span>, <span class="number">256</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>)</div><div class="line"></div><div class="line">        <span class="comment"># Lateral layers</span></div><div class="line">        <span class="comment"># 为了匹配channel dimension引入的1x1卷积</span></div><div class="line">        <span class="comment"># 注意这些backbone之外的extra conv，输出都是256 channel</span></div><div class="line">        self.latlayer1 = nn.Conv2d(<span class="number">1024</span>, <span class="number">256</span>, kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>, padding=<span class="number">0</span>)</div><div class="line">        self.latlayer2 = nn.Conv2d( <span class="number">512</span>, <span class="number">256</span>, kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>, padding=<span class="number">0</span>)</div><div class="line">        self.latlayer3 = nn.Conv2d( <span class="number">256</span>, <span class="number">256</span>, kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>, padding=<span class="number">0</span>)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_make_layer</span><span class="params">(self, block, planes, num_blocks, stride)</span>:</span></div><div class="line">        strides = [stride] + [<span class="number">1</span>]*(num_blocks<span class="number">-1</span>)</div><div class="line">        layers = []</div><div class="line">        <span class="keyword">for</span> stride <span class="keyword">in</span> strides:</div><div class="line">            layers.append(block(self.in_planes, planes, stride))</div><div class="line">            self.in_planes = planes * block.expansion</div><div class="line">        <span class="keyword">return</span> nn.Sequential(*layers)</div><div class="line"></div><div class="line">    <span class="comment">## FPN的lateral connection部分: upsample以后，element-wise相加</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_upsample_add</span><span class="params">(self, x, y)</span>:</span></div><div class="line">        <span class="string">'''Upsample and add two feature maps.</span></div><div class="line">        Args:</div><div class="line">          x: (Variable) top feature map to be upsampled.</div><div class="line">          y: (Variable) lateral feature map.</div><div class="line">        Returns:</div><div class="line">          (Variable) added feature map.</div><div class="line">        Note in PyTorch, when input size is odd, the upsampled feature map</div><div class="line">        with `F.upsample(..., scale_factor=2, mode='nearest')`</div><div class="line">        maybe not equal to the lateral feature map size.</div><div class="line">        e.g.</div><div class="line">        original input size: [N,_,15,15] -&gt;</div><div class="line">        conv2d feature map size: [N,_,8,8] -&gt;</div><div class="line">        upsampled feature map size: [N,_,16,16]</div><div class="line">        So we choose bilinear upsample which supports arbitrary output sizes.</div><div class="line">        '''</div><div class="line">        _,_,H,W = y.size()</div><div class="line">        <span class="keyword">return</span> F.upsample(x, size=(H,W), mode=<span class="string">'bilinear'</span>) + y</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></div><div class="line">        <span class="comment"># Bottom-up</span></div><div class="line">        c1 = F.relu(self.bn1(self.conv1(x)))</div><div class="line">        c1 = F.max_pool2d(c1, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>)</div><div class="line">        c2 = self.layer1(c1)</div><div class="line">        c3 = self.layer2(c2)</div><div class="line">        c4 = self.layer3(c3)</div><div class="line">        c5 = self.layer4(c4)</div><div class="line">        <span class="comment"># Top-down</span></div><div class="line">        <span class="comment"># P5: 金字塔最顶上的feature</span></div><div class="line">        p5 = self.toplayer(c5)</div><div class="line">        <span class="comment"># P4: 上一层 p5 + 侧边来的 c4</span></div><div class="line">        <span class="comment"># 其余同理</span></div><div class="line">        p4 = self._upsample_add(p5, self.latlayer1(c4))</div><div class="line">        p3 = self._upsample_add(p4, self.latlayer2(c3))</div><div class="line">        p2 = self._upsample_add(p3, self.latlayer3(c2))</div><div class="line">        <span class="comment"># Smooth</span></div><div class="line">        <span class="comment"># 输出做一下smooth</span></div><div class="line">        p4 = self.smooth1(p4)</div><div class="line">        p3 = self.smooth2(p3)</div><div class="line">        p2 = self.smooth3(p2)</div><div class="line">        <span class="keyword">return</span> p2, p3, p4, p5</div></pre></td></tr></table></figure></p><h2 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h2><p>下面作者会把FPN应用到FasterRCNN的两个重要步骤：RPN和Fast RCNN。</p><h3 id="FPN加持的RPN"><a href="#FPN加持的RPN" class="headerlink" title="FPN加持的RPN"></a>FPN加持的RPN</h3><p>在Faster RCNN中，RPN用来提供ROI的proposal。backbone网络输出的single feature map上接了$3\times 3$大小的卷积核来实现sliding window的功能，后面接两个$1\times 1$的卷积分别用来做objectness的分类和bounding box基于anchor box的回归。我们把最后的classifier和regressor部分叫做head。</p><p>使用FPN时，我们在金字塔每层的输出feature map上都接上这样的head结构（$3\times 3$的卷积 + two sibling $1\times 1$的卷积）。同时，我们不再使用多尺度的anchor box，而是在每个level上分别使用不同大小的anchor box。具体说，对应于特征金字塔的$5$个level的特征，<code>P2 - P6</code>，anchor box的大小分别是$32^2, 64^2, 128^2, 256^2, 512^2$。不过每层的anchor box仍然要照顾到不同的长宽比例，我们使用了$3$个不同的比例：$1:2, 1:1, 2:1$（和原来一样）。这样，我们一共有$5\times 3 = 15$个anchor box。</p><p>训练过程中，我们需要给anchor boxes赋上对应的正负标签。对于那些与ground truth有最大IoU或者与任意一个ground truth的IoU超过$0.7$的anchor boxes，是positive label；那些与所有ground truth的IoU都小于$0.3$的是negtive label。</p><p>有一个疑问是head的参数是否要在不同的level上共享。我们试验了共享与不共享两个方法，accuracy是相近的。这也说明不同level之间语义信息是相似的，只是resolution不同。</p><h3 id="FPN加持的Fast-RCNN"><a href="#FPN加持的Fast-RCNN" class="headerlink" title="FPN加持的Fast RCNN"></a>FPN加持的Fast RCNN</h3><p>Fast RCNN的原始方法是只在single scale的feature map上做的，要想使用FPN，首先应该解决的问题是前端提供的ROI proposal应该对应到pyramid的哪一个label。由于我们的网络基本都是在ImageNet训练的网络上做transfer learning得到的，我们就以base model在ImageNet上训练时候的输入$224\times 224$作为参考，依据当前ROI和它的大小比例，确定该把这个ROI对应到哪个level。如下所示：</p><script type="math/tex; mode=display">k = \lfloor k_0 + \log_2(\sqrt{wh}/224)\rfloor</script><p>后面接的predictor head我们这里直接连了两个$1024d$的fc layer，再接final classification和regression的部分。同样的，这些参数对于不同level来说是共享的。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;图像金字塔或特征金字塔是传统CV方法中常用的技巧，例如求取&lt;a href=&quot;https://xmfbit.github.io/2017/01/30/cs131-sift/&quot;&gt;SIFT特征&lt;/a&gt;就用到了DoG图像金字塔。但是在Deep Learning统治下的CV detection下，这种方法变得无人问津。一个重要的问题就是计算量巨大。而本文提出了一种仅用少量额外消耗建立特征金字塔的方法，提高了detector的性能。&lt;br&gt;
    
    </summary>
    
    
      <category term="paper" scheme="https://xmfbit.github.io/tags/paper/"/>
    
      <category term="deep learning" scheme="https://xmfbit.github.io/tags/deep-learning/"/>
    
      <category term="detection" scheme="https://xmfbit.github.io/tags/detection/"/>
    
  </entry>
  
  <entry>
    <title>论文 - YOLO v3</title>
    <link href="https://xmfbit.github.io/2018/04/01/paper-yolov3/"/>
    <id>https://xmfbit.github.io/2018/04/01/paper-yolov3/</id>
    <published>2018-04-01T08:48:45.000Z</published>
    <updated>2018-10-27T07:16:52.412Z</updated>
    
    <content type="html"><![CDATA[<p>YOLO的作者又放出了V3版本，在之前的版本上做出了一些改进，达到了更好的性能。这篇博客介绍这篇论文：<a href="https://pjreddie.com/media/files/papers/YOLOv3.pdf" target="_blank" rel="external">YOLOv3: An Incremental Improvement</a>。下面这张图是YOLO V3与RetinaNet的比较。<br><img src="/img/paper-yolov3-comparison-retinanet.png" alt="YOLO v3和RetinaNet的比较"></p><p>可以使用搜索功能，在本博客内搜索YOLO前作的论文阅读和代码。<br><a id="more"></a></p><h2 id="YOLO-v3比你们不知道高到哪里去了"><a href="#YOLO-v3比你们不知道高到哪里去了" class="headerlink" title="YOLO v3比你们不知道高到哪里去了"></a>YOLO v3比你们不知道高到哪里去了</h2><p>YOLO v3在保持其一贯的检测速度快的特点前提下，性能又有了提升：输入图像为$320\times 320$大小的图像，可以在$22$ms跑完，mAP达到了$28.2$，这个数据和SSD相同，但是快了$3$倍。在TitanX上，YOLO v3可以在$51$ms内完成，$AP_{50}$的值为$57.9$。而RetinaNet需要$198$ms，$AP_{50}$近似却略低，为$57.5$。</p><h3 id="ps：啥是AP"><a href="#ps：啥是AP" class="headerlink" title="ps：啥是AP"></a>ps：啥是AP</h3><p>AP就是average precision啦。在detection中，我们认为当预测的bounding box和ground truth的IoU大于某个阈值（如取为$0.5$）时，认为是一个True Positive。如果小于这个阈值，就是一个False Positive。</p><p>所谓precision，就是指检测出的框框中有多少是True Positive。另外，还有一个指标叫做recall，是指所有的ground truth里面，有多少被检测出来了。这两个概念都是来自于classification问题，通过设定上面IoU的阈值，就可以迁移到detection中了。</p><p>我们可以取不同的阈值，这样就可以绘出一条precisio vs recall的曲线，计算曲线下的面积，就是AP值。COCO中使用了<code>0.5:0.05:0.95</code>十个离散点近似计算（参考<a href="http://cocodataset.org/#detections-eval" target="_blank" rel="external">COCO的说明文档网页</a>）。detection中常常需要同时检测图像中多个类别的物体，我们将不同类别的AP求平均，就是mAP。</p><p>如果我们只看某个固定的阈值，如$0.5$，计算所有类别的平均AP，那么就用$AP_{50}$来表示。所以YOLO v3单拿出来$AP_{50}$说事，是为了证明虽然我的bounding box不如你RetinaNet那么精准（IoU相对较小），但是如果你对框框的位置不是那么敏感（$0.5$的阈值很多时候够用了），那么我是可以做到比你更好更快的。</p><h2 id="Bounding-Box位置的回归"><a href="#Bounding-Box位置的回归" class="headerlink" title="Bounding Box位置的回归"></a>Bounding Box位置的回归</h2><p>这里和原来v2基本没区别。仍然使用聚类产生anchor box的长宽（下式的$p_w$和$p_h$）。网络预测四个值：$t_x$，$t_y$，$t_w$，$t_h$。我们知道，YOLO网络最后输出是一个$M\times M$的feature map，对应于$M \times M$个cell。如果某个cell距离image的top left corner距离为$(c_x, c_y)$（也就是cell的坐标），那么该cell内的bounding box的位置和形状参数为：</p><script type="math/tex; mode=display">\begin{aligned}b_x &= \sigma(t_x) + c_x\\ b_y &= \sigma(t_y) + c_y\\ b_w &= p_w e^{t_w}\\ b_h &= p_h e^{t_h}\end{aligned}</script><p>PS：这里有一个问题，不管FasterRCNN还是YOLO，都不是直接回归bounding box的长宽（就像这样：$b_w = p_w t_w^\prime$），而是要做一个对数变换，实际预测的是$\log(\cdot)$。这里小小解释一下。</p><p>这是因为如果不做变换，直接预测相对形变$t_w^\prime$，那么要求$t_w^\prime &gt; 0$，因为你的框框的长宽不可能是负数。这样，是在做一个有不等式条件约束的优化问题，没法直接用SGD来做。所以先取一个对数变换，将其不等式约束去掉，就可以了。</p><p><img src="/img/paper=yolov3-bbox-regression.png" alt="bounding box的回归"></p><p>在训练的时候，使用平方误差损失。</p><p>另外，YOLO会对每个bounding box给出是否是object的置信度预测，用来区分objects和背景。这个值使用logistic回归。当某个bounding box与ground truth的IoU大于其他所有bounding box时，target给$1$；如果某个bounding box不是IoU最大的那个，但是IoU也大于了某个阈值（我们取$0.5$），那么我们忽略它（既不惩罚，也不奖励），这个做法是从Faster RCNN借鉴的。我们对每个ground truth只分配一个最好的bounding box与其对应（这与Faster RCNN不同）。如果某个bounding box没有倍assign到任何一个ground truth对应，那么它对边框位置大小的回归和class的预测没有贡献，我们只惩罚它的objectness，即试图减小其confidence。</p><h2 id="分类预测"><a href="#分类预测" class="headerlink" title="分类预测"></a>分类预测</h2><p>我们不用softmax做分类了，而是使用独立的logisitc做二分类。这种方法的好处是可以处理重叠的多标签问题，如Open Image Dataset。在其中，会出现诸如<code>Woman</code>和<code>Person</code>这样的重叠标签。</p><h2 id="FPN加持的多尺度预测"><a href="#FPN加持的多尺度预测" class="headerlink" title="FPN加持的多尺度预测"></a>FPN加持的多尺度预测</h2><p>之前YOLO的一个弱点就是缺少多尺度变换，使用<a href="https://arxiv.org/abs/1612.03144" target="_blank" rel="external">FPN</a>中的思路，v3在$3$个不同的尺度上做预测。在COCO上，我们每个尺度都预测$3$个框框，所以一共是$9$个。所以输出的feature map的大小是$N\times N\times [3\times (4+1+80)]$。</p><p>然后我们从两层前那里拿feature map，upsample 2x，并与更前面输出的feature map通过element-wide的相加做merge。这样我们能够从后面的层拿到更多的高层语义信息，也能从前面的层拿到细粒度的信息（更大的feature map，更小的感受野）。然后在后面接一些conv做处理，最终得到和上面相似大小的feature map，只不过spatial dimension变成了$2$倍。</p><p>照上一段所说方法，再一次在final scale尺度下给出预测。</p><h2 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h2><p>在v3中，作者新建了一个名为<code>yolo</code>的layer，其参数如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">[yolo]</div><div class="line">mask = 0,1,2</div><div class="line">## 9组anchor对应9个框框</div><div class="line">anchors = 10,13,  16,30,  33,23,  30,61,  62,45,  59,119,  116,90,  156,198,  373,326</div><div class="line">classes=20   ## VOC20类</div><div class="line">num=9</div><div class="line">jitter=.3</div><div class="line">ignore_thresh = .5</div><div class="line">truth_thresh = 1</div><div class="line">random=1</div></pre></td></tr></table></figure></p><p>打开<code>yolo_layer.c</code>文件，找到<code>forward</code><a href="">部分代码</a>。可以看到，首先，对输入进行activation。注意，如论文所说，对类别进行预测的时候，没有使用v2中的softmax或softmax tree，而是直接使用了logistic变换。<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">for</span> (b = <span class="number">0</span>; b &lt; l.batch; ++b)&#123;</div><div class="line">    <span class="keyword">for</span>(n = <span class="number">0</span>; n &lt; l.n; ++n)&#123;</div><div class="line">        <span class="keyword">int</span> index = entry_index(l, b, n*l.w*l.h, <span class="number">0</span>);</div><div class="line">        <span class="comment">// 对 tx, ty进行logistic变换</span></div><div class="line">        activate_array(l.output + index, <span class="number">2</span>*l.w*l.h, LOGISTIC);</div><div class="line">        index = entry_index(l, b, n*l.w*l.h, <span class="number">4</span>);</div><div class="line">        <span class="comment">// 对confidence和C类进行logistic变换</span></div><div class="line">        activate_array(l.output + index, (<span class="number">1</span>+l.classes)*l.w*l.h, LOGISTIC);</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p><p>我们看一下如何计算梯度。<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">for</span> (j = <span class="number">0</span>; j &lt; l.h; ++j) &#123;</div><div class="line">    <span class="keyword">for</span> (i = <span class="number">0</span>; i &lt; l.w; ++i) &#123;</div><div class="line">        <span class="keyword">for</span> (n = <span class="number">0</span>; n &lt; l.n; ++n) &#123;</div><div class="line">            <span class="comment">// 对每个预测的bounding box</span></div><div class="line">            <span class="comment">// 找到与其IoU最大的ground truth</span></div><div class="line">            <span class="keyword">int</span> box_index = entry_index(l, b, n*l.w*l.h + j*l.w + i, <span class="number">0</span>);</div><div class="line">            box pred = get_yolo_box(l.output, l.biases, l.mask[n], box_index, i, j, l.w, l.h, net.w, net.h, l.w*l.h);</div><div class="line">            <span class="keyword">float</span> best_iou = <span class="number">0</span>;</div><div class="line">            <span class="keyword">int</span> <span class="keyword">best_t</span> = <span class="number">0</span>;</div><div class="line">            <span class="keyword">for</span>(t = <span class="number">0</span>; t &lt; l.max_boxes; ++t)&#123;</div><div class="line">                box truth = float_to_box(net.truth + t*(<span class="number">4</span> + <span class="number">1</span>) + b*l.truths, <span class="number">1</span>);</div><div class="line">                <span class="keyword">if</span>(!truth.x) <span class="keyword">break</span>;</div><div class="line">                <span class="keyword">float</span> iou = box_iou(pred, truth);</div><div class="line">                <span class="keyword">if</span> (iou &gt; best_iou) &#123;</div><div class="line">                    best_iou = iou;</div><div class="line">                    <span class="keyword">best_t</span> = t;</div><div class="line">                &#125;</div><div class="line">            &#125;</div><div class="line">            <span class="keyword">int</span> obj_index = entry_index(l, b, n*l.w*l.h + j*l.w + i, <span class="number">4</span>);</div><div class="line">            avg_anyobj += l.output[obj_index];</div><div class="line">            <span class="comment">// 计算梯度</span></div><div class="line">            <span class="comment">// 如果大于ignore_thresh, 那么忽略</span></div><div class="line">            <span class="comment">// 如果小于ignore_thresh，target = 0</span></div><div class="line">            <span class="comment">// diff = -gradient = target - output</span></div><div class="line">            <span class="comment">// 为什么是上式，见下面的数学分析</span></div><div class="line">            l.delta[obj_index] = <span class="number">0</span> - l.output[obj_index];</div><div class="line">            <span class="keyword">if</span> (best_iou &gt; l.ignore_thresh) &#123;</div><div class="line">                l.delta[obj_index] = <span class="number">0</span>;</div><div class="line">            &#125;</div><div class="line">            <span class="comment">// 这里仍然有疑问，为何使用truth_thresh?这个值是1</span></div><div class="line">            <span class="comment">// 按道理，iou无论如何不可能大于1啊。。。</span></div><div class="line">            <span class="keyword">if</span> (best_iou &gt; l.truth_thresh) &#123;</div><div class="line">                <span class="comment">// confidence target = 1</span></div><div class="line">                l.delta[obj_index] = <span class="number">1</span> - l.output[obj_index];</div><div class="line">                <span class="keyword">int</span> <span class="keyword">class</span> = net.truth[<span class="keyword">best_t</span>*(<span class="number">4</span> + <span class="number">1</span>) + b*l.truths + <span class="number">4</span>];</div><div class="line">                <span class="keyword">if</span> (l.<span class="built_in">map</span>) <span class="keyword">class</span> = l.<span class="built_in">map</span>[<span class="keyword">class</span>];</div><div class="line">                <span class="keyword">int</span> class_index = entry_index(l, b, n*l.w*l.h + j*l.w + i, <span class="number">4</span> + <span class="number">1</span>);</div><div class="line">                <span class="comment">// 对class进行求导</span></div><div class="line">                delta_yolo_class(l.output, l.delta, class_index, <span class="keyword">class</span>, l.classes, l.w*l.h, <span class="number">0</span>);</div><div class="line">                box truth = float_to_box(net.truth + <span class="keyword">best_t</span>*(<span class="number">4</span> + <span class="number">1</span>) + b*l.truths, <span class="number">1</span>);</div><div class="line">                <span class="comment">// 对box位置参数进行求导</span></div><div class="line">                delta_yolo_box(truth, l.output, l.biases, l.mask[n], box_index, i, j, l.w, l.h, net.w, net.h, l.delta, (<span class="number">2</span>-truth.w*truth.h), l.w*l.h);</div><div class="line">            &#125;</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p><p>我们首先来说一下为何confidence（包括后面的classification）的<code>diff</code>计算为何是<code>target - output</code>的形式。对于logistic regression，假设logistic函数的输入是$o = f(x;\theta)$。其中，$\theta$是网络的参数。那么输出$y = h(o)$，其中$h$指logistic激活函数（或sigmoid函数）。那么，我们有：</p><script type="math/tex; mode=display">\begin{aligned}P(y=1|x) &= h(o)\\ P(y=0|x) &= 1-h(o)\end{aligned}</script><p>写出对数极大似然函数，我们有：</p><script type="math/tex; mode=display">\log L = \sum y\log h+(1-y)\log(1-h)</script><p>为了使用SGD，上式两边取相反数，我们有损失函数：</p><script type="math/tex; mode=display">J = -\log L = \sum -y\log h-(1-y)\log(1-h)</script><p>对第$i$个输入$o_i$求导，我们有：</p><script type="math/tex; mode=display">\begin{aligned}\frac{\partial J}{\partial o_i} &= \frac{\partial J}{\partial h_i}\frac{\partial h_i}{\partial o_i}\\&= [-y_i/h_i-(y_i-1)/(1-h_i)] \frac{\partial h_i}{\partial o_i} \\&= \frac{h_i-y_i}{h_i(1-h_i)} \frac{\partial h_i}{\partial o_i}\end{aligned}</script><p>根据logistic函数的求导性质，有：</p><script type="math/tex; mode=display">\frac{\partial h_i}{\partial o_i} = h_i(1-h_i)</script><p>所以，有</p><script type="math/tex; mode=display">\frac{\partial J}{\partial o_i} = h_i-y_i</script><p>其中，$h_i$即为logistic激活后的输出，$y_i$为target。由于YOLO代码中均使用<code>diff</code>，也就是<code>-gradient</code>，所以有<code>delta = target - output</code>。</p><p>关于logistic回归，还可以参考我的博客：<a href="https://xmfbit.github.io/2018/03/21/cs229-supervised-learning/">CS229 简单的监督学习方法</a>。</p><p>下面，我们看下两个关键的子函数，<code>delta_yolo_class</code>和<code>delta_yolo_box</code>的实现。<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// class是类别的ground truth</span></div><div class="line"><span class="comment">// classes是类别总数</span></div><div class="line"><span class="comment">// index是feature map一维数组里面class prediction的起始索引</span></div><div class="line"><span class="function"><span class="keyword">void</span> <span class="title">delta_yolo_class</span><span class="params">(<span class="keyword">float</span> *output, <span class="keyword">float</span> *delta, <span class="keyword">int</span> index, </span></span></div><div class="line">  <span class="keyword">int</span> <span class="keyword">class</span>, <span class="keyword">int</span> classes, <span class="keyword">int</span> stride, <span class="keyword">float</span> *avg_cat) &#123;</div><div class="line">    <span class="keyword">int</span> n;</div><div class="line">    <span class="comment">// 这里暂时不懂</span></div><div class="line">    <span class="keyword">if</span> (delta[index])&#123;</div><div class="line">        delta[index + stride*<span class="keyword">class</span>] = <span class="number">1</span> - output[index + stride*<span class="keyword">class</span>];</div><div class="line">        <span class="keyword">if</span>(avg_cat) *avg_cat += output[index + stride*<span class="keyword">class</span>];</div><div class="line">        <span class="keyword">return</span>;</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">for</span>(n = <span class="number">0</span>; n &lt; classes; ++n)&#123;</div><div class="line">        <span class="comment">// 见上，diff = target - prediction</span></div><div class="line">        delta[index + stride*n] = ((n == <span class="keyword">class</span>)?<span class="number">1</span> : <span class="number">0</span>) - output[index + stride*n];</div><div class="line">        <span class="keyword">if</span>(n == <span class="keyword">class</span> &amp;&amp; avg_cat) *avg_cat += output[index + stride*n];</div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"><span class="comment">// box delta这里没什么可说的，就是square error的求导</span></div><div class="line"><span class="function"><span class="keyword">float</span> <span class="title">delta_yolo_box</span><span class="params">(box truth, <span class="keyword">float</span> *x, <span class="keyword">float</span> *biases, <span class="keyword">int</span> n, </span></span></div><div class="line">  <span class="keyword">int</span> index, <span class="keyword">int</span> i, <span class="keyword">int</span> j, <span class="keyword">int</span> lw, <span class="keyword">int</span> lh, <span class="keyword">int</span> w, <span class="keyword">int</span> h, </div><div class="line">  <span class="keyword">float</span> *delta, <span class="keyword">float</span> scale, <span class="keyword">int</span> stride) &#123;</div><div class="line">    box pred = get_yolo_box(x, biases, n, index, i, j, lw, lh, w, h, stride);</div><div class="line">    <span class="keyword">float</span> iou = box_iou(pred, truth);</div><div class="line">    <span class="keyword">float</span> tx = (truth.x*lw - i);</div><div class="line">    <span class="keyword">float</span> ty = (truth.y*lh - j);</div><div class="line">    <span class="keyword">float</span> tw = <span class="built_in">log</span>(truth.w*w / biases[<span class="number">2</span>*n]);</div><div class="line">    <span class="keyword">float</span> th = <span class="built_in">log</span>(truth.h*h / biases[<span class="number">2</span>*n + <span class="number">1</span>]);</div><div class="line"></div><div class="line">    delta[index + <span class="number">0</span>*stride] = scale * (tx - x[index + <span class="number">0</span>*stride]);</div><div class="line">    delta[index + <span class="number">1</span>*stride] = scale * (ty - x[index + <span class="number">1</span>*stride]);</div><div class="line">    delta[index + <span class="number">2</span>*stride] = scale * (tw - x[index + <span class="number">2</span>*stride]);</div><div class="line">    delta[index + <span class="number">3</span>*stride] = scale * (th - x[index + <span class="number">3</span>*stride]);</div><div class="line">    <span class="keyword">return</span> iou;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p><p>上面，我们遍历了每一个prediction的bounding box，下面我们还要遍历每个ground truth，根据IoU，为其分配一个最佳的匹配。<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// 遍历ground truth</span></div><div class="line"><span class="keyword">for</span>(t = <span class="number">0</span>; t &lt; l.max_boxes; ++t)&#123;</div><div class="line">    box truth = float_to_box(net.truth + t*(<span class="number">4</span> + <span class="number">1</span>) + b*l.truths, <span class="number">1</span>);</div><div class="line">    <span class="keyword">if</span>(!truth.x) <span class="keyword">break</span>;</div><div class="line">    <span class="comment">// 找到iou最大的那个bounding box</span></div><div class="line">    <span class="keyword">float</span> best_iou = <span class="number">0</span>;</div><div class="line">    <span class="keyword">int</span> best_n = <span class="number">0</span>;</div><div class="line">    i = (truth.x * l.w);</div><div class="line">    j = (truth.y * l.h);</div><div class="line">    box truth_shift = truth;</div><div class="line">    truth_shift.x = truth_shift.y = <span class="number">0</span>;</div><div class="line">    <span class="keyword">for</span>(n = <span class="number">0</span>; n &lt; l.total; ++n)&#123;</div><div class="line">        box pred = &#123;<span class="number">0</span>&#125;;</div><div class="line">        pred.w = l.biases[<span class="number">2</span>*n]/net.w;</div><div class="line">        pred.h = l.biases[<span class="number">2</span>*n+<span class="number">1</span>]/net.h;</div><div class="line">        <span class="keyword">float</span> iou = box_iou(pred, truth_shift);</div><div class="line">        <span class="keyword">if</span> (iou &gt; best_iou)&#123;</div><div class="line">            best_iou = iou;</div><div class="line">            best_n = n;</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line">    </div><div class="line">    <span class="keyword">int</span> mask_n = int_index(l.mask, best_n, l.n);</div><div class="line">    <span class="keyword">if</span>(mask_n &gt;= <span class="number">0</span>)&#123;</div><div class="line">        <span class="keyword">int</span> box_index = entry_index(l, b, mask_n*l.w*l.h + j*l.w + i, <span class="number">0</span>);</div><div class="line">        <span class="keyword">float</span> iou = delta_yolo_box(truth, l.output, l.biases, best_n, </div><div class="line">          box_index, i, j, l.w, l.h, net.w, net.h, l.delta, </div><div class="line">          (<span class="number">2</span>-truth.w*truth.h), l.w*l.h);</div><div class="line">        <span class="keyword">int</span> obj_index = entry_index(l, b, mask_n*l.w*l.h + j*l.w + i, <span class="number">4</span>);</div><div class="line">        avg_obj += l.output[obj_index];</div><div class="line">        <span class="comment">// 对应objectness target = 1</span></div><div class="line">        l.delta[obj_index] = <span class="number">1</span> - l.output[obj_index];</div><div class="line">        <span class="keyword">int</span> <span class="keyword">class</span> = net.truth[t*(<span class="number">4</span> + <span class="number">1</span>) + b*l.truths + <span class="number">4</span>];</div><div class="line">        <span class="keyword">if</span> (l.<span class="built_in">map</span>) <span class="keyword">class</span> = l.<span class="built_in">map</span>[<span class="keyword">class</span>];</div><div class="line">        <span class="keyword">int</span> class_index = entry_index(l, b, mask_n*l.w*l.h + j*l.w + i, <span class="number">4</span> + <span class="number">1</span>);</div><div class="line">        delta_yolo_class(l.output, l.delta, class_index, <span class="keyword">class</span>, l.classes, l.w*l.h, &amp;avg_cat);</div><div class="line">        ++count;</div><div class="line">        ++class_count;</div><div class="line">        <span class="keyword">if</span>(iou &gt; <span class="number">.5</span>) recall += <span class="number">1</span>;</div><div class="line">        <span class="keyword">if</span>(iou &gt; <span class="number">.75</span>) recall75 += <span class="number">1</span>;</div><div class="line">        avg_iou += iou;</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p><h2 id="Darknet网络架构"><a href="#Darknet网络架构" class="headerlink" title="Darknet网络架构"></a>Darknet网络架构</h2><p>引入了ResidualNet的思路（$3\times 3$和$1\times 1$的卷积核，shortcut连接），构建了Darknet-53网络。<br><img src="/img/paper-yolov3-darknet53.png" alt="darknet-63"></p><h2 id="YOLO的优势和劣势"><a href="#YOLO的优势和劣势" class="headerlink" title="YOLO的优势和劣势"></a>YOLO的优势和劣势</h2><p>把YOLO v3和其他方法比较，优势在于快快快。当你不太在乎IoU一定要多少多少的时候，YOLO可以做到又快又好。作者还在文章的结尾发起了这样的牢骚：</p><blockquote><p>Russakovsky et al report that that humans have a hard time distinguishing an IOU of .3 from .5! “Training humans to visually inspect a bounding box with IOU of 0.3 and distinguish it from one with IOU 0.5 is surprisingly difficult.” [16] If humans have a hard time telling the difference, how much does it matter?</p></blockquote><p>使用了多尺度预测，v3对于小目标的检测结果明显变好了。不过对于medium和large的目标，表现相对不好。这是需要后续工作进一步挖局的地方。</p><p>下面是具体的数据比较。<br><img src="/img/paper-yolov3-comparisons.png" alt="具体数据比较"></p><h2 id="我们是身经百战，见得多了"><a href="#我们是身经百战，见得多了" class="headerlink" title="我们是身经百战，见得多了"></a>我们是身经百战，见得多了</h2><p>作者还贴心地给出了什么方法没有奏效。</p><ul><li>anchor box坐标$(x, y)$的预测。预测anchor box的offset，no stable，不好。</li><li>线性offset预测，而不是logistic。精度下降。</li><li>focal loss。精度下降。</li><li>双IoU阈值，像Faster RCNN那样。效果不好。</li></ul><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p>下面是一些可供利用的参考资料：</p><ul><li>YOLO的项目主页<a href="https://pjreddie.com/darknet/yolo/" target="_blank" rel="external">Darknet YOLO</a></li><li>作者主页上的<a href="https://pjreddie.com/media/files/papers/YOLOv3.pdf" target="_blank" rel="external">paper链接</a></li><li>知乎专栏上的<a href="https://zhuanlan.zhihu.com/p/34945787" target="_blank" rel="external">全文翻译</a></li><li>FPN论文<a href="https://arxiv.org/abs/1612.03144" target="_blank" rel="external">Feature pyramid networks for object detection</a></li><li>知乎上的解答：<a href="https://www.zhihu.com/question/41540197" target="_blank" rel="external">AP是什么，怎么计算</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;YOLO的作者又放出了V3版本，在之前的版本上做出了一些改进，达到了更好的性能。这篇博客介绍这篇论文：&lt;a href=&quot;https://pjreddie.com/media/files/papers/YOLOv3.pdf&quot;&gt;YOLOv3: An Incremental Improvement&lt;/a&gt;。下面这张图是YOLO V3与RetinaNet的比较。&lt;br&gt;&lt;img src=&quot;/img/paper-yolov3-comparison-retinanet.png&quot; alt=&quot;YOLO v3和RetinaNet的比较&quot;&gt;&lt;/p&gt;
&lt;p&gt;可以使用搜索功能，在本博客内搜索YOLO前作的论文阅读和代码。&lt;br&gt;
    
    </summary>
    
    
      <category term="paper" scheme="https://xmfbit.github.io/tags/paper/"/>
    
      <category term="deep learning" scheme="https://xmfbit.github.io/tags/deep-learning/"/>
    
      <category term="detection" scheme="https://xmfbit.github.io/tags/detection/"/>
    
      <category term="yolo" scheme="https://xmfbit.github.io/tags/yolo/"/>
    
  </entry>
  
  <entry>
    <title>论文 - SqueezeNet, AlexNet-level accuracy with 50x fewer parameters and &lt;0.5MB model size</title>
    <link href="https://xmfbit.github.io/2018/03/24/paper-squeezenet/"/>
    <id>https://xmfbit.github.io/2018/03/24/paper-squeezenet/</id>
    <published>2018-03-24T06:02:53.000Z</published>
    <updated>2018-10-27T07:16:52.408Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://arxiv.org/abs/1602.07360" target="_blank" rel="external">SqueezeNet</a>由HanSong等人提出，和AlexNet相比，用少于$50$倍的参数量，在ImageNet上实现了comparable的accuracy。比较本文和HanSoing其他的工作，可以看出，其他工作，如Deep Compression是对已有的网络进行压缩，减小模型size；而SqueezeNet是从网络设计入手，从设计之初就考虑如何使用较少的参数实现较好的性能。可以说是模型压缩的两个不同思路。</p><a id="more"></a><h2 id="模型压缩相关工作"><a href="#模型压缩相关工作" class="headerlink" title="模型压缩相关工作"></a>模型压缩相关工作</h2><p>模型压缩的好处主要有以下几点：</p><ul><li>更好的分布式训练。server之间的通信往往限制了分布式训练的提速比例，较少的网络参数能够降低对server间通信需求。</li><li>云端向终端的部署，需要更低的带宽，例如手机app更新或无人车的软件包更新。</li><li>更易于在FPGA等硬件上部署，因为它们往往都有着非常受限的片上RAM。</li></ul><p>相关工作主要有两个方向，即模型压缩和模型结构自身探索。</p><p>模型压缩方面的工作主要有，使用SVD分解，Deep Compression等。模型结构方面比较有意义的工作是GoogLeNet的Inception module（可在博客内搜索<em>Xception</em>查看Xception的作者是如何受此启发发明Xception结构的）。</p><p>本文的作者从网络设计角度出发，提出了名为SqueezeNet的网络结构，使用比AlexNet少$50$倍的参数，在ImageNet上取得了comparable的结果。此外，还探究了CNN的arch是如何影响model size和最终的accuracy的。主要从两个方面进行了探索，分别是<em>CNN microarch</em>和<em>CNN macroarch</em>。前者意为在更小的粒度上，如每一层的layer怎么设计，来考察；后者是在更为宏观的角度，如一个CNN中的不同layer该如何组织来考察。</p><p><em>PS: 吐槽：看完之后觉得基本没探索出什么太有用的可以迁移到其他地方的规律。。。只是比较了自己的SqueezeNet在不同参数下的性能，有些标题党之嫌，题目很大，但是里面的内容并不完全是这样。CNN的设计还是实验实验再实验。</em></p><h2 id="SqueezeNet"><a href="#SqueezeNet" class="headerlink" title="SqueezeNet"></a>SqueezeNet</h2><p>为了简单，下文简称<em>SNet</em>。SNet的基本组成是叫做<em>Fire</em>的module。我们知道，对于一个CONV layer，它的参数数量计算应该是：$K \times K \times M \times N$。其中，$K$是filter的spatial size，$M$和$N$分别是输入feature map和输出activation的channel size。由此，设计SNet时，作者的依据主要是以下几点：</p><ul><li>把$3\times 3$的卷积替换成$1\times 1$，相当于减小上式中的$K$。</li><li>减少$3\times 3$filter对应的输入feature map的channel，相当于减少上式的$M$。</li><li>delayed downsample。使得activation的feature map能够足够大，这样对提高accuracy有益。CNN中的downsample主要是通过CONV layer或pooling layer中stride设置大于$1$得到的，作者指出，应将这种操作尽量后移。</li></ul><blockquote><p>Our intuition is that large activation maps (due to delayed downsampling) can lead to higher classification accuracy, with all else held equal.</p></blockquote><h3 id="Fire-Module"><a href="#Fire-Module" class="headerlink" title="Fire Module"></a>Fire Module</h3><p>Fire Module是SNet的基本组成单元，如下图所示。可以分为两个部分，一个是上面的<em>squeeze</em>部分，是一组$1\times 1$的卷积，用来将输入的channel squeeze到一个较小的值。后面是<em>expand</em>部分，由$1\times 1$和$3\times 3$卷积mix起来。使用$s_{1 x 1}$，$e_{1x1}$和$e_{3x3}$表示squeeze和expand中两种不同卷积的channel数量，令$s_{1x1} &lt; e_{1x1} + e_{3x3}$，用来实现上述策略2.<br><img src="/img/paper-squeezenet-fire-module.png" alt="Fire Module示意"></p><p>下面，对照PyTorch实现的SNet代码看下Fire的实现，注意上面说的CONV后面都接了ReLU。<br><figure class="highlight py"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Fire</span><span class="params">(nn.Module)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, inplanes, squeeze_planes,</span></span></div><div class="line">                 expand1x1_planes, expand3x3_planes):</div><div class="line">        super(Fire, self).__init__()</div><div class="line">        self.inplanes = inplanes</div><div class="line">        <span class="comment">## squeeze 部分</span></div><div class="line">        self.squeeze = nn.Conv2d(inplanes, squeeze_planes, kernel_size=<span class="number">1</span>)</div><div class="line">        self.squeeze_activation = nn.ReLU(inplace=<span class="keyword">True</span>)</div><div class="line">        <span class="comment">## expand 1x1 部分</span></div><div class="line">        self.expand1x1 = nn.Conv2d(squeeze_planes, expand1x1_planes,</div><div class="line">                                   kernel_size=<span class="number">1</span>)</div><div class="line">        self.expand1x1_activation = nn.ReLU(inplace=<span class="keyword">True</span>)</div><div class="line">        <span class="comment">## expand 3x3部分</span></div><div class="line">        self.expand3x3 = nn.Conv2d(squeeze_planes, expand3x3_planes,</div><div class="line">                                   kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</div><div class="line">        self.expand3x3_activation = nn.ReLU(inplace=<span class="keyword">True</span>)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></div><div class="line">        x = self.squeeze_activation(self.squeeze(x))</div><div class="line">        <span class="comment">## 将expand 部分1x1和3x3的cat到一起</span></div><div class="line">        <span class="keyword">return</span> torch.cat([</div><div class="line">            self.expand1x1_activation(self.expand1x1(x)),</div><div class="line">            self.expand3x3_activation(self.expand3x3(x))], <span class="number">1</span>)</div></pre></td></tr></table></figure></p><h3 id="SNet"><a href="#SNet" class="headerlink" title="SNet"></a>SNet</h3><p>有了Fire Module这个基础材料，我们就可以搭建SNet了。一个单独的<code>conv1</code> layer，后面接了$8$个连续的Fire Module，最后再接一个<code>conv10</code> layer。此外，在<code>conv1</code>，<code>fire4</code>, <code>fire8</code>和<code>conv10</code>后面各有一个<code>stride=2</code>的MAX Pooling layer。这些pooling的位置相对靠后，是对上述策略$3$的实践。我们还可以在不同的Fire Module中加入ResNet中的bypass结构。这样，形成了下图三种不同的SNet结构。<br><img src="/img/paper-squeezenet-macroarch.png" alt="SNet的三种形式"></p><p>一些细节：</p><ul><li>为了使得$1\times 1$和$3\times 3$的卷积核能够有相同spatial size的输出，$3\times 3$的卷积输入加了<code>padding=1</code>。</li><li>在squeeze layer和expand layer中加入了ReLU。</li><li>在<code>fire 9</code>后加入了drop ratio为$0.5$的Dropout layer。</li><li>受NIN启发，SNet中没有fc层。</li><li>更多的细节和训练参数的设置可以参考GitHub上的<a href="https://github.com/DeepScale/SqueezeNet" target="_blank" rel="external">官方repo</a>。</li></ul><p>同样的，我们可以参考PyTorch中的实现。注意下面实现了v1.0和v1.1版本，两者略有不同。v1.1版本参数更少，也能够达到v1.0的精度。</p><blockquote><p>SqueezeNet v1.1 (in this repo), which requires 2.4x less computation than SqueezeNet v1.0 without diminshing accuracy.</p></blockquote><figure class="highlight py"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">SqueezeNet</span><span class="params">(nn.Module)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, version=<span class="number">1.0</span>, num_classes=<span class="number">1000</span>)</span>:</span></div><div class="line">        super(SqueezeNet, self).__init__()</div><div class="line">        <span class="keyword">if</span> version <span class="keyword">not</span> <span class="keyword">in</span> [<span class="number">1.0</span>, <span class="number">1.1</span>]:</div><div class="line">            <span class="keyword">raise</span> ValueError(<span class="string">"Unsupported SqueezeNet version &#123;version&#125;:"</span></div><div class="line">                             <span class="string">"1.0 or 1.1 expected"</span>.format(version=version))</div><div class="line">        self.num_classes = num_classes</div><div class="line">        <span class="keyword">if</span> version == <span class="number">1.0</span>:</div><div class="line">            self.features = nn.Sequential(</div><div class="line">                nn.Conv2d(<span class="number">3</span>, <span class="number">96</span>, kernel_size=<span class="number">7</span>, stride=<span class="number">2</span>),</div><div class="line">                nn.ReLU(inplace=<span class="keyword">True</span>),</div><div class="line">                nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, ceil_mode=<span class="keyword">True</span>),</div><div class="line">                Fire(<span class="number">96</span>, <span class="number">16</span>, <span class="number">64</span>, <span class="number">64</span>),</div><div class="line">                Fire(<span class="number">128</span>, <span class="number">16</span>, <span class="number">64</span>, <span class="number">64</span>),</div><div class="line">                Fire(<span class="number">128</span>, <span class="number">32</span>, <span class="number">128</span>, <span class="number">128</span>),</div><div class="line">                nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, ceil_mode=<span class="keyword">True</span>),</div><div class="line">                Fire(<span class="number">256</span>, <span class="number">32</span>, <span class="number">128</span>, <span class="number">128</span>),</div><div class="line">                Fire(<span class="number">256</span>, <span class="number">48</span>, <span class="number">192</span>, <span class="number">192</span>),</div><div class="line">                Fire(<span class="number">384</span>, <span class="number">48</span>, <span class="number">192</span>, <span class="number">192</span>),</div><div class="line">                Fire(<span class="number">384</span>, <span class="number">64</span>, <span class="number">256</span>, <span class="number">256</span>),</div><div class="line">                nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, ceil_mode=<span class="keyword">True</span>),</div><div class="line">                Fire(<span class="number">512</span>, <span class="number">64</span>, <span class="number">256</span>, <span class="number">256</span>),</div><div class="line">            )</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            self.features = nn.Sequential(</div><div class="line">                nn.Conv2d(<span class="number">3</span>, <span class="number">64</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>),</div><div class="line">                nn.ReLU(inplace=<span class="keyword">True</span>),</div><div class="line">                nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, ceil_mode=<span class="keyword">True</span>),</div><div class="line">                Fire(<span class="number">64</span>, <span class="number">16</span>, <span class="number">64</span>, <span class="number">64</span>),</div><div class="line">                Fire(<span class="number">128</span>, <span class="number">16</span>, <span class="number">64</span>, <span class="number">64</span>),</div><div class="line">                nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, ceil_mode=<span class="keyword">True</span>),</div><div class="line">                Fire(<span class="number">128</span>, <span class="number">32</span>, <span class="number">128</span>, <span class="number">128</span>),</div><div class="line">                Fire(<span class="number">256</span>, <span class="number">32</span>, <span class="number">128</span>, <span class="number">128</span>),</div><div class="line">                nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, ceil_mode=<span class="keyword">True</span>),</div><div class="line">                Fire(<span class="number">256</span>, <span class="number">48</span>, <span class="number">192</span>, <span class="number">192</span>),</div><div class="line">                Fire(<span class="number">384</span>, <span class="number">48</span>, <span class="number">192</span>, <span class="number">192</span>),</div><div class="line">                Fire(<span class="number">384</span>, <span class="number">64</span>, <span class="number">256</span>, <span class="number">256</span>),</div><div class="line">                Fire(<span class="number">512</span>, <span class="number">64</span>, <span class="number">256</span>, <span class="number">256</span>),</div><div class="line">            )</div><div class="line">        <span class="comment"># Final convolution is initialized differently form the rest</span></div><div class="line">        final_conv = nn.Conv2d(<span class="number">512</span>, self.num_classes, kernel_size=<span class="number">1</span>)</div><div class="line">        self.classifier = nn.Sequential(</div><div class="line">            nn.Dropout(p=<span class="number">0.5</span>),</div><div class="line">            final_conv,</div><div class="line">            nn.ReLU(inplace=<span class="keyword">True</span>),</div><div class="line">            nn.AvgPool2d(<span class="number">13</span>, stride=<span class="number">1</span>)</div><div class="line">)</div></pre></td></tr></table></figure><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p>把SNet和AlexNet分别经过Deep Compression，在ImageNet上测试结果如下。可以看到，未被压缩时，SNet比AlexNet少了$50$倍，accuracy是差不多的。经过压缩，SNet更是可以进一步瘦身成不到$0.5$M，比原始的AlexNet瘦身了$500+$倍。<br><img src="/img/paper-squeezenet-benchmark.png" alt="性能比较"></p><p>注意上述结果是使用HanSong的Deep Compression技术（聚类+codebook）得到的。这种方法得到的模型在通用计算平台（CPU/GPU）上的优势并不明显，需要在作者提出的EIE硬件上才能充分发挥其性能。对于线性的量化（直接用量化后的$8$位定点存储模型），<a href="http://lepsucd.com/?page_id=630" target="_blank" rel="external">Ristretto</a>实现了SNet的量化，但是有一个点的损失。</p><h2 id="Micro-Arch探索"><a href="#Micro-Arch探索" class="headerlink" title="Micro Arch探索"></a>Micro Arch探索</h2><p>所谓CNN的Micro Arch，是指如何确定各层的参数，如filter的个数，kernel size的大小等。在SNet中，主要是filter的个数，即上文提到的$s_{1x1}$，$e_{1x1}$和$e_{3x3}$。这样，$8$个Fire Module就有$24$个超参数，数量太多，我们需要加一些约束，暴露主要矛盾，把问题变简单一点。</p><p>我们设定$base_e$是第一个Fire Module的expand layer的filter个数，每隔$freq$个Fire Module，会加上$incr_e$这么多。那么任意一个Fire Module的expand layer filter的个数为$e_i = base_e + (incr_e \times \lfloor \frac{i}{freq}\rfloor)$。</p><p>在expand layer，我们有$e_i = e_{i,1x1} + e_{i,3x3}$，设定$pct_{3x3} = e_{i,3x3}/e_i$为$3\times 3$的conv占的比例。</p><p>设定$SR = s_{i,1x1} / e_i$，为squeeze和expand filter个数比例。</p><h3 id="SR的影响"><a href="#SR的影响" class="headerlink" title="SR的影响"></a>SR的影响</h3><p>$SR$于区间$[0.125, 1]$之间取，accuracy基本随着$SR$增大而提升，同时模型的size也在变大。但$SR$从$0.75$提升到$1.0$，accuracy无提升。publish的SNet使用了$SR=0.125$。<br><img src="/img/paper-squeeze-sr-impact.png" alt="SR"></p><h3 id="1X1和3x3的比例pct的影响"><a href="#1X1和3x3的比例pct的影响" class="headerlink" title="1X1和3x3的比例pct的影响"></a>1X1和3x3的比例pct的影响</h3><p>为了减少参数，我们把部分$3\times 3$的卷积换成了$1\times 1$的，构成了expand layer。那么两者的比例对模型的影响？$pct$在$[0.01, 0.99]$之间变化。同样，accuracy和model size基本都随着$pct$增大而提升。当大于$0.5$时，模型的accuracy基本无提升。<br><img src="/img/paper-squeezenet-pct-impact.png" alt="pct"></p><h2 id="Macro-Arch探索"><a href="#Macro-Arch探索" class="headerlink" title="Macro Arch探索"></a>Macro Arch探索</h2><p>这里主要讨论了是否使用ResNet中的bypass结构。<br><img src="/img/paper-squeezenet-bypass.png" alt="bypass比较"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1602.07360&quot;&gt;SqueezeNet&lt;/a&gt;由HanSong等人提出，和AlexNet相比，用少于$50$倍的参数量，在ImageNet上实现了comparable的accuracy。比较本文和HanSoing其他的工作，可以看出，其他工作，如Deep Compression是对已有的网络进行压缩，减小模型size；而SqueezeNet是从网络设计入手，从设计之初就考虑如何使用较少的参数实现较好的性能。可以说是模型压缩的两个不同思路。&lt;/p&gt;
    
    </summary>
    
    
      <category term="paper" scheme="https://xmfbit.github.io/tags/paper/"/>
    
      <category term="deep learning" scheme="https://xmfbit.github.io/tags/deep-learning/"/>
    
      <category term="model compression" scheme="https://xmfbit.github.io/tags/model-compression/"/>
    
  </entry>
  
  <entry>
    <title>论文 - MobileNets, Efficient Convolutional Neural Networks for Mobile Vision Applications</title>
    <link href="https://xmfbit.github.io/2018/03/23/paper-mobilenet/"/>
    <id>https://xmfbit.github.io/2018/03/23/paper-mobilenet/</id>
    <published>2018-03-23T02:53:43.000Z</published>
    <updated>2018-10-27T07:16:52.405Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://arxiv.org/abs/1704.04861" target="_blank" rel="external">MobileNet</a>是建立在Depthwise Separable Conv基础之上的一个轻量级网络。在本论文中，作者定量计算了使用这一技术带来的计算量节省，提出了MobileNet的结构，同时提出了两个简单的超参数，可以灵活地进行模型性能和inference时间的折中。后续改进的<a href="https://arxiv.org/abs/1801.04381" target="_blank" rel="external">MobileNet v2</a>以后讨论。<br><a id="more"></a></p><h2 id="Depthwise-Separable-Conv"><a href="#Depthwise-Separable-Conv" class="headerlink" title="Depthwise Separable Conv"></a>Depthwise Separable Conv</h2><p>Depthwise Separable Conv把卷积操作拆成两个部分。第一部分，depthwise conv时，每个filter只在一个channel上进行操作。第二部分，pointwise conv是使用$1\times 1$的卷积核做channel上的combination。在Caffe等DL框架中，一般是设定卷积层的<code>group</code>参数，使其等于input的channel数来实现depthwise conv的。而pointwise conv和使用标准卷积并无不同，只是需要设置<code>kernel size = 1</code>。如下，是使用PyTorch的一个<a href="https://github.com/marvis/pytorch-mobilenet/blob/master/main.py#L67" target="_blank" rel="external">例子</a>。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv_dw</span><span class="params">(inp, oup, stride)</span>:</span></div><div class="line">    <span class="keyword">return</span> nn.Sequential(</div><div class="line">        <span class="comment">## 通过设置group=input channels来实现depthwise conv</span></div><div class="line">        nn.Conv2d(inp, inp, <span class="number">3</span>, stride, <span class="number">1</span>, groups=inp, bias=<span class="keyword">False</span>),</div><div class="line">        nn.BatchNorm2d(inp),</div><div class="line">        nn.ReLU(inplace=<span class="keyword">True</span>),</div><div class="line">    </div><div class="line">        nn.Conv2d(inp, oup, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, bias=<span class="keyword">False</span>),</div><div class="line">        nn.BatchNorm2d(oup),</div><div class="line">        nn.ReLU(inplace=<span class="keyword">True</span>),</div><div class="line">    )</div></pre></td></tr></table></figure><p>这样做的好处就是能够大大减少计算量。假设原始conv的filter个数为$N$，kernel size大小为$D_k$，输入的维度为$D_F\times D_F\times M$，那么总的计算量是$D_K\times D_K\times M\times N\times D_F\times D_F$（设定<code>stride=1</code>，即输入输出的feature map在spatial两个维度上相同）。</p><p>改成上述Depthwise Separable Conv后，计算量变为两个独立操作之和，即$D_K\times D_K\times M\times D_F \times D_F + M\times N\times D_F\times D_F$，计算量是原来的$\frac{1}{N} + \frac{1}{D_K^2} &lt; 1$。<br><img src="/img/paper-mobilenet-depthwise-separable-conv.png" alt="Depthwise Separable Conv示意图"></p><p>在实际使用时，我们在两个卷积操作之间加上BN和非线性变换层，如下图所示：<br><img src="/img/paper-mobilenet-conv-unit.png" alt="Conv-BN-ReLU"></p><h2 id="MobileNet"><a href="#MobileNet" class="headerlink" title="MobileNet"></a>MobileNet</h2><p>下图展示了如何使用Depthwise Separable Conv构建MobileNet。表中的<code>dw</code>表示depthwise conv，后面接的<code>stride=1</code>的conv即为pointwise conv。可以看到，网络就是这样的单元堆叠而成的。最后使用了一个全局的均值pooling，后面接上fc-1000来做分类。<br><img src="/img/paper-mobilenet-net-arch.png" alt="body arch"></p><p>此外，作者指出目前的深度学习框架大多使用GEMM实现卷积层的计算（如Caffe等先使用im2col，再使用GEMM）。但是pointwis= conv其实不需要reordering，说明目前的框架这里还有提升的空间。（不清楚目前PyTorch，TensorFlow等对pointwise conv和depthwise conv的支持如何）</p><p>在训练的时候，一个注意的地方是，对depthwise conv layer，weight decay的参数要小，因为这层本来就没多少个参数。</p><p>这里，给出PyTorch的一个<a href="https://github.com/marvis/pytorch-mobilenet/blob/master/main.py#L78" target="_blank" rel="external">第三方实现</a>。<br><figure class="highlight py"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line">self.model = nn.Sequential(</div><div class="line">    conv_bn(  <span class="number">3</span>,  <span class="number">32</span>, <span class="number">2</span>), </div><div class="line">    conv_dw( <span class="number">32</span>,  <span class="number">64</span>, <span class="number">1</span>),</div><div class="line">    conv_dw( <span class="number">64</span>, <span class="number">128</span>, <span class="number">2</span>),</div><div class="line">    conv_dw(<span class="number">128</span>, <span class="number">128</span>, <span class="number">1</span>),</div><div class="line">    conv_dw(<span class="number">128</span>, <span class="number">256</span>, <span class="number">2</span>),</div><div class="line">    conv_dw(<span class="number">256</span>, <span class="number">256</span>, <span class="number">1</span>),</div><div class="line">    conv_dw(<span class="number">256</span>, <span class="number">512</span>, <span class="number">2</span>),</div><div class="line">    conv_dw(<span class="number">512</span>, <span class="number">512</span>, <span class="number">1</span>),</div><div class="line">    conv_dw(<span class="number">512</span>, <span class="number">512</span>, <span class="number">1</span>),</div><div class="line">    conv_dw(<span class="number">512</span>, <span class="number">512</span>, <span class="number">1</span>),</div><div class="line">    conv_dw(<span class="number">512</span>, <span class="number">512</span>, <span class="number">1</span>),</div><div class="line">    conv_dw(<span class="number">512</span>, <span class="number">512</span>, <span class="number">1</span>),</div><div class="line">    conv_dw(<span class="number">512</span>, <span class="number">1024</span>, <span class="number">2</span>),</div><div class="line">    conv_dw(<span class="number">1024</span>, <span class="number">1024</span>, <span class="number">1</span>),</div><div class="line">    nn.AvgPool2d(<span class="number">7</span>),</div><div class="line">)</div><div class="line">self.fc = nn.Linear(<span class="number">1024</span>, <span class="number">1000</span>)</div></pre></td></tr></table></figure></p><h3 id="网络设计超参数的影响"><a href="#网络设计超参数的影响" class="headerlink" title="网络设计超参数的影响"></a>网络设计超参数的影响</h3><h4 id="wider？or-thinner？"><a href="#wider？or-thinner？" class="headerlink" title="wider？or thinner？"></a>wider？or thinner？</h4><p>描述网络，除了常见的深度，还有一个指标就是宽度。网络的宽度受filter个数的影响。更多的filter，说明网络更胖，提取feature的能力”看起来“就会越强。MobileNet使用一个超参数$\alpha$来实验。某个层的filter个数越多，带来的结果就是下一层filter的input channel会变多，$\alpha$就是前后input channel的数量比例。可以得到，计算量会大致变为原来的$\alpha^2$倍。</p><h4 id="resolution"><a href="#resolution" class="headerlink" title="resolution"></a>resolution</h4><p>如果输入的spatial dimension变成原来的$\rho$倍，也就是$D_F$变了，那么会对计算量带来影响。利用上面总结的计算公式不难发现，和$\alpha$一样，计算量会变成原来的$\rho^2$倍。</p><p>实际中，我们令$\alpha$和$\rho$都小于$1$，构建了更少参数的mobilenet。下面是一个具体参数设置下，网络计算量和参数数目的变化情况。<br><img src="/img/paper-mobilenet-alpha-rho-effect.png" alt="具体参数设置下的reduce情况"></p><h3 id="Depthwise-Separable-Conv真的可以？"><a href="#Depthwise-Separable-Conv真的可以？" class="headerlink" title="Depthwise Separable Conv真的可以？"></a>Depthwise Separable Conv真的可以？</h3><p>同样的网络结构，区别在于使用/不使用Depthwise Separable Conv技术，在ImageNet上的精度相差很少（使用这一技术，下降了$1$个点），但是参数和计算量却节省了很多。<br><img src="/img/paper-mobilenet-depthwise-vs-full-conv.png" alt="Depthwise Separable vs Full Convolution MobileNet"></p><h3 id="更浅的网络还是更瘦的网络"><a href="#更浅的网络还是更瘦的网络" class="headerlink" title="更浅的网络还是更瘦的网络"></a>更浅的网络还是更瘦的网络</h3><p>如果我们要缩减网络的参数，是更浅的网络更好，还是更瘦的网络更好呢？作者设计了参数和计算量相近的两个网络进行了比较，结论是相对而言，缩减网络深度不是个好主意。<br><img src="/img/paper-mobilenet-narrow-vs-shallow-net.png" alt="Narrow vs Shallow MobileNet"></p><h3 id="alpha和rho的定量影响"><a href="#alpha和rho的定量影响" class="headerlink" title="alpha和rho的定量影响"></a>alpha和rho的定量影响</h3><p>定量地比较了不同$\alpha$和$\rho$的设置下，网络的性能。$\alpha$越小，网络精度越低，而且下降速度是加快的。<br><img src="/img/paper-mobilenet-alpha-compact.png" alt="MobileNet Width Multiplier"></p><p>输入图像的resolution越小，网络精度也越低。<br><img src="/img/paper-mobilenet-rho-compact.png" alt="MobileNet Resolution"></p><h3 id="和其他网络的对比"><a href="#和其他网络的对比" class="headerlink" title="和其他网络的对比"></a>和其他网络的对比</h3><p>这里只贴出结果。一个值得注意的地方是，SqueezeNet虽然参数很少，但是计算量却很大。而MobileNet可以达到参数也很少。这是通过depthwise separable conv带来的好处。<br><img src="/img/paper-mobilenet-comparision-with-other-model.png" alt="与其他主流模型的比较"></p><h2 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h2><p>接下来，论文讨论了MobileNet在多种不同任务上的表现，证明了它的泛化能力和良好表现。可以在网上找到很多基于MobileNet的detection，classification等的开源项目代码，这里就不再多说了。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1704.04861&quot;&gt;MobileNet&lt;/a&gt;是建立在Depthwise Separable Conv基础之上的一个轻量级网络。在本论文中，作者定量计算了使用这一技术带来的计算量节省，提出了MobileNet的结构，同时提出了两个简单的超参数，可以灵活地进行模型性能和inference时间的折中。后续改进的&lt;a href=&quot;https://arxiv.org/abs/1801.04381&quot;&gt;MobileNet v2&lt;/a&gt;以后讨论。&lt;br&gt;
    
    </summary>
    
    
      <category term="paper" scheme="https://xmfbit.github.io/tags/paper/"/>
    
      <category term="deep learning" scheme="https://xmfbit.github.io/tags/deep-learning/"/>
    
      <category term="model compression" scheme="https://xmfbit.github.io/tags/model-compression/"/>
    
      <category term="model arch" scheme="https://xmfbit.github.io/tags/model-arch/"/>
    
  </entry>
  
  <entry>
    <title>论文 - Xception, Deep Learning with Depthwise separable Convolution</title>
    <link href="https://xmfbit.github.io/2018/03/22/paper-xception/"/>
    <id>https://xmfbit.github.io/2018/03/22/paper-xception/</id>
    <published>2018-03-22T01:44:38.000Z</published>
    <updated>2018-10-27T07:16:52.411Z</updated>
    
    <content type="html"><![CDATA[<p>在MobileNet, ShuffleMet等轻量级网络中，<strong>depthwise separable conv</strong>是一个很流行的设计。借助<a href="https://arxiv.org/abs/1610.02357" target="_blank" rel="external">Xception: Deep Learning with Depthwise separable Convolution</a>，对这种分解卷积的思路做一个总结。<br><a id="more"></a></p><h2 id="起源"><a href="#起源" class="headerlink" title="起源"></a>起源</h2><p>自从AlexNet以来，DNN的网络设计经过了ZFNet-&gt;VGGNet-&gt;GoogLeNet-&gt;ResNet等几个发展阶段。本文作者的思路正是受GoogLeNet中Inception结构启发。Inception结构是最早有别于VGG等“直筒型”结构的网络module。以Inception V3为例，一个典型的Inception模块长下面这个样子：<br><img src="/img/paper-xception-inception-module.png" alt="一个典型的Inception结构"></p><p>对于一个CONV层来说，它要学习的是一个$3D$的filter，包括两个空间维度（spatial dimension），即width和height；以及一个channel dimension。这个filter和输入在$3$个维度上进行卷积操作，得到最终的输出。可以用伪代码表示如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">// 对于第i个filter</div><div class="line">// 计算输入中心点(x, y)对应的卷积结果</div><div class="line">sum = 0</div><div class="line">for c in 1:C</div><div class="line">  for h in 1:K</div><div class="line">    for w in 1:K</div><div class="line">      sum += in[c, y-K/2+h, x-K/2+w] * filter_i[c, h, w]</div><div class="line">out[i, y, x] = sum</div></pre></td></tr></table></figure></p><p>可以看到，在$3D$卷积中，channel这个维度和spatial的两个维度并无不同。</p><p>在Inception中，卷积操作更加轻量级。输入首先被$1\times 1$的卷积核处理，得到了跨channel的组合(cross-channel correlation)，同时将输入的channel dimension减少了$3\sim 4$倍（一会$4$个支路要做<code>concat</code>操作）。这个结果被后续的$3\times 3$卷积和$5\times 5$卷积核处理，处理方法和普通的卷积一样，见上。</p><p>由此作者想到，Inception能够work证明后面的一条假设就是：卷积的channel相关性和spatial相关性是可以解耦的，我们没必要要把它们一起完成。</p><h2 id="简化Inception，提取主要矛盾"><a href="#简化Inception，提取主要矛盾" class="headerlink" title="简化Inception，提取主要矛盾"></a>简化Inception，提取主要矛盾</h2><p>接着，为了更好地分析问题，作者将Inception结构做了简化，保留了主要结构，去掉了AVE Pooling操作，如下所示。<br><img src="/img/paper-xception-simplified-inception-module.png" alt="简化后的Inception"></p><p>好的，我们现在将底层的$3$个$1\times 1$的卷积核组合起来，其实上面的图和下图是等价的。一个“大的”$1\times 1$的卷积核（channels数目变多），它的输出结果在channel上被分为若干组（group），每组分别和不同的$3\times 3$卷积核做卷积，再将这$3$份输出拼接起来，得到最后的输出。<br><img src="/img/paper-xception-equivalent-inception-module.png" alt="另一种形式"></p><p>那么，如果我们把分组数目继续调大呢？极限情况，我们可以使得group number = channel number，如下所示：<br><img src="/img/paper-xception-extreme-version.png" alt="极限模式"></p><h2 id="Depthwise-Separable-Conv"><a href="#Depthwise-Separable-Conv" class="headerlink" title="Depthwise Separable Conv"></a>Depthwise Separable Conv</h2><p>这种结构和一种名为<strong>depthwise separable conv</strong>的技术很相似，即首先使用group conv在spatial dimension上卷积，然后使用$1\times 1$的卷积核做cross channel的卷积（又叫做<em>pointwise conv</em>）。主要有两点不同：</p><ul><li>操作的顺序。在TensorFlow等框架中，depthwise separable conv的实现是先使用channelwise的filter只在spatial dimension上做卷积，再使用$1\times 1$的卷积核做跨channel的融合。而Inception中先使用$1\times 1$的卷积核。</li><li>非线性变换的缺席。在Inception中，每个conv操作后面都有ReLU的非线性变换，而depthwise separable conv没有。</li></ul><p>第一点不同不太重要，尤其是在深层网络中，这些block都是堆叠在一起的。第二点论文后面通过实验进行了比较。可以看出，去掉中间的非线性激活，能够取得更好的结果。<br><img src="/img/paper-xception-experiment-intermediate-activation.png" alt="非线性激活的影响"></p><h2 id="Xception网络架构"><a href="#Xception网络架构" class="headerlink" title="Xception网络架构"></a>Xception网络架构</h2><p>基于上面的分析，作者认为这样的假设是合理的：cross channel的相关和spatial的相关可以<strong>完全</strong>解耦。</p><blockquote><p>we make the following hypothesis: that the mapping of cross-channels correlations and spatial correlations in the feature maps of convolutional neural networks can be <em>entirely</em> decoupled. </p></blockquote><p>Xception的结构基于ResNet，但是将其中的卷积层换成了depthwise separable conv。如下图所示。整个网络被分为了三个部分：Entry，Middle和Exit。</p><blockquote><p>The Xception architecture: the data first goes through the entry flow, then through the middle flow which is repeated eight times, and finally through the exit flow. Note that all Convolution and SeparableConvolution layers are followed by batch normalization [7] (not included in the diagram). All SeparableConvolution layers use a depth multiplier of 1 (no depth expansion).</p></blockquote><p><img src="/img/paper-xception-arch.png" alt="Xception的网络结构"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在MobileNet, ShuffleMet等轻量级网络中，&lt;strong&gt;depthwise separable conv&lt;/strong&gt;是一个很流行的设计。借助&lt;a href=&quot;https://arxiv.org/abs/1610.02357&quot;&gt;Xception: Deep Learning with Depthwise separable Convolution&lt;/a&gt;，对这种分解卷积的思路做一个总结。&lt;br&gt;
    
    </summary>
    
    
      <category term="paper" scheme="https://xmfbit.github.io/tags/paper/"/>
    
      <category term="deep learning" scheme="https://xmfbit.github.io/tags/deep-learning/"/>
    
      <category term="model compression" scheme="https://xmfbit.github.io/tags/model-compression/"/>
    
  </entry>
  
  <entry>
    <title>CS229 简单的监督学习方法</title>
    <link href="https://xmfbit.github.io/2018/03/21/cs229-supervised-learning/"/>
    <id>https://xmfbit.github.io/2018/03/21/cs229-supervised-learning/</id>
    <published>2018-03-21T03:08:14.000Z</published>
    <updated>2018-10-27T07:16:52.386Z</updated>
    
    <content type="html"><![CDATA[<p>回过头去复习一下基础的监督学习算法，主要包括最小二乘法和logistic回归。<br><a id="more"></a></p><h2 id="最小二乘法"><a href="#最小二乘法" class="headerlink" title="最小二乘法"></a>最小二乘法</h2><p>最小二乘法是一个线性模型，即：</p><script type="math/tex; mode=display">\hat{y} = h_\theta(x) = \sum_{i=1}^{m}\theta_i x_i = \theta^T x</script><p>定义损失函数为Mean Square Error(MSE)，如下所示。其中，不戴帽子的$y$表示给定的ground truth。</p><script type="math/tex; mode=display">J(\theta) = \frac{1}{2}(\hat{y}-y)^2</script><p>那么，最小二乘就是要找到这样的参数$\theta^*$，使得：</p><script type="math/tex; mode=display">\theta^* = \arg\min J(\theta)</script><h3 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h3><p>使用梯度下降方法求解上述优化问题，我们有：</p><script type="math/tex; mode=display">\theta_{i+1} = \theta_{i} - \alpha \nabla_\theta J(\theta)</script><p>求导，有：</p><script type="math/tex; mode=display">\begin{aligned}\nabla_\theta J(\theta) &= \frac{1}{2}\nabla_\theta (\theta^T x - y)^2 \\&= (\theta^T x - y) x\end{aligned}</script><p>由于这里的损失函数是一个凸函数，所以梯度下降方法能够保证到达全局的极值点。</p><p>上面的梯度下降只是对单个样本来做的。实际上，我们可以取整个训练集或者训练集的一部分，计算平均损失函数$J(\theta) = \frac{1}{N}\sum_{i=1}^{N}J_i(\theta)$，做梯度下降，道理是一样的，只不过相差了常数因子$\frac{1}{N}$。</p><h3 id="正则方程"><a href="#正则方程" class="headerlink" title="正则方程"></a>正则方程</h3><p>除了梯度下降方法之外，上述问题还存在着解析解。我们将所有的样本输入$x^{(i)}$作为行向量，构成矩阵$X \in \mathbb{R}^{N\times d}$。其中，$N$为样本总数，$d$为单个样本的特征个数。那么，对于参数$\theta\in\mathbb{R}^{d\times 1}$来说，$X\theta$的第$i$行就可以给出模型对第$i$个样本的预测结果。我们将ground truth排成一个$N\times 1$的矩阵，那么，损失函数可以写作：</p><script type="math/tex; mode=display">J(\theta) = \frac{1}{2N} \Vert X\theta-y \Vert_2^2</script><p>将$\Vert x\Vert_2^2$写作$x^T x$，同时略去常数项，我们有：</p><script type="math/tex; mode=display">\begin{aligned}J &= (X\theta - y)^T (X\theta - y) \\&= \theta^T X^T X\theta - 2\theta^T x^T y +y^T y\end{aligned}</script><p>对其求导，有：</p><script type="math/tex; mode=display">\nabla_\theta J = X^T X\theta - X^T y</script><p><img src="/img/cs229-supervised-learning-least-square-normal-equation.png" alt="具体计算过程贴图"></p><p>这其中，主要用到的矩阵求导性质如下：<br><img src="/img/cs229-supervised-learning-some-useful-matrix-derivatives.png" alt="一些典型求导结果"><br>令导数为$0$，求得极值点处：</p><script type="math/tex; mode=display">\theta^* = (X^TX)^{-1}X^T y</script><h3 id="概率解释"><a href="#概率解释" class="headerlink" title="概率解释"></a>概率解释</h3><p>这里对上述做法给出一个概率论上的解释。首先我们要引入似然函数（likelihood function）的概念。</p><p>似然函数是一个关于模型参数$\theta$的函数，它描述了某个参数$\theta$下，给出输入$x$，得到输出$y$的概率。用具体的公式表示如下：</p><script type="math/tex; mode=display">L(\theta) = \prod_{i=1}^{N}P(y^{(i)}|x^{(i)};\theta)</script><p>假设线性模型的预测结果和ground truth之间的误差服从Gaussian分布，也就是说，</p><script type="math/tex; mode=display">y - \theta^T x  =  \epsilon \sim \mathcal{N}(0, \sigma^2)</script><p>那么上面的似然函数可以写作：</p><script type="math/tex; mode=display">L(\theta) = \prod_{i=1}^{N}\frac{1}{\sqrt{2\pi\sigma}}\exp(\frac{(y^{(i)}-\theta^T x^{(i)})^2}{2\sigma^2})</script><p>如何估计参数$\theta$呢？我们可以认为，参数$\theta$使得出现样本点$(x^{(i)}, y^{(i)})$的概率变大，所以才能被我们观测到。自然，我们需要使得似然函数$L(\theta)$取得极大值，也就是说：</p><script type="math/tex; mode=display">\theta^* = \arg\max L(\theta)</script><p>通过引入$\log(\cdot)$，可以将连乘变成连加，同时不改变函数的单调性。这样，实际上我们操作的是对数似然函数$\log L(\theta)$。有：</p><script type="math/tex; mode=display">\begin{aligned} \mathcal{l} &= \log L(\theta) \\&= \sum_{i=1}^{N}\log \frac{1}{\sqrt{2\pi\sigma^2}} \exp (\frac{(y^{(i)}-\theta^T x^{(i)})^2}{2\sigma^2})\\&= N\log\frac{1}{\sqrt{2\pi\sigma^2}} -\frac{1}{\sigma^2}\frac{1}{2}\sum_{i=1}^{N}(y^{(i)}-\theta^T x^{(i)})^2 \end{aligned}</script><p>略去前面的常数项不管，后面一项正好是最小二乘法的损失函数。要想最大化对数似然函数，也就是要最小化上面的损失函数。</p><p>所以，最小二乘法的损失函数可以由数据集的噪声服从Gaussian分布自然地导出。</p><h3 id="加权最小二乘法"><a href="#加权最小二乘法" class="headerlink" title="加权最小二乘法"></a>加权最小二乘法</h3><p>加权最小二乘法是指对数据集中的数据赋予不同的权重，一个重要的用途是使用权重$w^{(i)} = \exp (-\frac{(x^{(i)}-x)^2}{2\tau^2})$做局部最小二乘。不再多说。</p><h2 id="logistic回归"><a href="#logistic回归" class="headerlink" title="logistic回归"></a>logistic回归</h2><p>虽然叫回归，但是logistic回归解决的问题是分类问题。</p><h3 id="logistic函数"><a href="#logistic函数" class="headerlink" title="logistic函数"></a>logistic函数</h3><p>logistic函数$\sigma(x) = \frac{1}{1+e^{-x}}$，又叫sigmoid函数，将输入$(-\infty, +\infty)$压缩到$(0, 1)$之间。它的形状如下：<br><img src="/img/cs229-supervised-learning-sigmoid.png" alt="sigmoid函数"></p><p>对其求导，发现导数值可以完全不依赖于输入$x$：</p><script type="math/tex; mode=display">\frac{d\sigma(x)} {dx} = \sigma(x)(1-\sigma(x))</script><p>我们将logistic函数的输入取做$x$的feature的线性组合，就得到了假设函数$h_\theta(x) = \sigma(\theta^T x)$。</p><h3 id="logistic回归-1"><a href="#logistic回归-1" class="headerlink" title="logistic回归"></a>logistic回归</h3><p>logistic函数的输出既然是在$(0,1)$上，我们可以将其作为概率。也就是说，我们认为它的输出是样本点属于类别$1$的概率：</p><script type="math/tex; mode=display">\begin{aligned}P(y=1|x) &= h_\theta(x) \\P(y=0|x) &= 1-h_\theta(x) \end{aligned}</script><p>或者我们写的更紧凑些：</p><script type="math/tex; mode=display">P(y|x) = (h_\theta(x))^y (1-h_\theta(x))^(1-y)</script><p>我们仍然使用上述极大似然的估计方法，求取参数$\theta$，为求简练，隐去了上标$(i)$。</p><script type="math/tex; mode=display">\begin{aligned}L(\theta) &= \prod_{i=1}^{N}P(y|x;\theta) \\&=\prod (h_\theta(x))^y (1-h_\theta(x))^{(1-y)} \end{aligned}</script><p>取对数：</p><script type="math/tex; mode=display">\log L(\theta) = \sum_{i=1}^{N}y\log(h(x)) + (1-y)\log(1-h(x))</script><p>所以，我们的损失函数为$J(\theta) = - [y\log(h(x)) + (1-y)\log(1-h(x))]$。把$h(x)$换成$P$，岂不就是深度学习中常用的交叉损失熵在二分类下的特殊情况？</p><p>回到logistic回归，使用梯度下降，我们可以得到更新参数的策略：</p><script type="math/tex; mode=display">\theta_{i+1} = \theta_i - \alpha (h_\theta(x) - y)x</script><p>啊哈！形式和最小二乘法完全一样。只不过要注意，现在的$h_\theta(x)$已经变成了一个非线性函数。</p><h3 id="感知机"><a href="#感知机" class="headerlink" title="感知机"></a>感知机</h3><p>在上述logistic回归基础上，我们强制将其输出映射到$\lbrace 1, -1\rbrace$。即将$\sigma(x)$换成$g(x)$：</p><script type="math/tex; mode=display">g(x) = \begin{cases} 1, \quad\text{if}\quad x \ge 0\\ 0, \quad\text{if}\quad x < 0\end{cases}</script><p>使用同样的更新方法，我们就得到了感知机模型（perceptron machine）。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;回过头去复习一下基础的监督学习算法，主要包括最小二乘法和logistic回归。&lt;br&gt;
    
    </summary>
    
    
      <category term="公开课" scheme="https://xmfbit.github.io/tags/%E5%85%AC%E5%BC%80%E8%AF%BE/"/>
    
      <category term="cs229" scheme="https://xmfbit.github.io/tags/cs229/"/>
    
  </entry>
  
  <entry>
    <title>Hack PyCaffe</title>
    <link href="https://xmfbit.github.io/2018/03/16/caffe-hack-python-interface/"/>
    <id>https://xmfbit.github.io/2018/03/16/caffe-hack-python-interface/</id>
    <published>2018-03-16T11:01:32.000Z</published>
    <updated>2018-10-27T07:16:52.378Z</updated>
    
    <content type="html"><![CDATA[<p>这篇文章主要是<a href="https://github.com/nitnelave/pycaffe_tutorial/blob/master/04%20Hacking%20the%20Python%20API.ipynb" target="_blank" rel="external">Github: PyCaffe Tutorial</a>中Hack Pycaffe的翻译整理。后续可能会加上一些使用boost和C++为Python接口提供后端的解释。这里主要讨论如何为Pycaffe添加自己想要的功能。至于Pycaffe的使用，留待以后的文章整理。<br><img src="/img/caffe-hack-pycaffe-python-cpp-binding.jpg" alt="Python&amp;&amp;CPP binding"><br><a id="more"></a></p><h2 id="PyCaffe的代码组织结构"><a href="#PyCaffe的代码组织结构" class="headerlink" title="PyCaffe的代码组织结构"></a>PyCaffe的代码组织结构</h2><p>见Caffe的<code>python</code>目录。下面这张图是与PyCaffe相关的代码的分布。其中<code>src</code>和<code>include</code>是Caffe框架的后端C++实现，<code>python</code>目录中是与PyCaffe关系更密切的代码。可以看到，除了<code>_caffe.cpp</code>以外，其他都是纯python代码。<code>_caffe.cpp</code>使用boost提供了C++与python的绑定，而其他python脚本在此层的抽象隔离之上，继续完善了相关功能，提供了更加丰富的API、<br><img src="/img/hack-pycaffe-code-organization.png" alt="代码组织结构"></p><h2 id="添加纯Python功能"><a href="#添加纯Python功能" class="headerlink" title="添加纯Python功能"></a>添加纯Python功能</h2><p>首先，我们介绍如何在C++构建的PyCaffe隔离之上，用纯python实现想要的功能。</p><h3 id="添加的功能和PyCaffe基本平行，不需要改变已有代码"><a href="#添加的功能和PyCaffe基本平行，不需要改变已有代码" class="headerlink" title="添加的功能和PyCaffe基本平行，不需要改变已有代码"></a>添加的功能和PyCaffe基本平行，不需要改变已有代码</h3><p>有的时候想加入的功能和PyCaffe的关系基本是平行的，比如想仿照<code>PyTorch</code>等框架，加入对数据进行预处理的<code>Transformer</code>功能（这个API其实已经在PyCaffe中实现了，这里只是举个例子）。为了实现这个功能，我们可能需要使用<code>numpy</code>和<code>opencv</code>等包装图像的预处理操作，但是和Caffe本身基本没什么关系。在这样的情况下，我们直接编写即可。要注意在<code>python/caffe/__init__.py</code>中import相关的子模块或函数。这个例子可以参考<code>caffe.io</code>的实现（见<code>python/caffe/io.py</code>文件）。</p><h3 id="添加的功能需要Caffe的支持，向已有的类中添加函数"><a href="#添加的功能需要Caffe的支持，向已有的类中添加函数" class="headerlink" title="添加的功能需要Caffe的支持，向已有的类中添加函数"></a>添加的功能需要Caffe的支持，向已有的类中添加函数</h3><p>如果添加的功能需要Caffe的支持，可以在<code>pycaffe.py</code>内添加，详见<code>Net</code>的例子。由于python的灵活性，我们可以参考<code>Net</code>的实现方式，待函数实现完成后，使用<code>&lt;class&gt;.&lt;function&gt; = my_function</code>动态地添加。如下所示，注意<code>_Net_forward</code>函数的第一个参数必须是<code>self</code>。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">_Net_forward</span><span class="params">(self, blobs=None, start=None, end=None, **kwargs)</span>:</span></div><div class="line">    <span class="comment"># do something</span></div><div class="line">Net.forward = _Net_forward</div></pre></td></tr></table></figure><p>与之相似，我们还可以为已经存在的类添加字段。注意，函数用<code>@property</code>装饰，且参数有且只有一个<code>self</code>，</p><figure class="highlight py"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># This function will be called when accessing net.blobs</span></div><div class="line"><span class="meta">@property</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">_Net_blobs</span><span class="params">(self)</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line">    An OrderedDict (bottom to top, i.e., input to output) of network</div><div class="line">    blobs indexed by name</div><div class="line">    """</div><div class="line">    <span class="keyword">if</span> <span class="keyword">not</span> hasattr(self, <span class="string">'_blobs_dict'</span>):</div><div class="line">        self._blobs_dict = OrderedDict(zip(self._blob_names, self._blobs))</div><div class="line">    <span class="keyword">return</span> self._blobs_dict </div><div class="line"></div><div class="line"><span class="comment"># Set the field `blobs` to call _Net_blobs</span></div><div class="line">Net.blobs = _Net_blobs</div></pre></td></tr></table></figure><p>PyCaffe中已经实现的类主要有：<code>Net, SGDSolver, NesterovSolver, AdaGradSolver, RMSPropSolver, AdaDeltaSolver, AdamSolver</code>。</p><h2 id="使用C-添加功能"><a href="#使用C-添加功能" class="headerlink" title="使用C++添加功能"></a>使用C++添加功能</h2><p>当遇到如下情况时，可能需要修改C++代码：</p><ul><li>为了获取更底层的权限控制，如一些私有字段。</li><li>性能考虑。</li></ul><p>这时，你应该去修改<code>python/caffe/_caffe.cpp</code>文件。这个文件使用了boost实现了python与C++的绑定。</p><p>为了添加一个字段，可以在<code>Blob</code>部分添加如下的代码。这样，就会将python中<code>Blob</code>类的<code>num</code>字段绑定到C++的<code>Blob&lt;Dtype&gt;::num()</code>方法上。<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">.add_property(<span class="string">"num"</span>, &amp;Blob&lt;Dtype&gt;::num)</div></pre></td></tr></table></figure></p><p>使用<code>.def</code>可以为python相应的类绑定方法。在下面的代码中，首先实现了<code>Net_Save</code>方法，然后将其绑定到了python中<code>Net</code>类的<code>save</code>方法上。这样，通过python调用<code>net.save(filename)</code>即可。</p><p>注意，当你修改了<code>_caffe,cpp</code>后，记得使用<code>make pycaffe</code>重新生成动态链接库。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"># <span class="function">Declare the function</span></div><div class="line"><span class="keyword">void</span> <span class="title">Net_Save</span><span class="params">(<span class="keyword">const</span> Net&lt;Dtype&gt;&amp; net, <span class="built_in">string</span> filename)</span> &#123;</div><div class="line">    <span class="comment">// ...</span></div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">// ...</span></div><div class="line"></div><div class="line">bp::class_&lt;Net&lt;Dtype&gt;&gt;(<span class="string">"Net"</span>, bp::no_init)</div><div class="line"># Now we can call net.save(file)</div><div class="line">.def(<span class="string">"save"</span>, &amp;Net_Save)</div></pre></td></tr></table></figure><p>当然，上面介绍的这些还很基础，关于boost的python绑定，可以参考官方的文档：<a href="http://www.boost.org/doc/libs/1_58_0/libs/python/doc/tutorial/doc/html/index.html" target="_blank" rel="external">boost: python binding</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这篇文章主要是&lt;a href=&quot;https://github.com/nitnelave/pycaffe_tutorial/blob/master/04%20Hacking%20the%20Python%20API.ipynb&quot;&gt;Github: PyCaffe Tutorial&lt;/a&gt;中Hack Pycaffe的翻译整理。后续可能会加上一些使用boost和C++为Python接口提供后端的解释。这里主要讨论如何为Pycaffe添加自己想要的功能。至于Pycaffe的使用，留待以后的文章整理。&lt;br&gt;&lt;img src=&quot;/img/caffe-hack-pycaffe-python-cpp-binding.jpg&quot; alt=&quot;Python&amp;amp;&amp;amp;CPP binding&quot;&gt;&lt;br&gt;
    
    </summary>
    
    
      <category term="caffe" scheme="https://xmfbit.github.io/tags/caffe/"/>
    
      <category term="python" scheme="https://xmfbit.github.io/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>论文 - Learning both Weights and Connections for Efficient Neural Networks</title>
    <link href="https://xmfbit.github.io/2018/03/14/paper-network-prune-hansong/"/>
    <id>https://xmfbit.github.io/2018/03/14/paper-network-prune-hansong/</id>
    <published>2018-03-14T08:18:53.000Z</published>
    <updated>2018-10-27T07:16:52.406Z</updated>
    
    <content type="html"><![CDATA[<p>Han Song的Deep Compression是模型压缩方面很重要的论文。在Deep Compression中，作者提出了三个步骤来进行模型压缩：剪枝，量化和霍夫曼编码。其中，剪枝对应的方法就是基于本文要总结的这篇论文：<a href="https://arxiv.org/abs/1506.02626" target="_blank" rel="external">Learning both Weights and Connections for Efficient Neural Networks</a>。在这篇论文中，作者介绍了如何在不损失精度的前提下，对深度学习的网络模型进行剪枝，从而达到减小模型大小的目的。<br><img src="/img/paper-pruning-network-demo.png" alt="Pruning的主要过程"><br><a id="more"></a></p><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>DNN虽然能够解决很多以前很难解决的问题，但是一个应用方面的问题就是这些模型通常都太大了。尤其是当运行在手机等移动设备上时，对电源和网络带宽都是负担。对于电源来说，由于模型巨大，所以只能在外部内存DRAM中加载，造成能耗上升。具体数值见下表。所以模型压缩很有必要。本文就是使用剪枝的方法，将模型中不重要的权重设置为$0$，将原来的dense model转变为sparse model，达到压缩的目的。<br><img src="/img/paper-pruning-network-energy-for-different-memory-hieracy.png" alt="操作数地址的不同造成的功耗对比"></p><h3 id="解决什么问题？"><a href="#解决什么问题？" class="headerlink" title="解决什么问题？"></a>解决什么问题？</h3><p>如何在不损失精度的前提下，对DNN进行剪枝（或者说稀疏化），从而压缩模型。</p><h3 id="为什么剪枝是work的？"><a href="#为什么剪枝是work的？" class="headerlink" title="为什么剪枝是work的？"></a>为什么剪枝是work的？</h3><p>为什么能够通过剪枝的方法来压缩模型呢？难道剪掉的那些连接真的不重要到可以去掉吗？论文中，作者指出，DNN模型广泛存在着参数过多的问题，具有很大的冗余（见参考文献NIPS 2013的一篇文章<a href="https://arxiv.org/abs/1306.0543" target="_blank" rel="external">Predicting parameters in deep learning</a>）。</p><blockquote><p>Neural networks are typically over-parameterized, and there is significant redundancy for deep learning models </p></blockquote><p>另外，作者也为自己的剪枝方法找到了生理学上的依据，生理学上发现，对于哺乳动物来说，婴儿期会产生许多的突触连接，在后续的成长过程中，不怎么用的那些突出会退化消失。</p><h3 id="怎么做"><a href="#怎么做" class="headerlink" title="怎么做"></a>怎么做</h3><p>作者的方法分为三个步骤：</p><ul><li>Train Connectivity: 按照正常方法训练初始模型。作者认为该模型中权重的大小表征了其重要程度</li><li>Prune Connection: 将初始模型中那些低于某个阈值的的权重参数置成$0$（即所谓剪枝）</li><li>Re-Train: 重新训练，以期其他未被剪枝的权重能够补偿pruning带来的精度下降</li></ul><p>为了达到一个满意的压缩比例和精度要求，$2$和$3$要重复多次。</p><h2 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h2><p>为了减少网络的冗余，减小模型的size，有以下相关工作：</p><ul><li>定点化。将weight使用8bit定点表示，32bit浮点表示activation。</li><li>低秩近似。使用矩阵分解等方法。</li><li>网络设计上，NIN等使用Global Average Pooling取代FC层，可以大大减少参数量，这种结构已经得到了广泛使用。而FC也并非无用。在Pooling后面再接一个fc层，便于后续做迁移学习transfer learning。</li><li>从优化上下手，使用损失函数的Hessian矩阵，比直接用weight decay更好。</li><li>HashedNet等工作，这里不再详述。</li></ul><h2 id="如何Prune"><a href="#如何Prune" class="headerlink" title="如何Prune"></a>如何Prune</h2><p>主要分为三步，上面 概述 中 怎么做 部分已经简单列出。下面的算法流程摘自作者的博士论文，可能更加详细清楚。<br><img src="/img/paper-pruning-network-algrithem.png" alt="剪枝算法"></p><h3 id="正则项的选择"><a href="#正则项的选择" class="headerlink" title="正则项的选择"></a>正则项的选择</h3><p>L1和L2都可以用来做正则，惩罚模型的复杂度。使用不同的正则方法会对pruning和retraining产生影响。实验发现，采用L2做正则项较好。见下图，可以看到详细的比较结果，分别是with/without retrain下L1和L2正则对精度的影响。还可以看到一个共性的地方，就是当pruning的比例大于某个阈值后，模型的精度会快速下降。</p><p><img src="/img/paper-pruning-network-regularization.png" alt="L1/L2 Regularization"></p><h3 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a>Dropout</h3><p>Dropout是一项防止过拟合的技术。要注意的是，在retraining的时候，我们需要对Dropout ratio做出调整。因为网络中的很多连接都被剪枝剪下来了，所以dropout的比例要变小。下面给出定量的估计。</p><p>对于FC层来说，如果第$i$层的神经元个数是$N_i$，那么该层的连接数$C_i$用乘法原理可以很容易得到：$C_i = N_{i-1}N_i$。也就是说，连接数$C\sim N^2$。而dropout是作用于神经元的（dropout是将$N_i$个神经元输出按照概率dropout掉）。所以，比例$D^2 \sim C$，最后得到：</p><script type="math/tex; mode=display">D_r = D_o \sqrt{\frac{C_{ir}}{C_{io}}}</script><p>其中，下标$r$表示retraining，$o$表示初始模型(original)。</p><h2 id="Local-Pruning"><a href="#Local-Pruning" class="headerlink" title="Local Pruning"></a>Local Pruning</h2><p>在retraining部分，在初始模型基础上继续fine tune较好。为了能够更有效地训练，在训练FC层的时候，可以将CONV的参数固定住。反之亦然。</p><p>另外，不同深度和类型的layer对剪枝的敏感度是不一样的。作者指出，CONV比FC更敏感，第$1$个CONV比后面的要敏感。下图是AlexNet中各个layer剪枝比例和模型精度下降之间的关系。可以印证上面的结论。</p><p><img src="/img/paper-pruning-network-layer-sensitivity.png" alt="CONV和FC的prune和精度下降的关系"></p><h2 id="多次迭代剪枝"><a href="#多次迭代剪枝" class="headerlink" title="多次迭代剪枝"></a>多次迭代剪枝</h2><p>应该迭代地进行多次剪枝 + 重新训练这套组合拳。作者还尝试过根据参数的绝对值依概率进行剪枝，效果不好。<br><img src="/img/paper-pruning-network-iterative-pruning.png" alt="迭代剪枝"></p><h2 id="对神经元进行剪枝"><a href="#对神经元进行剪枝" class="headerlink" title="对神经元进行剪枝"></a>对神经元进行剪枝</h2><p>将神经元之间的connection剪枝后（或者说将权重稀疏化了），那些$0$输入$0$输出的神经元也应该被剪枝了。然后，我们又可以继续以这个神经元出发，剪掉与它相关的connection。这个步骤可以在训练的时候自动发生。因为如果某个神经元已经是dead状态，那么它的梯度也会是$0$。那么只有正则项推着它向$0$的方向。</p><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p>使用Caffe实现，需要加入一个<code>mask</code>来表示剪枝。剪枝的阈值，是该layer的权重标准差乘上某个超参数。这里：<a href="https://github.com/BVLC/caffe/pull/4294/files" target="_blank" rel="external">Add pruning possibilities at inner_product_layer #4294 </a>，有人基于Caffe官方的repo给FC层加上了剪枝。这里：<a href="https://github.com/may0324/DeepCompression-caffe" target="_blank" rel="external">Github: DeepCompression</a>,，有人实现了Deep Compression，可以参考他们的实现思路。</p><p>对于实验结果，论文中比对了LeNet和AlexNet。此外，作者的博士论文中给出了更加详细的实验结果，在更多的流行的模型上取得了不错的压缩比例。直接引用如下，做一个mark：</p><blockquote><p>On the ImageNet dataset, the pruning method reduced the number of parameters of AlexNet by a factor of 9× (61 to 6.7 million), without incurring accuracy loss. Similar experiments with VGG-16 found that the total number of parameters can be reduced by 13× (138 to 10.3 million), again with no loss of accuracy. We also experimented with the more efficient fully-convolutional neural networks: GoogleNet (Inception-V1), SqueezeNet, and ResNet-50, which have zero or very thin fully connected layers. From these experiments we find that they share very similar pruning ratios before the accuracy drops: 70% of the parameters in those fully-convolutional neural networks can be pruned. GoogleNet is pruned from 7 million to 2 million parameters, SqueezeNet from 1.2 million to 0.38 million, and ResNet-50 from 25.5 million to 7.47 million, all with no loss of Top-1 and Top-5 accuracy on Imagenet.</p></blockquote><p><img src="/img/paper-pruning-network-results.png" alt="Results"></p><p>下面 参考资料 部分也给出了作者在GitHub上放出的Deep Compression的结果，可以前去参考。</p><h3 id="学习率的设置"><a href="#学习率的设置" class="headerlink" title="学习率的设置"></a>学习率的设置</h3><p>跑模型跑实验，一个重要的超参数就是学习率$LR$。这里作者也给了一个经验规律。一般在训练初始模型的时候，学习率都是逐渐下降的。刚开始是一个较大的值$LR_1$，最后是一个较小的值$LR_2$。它们之间可能有数量级的差别。作者指出，retraining的学习率应该介于两者之间。可以取做比$LR_1$小$1 \sim 2$个数量级。</p><h3 id="RNN和LSTM"><a href="#RNN和LSTM" class="headerlink" title="RNN和LSTM"></a>RNN和LSTM</h3><p>在博士论文中，作者还是用这一技术对RNN/LSTM在Neural Talk任务上做了剪枝，取得了不错的结果。<br><img src="/img/paper-pruning-network-lstm.png" alt="LSTM"></p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul><li>HanSong的个人主页：<a href="http://stanford.edu/~songhan/" target="_blank" rel="external">Homepage</a></li><li>HanSong的博士论文：<a href="https://purl.stanford.edu/qf934gh3708" target="_blank" rel="external">Efficient Methods and Hardware for Deep Learning</a></li><li>后续的Deep Compression论文：<a href="https://arxiv.org/abs/1510.00149" target="_blank" rel="external">DEEP COMPRESSION- COMPRESSING DEEP NEURAL NETWORKS WITH PRUNING, TRAINED QUANTIZATION AND HUFFMAN CODING</a></li><li>Deep Compression AlexNet: <a href="https://github.com/songhan/Deep-Compression-AlexNet" target="_blank" rel="external">Github: Deep-Compression-AlexNet</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Han Song的Deep Compression是模型压缩方面很重要的论文。在Deep Compression中，作者提出了三个步骤来进行模型压缩：剪枝，量化和霍夫曼编码。其中，剪枝对应的方法就是基于本文要总结的这篇论文：&lt;a href=&quot;https://arxiv.org/abs/1506.02626&quot;&gt;Learning both Weights and Connections for Efficient Neural Networks&lt;/a&gt;。在这篇论文中，作者介绍了如何在不损失精度的前提下，对深度学习的网络模型进行剪枝，从而达到减小模型大小的目的。&lt;br&gt;&lt;img src=&quot;/img/paper-pruning-network-demo.png&quot; alt=&quot;Pruning的主要过程&quot;&gt;&lt;br&gt;
    
    </summary>
    
    
      <category term="paper" scheme="https://xmfbit.github.io/tags/paper/"/>
    
      <category term="deep learning" scheme="https://xmfbit.github.io/tags/deep-learning/"/>
    
      <category term="model compression" scheme="https://xmfbit.github.io/tags/model-compression/"/>
    
  </entry>
  
  <entry>
    <title>Caffe中的Net实现</title>
    <link href="https://xmfbit.github.io/2018/02/28/caffe-net/"/>
    <id>https://xmfbit.github.io/2018/02/28/caffe-net/</id>
    <published>2018-02-28T02:16:43.000Z</published>
    <updated>2018-10-27T07:16:52.379Z</updated>
    
    <content type="html"><![CDATA[<p>Caffe中使用<code>Net</code>实现神经网络，这篇文章对应Caffe代码总结<code>Net</code>的实现。<br><img src="/img/caffe-net-demo.jpg" width="300" height="200" alt="Net示意" align="center"><br><a id="more"></a></p><h2 id="proto中定义的参数"><a href="#proto中定义的参数" class="headerlink" title="proto中定义的参数"></a>proto中定义的参数</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div></pre></td><td class="code"><pre><div class="line">message NetParameter &#123;</div><div class="line">  // net的名字</div><div class="line">  optional string name = 1; // consider giving the network a name</div><div class="line">  // 以下几个都是弃用的参数，为了定义输入的blob（大小）</div><div class="line">  // 下面有使用推荐`InputParameter`进行输入设置的方法</div><div class="line">  // DEPRECATED. See InputParameter. The input blobs to the network.</div><div class="line">  repeated string input = 3;</div><div class="line">  // DEPRECATED. See InputParameter. The shape of the input blobs.</div><div class="line">  repeated BlobShape input_shape = 8;</div><div class="line"></div><div class="line">  // 4D input dimensions -- deprecated.  Use &quot;input_shape&quot; instead.</div><div class="line">  // If specified, for each input blob there should be four</div><div class="line">  // values specifying the num, channels, height and width of the input blob.</div><div class="line">  // Thus, there should be a total of (4 * #input) numbers.</div><div class="line">  repeated int32 input_dim = 4;</div><div class="line">  </div><div class="line">  // Whether the network will force every layer to carry out backward operation.</div><div class="line">  // If set False, then whether to carry out backward is determined</div><div class="line">  // automatically according to the net structure and learning rates.</div><div class="line">  optional bool force_backward = 5 [default = false];</div><div class="line">  // The current &quot;state&quot; of the network, including the phase, level, and stage.</div><div class="line">  // Some layers may be included/excluded depending on this state and the states</div><div class="line">  // specified in the layers&apos; include and exclude fields.</div><div class="line">  optional NetState state = 6;</div><div class="line"></div><div class="line">  // Print debugging information about results while running Net::Forward,</div><div class="line">  // Net::Backward, and Net::Update.</div><div class="line">  optional bool debug_info = 7 [default = false];</div><div class="line"></div><div class="line">  // The layers that make up the net.  Each of their configurations, including</div><div class="line">  // connectivity and behavior, is specified as a LayerParameter.</div><div class="line">  repeated LayerParameter layer = 100;  // ID 100 so layers are printed last.</div><div class="line"></div><div class="line">  // DEPRECATED: use &apos;layer&apos; instead.</div><div class="line">  repeated V1LayerParameter layers = 2;</div><div class="line">&#125;</div></pre></td></tr></table></figure><h3 id="Input的定义"><a href="#Input的定义" class="headerlink" title="Input的定义"></a>Input的定义</h3><p>在<code>train</code>和<code>deploy</code>的时候，输入的定义常常是不同的。在<code>train</code>时，我们需要提供数据$x$和真实值$y$，这样网络的输出$\hat{y} = \mathcal{F}_\theta (x)$与真实值$y$计算损失，bp，更新网络参数$\theta$。</p><p>在<code>deploy</code>时，推荐使用<code>InputLayer</code>定义网络的输入，下面是<code>$CAFFE/models/bvlc_alexnet/deploy.prototxt</code>中的输入定义：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">layer &#123;</div><div class="line">  name: &quot;data&quot;</div><div class="line">  type: &quot;Input&quot;</div><div class="line">  // 该层layer的输出blob名称为data，供后续layer使用</div><div class="line">  top: &quot;data&quot;</div><div class="line">  // 定义输入blob的大小：10 x 3 x 227 x 227</div><div class="line">  // 说明batch size = 10</div><div class="line">  // 输入彩色图像，channel = 3, RGB</div><div class="line">  // 输入image的大小：227 x 227</div><div class="line">  input_param &#123; shape: &#123; dim: 10 dim: 3 dim: 227 dim: 227 &#125; &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p><h2 id="头文件"><a href="#头文件" class="headerlink" title="头文件"></a>头文件</h2><p><code>Net</code>的描述头文件位于<code>$CAFFE/include/caffe/net.hpp</code>中。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Caffe中使用&lt;code&gt;Net&lt;/code&gt;实现神经网络，这篇文章对应Caffe代码总结&lt;code&gt;Net&lt;/code&gt;的实现。&lt;br&gt;&lt;img src=&quot;/img/caffe-net-demo.jpg&quot; width = &quot;300&quot; height = &quot;200&quot; alt=&quot;Net示意&quot; align=center /&gt;&lt;br&gt;
    
    </summary>
    
    
      <category term="caffe" scheme="https://xmfbit.github.io/tags/caffe/"/>
    
  </entry>
  
</feed>
