<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>来呀，快活呀~</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://xmfbit.github.io/"/>
  <updated>2018-03-14T08:25:56.394Z</updated>
  <id>https://xmfbit.github.io/</id>
  
  <author>
    <name>一个脱离了高级趣味的人</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>论文 - Learning both Weights and Connections for Efficient Neural Networks</title>
    <link href="https://xmfbit.github.io/2018/03/14/paper-network-prune-hansong/"/>
    <id>https://xmfbit.github.io/2018/03/14/paper-network-prune-hansong/</id>
    <published>2018-03-14T08:18:53.000Z</published>
    <updated>2018-03-14T08:25:56.394Z</updated>
    
    <content type="html"><![CDATA[<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul><li>HanSong的个人主页：<a href="http://stanford.edu/~songhan/" target="_blank" rel="external">Homepage</a></li><li>HanSong的博士论文：<a href="https://purl.stanford.edu/qf934gh3708" target="_blank" rel="external">Efficient Methods and Hardware for Deep Learning</a></li><li>后续的Deep Compression论文：<a href="https://arxiv.org/abs/1510.00149" target="_blank" rel="external">DEEP COMPRESSION- COMPRESSING DEEP NEURAL NETWORKS WITH PRUNING, TRAINED QUANTIZATION AND HUFFMAN CODING</a></li><li>Deep Compression AlexNet: <a href="https://github.com/songhan/Deep-Compression-AlexNet" target="_blank" rel="external">Deep-Compression-AlexNet</a></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;参考资料&quot;&gt;&lt;a href=&quot;#参考资料&quot; class=&quot;headerlink&quot; title=&quot;参考资料&quot;&gt;&lt;/a&gt;参考资料&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;HanSong的个人主页：&lt;a href=&quot;http://stanford.edu/~songhan/&quot; tar
      
    
    </summary>
    
    
      <category term="deep learning" scheme="https://xmfbit.github.io/tags/deep-learning/"/>
    
      <category term="paper" scheme="https://xmfbit.github.io/tags/paper/"/>
    
  </entry>
  
  <entry>
    <title>Caffe中的Net实现</title>
    <link href="https://xmfbit.github.io/2018/02/28/caffe-net/"/>
    <id>https://xmfbit.github.io/2018/02/28/caffe-net/</id>
    <published>2018-02-28T02:16:43.000Z</published>
    <updated>2018-03-14T06:04:00.123Z</updated>
    
    <content type="html"><![CDATA[<p>Caffe中使用<code>Net</code>实现神经网络，这篇文章对应Caffe代码总结<code>Net</code>的实现。<br><img src="/img/caffe-net-demo.jpg" width="300" height="200" alt="Net示意" align="center"><br><a id="more"></a></p><h2 id="proto中定义的参数"><a href="#proto中定义的参数" class="headerlink" title="proto中定义的参数"></a>proto中定义的参数</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div></pre></td><td class="code"><pre><div class="line">message NetParameter &#123;</div><div class="line">  // net的名字</div><div class="line">  optional string name = 1; // consider giving the network a name</div><div class="line">  // 以下几个都是弃用的参数，为了定义输入的blob（大小）</div><div class="line">  // 下面有使用推荐`InputParameter`进行输入设置的方法</div><div class="line">  // DEPRECATED. See InputParameter. The input blobs to the network.</div><div class="line">  repeated string input = 3;</div><div class="line">  // DEPRECATED. See InputParameter. The shape of the input blobs.</div><div class="line">  repeated BlobShape input_shape = 8;</div><div class="line"></div><div class="line">  // 4D input dimensions -- deprecated.  Use &quot;input_shape&quot; instead.</div><div class="line">  // If specified, for each input blob there should be four</div><div class="line">  // values specifying the num, channels, height and width of the input blob.</div><div class="line">  // Thus, there should be a total of (4 * #input) numbers.</div><div class="line">  repeated int32 input_dim = 4;</div><div class="line">  </div><div class="line">  // Whether the network will force every layer to carry out backward operation.</div><div class="line">  // If set False, then whether to carry out backward is determined</div><div class="line">  // automatically according to the net structure and learning rates.</div><div class="line">  optional bool force_backward = 5 [default = false];</div><div class="line">  // The current &quot;state&quot; of the network, including the phase, level, and stage.</div><div class="line">  // Some layers may be included/excluded depending on this state and the states</div><div class="line">  // specified in the layers&apos; include and exclude fields.</div><div class="line">  optional NetState state = 6;</div><div class="line"></div><div class="line">  // Print debugging information about results while running Net::Forward,</div><div class="line">  // Net::Backward, and Net::Update.</div><div class="line">  optional bool debug_info = 7 [default = false];</div><div class="line"></div><div class="line">  // The layers that make up the net.  Each of their configurations, including</div><div class="line">  // connectivity and behavior, is specified as a LayerParameter.</div><div class="line">  repeated LayerParameter layer = 100;  // ID 100 so layers are printed last.</div><div class="line"></div><div class="line">  // DEPRECATED: use &apos;layer&apos; instead.</div><div class="line">  repeated V1LayerParameter layers = 2;</div><div class="line">&#125;</div></pre></td></tr></table></figure><h3 id="Input的定义"><a href="#Input的定义" class="headerlink" title="Input的定义"></a>Input的定义</h3><p>在<code>train</code>和<code>deploy</code>的时候，输入的定义常常是不同的。在<code>train</code>时，我们需要提供数据$x$和真实值$y$，这样网络的输出$\hat{y} = \mathcal{F}_\theta (x)$与真实值$y$计算损失，bp，更新网络参数$\theta$。</p><p>在<code>deploy</code>时，推荐使用<code>InputLayer</code>定义网络的输入，下面是<code>$CAFFE/models/bvlc_alexnet/deploy.prototxt</code>中的输入定义：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">layer &#123;</div><div class="line">  name: &quot;data&quot;</div><div class="line">  type: &quot;Input&quot;</div><div class="line">  // 该层layer的输出blob名称为data，供后续layer使用</div><div class="line">  top: &quot;data&quot;</div><div class="line">  // 定义输入blob的大小：10 x 3 x 227 x 227</div><div class="line">  // 说明batch size = 10</div><div class="line">  // 输入彩色图像，channel = 3, RGB</div><div class="line">  // 输入image的大小：227 x 227</div><div class="line">  input_param &#123; shape: &#123; dim: 10 dim: 3 dim: 227 dim: 227 &#125; &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p><h2 id="头文件"><a href="#头文件" class="headerlink" title="头文件"></a>头文件</h2><p><code>Net</code>的描述头文件位于<code>$CAFFE/include/caffe/net.hpp</code>中。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Caffe中使用&lt;code&gt;Net&lt;/code&gt;实现神经网络，这篇文章对应Caffe代码总结&lt;code&gt;Net&lt;/code&gt;的实现。&lt;br&gt;&lt;img src=&quot;/img/caffe-net-demo.jpg&quot; width = &quot;300&quot; height = &quot;200&quot; alt=&quot;Net示意&quot; align=center /&gt;&lt;br&gt;
    
    </summary>
    
    
      <category term="caffe" scheme="https://xmfbit.github.io/tags/caffe/"/>
    
  </entry>
  
  <entry>
    <title>捉bug记 - JupyterNotebook中使用pycaffe加载多个模型一直等待的现象</title>
    <link href="https://xmfbit.github.io/2018/02/27/bug-pycaffe-jupyternotebook-awaiting-for-data/"/>
    <id>https://xmfbit.github.io/2018/02/27/bug-pycaffe-jupyternotebook-awaiting-for-data/</id>
    <published>2018-02-27T05:30:25.000Z</published>
    <updated>2018-03-14T06:04:00.121Z</updated>
    
    <content type="html"><![CDATA[<p>JupyteNotebook是个很好的工具，但是在使用pycaffe试图在notebook中同时加载多个caffemodel模型的时候，却出现了无法加载的问题。</p><h2 id="bug重现"><a href="#bug重现" class="headerlink" title="bug重现"></a>bug重现</h2><p>我想在notebook中比较两个使用不同方法训练出来的模型，它们使用了同样的LMDB文件进行训练。加载第一个模型没有问题，但当加载第二个模型时，却一直等待。在StackOverflow上我发现了类似的问题，可以见：<a href="https://stackoverflow.com/questions/37260158/cant-load-2-models-in-pycaffe" target="_blank" rel="external">Can’t load 2 models in pycaffe</a>。</p><h2 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a>解决方法</h2><p>这是由于pycaffe（是否要加上jupyter-notebook？因为不用notebook，以前没有出现过类似问题）不能并发读取同样的LMDB所导致的。但是很遗憾，没有发现太好的解决办法。最后只能是将LMDB重新copy了一份，并修改prototxt文件，使得两个模型分别读取不同的LMDB。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;JupyteNotebook是个很好的工具，但是在使用pycaffe试图在notebook中同时加载多个caffemodel模型的时候，却出现了无法加载的问题。&lt;/p&gt;
&lt;h2 id=&quot;bug重现&quot;&gt;&lt;a href=&quot;#bug重现&quot; class=&quot;headerlink&quot; t
      
    
    </summary>
    
    
      <category term="caffe" scheme="https://xmfbit.github.io/tags/caffe/"/>
    
  </entry>
  
  <entry>
    <title>Caffe中卷积的大致实现思路</title>
    <link href="https://xmfbit.github.io/2018/02/26/conv-in-caffe/"/>
    <id>https://xmfbit.github.io/2018/02/26/conv-in-caffe/</id>
    <published>2018-02-26T07:26:09.000Z</published>
    <updated>2018-03-14T06:04:00.124Z</updated>
    
    <content type="html"><![CDATA[<p>参考资料：知乎：<a href="https://www.zhihu.com/question/28385679" target="_blank" rel="external">在Caffe中如何计算卷积</a>。<br><img src="/img/conv-in-caffe-naive-loop.png" alt="Naive Loop"><br><a id="more"></a></p><p>使用<code>im2col</code>将输入的图像或特征图转换为矩阵，后续就可以使用现成的线代运算优化库，如BLAS中的GEMM，来快速计算。<br><img src="/img/conv-in-caffe-im2col-followed-gemm.png" alt="im2col-&gt;gemm"></p><p>im2col的工作原理如下：每个要和卷积核做卷积的patch被抻成了一个feature vector。不同位置的patch，顺序堆叠起来，<br><img src="/img/conv-in-caffe-im2col-1.png" alt="patches堆起来"></p><p>最后就变成了这样：<br><img src="/img/conv-in-caffe-im2col-2.png" alt="最后的样子"></p><p>同样的，对卷积核也做类似的变换。将单一的卷积核抻成一个行向量，然后把<code>c_out</code>个卷积核顺序排列起来。<br><img src="/img/conv-in-caffe-im2col-3.png" alt="卷积核 to col"></p><p>我们记图像那个矩阵是<code>A</code>，记卷积那个矩阵是<code>F</code>。那么，对于第<code>i</code>个卷积核来说，它现在实际上是<code>F</code>里面的第<code>i</code>个行向量。为了计算它在原来图像上的各个位置的卷积，现在我们需要它和矩阵<code>A</code>中的每行做点积。也就是 <code>F_i * [A_1^T, A_2^T, … A_i^T]</code> （也就是<code>A</code>的转置）。推广到其他的卷积核，就是说，最后的结果是<code>F*A^T</code>.</p><p>我们可以用矩阵维度验证。<code>F</code>的维度是<code>Cout x (C x K x K)</code>. 输入的Feature map matrix的维度是<code>(H x W) x (C x K x K)</code>。那么上述矩阵乘法的结果就是 <code>Cout x (H x W)</code>。正好可以看做输出的三维blob的大小：<code>Cout x H x W</code>。</p><p>这里<a href="https://github.com/Yangqing/caffe/wiki/Convolution-in-Caffe:-a-memo" target="_blank" rel="external">Convolution in Caffe: a memo</a>还有贾扬清对于自己当时在caffe中实现conv的”心路历程“，题图出自此处。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;参考资料：知乎：&lt;a href=&quot;https://www.zhihu.com/question/28385679&quot;&gt;在Caffe中如何计算卷积&lt;/a&gt;。&lt;br&gt;&lt;img src=&quot;/img/conv-in-caffe-naive-loop.png&quot; alt=&quot;Naive Loop&quot;&gt;&lt;br&gt;
    
    </summary>
    
    
      <category term="caffe" scheme="https://xmfbit.github.io/tags/caffe/"/>
    
      <category term="deep learning" scheme="https://xmfbit.github.io/tags/deep-learning/"/>
    
  </entry>
  
  <entry>
    <title>论文 - Learning Structured Sparsity in Deep Neural Networks</title>
    <link href="https://xmfbit.github.io/2018/02/24/paper-ssl-dnn/"/>
    <id>https://xmfbit.github.io/2018/02/24/paper-ssl-dnn/</id>
    <published>2018-02-24T02:21:14.000Z</published>
    <updated>2018-03-14T06:04:00.133Z</updated>
    
    <content type="html"><![CDATA[<p>DNN的稀疏化？用L1正则项不就好了？在很多场合，这种方法的确可行。但是当试图使用FPGA/AISC加速DNN的前向计算时，我们希望DNN的参数能有一些结构化的稀疏性质。这样才能减少不必要的cache missing等问题。在<a href="https://arxiv.org/pdf/1608.03665.pdf" target="_blank" rel="external">这篇文章</a>中，作者提出了一种结构化稀疏的方法，能够在不损失精度的前提下，对深度神经网络进行稀疏化，达到加速的目的。本文作者<a href="http://www.pittnuts.com/" target="_blank" rel="external">温伟</a>，目前是杜克大学Chen Yiran组的博士生，做了很多关于结构化稀疏和DNN加速相关的工作。本文发表在NIPS 2016上。本文的代码已经公开：<a href="https://github.com/wenwei202/caffe/tree/scnn" target="_blank" rel="external">GitHub</a><br><img src="/img/paper-ssldnn.png" alt="SSL的原理示意图"><br><a id="more"></a></p><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>为了满足DNN的计算速度要求，我们提出了Structure Sparisity Learning (SSL)技术来正则化DNN的“结构”（例如CNN filter的参数，filter的形状，包括channel和网络层深）。它可以带来：</p><ul><li>大的DNN —&gt; 紧凑的模型 —&gt; 计算开销节省</li><li>硬件友好的结构化稀疏 —&gt; 便于在专用硬件上加速</li><li>提供了正则化，提高网络泛化能力 —&gt; 提高了精度</li></ul><p>实验结果显示，这种方法可以在CPU/GPU上对AlexNet分别达到平均$5.1$和$3,1$倍的加速。在CIFAR10上训练ResNet，从$20$层减少到$18$层，并提高了精度。</p><h2 id="LASSO"><a href="#LASSO" class="headerlink" title="LASSO"></a>LASSO</h2><p>SSL是基于Group LASSO的，所以正式介绍文章之前，首先简单介绍LASSO和Group LASSO，<br><a href="https://en.wikipedia.org/wiki/Lasso_(statistics" target="_blank" rel="external">LASSO</a>)(least absolute shrinkage and selection operator)是指统计学习中的特征选择方法。以最小二乘法求解线性模型为例，可以加上L1 norm作为正则化约束，见下式，其中$\beta$是模型的参数。具体推导过程可以参见wiki页面。</p><script type="math/tex; mode=display">\min_{\beta \in R^p}\frac{1}{N} \Vert(y-X\beta)\Vert_2^2 + \lambda \Vert \beta \Vert_1</script><p>而Group LASSO就是将参数分组，进行LASSO操作。</p><p>这里只是简单介绍一下LASSO。SSL下面还会详细介绍，不必过多执着于LASSO。</p><h2 id="结构化稀疏"><a href="#结构化稀疏" class="headerlink" title="结构化稀疏"></a>结构化稀疏</h2><p>DNN通常参数很多，计算量很大。为了减少计算开销，目前的研究包括：稀疏化，connection pruning, low rank approximation等。然而前两种方法只能得到随机的稀疏，无规律的内存读取仍然制约了速度。下面这种图是一个例子。我们使用了L1正则进行稀疏。和原模型相比，精度损失了$2$个点。虽然稀疏度比较高，但是实际加速效果却很差。可以看到，在<code>conv3</code>，<code>conv4</code>和<code>conv5</code>中，有的情况下反而加速比是大于$1$的。<br><img src="/img/paper-ssldnn-random-sparity-is-bad.png" alt="随机稀疏的实际加速效果"></p><p>low rank approx利用矩阵分解，将预训练好的模型的参数分解为小矩阵的乘积。这种方法需要较多的迭代次数，同时，网络结构是不能更改的。</p><p>基于下面实验中观察的事实，我们提出了SSL来直接学习结构化稀疏。</p><ul><li>网络中的filter和channel存在冗余</li><li>filter稀疏化为别的形状能够去除不必要的计算</li><li>网络的深度虽然重要，但是并不意味着深层的layer对网络性能一定是好的</li></ul><p>假设第$l$个卷积层的参数是一个$4D$的Tensor，$W^{(l)}\in R^{N_l \times C_l \times M_l \times N_l}$，那么SSL方法可以表示为优化下面这个损失函数：</p><script type="math/tex; mode=display">E(W)=E_D{W} + \lambda R(W) + \lambda_g \sum_{l=1}^{L}R_g(W^{(l)})</script><p>这里，$W$代表DNN中所有权重的集合。$E_D(W)$代表在训练集上的loss。$R$是非结构化的正则项，例如L2 norm。$R_g$是指结构化稀疏的正则项，注意是逐层计算的。对于每一层来说（也就是上述最后一项求和的每一项），group LASSO可以表示为：</p><script type="math/tex; mode=display">R_g(w) = \sum_{g=1}^{G}\Vert w^{(g)} \Vert_g</script><p>其中，$w^{(g)}$是该层权重$W^{(l)}$的一部分，不同的分组可以重叠。$G$是分组的组数。$\Vert \cdot \Vert_g$指的是group LASSO，这里使用的是$\Vert w^{(g)}\Vert_g = \sqrt{\sum_{i=1}^{|w^{(g)}|}(w_i^{(g)})^2}$，也就是$2$范数。</p><h2 id="SSL"><a href="#SSL" class="headerlink" title="SSL"></a>SSL</h2><p>有了上面的损失函数，SSL就取决于如何对weight进行分组。对不同的分组情况分类讨论如下。图示见博客开头的题图。</p><h3 id="惩罚不重要的filter和channel"><a href="#惩罚不重要的filter和channel" class="headerlink" title="惩罚不重要的filter和channel"></a>惩罚不重要的filter和channel</h3><p>假设$W^{(l)}_{n_l,:,:,:}$是第$n$个filter，$W^{(l)}_{:, c_l, :,:}$是所有weight的第$c$个channel。可以通过下面的约束来去除相对不重要的filter和channel。注意，如果第$l$层的weight中某个filter变成了$0$，那么输出的feature map中就有一个全$0$，所以filter和channel的结构化稀疏要放到一起。下面是这种形式下的损失函数。为了简单，后面的讨论中都略去了正常的正则化项$R(W)$。</p><script type="math/tex; mode=display">E(W) = E_D(W) + \lambda_n \sum_{l=1}^{L}(\sum_{n_l=1}^{N_l}\Vert W^{(l)}_{n_l,:,:,:}\Vert_g) + \lambda_c\sum_{l=1}^{L}(\sum_{cl=1}^{C_l}\Vert W^{(l)}_{:,c_l,:,:}\Vert_g)</script><h3 id="任意形状的filter"><a href="#任意形状的filter" class="headerlink" title="任意形状的filter"></a>任意形状的filter</h3><p>所谓任意形状的filter，就是将filter中的一些权重置为$0$。可以使用下面的分组方法：</p><script type="math/tex; mode=display">E(W) = E_D(W) + \lambda_s \sum_{l=1}^{L}(\sum_{c_l=1}^{C_l}\sum_{m_l=1}^{M_l}\sum_{k_l=1}^{K_l})\Vert W^{(l)}_{:,c_l,m_l,k_l} \Vert_g</script><h3 id="网络深度"><a href="#网络深度" class="headerlink" title="网络深度"></a>网络深度</h3><p>损失函数如下：</p><script type="math/tex; mode=display">E(W) = E_D(W) + \lambda_d \sum_{l=1}^{L}\Vert W^{(l)}\Vert_g</script><p>不过要注意的是，某个layer被稀疏掉了，会切断信息的流通。所以受ResNet启发，加上了short-cut结构。即使SSL移去了该layer所有的filter，上层的feature map仍然可以传导到后面。</p><h3 id="两类特殊的稀疏规则"><a href="#两类特殊的稀疏规则" class="headerlink" title="两类特殊的稀疏规则"></a>两类特殊的稀疏规则</h3><p>特意提出下面两种稀疏规则，下面的实验即是基于这两种特殊的稀疏结构。</p><h4 id="2D-filter-sparsity"><a href="#2D-filter-sparsity" class="headerlink" title="2D filter sparsity"></a>2D filter sparsity</h4><p>卷积层中的3D卷积可以看做是2D卷积的组合（做卷积的时候spatial和channel是不相交的）。这种结构化稀疏是将该卷积层中的每个2D的filter，$W^{(l)}_{n_l,c_l,:,:}$，看做一个group，做group LASSO。这相当于是上述filter-wise和channel-wise的组合。</p><h4 id="filter-wise和shape-wise的组合加速GEMM"><a href="#filter-wise和shape-wise的组合加速GEMM" class="headerlink" title="filter-wise和shape-wise的组合加速GEMM"></a>filter-wise和shape-wise的组合加速GEMM</h4><p>在Caffe中，3D的权重tensor是reshape成了一个行向量，然后$N<em>l$个filter的行向量堆叠在一起，就成了一个2D的矩阵。这个矩阵的每一列对应的是$W^{(l)}</em>{:,c_l,m_l,k_l}$，称为shape sparsity。两者组合，矩阵的零行和零列可以被抽去，相当于GEMM的矩阵行列数少了，起到了加速的效果。</p><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p>分别在MNIST，CIFAR10和ImageNet上做了实验，使用公开的模型做baseline，并以此为基础使用SSL训练。</p><h3 id="LeNet-amp-MLP-MNIST"><a href="#LeNet-amp-MLP-MNIST" class="headerlink" title="LeNet&amp;MLP@MNIST"></a>LeNet&amp;MLP@MNIST</h3><p>分别使用Caffe中实现的LeNet和MLP做实验。</p><h4 id="LeNet"><a href="#LeNet" class="headerlink" title="LeNet"></a>LeNet</h4><p>限制SSL为filter-wise和channel-wise稀疏化，来惩罚不重要的filter。下表中，LeNet-1是baseline，2和3是使用不同强度得到的稀疏化结果。可以看到，精度基本没有损失($0.1%$)，但是filter和channel数量都有了较大减少，FLOP大大减少，加速效果比较明显。<br><img src="/img/paper-ssldnn-lenet-penalizing-unimportant-filter-channel.png" alt="实验结果1"></p><p>将网络<code>conv1</code>的filter可视化如下。可以看到，对于LeNet2来说，大多数filter都被稀疏掉了。<br><img src="/img/paper-ssldnn-experiment-on-lenet.png" alt="LeNet的实验结果"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;DNN的稀疏化？用L1正则项不就好了？在很多场合，这种方法的确可行。但是当试图使用FPGA/AISC加速DNN的前向计算时，我们希望DNN的参数能有一些结构化的稀疏性质。这样才能减少不必要的cache missing等问题。在&lt;a href=&quot;https://arxiv.org/pdf/1608.03665.pdf&quot;&gt;这篇文章&lt;/a&gt;中，作者提出了一种结构化稀疏的方法，能够在不损失精度的前提下，对深度神经网络进行稀疏化，达到加速的目的。本文作者&lt;a href=&quot;http://www.pittnuts.com/&quot;&gt;温伟&lt;/a&gt;，目前是杜克大学Chen Yiran组的博士生，做了很多关于结构化稀疏和DNN加速相关的工作。本文发表在NIPS 2016上。本文的代码已经公开：&lt;a href=&quot;https://github.com/wenwei202/caffe/tree/scnn&quot;&gt;GitHub&lt;/a&gt;&lt;br&gt;&lt;img src=&quot;/img/paper-ssldnn.png&quot; alt=&quot;SSL的原理示意图&quot;&gt;&lt;br&gt;
    
    </summary>
    
    
      <category term="deep learning" scheme="https://xmfbit.github.io/tags/deep-learning/"/>
    
      <category term="paper" scheme="https://xmfbit.github.io/tags/paper/"/>
    
  </entry>
  
  <entry>
    <title>捉bug记 - Cannot create Cublas handle. Cublas won&#39;t be available.</title>
    <link href="https://xmfbit.github.io/2018/02/08/bug-pycaffe-using-cublas/"/>
    <id>https://xmfbit.github.io/2018/02/08/bug-pycaffe-using-cublas/</id>
    <published>2018-02-08T06:16:53.000Z</published>
    <updated>2018-03-14T06:04:00.121Z</updated>
    
    <content type="html"><![CDATA[<p>这两天在使用Caffe的时候出现了一个奇怪的bug。当使用C++接口时，完全没有问题；但是当使用python接口时，会出现错误提示如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">common.cpp:114] Cannot create Cublas handle. Cublas won&apos;t be available.</div><div class="line">common.cpp:121] Cannot create Curand generator. Curand won&apos;t be available.</div></pre></td></tr></table></figure></p><a id="more"></a><p>令人疑惑的是，这个python脚本我前段时间已经用过几次了，却没有这样的问题。</p><p>如果在Google上搜索这个问题，很多讨论都是把锅推给了驱动，不过我使用的这台服务器并没有更新过驱动或系统。本来想要试试重启大法，但是上面还有其他人在跑的任务，所以重启不太现实。</p><p>最后找到了这个issue: <a href="https://github.com/BVLC/caffe/issues/440" target="_blank" rel="external">Cannot use Caffe on another GPU when GPU 0 has full memory</a>。联想到我目前使用的服务器上GPU０也正是在跑着一项很吃显存的任务（如下所示），所以赶紧试了一下里面@longjon的方法。<br><img src="/img/bug_pycaffe_nvidia_smi_result.png" alt="nvidia-smi给出的显卡使用信息"></p><p>使用<code>CUDA_VISIBLE_DEVICES</code>变量，指定Caffe能看到的显卡设备。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">CUDA_VISIBLE_DEVICES=2 python my_script.py --gpu_id=0</div></pre></td></tr></table></figure></p><p>果然就可以了！</p><p>这个问题应该出在pycaffe的初始化上。这里不再深究。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这两天在使用Caffe的时候出现了一个奇怪的bug。当使用C++接口时，完全没有问题；但是当使用python接口时，会出现错误提示如下：&lt;br&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;common.cpp:114] Cannot create Cublas handle. Cublas won&amp;apos;t be available.&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;common.cpp:121] Cannot create Curand generator. Curand won&amp;apos;t be available.&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="caffe" scheme="https://xmfbit.github.io/tags/caffe/"/>
    
  </entry>
  
  <entry>
    <title>论文 - Visualizing and Understanding ConvNet</title>
    <link href="https://xmfbit.github.io/2018/02/08/paper-visualize-convnet/"/>
    <id>https://xmfbit.github.io/2018/02/08/paper-visualize-convnet/</id>
    <published>2018-02-08T02:48:21.000Z</published>
    <updated>2018-03-14T06:04:00.133Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://arxiv.org/pdf/1311.2901.pdf" target="_blank" rel="external">Visualizing &amp; Understanding ConvNet</a>这篇文章是比较早期关于CNN调试的文章，作者利用可视化方法，设计了一个超过AlexNet性能的网络结构。</p><p><img src="/img/paper_visconvnet_demo.png" alt="可视化结果"><br><a id="more"></a></p><h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><p>继AlexNet之后，CNN在ImageNet竞赛中得到了广泛应用。AlextNet成功的原因包括以下三点：</p><ul><li>Large data。</li><li>硬件GPU性能。</li><li>一些技巧提升了模型的泛化能力，如Dropout技术。</li></ul><p>不过CNN仍然像一只黑盒子，缺少可解释性。这使得对CNN的调试变得比较困难。我们提出了一种思路，可以找出究竟input中的什么东西对应了激活后的Feature map。</p><p>(对于神经网络的可解释性，可以从基础理论入手，也可以从实践中的经验入手。本文作者实际上就是在探索如何能够更好得使用经验对CNN进行调试。这种方法仍然没有触及到CNN本质的可解释性的东西，不过仍然在工程实践中有很大的意义，相当于将黑盒子变成了灰盒子。从人工取火到炼金术到现代化学，也不是这么一个过程吗？)</p><p>在AlexNet中，每个卷积单元常常由以下几个部分组成：</p><ul><li>卷积层，使用一组学习到的$3D$滤波器与输入（上一层的输出或网络输入的数据）做卷积操作。</li><li>非线性激活层，通常使用<code>ReLU(x) = max(0, x)</code>。</li><li>可选的，池化层，缩小Feature map的尺寸。</li><li>可选的，LRN层（现在已经基本不使用）。</li></ul><h2 id="DeconvNet"><a href="#DeconvNet" class="headerlink" title="DeconvNet"></a>DeconvNet</h2><p>我们使用DeconvNet这项技术，寻找与输出的激活对应的输入模式。这样，我们可以看到，输入中的哪个部分被神经元捕获，产生了较强的激活。</p><p>如图所示，展示了DeconvNet是如何构造的。<br><img src="/img/paer_visconvnet_deconvnet_structure.png" alt="DeconvNet的构造"></p><p>首先，图像被送入卷积网络中，得到输出的feature map。对于输出的某个激活，我们可以将其他激活值置成全$0$，然后顺着deconvNet计算，得到与之对应的输入。具体来说，我们需要对三种不同的layer进行反向操作。</p><h3 id="Uppooling"><a href="#Uppooling" class="headerlink" title="Uppooling"></a>Uppooling</h3><p>在CNN中，max pooling操作是不可逆的（信息丢掉了）。我们可以使用近似操作：记录最大值的位置；在deconvNet中，保留该标记位置处的激活值。如下图所示。右侧为CNN中的max pooling操作。中间switches显示的是最大值的位置（用灰色标出）。在左侧的deconvNet中，激活值对应给到相应的灰色位置。这个操作被称为Uppooing。<br><img src="/img/paper_visconvnet_uppooling.png" alt="Uppooling示意图"></p><h3 id="Rectification"><a href="#Rectification" class="headerlink" title="Rectification"></a>Rectification</h3><p>在CNN中，一般使用relu作为非线性激活。deconvNet中也做同样的处理。</p><h3 id="Filtering"><a href="#Filtering" class="headerlink" title="Filtering"></a>Filtering</h3><p>在CNN中，一组待学习的filter用来与输入的feature map做卷积。得到输出。在deconvNet中，使用deconv操作，输入是整流之后的feature map。</p><p>对于最终输出的activation中的每个值，经过deconv的作用，最终会对应到输入pixel space上的一小块区域，显示了它们对最终输出的贡献。</p><h2 id="CNN的可视化"><a href="#CNN的可视化" class="headerlink" title="CNN的可视化"></a>CNN的可视化</h2><p>要想可视化，先要有训练好的CNN模型。这里用作可视化的模型基于AlexNet，但是去掉了group。另外，为了可视化效果，将layer $1$的filter size从$11\times 11$变成$7\times 7$，步长变成$2$。具体训练过程不再详述。</p><p>训练完之后，我们将ImageNet的validation数据集送入到网络中进行前向计算，</p><p>如下所示，是layer $1$的可视化结果。可以看到，右下方的可视化结果被分成了$9\times 9$的方格，每个方格内又细分成了$9\times 9$的小格子。其中，大格子对应的是$9$个filter，小格子对应的是top 9的激活利用deconvNet反算回去对应的image patch、因为layer 1的filter个数正好也是$9$，所以可能稍显迷惑。<br><img src="/img/paper_visconvnet_layer1_demo.png" alt="layer 1的可视化"></p><h2 id="附录"><a href="#附录" class="headerlink" title="附录"></a>附录</h2><p>这里是关于CNN可视化的一些额外资料：</p><ul><li>Zeiler关于本文的talk：<a href="https://www.youtube.com/watch?v=ghEmQSxT6tw" target="_blank" rel="external">Visualizing and Understanding Deep Neural Networks by Matt Zeiler</a></li><li>斯坦福CS231课程的讲义：<a href="http://cs231n.github.io/understanding-cnn/" target="_blank" rel="external">Visualizing what ConvNets learn</a></li><li>ICML 2015上的另一篇CNN可视化的paper：<a href="https://arxiv.org/pdf/1506.06579.pdf" target="_blank" rel="external">Understanding Neural Networks Through Deep Visualization</a>以及他们的开源工具：<a href="https://github.com/yosinski/deep-visualization-toolbox" target="_blank" rel="external">deep-visualization-toolbox</a></li><li>一篇知乎专栏的文章：<a href="https://zhuanlan.zhihu.com/p/24833574" target="_blank" rel="external">Deep Visualization:可视化并理解CNN</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1311.2901.pdf&quot;&gt;Visualizing &amp;amp; Understanding ConvNet&lt;/a&gt;这篇文章是比较早期关于CNN调试的文章，作者利用可视化方法，设计了一个超过AlexNet性能的网络结构。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/img/paper_visconvnet_demo.png&quot; alt=&quot;可视化结果&quot;&gt;&lt;br&gt;
    
    </summary>
    
    
      <category term="deep learning" scheme="https://xmfbit.github.io/tags/deep-learning/"/>
    
      <category term="paper" scheme="https://xmfbit.github.io/tags/paper/"/>
    
  </entry>
  
  <entry>
    <title>Incremental Network Quantization 论文阅读</title>
    <link href="https://xmfbit.github.io/2018/01/25/inq-paper/"/>
    <id>https://xmfbit.github.io/2018/01/25/inq-paper/</id>
    <published>2018-01-25T07:30:28.000Z</published>
    <updated>2018-03-14T06:04:00.131Z</updated>
    
    <content type="html"><![CDATA[<p>卷积神经网络虽然已经在很多任务上取得了很棒的效果，但是模型大小和运算量限制了它们在移动设备和嵌入式设备上的使用。模型量化压缩等问题自然引起了大家的关注。<a href="https://arxiv.org/abs/1702.03044" target="_blank" rel="external">Incremental Network Quantization</a>这篇文章关注的是如何使用更少的比特数进行模型参数量化以达到压缩模型，减少模型大小同时使得模型精度无损。如下图所示，使用5bit位数，INQ在多个模型上都取得了不逊于原始FP32模型的精度。实验结果还是很有说服力的。作者将其开源在了GitHub上，见<a href="https://github.com/Zhouaojun/Incremental-Network-Quantization" target="_blank" rel="external">Incremental-Network-Quantization</a>。<br><img src="/img/paper-inq-result.png" alt="实验结果"><br><a id="more"></a></p><h2 id="量化方法"><a href="#量化方法" class="headerlink" title="量化方法"></a>量化方法</h2><p>INQ论文中，作者采用的量化方法是将权重量化为$2$的幂次或$0$。具体来说，是将权重$W_l$（表示第$l$层的参数权重）舍入到下面这个有限集合中的元素（在下面的讨论中，我们认为$n_1 &gt; n_2$）：<br><img src="/img/paper-inq-quantize-set.png" alt="权重集合"></p><p>假设用$b$bit表示权重，我们分出$1$位单独表示$0$。</p><p>PS：这里插一句。关于为什么要单独分出$1$位表示$0$，毕竟这样浪费了($2^b$ vs $2^{b-1}+1$)。GitHub上有人发<a href="https://github.com/Zhouaojun/Incremental-Network-Quantization/issues/12" target="_blank" rel="external">issue</a>问，作者也没有正面回复这样做的原因。以我的理解，是方便判定$0$和移位。因为作者将权重都舍入到了$2$的幂次，那肯定是为了后续将乘法变成移位操作。而使用剩下的$b-1$表示，可以方便地读出移位的位数，进行操作。</p><p>这样，剩下的$b-1$位用来表示$2$的幂次。我们需要决定$n_1$和$n_2$。因为它俩决定了表示范围。它们之间的关系为：</p><script type="math/tex; mode=display">(n_1-n_2 + 1) \times 2 = 2^{b-1}</script><p>其中，乘以$2$是考虑到正负对称的表示范围。</p><p>如何确定$n_1$呢（由上式可知，有了$b$和$n_1$，$n_2$就确定了）。作者考虑了待量化权重中的最大值，我们需要设置$n_1$，使其刚好不溢出。所以有：</p><script type="math/tex; mode=display">n_1 = \lfloor \log_2(4s/3) \rfloor</script><p>其中，$s$是权重当中绝对值最大的那个，即$s = \max \vert W_l\vert$。</p><p>之后做最近舍入就可以了。对于小于最小分辨力$2^{n_2}$的那些权重，将其直接截断为$0$。</p><h2 id="训练方法"><a href="#训练方法" class="headerlink" title="训练方法"></a>训练方法</h2><p>量化完成后，网络的精度必然会下降。我们需要对其进行调整，使其精度能够恢复原始模型的水平。为此，作者提出了三个主要步骤，迭代地进行。即 weight partition（权重划分）, group-wise quantization（分组量化） 和re-training（训练）。</p><p>re-training好理解，就是量化之后要继续做finetuning。前面两个名词解释如下：weight partition是指我们不是对整个权重一股脑地做量化，而是将其划分为两个不相交的集合。group-wise quantization是指对其中一个集合中的权重做量化，另一组集合中的权重不变，仍然为FP32。注意，在re-training中，我们只对没有量化的那组参数做参数更新。下面是论文中的表述。</p><blockquote><p>Weight partition is to divide the weights in each layer of a pre-trained full-precision CNN model into two disjoint groups which play comple- mentary roles in our INQ. The weights in the first group are responsible for forming a low-precision base for the original model, thus they are quantized by using Equation (4). The weights in the second group adapt to compensate for the loss in model accuracy, thus they are the ones to be re-trained.</p></blockquote><p>训练步骤可以用下图来表示。在第一个迭代中，将所有的权重划分为黑色和白色两个部分（图$1$）。黑色部分的权重进行量化，白色部分不变（图$2$）。然后，使用SGD更新那些白色部分的权重（图$3$）。在第二次迭代中，我们扩大量化权重的范围，重复进行迭代$1$中的操作。在后面的迭代中，以此类推，只不过要不断调大量化权重的比例，最终使得所有权重都量化为止。<br><img src="/img/paper-inq-algorithm-demo.png" alt="训练图解"></p><h3 id="pruning-inspired-strategy"><a href="#pruning-inspired-strategy" class="headerlink" title="pruning-inspired strategy"></a>pruning-inspired strategy</h3><p>在权重划分步骤，作者指出，随机地将权重量化，不如根据权重的幅值，优先量化那些绝对值比较大的权重。比较结果见下图。<br><img src="/img/paper-inq-different-quantize.png" alt="两种量化方法的比较"></p><p>在代码部分，INQ基于Caffe框架，主要修改的地方集中于<code>blob.cpp</code>和<code>sgd_solver.cpp</code>中。量化部分的代码如下，首先根据要划分的比例计算出两个集合分界点处的权重大小。然后将大于该值的权重进行量化，小于该值的权重保持不变。下面的代码其实有点小问题，<code>data_copy</code>使用完之后没有释放。关于代码中<code>mask</code>的作用，下文介绍。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// blob.cpp</span></div><div class="line"><span class="comment">// INQ  </span></div><div class="line"><span class="keyword">if</span>(is_quantization)</div><div class="line">&#123;</div><div class="line">  Dtype* data_copy=(Dtype*) <span class="built_in">malloc</span>(count_*<span class="keyword">sizeof</span>(Dtype));</div><div class="line">  caffe_copy(count_,data_vec,data_copy);</div><div class="line">  caffe_abs(count_,data_copy,data_copy);</div><div class="line">  <span class="built_in">std</span>::sort(data_copy,data_copy+count_); <span class="comment">//data_copy order from small to large</span></div><div class="line">  </div><div class="line">  <span class="comment">//caculate the n1</span></div><div class="line">  Dtype max_data=data_copy[count_<span class="number">-1</span>];</div><div class="line">  <span class="keyword">int</span> n1=(<span class="keyword">int</span>)<span class="built_in">floor</span>(log2(max_data*<span class="number">4.0</span>/<span class="number">3.0</span>));</div><div class="line">  </div><div class="line">  <span class="comment">//quantizate the top 30% of each layer, change the "partition" until partition=0</span></div><div class="line">  <span class="keyword">int</span> partition=<span class="keyword">int</span>(count_*<span class="number">0.7</span>)<span class="number">-1</span>;</div><div class="line"></div><div class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; (count_); ++i) &#123;</div><div class="line">  </div><div class="line">    <span class="keyword">if</span>(<span class="built_in">std</span>::<span class="built_in">abs</span>(data_vec[i])&gt;=data_copy[partition])</div><div class="line">      &#123;</div><div class="line">        data_vec[i] = weightCluster_zero(data_vec[i],n1);</div><div class="line"> </div><div class="line">        mask_vec[i]=<span class="number">0</span>;</div><div class="line">      &#125;</div><div class="line">  &#125;</div></pre></td></tr></table></figure><h3 id="参数更新"><a href="#参数更新" class="headerlink" title="参数更新"></a>参数更新</h3><p>在re-training中，我们只对未量化的那些参数进行更新。待更新的参数，<code>mask</code>中的值都是$1$，这样和<code>diff</code>相乘仍然不变；不更新的参数，<code>mask</code>中的值都是$0$，和<code>diff</code>乘起来，相当于强制把梯度变成了$0$。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">// sgd_solver.cpp</div><div class="line">caffe_gpu_mul(net_params[param_id]-&gt;count(),net_params[param_id]-&gt;gpu_mask(),net_params[param_id]-&gt;mutable_gpu_diff(),net_params[param_id]-&gt;mutable_gpu_diff());</div></pre></td></tr></table></figure><h2 id="结语"><a href="#结语" class="headerlink" title="结语"></a>结语</h2><p>论文中还有一些其他的小细节，这里不再多说。本文的作者还维护了一个关于模型量化压缩相关的<a href="https://github.com/Zhouaojun/Efficient-Deep-Learning" target="_blank" rel="external">repo</a>，也可以作为参考。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;卷积神经网络虽然已经在很多任务上取得了很棒的效果，但是模型大小和运算量限制了它们在移动设备和嵌入式设备上的使用。模型量化压缩等问题自然引起了大家的关注。&lt;a href=&quot;https://arxiv.org/abs/1702.03044&quot;&gt;Incremental Network Quantization&lt;/a&gt;这篇文章关注的是如何使用更少的比特数进行模型参数量化以达到压缩模型，减少模型大小同时使得模型精度无损。如下图所示，使用5bit位数，INQ在多个模型上都取得了不逊于原始FP32模型的精度。实验结果还是很有说服力的。作者将其开源在了GitHub上，见&lt;a href=&quot;https://github.com/Zhouaojun/Incremental-Network-Quantization&quot;&gt;Incremental-Network-Quantization&lt;/a&gt;。&lt;br&gt;&lt;img src=&quot;/img/paper-inq-result.png&quot; alt=&quot;实验结果&quot;&gt;&lt;br&gt;
    
    </summary>
    
    
      <category term="paper" scheme="https://xmfbit.github.io/tags/paper/"/>
    
      <category term="quantize" scheme="https://xmfbit.github.io/tags/quantize/"/>
    
  </entry>
  
  <entry>
    <title>Caffe 中的 SyncedMem介绍</title>
    <link href="https://xmfbit.github.io/2018/01/12/caffe-syncedmem/"/>
    <id>https://xmfbit.github.io/2018/01/12/caffe-syncedmem/</id>
    <published>2018-01-12T06:05:59.000Z</published>
    <updated>2018-03-14T06:04:00.123Z</updated>
    
    <content type="html"><![CDATA[<p><code>Blob</code>是Caffe中的基本数据结构，类似于TensorFlow和PyTorch中的Tensor。图像读入后作为<code>Blob</code>，开始在各个<code>Layer</code>之间传递，最终得到输出。下面这张图展示了<code>Blob</code>和<code>Layer</code>之间的关系：<br> <img src="/img/caffe_syncedmem_blob_flow.jpg" width="300" height="200" alt="blob的流动" align="center"></p><p>Caffe中的<code>Blob</code>在实现的时候，使用了<code>SyncedMem</code>管理内存，并在内存（Host）和显存（device）之间同步。这篇博客对Caffe中<code>SyncedMem</code>的实现做一总结。<br><a id="more"></a></p><h2 id="SyncedMem的作用"><a href="#SyncedMem的作用" class="headerlink" title="SyncedMem的作用"></a>SyncedMem的作用</h2><p><code>Blob</code>是一个多维的数组，可以位于内存，也可以位于显存（当使用GPU时）。一方面，我们需要对底层的内存进行管理，包括何何时开辟内存空间。另一方面，我们的训练数据常常是首先由硬盘读取到内存中，而训练又经常使用GPU，最终结果的保存或可视化又要求数据重新传回内存，所以涉及到Host和Device内存的同步问题。</p><h2 id="同步的实现思路"><a href="#同步的实现思路" class="headerlink" title="同步的实现思路"></a>同步的实现思路</h2><p>在<code>SyncedMem</code>的实现代码中，作者使用一个枚举量<code>head_</code>来标记当前的状态。如下所示：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// in SyncedMem</span></div><div class="line"><span class="keyword">enum</span> SyncedHead &#123; UNINITIALIZED, HEAD_AT_CPU, HEAD_AT_GPU, SYNCED &#125;;</div><div class="line"><span class="comment">// 使用过Git吗？ 在Git中那个标志着repo最新版本状态的变量就叫 HEAD</span></div><div class="line"><span class="comment">// 这里也是一样，标志着最新的数据位于哪里</span></div><div class="line">SyncedHead head_;</div></pre></td></tr></table></figure><p>这样，利用<code>head_</code>变量，就可以构建一个状态转移图，在不同状态切换时进行必要的同步操作等。<br><img src="/img/caffe_syncedmem_transfer.png" alt="状态转换图"></p><h2 id="具体实现"><a href="#具体实现" class="headerlink" title="具体实现"></a>具体实现</h2><p><code>SyncedMem</code>的类声明如下：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line"> * @brief Manages memory allocation and synchronization between the host (CPU)</div><div class="line"> *        and device (GPU).</div><div class="line"> *</div><div class="line"> * TODO(dox): more thorough description.</div><div class="line"> */</div><div class="line"><span class="keyword">class</span> SyncedMemory &#123;</div><div class="line"> <span class="keyword">public</span>:</div><div class="line">  SyncedMemory();</div><div class="line">  <span class="function"><span class="keyword">explicit</span> <span class="title">SyncedMemory</span><span class="params">(<span class="keyword">size_t</span> size)</span></span>;</div><div class="line">  ~SyncedMemory();</div><div class="line">  <span class="comment">// 获取CPU data指针</span></div><div class="line">  <span class="function"><span class="keyword">const</span> <span class="keyword">void</span>* <span class="title">cpu_data</span><span class="params">()</span></span>;</div><div class="line">  <span class="comment">// 设置CPU data指针</span></div><div class="line">  <span class="function"><span class="keyword">void</span> <span class="title">set_cpu_data</span><span class="params">(<span class="keyword">void</span>* data)</span></span>;</div><div class="line">  <span class="comment">// 获取GPU data指针</span></div><div class="line">  <span class="function"><span class="keyword">const</span> <span class="keyword">void</span>* <span class="title">gpu_data</span><span class="params">()</span></span>;</div><div class="line">  <span class="comment">// 设置GPU data指针</span></div><div class="line">  <span class="function"><span class="keyword">void</span> <span class="title">set_gpu_data</span><span class="params">(<span class="keyword">void</span>* data)</span></span>;</div><div class="line">  <span class="comment">// 获取CPU data指针，并在后续将改变指针所指向内存的值</span></div><div class="line">  <span class="function"><span class="keyword">void</span>* <span class="title">mutable_cpu_data</span><span class="params">()</span></span>;</div><div class="line">  <span class="comment">// 获取GPU data指针，并在后续将改变指针所指向内存的值</span></div><div class="line">  <span class="function"><span class="keyword">void</span>* <span class="title">mutable_gpu_data</span><span class="params">()</span></span>;</div><div class="line">  <span class="comment">// CPU 和 GPU的同步状态：未初始化，在CPU（未同步），在GPU（未同步），已同步</span></div><div class="line">  <span class="keyword">enum</span> SyncedHead &#123; UNINITIALIZED, HEAD_AT_CPU, HEAD_AT_GPU, SYNCED &#125;;</div><div class="line">  <span class="function">SyncedHead <span class="title">head</span><span class="params">()</span> </span>&#123; <span class="keyword">return</span> head_; &#125;</div><div class="line">  <span class="comment">// 内存大小</span></div><div class="line">  <span class="keyword">size_t</span> size() &#123; <span class="keyword">return</span> size_; &#125;</div><div class="line"></div><div class="line"><span class="meta">#<span class="meta-keyword">ifndef</span> CPU_ONLY</span></div><div class="line">  <span class="function"><span class="keyword">void</span> <span class="title">async_gpu_push</span><span class="params">(<span class="keyword">const</span> cudaStream_t&amp; stream)</span></span>;</div><div class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></div><div class="line"></div><div class="line"> <span class="keyword">private</span>:</div><div class="line">  <span class="function"><span class="keyword">void</span> <span class="title">check_device</span><span class="params">()</span></span>;</div><div class="line"></div><div class="line">  <span class="function"><span class="keyword">void</span> <span class="title">to_cpu</span><span class="params">()</span></span>;</div><div class="line">  <span class="function"><span class="keyword">void</span> <span class="title">to_gpu</span><span class="params">()</span></span>;</div><div class="line">  <span class="keyword">void</span>* cpu_ptr_;</div><div class="line">  <span class="keyword">void</span>* gpu_ptr_;</div><div class="line">  <span class="keyword">size_t</span> size_;</div><div class="line">  SyncedHead head_;</div><div class="line">  <span class="keyword">bool</span> own_cpu_data_;</div><div class="line">  <span class="keyword">bool</span> cpu_malloc_use_cuda_;</div><div class="line">  <span class="keyword">bool</span> own_gpu_data_;</div><div class="line">  <span class="comment">// GPU设备编号</span></div><div class="line">  <span class="keyword">int</span> device_;</div><div class="line"></div><div class="line">  DISABLE_COPY_AND_ASSIGN(SyncedMemory);</div><div class="line">&#125;;  <span class="comment">// class SyncedMemory</span></div></pre></td></tr></table></figure><p>我们以<code>to_cpu()</code>为例，看一下如何在不同状态之间切换。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">inline</span> <span class="keyword">void</span> SyncedMemory::to_gpu() &#123;</div><div class="line">  <span class="comment">// 检查设备状态（使用条件编译，只在DEBUG中使能）</span></div><div class="line">  check_device();</div><div class="line"><span class="meta">#<span class="meta-keyword">ifndef</span> CPU_ONLY</span></div><div class="line">  <span class="keyword">switch</span> (head_) &#123;</div><div class="line">  <span class="keyword">case</span> UNINITIALIZED:</div><div class="line">    <span class="comment">// 还没有初始化呢~所以内存啥的还没开</span></div><div class="line">    <span class="comment">// 先在GPU上开块显存吧~</span></div><div class="line">    CUDA_CHECK(cudaMalloc(&amp;gpu_ptr_, size_));</div><div class="line">    caffe_gpu_memset(size_, <span class="number">0</span>, gpu_ptr_);</div><div class="line">    <span class="comment">// 接着，改变状态标志</span></div><div class="line">    head_ = HEAD_AT_GPU;</div><div class="line">    own_gpu_data_ = <span class="literal">true</span>;</div><div class="line">    <span class="keyword">break</span>;</div><div class="line">  <span class="keyword">case</span> HEAD_AT_CPU:</div><div class="line">    <span class="comment">// 数据在CPU上~如果需要，先在显存上开内存</span></div><div class="line">    <span class="keyword">if</span> (gpu_ptr_ == <span class="literal">NULL</span>) &#123;</div><div class="line">      CUDA_CHECK(cudaMalloc(&amp;gpu_ptr_, size_));</div><div class="line">      own_gpu_data_ = <span class="literal">true</span>;</div><div class="line">    &#125;</div><div class="line">    <span class="comment">// 数据拷贝</span></div><div class="line">    caffe_gpu_memcpy(size_, cpu_ptr_, gpu_ptr_);</div><div class="line">    <span class="comment">// 改变状态变量</span></div><div class="line">    head_ = SYNCED;</div><div class="line">    <span class="keyword">break</span>;</div><div class="line">  <span class="comment">// 已经在GPU或者已经同步了，什么都不做</span></div><div class="line">  <span class="keyword">case</span> HEAD_AT_GPU:</div><div class="line">  <span class="keyword">case</span> SYNCED:</div><div class="line">    <span class="keyword">break</span>;</div><div class="line">  &#125;</div><div class="line"><span class="meta">#<span class="meta-keyword">else</span></span></div><div class="line">  <span class="comment">// NO_GPU 是一个宏，打印FATAL ERROR日志信息</span></div><div class="line">  <span class="comment">// 编译选项没有开GPU支持，只能说 无可奉告</span></div><div class="line">  NO_GPU;</div><div class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></div><div class="line">&#125;</div></pre></td></tr></table></figure><p>注意到，除了<code>head_</code>以外，<code>SyncedMemory</code>中还有<code>own_gpu_data_</code>（同样，也有<code>own_cpu_data_</code>）的成员。这个变量是用来标志当前CPU或GPU上有没有分配内存，从而当我们使用<code>set_c/gpu_data</code>或析构函数被调用的时候，能够正确释放内存/显存的。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;code&gt;Blob&lt;/code&gt;是Caffe中的基本数据结构，类似于TensorFlow和PyTorch中的Tensor。图像读入后作为&lt;code&gt;Blob&lt;/code&gt;，开始在各个&lt;code&gt;Layer&lt;/code&gt;之间传递，最终得到输出。下面这张图展示了&lt;code&gt;Blob&lt;/code&gt;和&lt;code&gt;Layer&lt;/code&gt;之间的关系：&lt;br&gt; &lt;img src=&quot;/img/caffe_syncedmem_blob_flow.jpg&quot; width = &quot;300&quot; height = &quot;200&quot; alt=&quot;blob的流动&quot; align=center /&gt;&lt;/p&gt;
&lt;p&gt;Caffe中的&lt;code&gt;Blob&lt;/code&gt;在实现的时候，使用了&lt;code&gt;SyncedMem&lt;/code&gt;管理内存，并在内存（Host）和显存（device）之间同步。这篇博客对Caffe中&lt;code&gt;SyncedMem&lt;/code&gt;的实现做一总结。&lt;br&gt;
    
    </summary>
    
    
      <category term="caffe" scheme="https://xmfbit.github.io/tags/caffe/"/>
    
  </entry>
  
  <entry>
    <title>Caffe中的BatchNorm实现</title>
    <link href="https://xmfbit.github.io/2018/01/08/caffe-batch-norm/"/>
    <id>https://xmfbit.github.io/2018/01/08/caffe-batch-norm/</id>
    <published>2018-01-08T12:12:44.000Z</published>
    <updated>2018-03-14T06:04:00.122Z</updated>
    
    <content type="html"><![CDATA[<p>这篇博客总结了Caffe中BN的实现。<br><a id="more"></a></p><h2 id="BN简介"><a href="#BN简介" class="headerlink" title="BN简介"></a>BN简介</h2><p>由于BN技术已经有很广泛的应用，所以这里只对BN做一个简单的介绍。</p><p>BN是Batch Normalization的简称，来源于Google研究人员的论文：<a href="https://arxiv.org/pdf/1502.03167.pdf" target="_blank" rel="external">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</a>。对于网络的输入层，我们可以采用减去均值除以方差的方法进行归一化，对于网络中间层，BN可以实现类似的功能。</p><p>在BN层中，训练时，会对输入blob各个channel的均值和方差做一统计。在做inference的时候，我们就可以利用均值和方法，对输入$x$做如下的归一化操作。其中，$\epsilon$是为了防止除数是$0$，$i$是channel的index。</p><script type="math/tex; mode=display">\hat{x_i} = \frac{x_i-\mu_i}{\sqrt{Var(x_i)+\epsilon}}</script><p>不过如果只是做如上的操作，会影响模型的表达能力。例如，Identity Map($y = x$)就不能表示了。所以，作者提出还需要在后面添加一个线性变换，如下所示。其中，$\gamma$和$\beta$都是待学习的参数，使用梯度下降进行更新。BN的最终输出就是$y$。</p><script type="math/tex; mode=display">y_i = \gamma \hat{x_i} + \beta</script><p>如下图所示，展示了BN变换的过程。<br><img src="/img/caffe_bn_what_is_bn.jpg" alt="BN变换"></p><p>上面，我们讲的还是inference时候BN变换是什么样子的。那么，训练时候，BN是如何估计样本均值和方差的呢？下面，结合Caffe的代码进行梳理。</p><h2 id="BN-in-Caffe"><a href="#BN-in-Caffe" class="headerlink" title="BN in Caffe"></a>BN in Caffe</h2><p>在BVLC的Caffe实现中，BN层需要和Scale层配合使用。在这里，BN层专门用来做“Normalization”操作（确实是人如其名了），而后续的线性变换层，交给Scale层去做。</p><p>下面的这段代码取自He Kaiming的Residual Net50的<a href="https://github.com/KaimingHe/deep-residual-networks/blob/master/prototxt/ResNet-50-deploy.prototxt#L21" target="_blank" rel="external">模型定义文件</a>。在这里，设置<code>batch_norm_param</code>中<code>use_global_stats</code>为<code>true</code>，是指在inference阶段，我们只使用已经得到的均值和方差统计量，进行归一化处理，而不再更新这两个统计量。后面Scale层设置的<code>bias_term: true</code>是不可省略的。这个选项将其配置为线性变换层。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line">layer &#123;</div><div class="line">bottom: &quot;conv1&quot;</div><div class="line">top: &quot;conv1&quot;</div><div class="line">name: &quot;bn_conv1&quot;</div><div class="line">type: &quot;BatchNorm&quot;</div><div class="line">batch_norm_param &#123;</div><div class="line">use_global_stats: true</div><div class="line">&#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">bottom: &quot;conv1&quot;</div><div class="line">top: &quot;conv1&quot;</div><div class="line">name: &quot;scale_conv1&quot;</div><div class="line">type: &quot;Scale&quot;</div><div class="line">scale_param &#123;</div><div class="line">bias_term: true</div><div class="line">&#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure><p>这就是Caffe中BN层的固定搭配方法。这里只是简单提到，具体参数的意义待我们深入代码可以分析。</p><h2 id="BatchNorm-层的实现"><a href="#BatchNorm-层的实现" class="headerlink" title="BatchNorm 层的实现"></a>BatchNorm 层的实现</h2><p>上面说过，Caffe中的BN层与原始论文稍有不同，只是做了输入的归一化，而后续的线性变换是交由后续的Scale层实现的。</p><h3 id="proto定义的相关参数"><a href="#proto定义的相关参数" class="headerlink" title="proto定义的相关参数"></a>proto定义的相关参数</h3><p>我们首先看一下<code>caffe.proto</code>中关于BN层参数的描述。保留了原始的英文注释，并添加了中文解释。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div></pre></td><td class="code"><pre><div class="line">message BatchNormParameter &#123;</div><div class="line">  // If false, normalization is performed over the current mini-batch</div><div class="line">  // and global statistics are accumulated (but not yet used) by a moving</div><div class="line">  // average.</div><div class="line">  // If true, those accumulated mean and variance values are used for the</div><div class="line">  // normalization.</div><div class="line">  // By default, it is set to false when the network is in the training</div><div class="line">  // phase and true when the network is in the testing phase.</div><div class="line">  // 设置为False的话，更新全局统计量，对当前的mini-batch进行规范化时，不使用全局统计量，而是</div><div class="line">  // 当前batch的均值和方差。</div><div class="line">  // 设置为True，使用全局统计量做规范化。</div><div class="line">  // 后面在BN的实现代码我们会看到，这个变量默认随着当前网络在train或test phase而变化。</div><div class="line">  // 当train时为false，当test时为true。</div><div class="line">  optional bool use_global_stats = 1;</div><div class="line">  </div><div class="line">  // What fraction of the moving average remains each iteration?</div><div class="line">  // Smaller values make the moving average decay faster, giving more</div><div class="line">  // weight to the recent values.</div><div class="line">  // Each iteration updates the moving average @f$S_&#123;t-1&#125;@f$ with the</div><div class="line">  // current mean @f$ Y_t @f$ by</div><div class="line">  // @f$ S_t = (1-\beta)Y_t + \beta \cdot S_&#123;t-1&#125; @f$, where @f$ \beta @f$</div><div class="line">  // is the moving_average_fraction parameter.</div><div class="line">  // BN在统计全局均值和方差信息时，使用的是滑动平均法，也就是</div><div class="line">  // St = (1-beta)*Yt + beta*S_&#123;t-1&#125;</div><div class="line">  // 其中St为当前估计出来的全局统计量（均值或方差），Yt为当前batch的均值或方差</div><div class="line">  // beta是滑动因子。其实这是一种很常见的平滑滤波的方法。</div><div class="line">  optional float moving_average_fraction = 2 [default = .999];</div><div class="line">  </div><div class="line">  // Small value to add to the variance estimate so that we don&apos;t divide by</div><div class="line">  // zero.</div><div class="line">  // 防止除数为0加上去的eps</div><div class="line">  optional float eps = 3 [default = 1e-5];</div><div class="line">&#125;</div></pre></td></tr></table></figure></p><p>OK。现在可以进入BN的代码实现了。阅读大部分代码都没有什么难度，下面主要结合代码讲解<code>use_global_stats</code>变量的作用和均值（方差同理）的计算。由于均值和方差的计算原理相近，所以下面只会详细介绍均值的计算。</p><h3 id="SetUp"><a href="#SetUp" class="headerlink" title="SetUp"></a>SetUp</h3><p>BN层的SetUp代码如下。首先，会根据当前处于train还是test决定是否使用全局的统计量。如果prototxt文件中设置了<code>use_global_stats</code>标志，则会使用用户给定的配置。所以一般在使用BN时，无需对<code>use_global_stats</code>进行配置。</p><p>这里有一个地方容易迷惑。BN中要对样本的均值和方差进行统计，即我们需要两个blob来存储。但是从下面的代码可以看到，BN一共有3个blob作为参数。这里做一解释，主要参考了wiki的<a href="https://wiki2.org/en/Moving_average" target="_blank" rel="external">moving average条目</a>。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="keyword">void</span> BatchNormLayer&lt;Dtype&gt;::LayerSetUp(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,</div><div class="line">      <span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; top) &#123;</div><div class="line">  BatchNormParameter param = <span class="keyword">this</span>-&gt;layer_param_.batch_norm_param();</div><div class="line">  moving_average_fraction_ = param.moving_average_fraction();</div><div class="line">  <span class="comment">// 默认根据当前是否处在TEST模式而决定是否使用全局mean和var</span></div><div class="line">  use_global_stats_ = <span class="keyword">this</span>-&gt;phase_ == TEST;</div><div class="line">  <span class="keyword">if</span> (param.has_use_global_stats())</div><div class="line">    use_global_stats_ = param.use_global_stats();</div><div class="line">  <span class="comment">// 得到channels数量</span></div><div class="line">  <span class="comment">// 为了防止越界，首先检查输入是否为1D</span></div><div class="line">  <span class="keyword">if</span> (bottom[<span class="number">0</span>]-&gt;num_axes() == <span class="number">1</span>)</div><div class="line">    channels_ = <span class="number">1</span>;</div><div class="line">  <span class="keyword">else</span></div><div class="line">    channels_ = bottom[<span class="number">0</span>]-&gt;shape(<span class="number">1</span>);</div><div class="line">  eps_ = param.eps();</div><div class="line">  <span class="keyword">if</span> (<span class="keyword">this</span>-&gt;blobs_.size() &gt; <span class="number">0</span>) &#123;</div><div class="line">    LOG(INFO) &lt;&lt; <span class="string">"Skipping parameter initialization"</span>;</div><div class="line">  &#125; <span class="keyword">else</span> &#123;</div><div class="line">    <span class="comment">// 参数共3个</span></div><div class="line">    <span class="keyword">this</span>-&gt;blobs_.resize(<span class="number">3</span>);</div><div class="line">    <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; sz;</div><div class="line">    sz.push_back(channels_);</div><div class="line">    <span class="comment">// mean 和var都是1D的长度为channels的向量</span></div><div class="line">    <span class="comment">// 因为在规范化过程中，要逐channel进行，即：</span></div><div class="line">    <span class="comment">// for c in range(channels):</span></div><div class="line">    <span class="comment">//     x_hat[c] = (x[c] - mean[c]) / std[c]</span></div><div class="line">    <span class="keyword">this</span>-&gt;blobs_[<span class="number">0</span>].reset(<span class="keyword">new</span> Blob&lt;Dtype&gt;(sz));</div><div class="line">    <span class="keyword">this</span>-&gt;blobs_[<span class="number">1</span>].reset(<span class="keyword">new</span> Blob&lt;Dtype&gt;(sz));</div><div class="line">    <span class="comment">// 这里的解释见下</span></div><div class="line">    sz[<span class="number">0</span>] = <span class="number">1</span>;</div><div class="line">    <span class="keyword">this</span>-&gt;blobs_[<span class="number">2</span>].reset(<span class="keyword">new</span> Blob&lt;Dtype&gt;(sz));</div><div class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">3</span>; ++i) &#123;</div><div class="line">      caffe_set(<span class="keyword">this</span>-&gt;blobs_[i]-&gt;count(), Dtype(<span class="number">0</span>),</div><div class="line">                <span class="keyword">this</span>-&gt;blobs_[i]-&gt;mutable_cpu_data());</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">  <span class="comment">// Mask statistics from optimization by setting local learning rates</span></div><div class="line">  <span class="comment">// for mean, variance, and the bias correction to zero.</span></div><div class="line">  <span class="comment">// mean 和 std在训练的时候是不需要梯度下降来更新的，这里强制把其learning rate</span></div><div class="line">  <span class="comment">// 设置为0</span></div><div class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="keyword">this</span>-&gt;blobs_.size(); ++i) &#123;</div><div class="line">    <span class="keyword">if</span> (<span class="keyword">this</span>-&gt;layer_param_.param_size() == i) &#123;</div><div class="line">      ParamSpec* fixed_param_spec = <span class="keyword">this</span>-&gt;layer_param_.add_param();</div><div class="line">      fixed_param_spec-&gt;set_lr_mult(<span class="number">0.f</span>);</div><div class="line">    &#125; <span class="keyword">else</span> &#123;</div><div class="line">      CHECK_EQ(<span class="keyword">this</span>-&gt;layer_param_.param(i).lr_mult(), <span class="number">0.f</span>)</div><div class="line">          &lt;&lt; <span class="string">"Cannot configure batch normalization statistics as layer "</span></div><div class="line">          &lt;&lt; <span class="string">"parameters."</span>;</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure><p>在求取某个流数据（stream）的平均值的时候，常用的一种方法是滑动平均法，也就是使用系数$\alpha$来做平滑滤波，如下所示：</p><script type="math/tex; mode=display">S_t = \alpha Y_t + (1-\alpha) S_{t-1}</script><p>上面的式子等价于：</p><script type="math/tex; mode=display">S_t = \frac{\text{WeightedSum}_n}{\text{WeightedCount}_n}</script><p>其中，<script type="math/tex">\text{WeightedSum}_n = Y_t + (1-\alpha) \text{WeightedSum}_{n-1}</script></p><script type="math/tex; mode=display">\text{WeightedCount}_n = 1 + (1-\alpha) \text{WeightedCount}_{n-1}</script><p>而Caffe中BN的实现中，<code>blobs_[0]</code>和<code>blobs_[1]</code>中存储的实际是$\text{WeightedSum}_n$，而<code>blos_[2]</code>中存储的是$\text{WeightedCount}_n$。所以，真正的mean和var是两者相除的结果。即：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">mu = blobs_[0] / blobs_[2]</div><div class="line">var = blobs_[1] / blobs_[2]</div></pre></td></tr></table></figure></p><h3 id="Forward"><a href="#Forward" class="headerlink" title="Forward"></a>Forward</h3><p>下面是Forward CPU的代码。主要应该注意当前batch的mean和var的求法。<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="keyword">void</span> BatchNormLayer&lt;Dtype&gt;::Forward_cpu(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,</div><div class="line">    <span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; top) &#123;</div><div class="line">  <span class="keyword">const</span> Dtype* bottom_data = bottom[<span class="number">0</span>]-&gt;cpu_data();</div><div class="line">  Dtype* top_data = top[<span class="number">0</span>]-&gt;mutable_cpu_data();</div><div class="line">  <span class="keyword">int</span> num = bottom[<span class="number">0</span>]-&gt;shape(<span class="number">0</span>);</div><div class="line">  <span class="keyword">int</span> spatial_dim = bottom[<span class="number">0</span>]-&gt;count()/(bottom[<span class="number">0</span>]-&gt;shape(<span class="number">0</span>)*channels_);</div><div class="line"></div><div class="line">  <span class="comment">// 如果不是就地操作，首先将bottom的数据复制到top</span></div><div class="line">  <span class="keyword">if</span> (bottom[<span class="number">0</span>] != top[<span class="number">0</span>]) &#123;</div><div class="line">    caffe_copy(bottom[<span class="number">0</span>]-&gt;count(), bottom_data, top_data);</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="comment">// 如果使用全局统计量，我们需要先计算出真正的mean和var</span></div><div class="line">  <span class="keyword">if</span> (use_global_stats_) &#123;</div><div class="line">    <span class="comment">// use the stored mean/variance estimates.</span></div><div class="line">    <span class="keyword">const</span> Dtype scale_factor = <span class="keyword">this</span>-&gt;blobs_[<span class="number">2</span>]-&gt;cpu_data()[<span class="number">0</span>] == <span class="number">0</span> ?</div><div class="line">        <span class="number">0</span> : <span class="number">1</span> / <span class="keyword">this</span>-&gt;blobs_[<span class="number">2</span>]-&gt;cpu_data()[<span class="number">0</span>];</div><div class="line">    <span class="comment">// mean = blobs[0] / blobs[2]</span></div><div class="line">    caffe_cpu_scale(variance_.count(), scale_factor,</div><div class="line">        <span class="keyword">this</span>-&gt;blobs_[<span class="number">0</span>]-&gt;cpu_data(), mean_.mutable_cpu_data());</div><div class="line">    <span class="comment">// var = blobs[1] / blobs[2]</span></div><div class="line">    caffe_cpu_scale(variance_.count(), scale_factor,</div><div class="line">        <span class="keyword">this</span>-&gt;blobs_[<span class="number">1</span>]-&gt;cpu_data(), variance_.mutable_cpu_data());</div><div class="line">  &#125; <span class="keyword">else</span> &#123;</div><div class="line">    <span class="comment">// 不使用全局统计量时，我们要根据当前batch的mean和var做规范化</span></div><div class="line">    <span class="comment">// compute mean</span></div><div class="line">    <span class="comment">// spatial_sum_multiplier_是全1向量</span></div><div class="line">    <span class="comment">// batch_sum_multiplier_也是全1向量</span></div><div class="line">    <span class="comment">// gemv做矩阵与向量相乘 y = alpha*A*x + beta*y。</span></div><div class="line">    <span class="comment">// 下面式子是将bottom_data这个矩阵与一个全1向量相乘，</span></div><div class="line">    <span class="comment">// 相当于是在统计行和。</span></div><div class="line">    <span class="comment">// 注意第二个参数channels_ * num指矩阵的行数，第三个参数是矩阵的列数</span></div><div class="line">    <span class="comment">// 所以这是在计算每个channel的feature map的和</span></div><div class="line">    <span class="comment">// 结果out[n][c]是指输入第n个sample的第c个channel的和</span></div><div class="line">    <span class="comment">// 同时，传入了 1. / (num * spatial_dim) 作为因子乘到结果上面，作用见下面</span></div><div class="line">    caffe_cpu_gemv&lt;Dtype&gt;(CblasNoTrans, channels_ * num, spatial_dim,</div><div class="line">        <span class="number">1.</span> / (num * spatial_dim), bottom_data,</div><div class="line">        spatial_sum_multiplier_.cpu_data(), <span class="number">0.</span>,</div><div class="line">        num_by_chans_.mutable_cpu_data());</div><div class="line">    <span class="comment">// 道理和上面相同，注意下面通过传入CblasTrans，指定了矩阵要转置。所以是在求列和</span></div><div class="line">    <span class="comment">// 这样，就求出了各个channel的和。</span></div><div class="line">    <span class="comment">// 上面不是已经除了 num * spatial_dim 吗？这就是求和元素的总数量</span></div><div class="line">    <span class="comment">// 到此，我们就完成了对当前batch的平均值的求解</span></div><div class="line">    caffe_cpu_gemv&lt;Dtype&gt;(CblasTrans, num, channels_, <span class="number">1.</span>,</div><div class="line">        num_by_chans_.cpu_data(), batch_sum_multiplier_.cpu_data(), <span class="number">0.</span>,</div><div class="line">        mean_.mutable_cpu_data());</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="comment">// subtract mean</span></div><div class="line">  <span class="comment">// gemm是在做矩阵与矩阵相乘 C = alpha*A*B + beta*C</span></div><div class="line">  <span class="comment">// 下面这个是在做broadcasting subtraction</span></div><div class="line">  caffe_cpu_gemm&lt;Dtype&gt;(CblasNoTrans, CblasNoTrans, num, channels_, <span class="number">1</span>, <span class="number">1</span>,</div><div class="line">      batch_sum_multiplier_.cpu_data(), mean_.cpu_data(), <span class="number">0.</span>,</div><div class="line">      num_by_chans_.mutable_cpu_data());</div><div class="line">  caffe_cpu_gemm&lt;Dtype&gt;(CblasNoTrans, CblasNoTrans, channels_ * num,</div><div class="line">      spatial_dim, <span class="number">1</span>, <span class="number">-1</span>, num_by_chans_.cpu_data(),</div><div class="line">      spatial_sum_multiplier_.cpu_data(), <span class="number">1.</span>, top_data);</div><div class="line"></div><div class="line">  <span class="comment">// 计算当前的var</span></div><div class="line">  <span class="keyword">if</span> (!use_global_stats_) &#123;</div><div class="line">    <span class="comment">// compute variance using var(X) = E((X-EX)^2)</span></div><div class="line">    caffe_sqr&lt;Dtype&gt;(top[<span class="number">0</span>]-&gt;count(), top_data,</div><div class="line">                     temp_.mutable_cpu_data());  <span class="comment">// (X-EX)^2</span></div><div class="line">    caffe_cpu_gemv&lt;Dtype&gt;(CblasNoTrans, channels_ * num, spatial_dim,</div><div class="line">        <span class="number">1.</span> / (num * spatial_dim), temp_.cpu_data(),</div><div class="line">        spatial_sum_multiplier_.cpu_data(), <span class="number">0.</span>,</div><div class="line">        num_by_chans_.mutable_cpu_data());</div><div class="line">    caffe_cpu_gemv&lt;Dtype&gt;(CblasTrans, num, channels_, <span class="number">1.</span>,</div><div class="line">        num_by_chans_.cpu_data(), batch_sum_multiplier_.cpu_data(), <span class="number">0.</span>,</div><div class="line">        variance_.mutable_cpu_data());  <span class="comment">// E((X_EX)^2)</span></div><div class="line"></div><div class="line">    <span class="comment">// compute and save moving average</span></div><div class="line">    <span class="comment">// 做滑动平均，更新全局统计量，这里可以参见上面的式子</span></div><div class="line">    <span class="keyword">this</span>-&gt;blobs_[<span class="number">2</span>]-&gt;mutable_cpu_data()[<span class="number">0</span>] *= moving_average_fraction_;</div><div class="line">    <span class="keyword">this</span>-&gt;blobs_[<span class="number">2</span>]-&gt;mutable_cpu_data()[<span class="number">0</span>] += <span class="number">1</span>;</div><div class="line">    caffe_cpu_axpby(mean_.count(), Dtype(<span class="number">1</span>), mean_.cpu_data(),</div><div class="line">        moving_average_fraction_, <span class="keyword">this</span>-&gt;blobs_[<span class="number">0</span>]-&gt;mutable_cpu_data());</div><div class="line">    <span class="keyword">int</span> m = bottom[<span class="number">0</span>]-&gt;count()/channels_;</div><div class="line">    Dtype bias_correction_factor = m &gt; <span class="number">1</span> ? Dtype(m)/(m<span class="number">-1</span>) : <span class="number">1</span>;</div><div class="line">    caffe_cpu_axpby(variance_.count(), bias_correction_factor,</div><div class="line">        variance_.cpu_data(), moving_average_fraction_,</div><div class="line">        <span class="keyword">this</span>-&gt;blobs_[<span class="number">1</span>]-&gt;mutable_cpu_data());</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="comment">// normalize variance</span></div><div class="line">  caffe_add_scalar(variance_.count(), eps_, variance_.mutable_cpu_data());</div><div class="line">  caffe_sqrt(variance_.count(), variance_.cpu_data(),</div><div class="line">             variance_.mutable_cpu_data());</div><div class="line"></div><div class="line">  <span class="comment">// replicate variance to input size</span></div><div class="line">  <span class="comment">// 同样是在做broadcasting</span></div><div class="line">  caffe_cpu_gemm&lt;Dtype&gt;(CblasNoTrans, CblasNoTrans, num, channels_, <span class="number">1</span>, <span class="number">1</span>,</div><div class="line">      batch_sum_multiplier_.cpu_data(), variance_.cpu_data(), <span class="number">0.</span>,</div><div class="line">      num_by_chans_.mutable_cpu_data());</div><div class="line">  caffe_cpu_gemm&lt;Dtype&gt;(CblasNoTrans, CblasNoTrans, channels_ * num,</div><div class="line">      spatial_dim, <span class="number">1</span>, <span class="number">1.</span>, num_by_chans_.cpu_data(),</div><div class="line">      spatial_sum_multiplier_.cpu_data(), <span class="number">0.</span>, temp_.mutable_cpu_data());</div><div class="line">  caffe_div(temp_.count(), top_data, temp_.cpu_data(), top_data);</div><div class="line">  <span class="comment">// TODO(cdoersch): The caching is only needed because later in-place layers</span></div><div class="line">  <span class="comment">//                 might clobber the data.  Can we skip this if they won't?</span></div><div class="line">  caffe_copy(x_norm_.count(), top_data,</div><div class="line">      x_norm_.mutable_cpu_data());</div><div class="line">&#125;</div></pre></td></tr></table></figure></p><p>由上面的计算过程不难得出，当经过很多轮迭代之后，<code>blobs_[2]</code>的值会趋于稳定。下面我们使用$m_t$来表示第$t$轮迭代后的<code>blobs_[2]</code>的值，也就是$\text{WeightedCount}_n$，使用$\alpha$表示<code>moving_average_fraction_</code>，那么我们有：</p><script type="math/tex; mode=display">m_t = 1 + \alpha m_{t-1}</script><p>可以求取$m_t$的通项后令$t=\infty$，可以得到，$m_{\infty}=\frac{1}{1-\alpha}$。</p><h3 id="Backward"><a href="#Backward" class="headerlink" title="Backward"></a>Backward</h3><p>在做BP的时候，我们需要分情况讨论。</p><ul><li>当<code>use_global_stats == true</code>的时候，BN所做的操作是一个线性变换<script type="math/tex; mode=display">BN(x) = \frac{x-\mu}{\sqrt{Var}}</script>所以<script type="math/tex; mode=display">\frac{\partial L}{\partial x} = \frac{1}{\sqrt{Var}}\frac{\partial L}{\partial y}</script></li></ul><p>对应的代码如下。其中，<code>temp_</code>是broadcasting之后的输入<code>x</code>的标准差（见上面<code>Forward</code>部分的代码最后），做逐元素的除法即可。<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">if</span> (use_global_stats_) &#123;</div><div class="line">  caffe_div(temp_.count(), top_diff, temp_.cpu_data(), bottom_diff);</div><div class="line">  <span class="keyword">return</span>;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p><ul><li>当<code>use_global_stats == false</code>的时候，BN所做操作虽然也是上述线性变换。但是注意，现在式子里面的$\mu$和$Var(x)$都是当前batch计算出来的，也就是它们都是输入<code>x</code>的函数。所以就麻烦了不少。这里我并没有推导，而是看了<a href="https://kevinzakka.github.io/2016/09/14/batch_normalization/" target="_blank" rel="external">这篇博客</a>，里面有详细的推导过程，写的很易懂。我将最后的结果贴在下面，对计算过程感兴趣的可以去原文章查看。<br><img src="/img/caffe_bn_bp_of_bn.jpg" alt="BP的推导结果"></li></ul><p>我们使用$y$来代替上面的$\hat{x_i}$，并且上下同时除以$m$，就可以得到Caffe BN代码中所给的BP式子：</p><script type="math/tex; mode=display">\frac{\partial f}{\partial x_i} = \frac{\frac{\partial f}{\partial y}-E[\frac{\partial f}{\partial y}]-yE[\frac{\partial f}{\partial y}y]}{\sqrt{\sigma^2+\epsilon}}</script><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// if Y = (X-mean(X))/(sqrt(var(X)+eps)), then</span></div><div class="line"><span class="comment">//</span></div><div class="line"><span class="comment">// dE(Y)/dX =</span></div><div class="line"><span class="comment">//   (dE/dY - mean(dE/dY) - mean(dE/dY \cdot Y) \cdot Y)</span></div><div class="line"><span class="comment">//     ./ sqrt(var(X) + eps)</span></div><div class="line"><span class="comment">//</span></div><div class="line"><span class="comment">// where \cdot and ./ are hadamard product and elementwise division,</span></div><div class="line"><span class="comment">// respectively, dE/dY is the top diff, and mean/var/sum are all computed</span></div><div class="line"><span class="comment">// along all dimensions except the channels dimension.  In the above</span></div><div class="line"><span class="comment">// equation, the operations allow for expansion (i.e. broadcast) along all</span></div><div class="line"><span class="comment">// dimensions except the channels dimension where required.</span></div></pre></td></tr></table></figure><p>下面的代码部分就是实现上面这个式子的内容，注释很详细，要解决的一个比较棘手的问题就是broadcasting，这个有兴趣可以看一下。对Caffe中BN的介绍就到这里。下面介绍与BN经常成对出现的Scale层。</p><h2 id="Scale层的实现"><a href="#Scale层的实现" class="headerlink" title="Scale层的实现"></a>Scale层的实现</h2><p>Caffe中将后续的线性变换使用单独的Scale层实现。Caffe中的Scale可以根据需要配置成不同的模式：</p><ul><li>当输入blob为两个时，计算输入blob的逐元素乘的结果（维度不相同时，第二个blob可以做broadcasting）。</li><li>当输入blob为一个时，计算输入blob与一个可学习参数<code>gamma</code>的按元素相乘结果。</li><li>当设置<code>bias_term: true</code>时，添加一个偏置项。</li></ul><p>用于BN的线性变换的计算方法很直接，这里不再多说了。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这篇博客总结了Caffe中BN的实现。&lt;br&gt;
    
    </summary>
    
    
      <category term="caffe" scheme="https://xmfbit.github.io/tags/caffe/"/>
    
  </entry>
  
  <entry>
    <title>shell编程</title>
    <link href="https://xmfbit.github.io/2017/11/10/shell-programming/"/>
    <id>https://xmfbit.github.io/2017/11/10/shell-programming/</id>
    <published>2017-11-10T05:06:30.000Z</published>
    <updated>2018-03-14T06:04:00.136Z</updated>
    
    <content type="html"><![CDATA[<p>介绍基本的shell编程方法，参考的教程是<a href="http://www.freeos.com/guides/lsst/" target="_blank" rel="external">Linux Shell Scripting Tutorial, A Beginner’s handbook</a>。<br><img src="/img/shell-programming-bash-logo.png" alt="Bash Logo"><br><a id="more"></a></p><h2 id="变量"><a href="#变量" class="headerlink" title="变量"></a>变量</h2><p>变量是代码的基本组成元素。可以认为shell中的变量类型都是字符串。</p><p>shell中的变量可以分为两类：系统变量和用户自定义变量。下面分别进行介绍。</p><p>在代码中使用变量值的时候，需要在前面加上<code>$</code>。<code>echo</code>命令可以在控制台打印相应输出。所以使用<code>echo $var</code>就可以输出变量<code>var</code>的值。</p><h3 id="系统变量"><a href="#系统变量" class="headerlink" title="系统变量"></a>系统变量</h3><p>系统变量是指Linux中自带的一些变量。例如<code>HOME</code>,<code>PATH</code>等。其中<code>PATH</code>又叫环境变量。更多的系统变量见下表：<br><img src="/img/shell-programming-system-variables.jpg" alt="系统变量列表"></p><h3 id="用户定义的变量"><a href="#用户定义的变量" class="headerlink" title="用户定义的变量"></a>用户定义的变量</h3><p>用户自定义变量是用户命名并赋值的变量。使用下面的方法定义：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 注意不要在等号两边插入空格</span></div><div class="line">name=value</div><div class="line"><span class="comment"># 如 n=10</span></div></pre></td></tr></table></figure><h3 id="局部变量和全局变量"><a href="#局部变量和全局变量" class="headerlink" title="局部变量和全局变量"></a>局部变量和全局变量</h3><p>局部变量是指在当前代码块内可见的变量，使用<code>local</code>声明。例如下面的代码，将依次输出：111, 222, 111.<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#! /bin/sh</span></div><div class="line">num=111 <span class="comment"># 全局变量</span></div><div class="line"><span class="function"><span class="title">func1</span></span>()</div><div class="line">&#123;</div><div class="line">  <span class="built_in">local</span> num=222 <span class="comment"># 局部变量</span></div><div class="line">  <span class="built_in">echo</span> <span class="variable">$num</span></div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="built_in">echo</span> <span class="string">"before---<span class="variable">$num</span>"</span></div><div class="line">func1</div><div class="line"><span class="built_in">echo</span> <span class="string">"after---<span class="variable">$num</span>"</span></div></pre></td></tr></table></figure></p><h3 id="变量之间的运算"><a href="#变量之间的运算" class="headerlink" title="变量之间的运算"></a>变量之间的运算</h3><p>使用<code>expr</code>可以进行变量之间的运算，如下所示：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 注意要在操作符两边空余空格</span></div><div class="line">expr 1 + 3</div><div class="line"><span class="comment"># 由于*是特殊字符，所以乘法要使用转义</span></div><div class="line">expr 10 \* 2</div></pre></td></tr></table></figure><h3 id="和””"><a href="#和””" class="headerlink" title="``和””"></a>``和””</h3><p>使用``（也就是TAB键上面的那个）包起来的部分，是可执行的命令。而使用””（引号）包起来的部分，是字符串。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">a=`expr 10 \* 3`</div><div class="line"><span class="comment"># output: 3</span></div><div class="line"><span class="built_in">echo</span> <span class="variable">$a</span></div><div class="line"><span class="comment"># output: a</span></div><div class="line"><span class="built_in">echo</span> a</div><div class="line"><span class="comment"># output: expr 10 \* 3</span></div><div class="line">a=<span class="string">"expr 10 \* 3"</span></div><div class="line"><span class="built_in">echo</span> <span class="variable">$a</span></div></pre></td></tr></table></figure><p>另外，使用””（双引号）括起来的字符串会发生变量替换，而用’’（单引号）括起来的字符串则不会。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">a=1</div><div class="line"><span class="built_in">echo</span> <span class="string">"<span class="variable">$a</span>"</span>  <span class="comment"># 输出 1</span></div><div class="line"><span class="built_in">echo</span> <span class="string">'$a'</span>  <span class="comment"># 输出 $a</span></div></pre></td></tr></table></figure><h3 id="读取输入"><a href="#读取输入" class="headerlink" title="读取输入"></a>读取输入</h3><p>使用<code>read var1, var2, ...</code>的方式从键盘的输入读取变量的值。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># input a=1</span></div><div class="line"><span class="built_in">read</span> a</div><div class="line"><span class="comment"># ouptut: 2</span></div><div class="line"><span class="built_in">echo</span> `expr <span class="variable">$a</span> + 1`</div></pre></td></tr></table></figure><h2 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h2><h3 id="命令的返回值"><a href="#命令的返回值" class="headerlink" title="命令的返回值"></a>命令的返回值</h3><p>当bash命令成功执行后，返回给系统的返回值为<code>0</code>；否则为非零。可以据此判断上步操作的状态。使用<code>$?</code>可以取出上一步执行的返回值。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 将echo 错输为ecoh</span></div><div class="line">ecoh <span class="string">"hello"</span></div><div class="line"><span class="comment"># output: 非零(127)</span></div><div class="line"><span class="built_in">echo</span> $?</div><div class="line"><span class="comment"># output: 0</span></div><div class="line"><span class="built_in">echo</span> $?</div></pre></td></tr></table></figure><h3 id="通配符"><a href="#通配符" class="headerlink" title="通配符"></a>通配符</h3><p>通配符是指<code>*</code>,<code>?</code>和<code>[...]</code>这三类。</p><p><code>*</code>可以匹配任意多的字符，<code>?</code>用来匹配一个字符。<code>[...]</code>用来匹配括号内的字符。见下表。<br><img src="/img/shell-programming-wild-cards.jpg" alt="通配符"></p><p><code>[...]</code>表示法还有如下变形：</p><ul><li>使用<code>-</code>用来指示范围。如<code>[a-z]</code>，表示<code>a</code>到<code>z</code>间任意一个字符。</li><li>使用<code>^</code>或<code>!</code>表示取反。如<code>[!a-p]</code>表示除了<code>a</code>到<code>p</code>间字符的其他字符。</li></ul><h3 id="输入输出重定向"><a href="#输入输出重定向" class="headerlink" title="输入输出重定向"></a>输入输出重定向</h3><p>重定向是指改变命令的输出位置。使用<code>&gt;</code>进行输出重定向。使用<code>&lt;</code>进行输入重定向。例如，<code>ls -l &gt; a.txt</code>，将本目录下的文件信息输出到文本文件<code>a.txt</code>中，而不再输出到终端。</p><p>此外，<code>&gt;&gt;</code>同样是输出重定向。但是它会在文件末尾追加写入，不会覆盖文件的原有内容。</p><p>搭配使用<code>&lt;</code>和<code>&gt;</code>可以做文件处理。例如，<code>tr group1 group2</code>命令可以将<code>group1</code>中的字符变换为<code>group2</code>中对应位置的字符。使用如下命令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">tr <span class="string">"[a-z]"</span> <span class="string">"A-Z"</span> &lt; ori.txt &gt; out.txt</div></pre></td></tr></table></figure><p>可以将<code>ori.txt</code>中的小写字母转换为大写字母输出到<code>out.txt</code>中。</p><h3 id="管道（pipeline）"><a href="#管道（pipeline）" class="headerlink" title="管道（pipeline）"></a>管道（pipeline）</h3><p>管道<code>|</code>可以将第一个程序的输出作为第二个程序的输入。例如：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">cat ori.txt | tr <span class="string">"[a-z]"</span> <span class="string">"A-Z"</span></div></pre></td></tr></table></figure><p>会将<code>ori.txt</code>中的小写字母转换为大写，并在终端输出。</p><h3 id="过滤器（Filter）"><a href="#过滤器（Filter）" class="headerlink" title="过滤器（Filter）"></a>过滤器（Filter）</h3><p>Filter是指那些输入和输出都是控制台的命令。通过Filter和输入输出重定向，可以很方便地对文件内容进行整理。例如：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sort &lt; names.txt | uniq &gt; u_names.txt</div></pre></td></tr></table></figure><p><code>uniq</code>命令可以实现去重，但是需要首先对输入数据进行排序。上面的Filter可以将输入文件<code>names.txt</code>中的行文本去重后输出到<code>u_names.txt</code>中去。</p><h2 id="控制流"><a href="#控制流" class="headerlink" title="控制流"></a>控制流</h2><h3 id="if-条件控制"><a href="#if-条件控制" class="headerlink" title="if 条件控制"></a>if 条件控制</h3><p>在bash中使用<code>if</code>条件控制的语法和MATLAB等很像，要在末尾加上类似<code>end</code>的指示符，如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">if</span> condition</div><div class="line"><span class="keyword">then</span> </div><div class="line">XXX</div><div class="line"><span class="keyword">fi</span></div></pre></td></tr></table></figure><p>或者加上<code>else</code>，使用如下的形式：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">if</span> condition</div><div class="line"><span class="keyword">then</span></div><div class="line">    <span class="keyword">do</span> something</div><div class="line"><span class="keyword">elif</span> condition</div><div class="line"><span class="keyword">then</span></div><div class="line">    <span class="keyword">do</span> something</div><div class="line"><span class="keyword">else</span></div><div class="line">    <span class="keyword">do</span> something</div><div class="line"><span class="keyword">fi</span></div></pre></td></tr></table></figure></p><p>那么，如何做逻辑运算呢？需要借助<code>test</code>关键字。</p><p>对于整数来说，我们可以使用<code>if test op1 oprator op2</code>的方式，判断操作数<code>op1</code>和<code>op2</code>的大小关系。其中，<code>operator</code>可以是<code>-gt</code>，<code>-eq</code>等。</p><p>或者另一种写法：<code>if [ op1 operator op2 ]</code>，但是注意后者<code>[]</code>与操作数之间有空格。<br>如下表所示（点击可放大）：</p><p><img src="/img/shell-programming-if-operators.jpg" alt="比较整数的逻辑运算"></p><p>对于字符串，支持的逻辑判断如下：<br><img src="/img/bash-programming-comparing-string.jpg" alt="比较字符串的逻辑运算"></p><p>举个例子，我们想判断输入的值是否为1或2，可以使用如下的脚本。注意<code>[]</code>的两边一定要加空格。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#! /bin/bash</span></div><div class="line">a=1</div><div class="line"><span class="keyword">if</span> [ <span class="variable">$1</span>=<span class="variable">$a</span> ]</div><div class="line"><span class="keyword">then</span></div><div class="line">    <span class="built_in">echo</span> <span class="string">"you input 1"</span></div><div class="line"><span class="keyword">elif</span> [ <span class="variable">$1</span>=2 ]</div><div class="line"><span class="keyword">then</span></div><div class="line">    <span class="built_in">echo</span> <span class="string">"you input 2"</span></div><div class="line"><span class="keyword">else</span></div><div class="line">    <span class="built_in">echo</span> <span class="string">"you input <span class="variable">$1</span>"</span></div><div class="line"><span class="keyword">fi</span></div></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;介绍基本的shell编程方法，参考的教程是&lt;a href=&quot;http://www.freeos.com/guides/lsst/&quot;&gt;Linux Shell Scripting Tutorial, A Beginner’s handbook&lt;/a&gt;。&lt;br&gt;&lt;img src=&quot;/img/shell-programming-bash-logo.png&quot; alt=&quot;Bash Logo&quot;&gt;&lt;br&gt;
    
    </summary>
    
    
      <category term="linux" scheme="https://xmfbit.github.io/tags/linux/"/>
    
      <category term="shell" scheme="https://xmfbit.github.io/tags/shell/"/>
    
  </entry>
  
  <entry>
    <title>过秦论</title>
    <link href="https://xmfbit.github.io/2017/10/27/fuck-gfw/"/>
    <id>https://xmfbit.github.io/2017/10/27/fuck-gfw/</id>
    <published>2017-10-27T01:56:00.000Z</published>
    <updated>2018-03-14T06:04:00.129Z</updated>
    
    <content type="html"><![CDATA[<p>秦以耕战立国，起于西北，从边陲弱小诸侯，借以天时地利，用商鞅、李斯，举法家大旗。而后逐鹿中原，坑赵括，灌大梁，掳六国贵族，结束战乱，建立大一统帝国。然而，强秦却二世而亡。“始皇之心，自以为关中之固，金城千里，子孙帝王万世之业也。”封建王朝的皇帝总觉得自己的国祚能够绵延不绝，历史却一次次地打他们的脸。<br><img src="/img/just-a-joke.png" alt="just a joke"></p><a id="more"></a><p>　　秦孝公据崤函之固，拥雍州之地，君臣固守以窥周室，有席卷天下，包举宇内，囊括四海之意，并吞八荒之心。当是时也，商君佐之，内立法度，务耕织，修守战之具；外连衡而斗诸侯。于是秦人拱手而取西河之外。 </p><p>　　孝公既没，惠文、武、昭襄蒙故业，因遗策，南取汉中，西举巴、蜀，东割膏腴之地，北收要害之郡。诸侯恐惧，会盟而谋弱秦，不爱珍器重宝肥饶之地，以致天下之士，合从缔交，相与为一。当此之时，齐有孟尝，赵有平原，楚有春申，魏有信陵。此四君者，皆明智而忠信，宽厚而爱人，尊贤而重士，约从离衡，兼韩、魏、燕、楚、齐、赵、宋、卫、中山之众。于是六国之士，有宁越、徐尚、苏秦、杜赫之属为之谋，齐明、周最、陈轸、召滑、楼缓、翟景、苏厉、乐毅之徒通其意，吴起、孙膑、带佗、倪良、王廖、田忌、廉颇、赵奢之伦制其兵。尝以十倍之地，百万之众，叩关而攻秦。秦人开关延敌，九国之师，逡巡而不敢进。秦无亡矢遗镞之费，而天下诸侯已困矣。于是从散约败，争割地而赂秦。秦有余力而制其弊，追亡逐北，伏尸百万，流血漂橹。因利乘便，宰割天下，分裂山河。强国请服，弱国入朝。 </p><p>　　延及孝文王、庄襄王，享国之日浅，国家无事。及至始皇，奋六世之余烈，振长策而御宇内，吞二周而亡诸侯，履至尊而制六合，执敲扑而鞭笞天下，威振四海。南取百越之地，以为桂林、象郡；百越之君，俯首系颈，委命下吏。乃使蒙恬北筑长城而守藩篱，却匈奴七百余里。胡人不敢南下而牧马，士不敢弯弓而报怨。于是废先王之道，焚百家之言，以愚黔首；隳名城，杀豪杰，收天下之兵，聚之咸阳，销锋镝，铸以为金人十二，以弱天下之民。然后践华为城，因河为池，据亿丈之城，临不测之渊，以为固。良将劲弩守要害之处，信臣精卒陈利兵而谁何。天下已定，始皇之心，自以为关中之固，金城千里，子孙帝王万世之业也。</p><p>　　始皇既没，余威震于殊俗。然陈涉瓮牖绳枢之子，氓隶之人，而迁徙之徒也；才能不及中人，非有仲尼、墨翟之贤，陶朱、猗顿之富；蹑足行伍之间，而倔起阡陌之中，率疲弊之卒，将数百之众，转而攻秦，斩木为兵，揭竿为旗，天下云集响应，赢粮而景从。山东豪俊遂并起而亡秦族矣。 </p><p>　　且夫天下非小弱也，雍州之地，崤函之固，自若也。陈涉之位，非尊于齐、楚、燕、赵、韩、魏、宋、卫、中山之君也；锄櫌棘矜，非铦于钩戟长铩也；谪戍之众，非抗于九国之师也；深谋远虑，行军用兵之道，非及向时之士也。然而成败异变，功业相反，何也？试使山东之国与陈涉度长絜大，比权量力，则不可同年而语矣。然秦以区区之地，致万乘之势，序八州而朝同列，百有余年矣；然后以六合为家，崤函为宫；一夫作难而七庙隳，身死人手，为天下笑者，何也？仁义不施而攻守之势异也。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;秦以耕战立国，起于西北，从边陲弱小诸侯，借以天时地利，用商鞅、李斯，举法家大旗。而后逐鹿中原，坑赵括，灌大梁，掳六国贵族，结束战乱，建立大一统帝国。然而，强秦却二世而亡。“始皇之心，自以为关中之固，金城千里，子孙帝王万世之业也。”封建王朝的皇帝总觉得自己的国祚能够绵延不绝，历史却一次次地打他们的脸。&lt;br&gt;&lt;img src=&quot;/img/just-a-joke.png&quot; alt=&quot;just a joke&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="扯淡" scheme="https://xmfbit.github.io/tags/%E6%89%AF%E6%B7%A1/"/>
    
  </entry>
  
  <entry>
    <title>Ubuntu/Mac 工具软件列表</title>
    <link href="https://xmfbit.github.io/2017/10/22/useful-tools-list/"/>
    <id>https://xmfbit.github.io/2017/10/22/useful-tools-list/</id>
    <published>2017-10-22T12:12:09.000Z</published>
    <updated>2018-03-14T06:04:00.137Z</updated>
    
    <content type="html"><![CDATA[<p>工作环境大部分在Ubuntu和MacOS上进行，这里是一个我觉得在这两个平台上很有用的工具的整理列表，大部分都可以在两个系统上找到。这里并不会给出具体安装方式，因为最好的文档总是软件的官方Document或者GitHub的README。<br><a id="more"></a></p><h2 id="zsh和Oh-my-zsh"><a href="#zsh和Oh-my-zsh" class="headerlink" title="zsh和Oh-my-zsh"></a>zsh和Oh-my-zsh</h2><p>如果经常在终端敲命令而且还在用系统自带的Bash？可以考虑试一下zsh替代bash，并使用<a href="https://github.com/robbyrussell/oh-my-zsh" target="_blank" rel="external">oh-my-zsh</a>武装zsh。</p><p>关于oh-my-zsh的帖子网上已经有很多，不过我还并没有用到太多的功能。oh-my-zsh中可以配置插件，不过我只是使用了<code>colored-man-pages</code>。顾名思义，它可以将使用<code>man</code>查询时的页面彩色输出。如下所示。<br><img src="/img/useful_tools_colored_man_pages.jpg" alt="彩色的cp man页面"></p><h2 id="autojump"><a href="#autojump" class="headerlink" title="autojump"></a>autojump</h2><p>使用<a href="https://github.com/wting/autojump" target="_blank" rel="external">autojump</a>，可以很方便地在已经访问过的文件夹间快速跳转。甚至都不需要输入目标文件夹的全名，支持自动联想。</p><p>除了自动跳转功能，我还将其作为终端到文件资源管理器(Mac: Finder)的跳转功能。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"># 跳转到path并使用文件资源管理器打开</div><div class="line">jo path</div></pre></td></tr></table></figure></p><h2 id="tldr"><a href="#tldr" class="headerlink" title="tldr"></a>tldr</h2><p><a href="https://github.com/tldr-pages/tldr" target="_blank" rel="external">tldr</a> (too long don’t read)是一款能够给出bash命令常用功能的工具。在Linux系统中，很多命令都有一长串参数。这其中很多都是不常用的。而我们使用时，常常是使用某几个常见的功能选项。tldr就能够给出命令的简要描述和例子。</p><p>例如，使用其查询<code>tar</code>的常用方法：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line">tldr tar</div><div class="line"><span class="comment"># output</span></div><div class="line">tar</div><div class="line"></div><div class="line">Archiving utility.</div><div class="line">Often combined with a compression method, such as gzip or bzip.</div><div class="line"></div><div class="line">- Create an archive from files:</div><div class="line">    tar cf target.tar file1 file2 file3</div><div class="line"></div><div class="line">- Create a gzipped archive:</div><div class="line">    tar czf target.tar.gz file1 file2 file3</div><div class="line"></div><div class="line">- Extract an archive <span class="keyword">in</span> a target folder:</div><div class="line">    tar xf source.tar -C folder</div><div class="line"></div><div class="line">- Extract a gzipped archive <span class="keyword">in</span> the current directory:</div><div class="line">    tar xzf source.tar.gz</div><div class="line"></div><div class="line">- Extract a bzipped archive <span class="keyword">in</span> the current directory:</div><div class="line">    tar xjf source.tar.bz2</div><div class="line"></div><div class="line">- Create a compressed archive, using archive suffix to determine the compression program:</div><div class="line">    tar caf target.tar.xz file1 file2 file3</div><div class="line"></div><div class="line">- List the contents of a tar file:</div><div class="line">    tar tvf source.tar</div></pre></td></tr></table></figure><p>tldr支持多种语言，我使用了python包安装。但是不知为何，tldr在我这里总显示奇怪的背景颜色，看上去很别扭。所以我实际使用的是<a href="https://github.com/lord63/tldr.py" target="_blank" rel="external">tldr-py</a>。</p><h2 id="tmux"><a href="#tmux" class="headerlink" title="tmux"></a>tmux</h2><p>用SSH登录到服务器上时，如果网络连接不稳定或是自己的主机意外断电，会造成正在跑的代码死掉。因为进程是依附于SSH的会话Session的。tmux是一个终端的“分线器”，可以很方便地将正在进行的终端会话detach掉，使其转入后台运行。正是有这一特点，所以我们可以在SSH会话时，新建tmux会话，在其中跑一些耗时很长的代码，而不必担心SSH掉线。当然，也可以将tmux作为一款终端多任务的管理软件，方便地在多个任务中进行跳转。不过这个功能，我更加常用的是下面的guake。</p><p>虽然Ubuntu14.04可以通过<code>apt-get</code>的方式安装tmux，不过为了能够使用一款好用的配置<a href="https://github.com/gpakosz/.tmux" target="_blank" rel="external">oh-my-tmux</a>（要求tmux&gt;=2.1），还是推荐去GitHub上自己编译安装<a href="https://github.com/tmux/tmux" target="_blank" rel="external">tmux</a>。</p><h2 id="guake"><a href="#guake" class="headerlink" title="guake"></a>guake</h2><p><a href="https://github.com/Guake/guake" target="_blank" rel="external">guake</a>是一款Ubuntu上可以方便呼出终端的应用（按下F12，终端将以全屏的方式铺满桌面，F11可以切换全屏或半屏）。</p><h2 id="Dash-Zeal"><a href="#Dash-Zeal" class="headerlink" title="Dash/Zeal"></a>Dash/Zeal</h2><p>Dash是Mac上一款用于查询API文档的软件。在Ubuntu或Windows上，我们可以使用Zeal这个替代软件。Zeal和Dash基本上无缝衔接，但是却是免费的（Mac上的软件真是好贵。。。）。之前我已经写过一篇<a href="https://xmfbit.github.io/2017/08/26/doc2dash-usage/">博客</a>，介绍如何自己制作文档导入Zeal中。</p><h2 id="sshfs"><a href="#sshfs" class="headerlink" title="sshfs"></a>sshfs</h2><p>使用sshfs可以在本地机器上挂载远程服务器某个文件夹。这样，操作本地的该文件夹就相当于操作远程服务器上的该文件夹（小心使用<code>rm</code>）。</p><h2 id="Alfred-Mutate"><a href="#Alfred-Mutate" class="headerlink" title="Alfred/Mutate"></a>Alfred/Mutate</h2><p>Alfred是Mac上一款非常好用的软件，就像蝙蝠侠身边的老管家一样，可以帮你自动化处理很多事情。除了原生<br>功能，还可以自己编写脚本实现扩展。例如查询豆瓣电影，查询ip，计算器等。鉴于这款软件的大名，这里不再多说。</p><p><a href="https://github.com/qdore/Mutate" target="_blank" rel="external">Mutate</a>是Ubuntu上的一款替代软件。同时，它也提供了方便的扩展接口，只需要按照模板编写python/shell代码，可以很方便地将自己的自动化处理功能加入软件中。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;工作环境大部分在Ubuntu和MacOS上进行，这里是一个我觉得在这两个平台上很有用的工具的整理列表，大部分都可以在两个系统上找到。这里并不会给出具体安装方式，因为最好的文档总是软件的官方Document或者GitHub的README。&lt;br&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>学一点PyQT</title>
    <link href="https://xmfbit.github.io/2017/08/29/learn-pyqt/"/>
    <id>https://xmfbit.github.io/2017/08/29/learn-pyqt/</id>
    <published>2017-08-29T06:07:55.000Z</published>
    <updated>2018-03-14T06:04:00.132Z</updated>
    
    <content type="html"><![CDATA[<p>Qt是一个流行的GUI框架，支持C++/Python。这篇文章是我在这两天通过PyQT制作一个串口通信并画图的小程序的时候，阅读<a href="http://zetcode.com/gui/pyqt5/introduction/" target="_blank" rel="external">PyQT5的一篇教程</a>时候的记录。<br><a id="more"></a></p><h2 id="主要模块"><a href="#主要模块" class="headerlink" title="主要模块"></a>主要模块</h2><p>PyQt5中的主要三个模块如下：</p><ul><li><code>QtCore</code>: 和GUI无关的核心功能：文件，时间，多线程等</li><li><code>QtGui</code>：和GUI相关的的东西：事件处理，2D图形，字体和文本等</li><li><code>QtWidget</code>：GUI中的相关组件，例如按钮，窗口等。</li></ul><p>其他模块还有<code>QtBluetooth</code>，<code>QtNetwork</code>等，都是比较专用的模块，用到再说。</p><h2 id="HelloWorld"><a href="#HelloWorld" class="headerlink" title="HelloWorld"></a>HelloWorld</h2><p>这里首先给出一段简单的程序，可以在桌面上显示一个窗口。<br><figure class="highlight py"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> sys</div><div class="line"><span class="keyword">from</span> PyQt5.QtWidgets <span class="keyword">import</span> QApplication, QWidget</div><div class="line"></div><div class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</div><div class="line">    app = QApplication(sys.argv)</div><div class="line">    w = QWidget()</div><div class="line">    w.resize(<span class="number">250</span>, <span class="number">150</span>)</div><div class="line">    w.move(<span class="number">300</span>, <span class="number">300</span>)</div><div class="line">    w.setWindowTitle(<span class="string">'Simple'</span>)</div><div class="line">    w.show()</div><div class="line"></div><div class="line">    sys.exit(app.exec_())</div></pre></td></tr></table></figure></p><p>下面介绍上面代码的含义：<br><figure class="highlight py"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">app = QApplication(sys.argv)</div></pre></td></tr></table></figure></p><p>每个Qt5应用必须首先创建一个application，后面会用到。<br><figure class="highlight py"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">w = QWidget()</div><div class="line">w.resize(<span class="number">250</span>, <span class="number">150</span>)</div><div class="line">w.move(<span class="number">300</span>, <span class="number">300</span>)</div><div class="line">w.setWindowTitle(<span class="string">'Simple'</span>)</div><div class="line">w.show()</div></pre></td></tr></table></figure></p><p><code>QtWidget</code>是所有组件的父类，我们创建了一个<code>Widget</code>。没有任何parent widget的Widget会作为窗口出现。接下来，调用其成员函数实现调整大小等功能。最后使用<code>show()</code>将其显示出来。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sys.exit(app.exec_())</div></pre></td></tr></table></figure><p>进入application的主循环，等待事件的触发。当退出程序（也许是通过Ctrl+C实现的）或者关闭窗口（点击关闭）后，主循环退出。</p><h2 id="添加一个按钮"><a href="#添加一个按钮" class="headerlink" title="添加一个按钮"></a>添加一个按钮</h2><p>下面，我们为窗口添加按钮，并为其添加事件响应动作。</p><p>参考文档可知，按钮<code>QPushButton</code>存在这样的构造函数：<br><figure class="highlight py"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">__init__ (self, QWidget parent = <span class="keyword">None</span>)</div></pre></td></tr></table></figure></p><p>下面的代码在初始化<code>QPushButton</code>实例<code>btn</code>时，将<code>self</code>作为参数传入，指定了其parent。另外，在指定按钮大小的时候，使用了<code>sizeHint()</code>方法自适应调节其大小。</p><p>同时，为按钮关联了点击动作。Qt中的事件响应机制通过信号和槽实现。点击事件一旦发生，信号<code>clicked</code>会被释放。然后槽相对的处理函数被调用。所谓的槽可以使PyQt提供的slot，或者是任何Python的可调用对象（函数或者实现了<code>__call__()</code>方法的对象）。</p><p>我们调用了现成的处理函数，来达到关闭窗口的目的。使用<code>instance()</code>可以得到当前application实例，调用其<code>quit()</code>方法即是退出当前应用，自然窗口就被关闭了。<br><figure class="highlight py"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> sys</div><div class="line"><span class="keyword">from</span> PyQt5.QtWidgets <span class="keyword">import</span> QApplication, QWidget, QPushButton, QToolTip</div><div class="line"><span class="keyword">from</span> PyQt5.QtCore <span class="keyword">import</span> QCoreApplication</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyWindow</span><span class="params">(QWidget)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></div><div class="line">        super(MyWindow, self).__init__()</div><div class="line">        self._init_ui()</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_init_ui</span><span class="params">(self)</span>:</span></div><div class="line">        btn = QPushButton(<span class="string">'quit'</span>, self)</div><div class="line">        btn.clicked.connect(QCoreApplication.instance().quit)</div><div class="line">        btn.setToolTip(<span class="string">'This is a &lt;b&gt;QPushButton&lt;/b&gt; widget'</span>)</div><div class="line">        btn.move(<span class="number">50</span>, <span class="number">50</span>)</div><div class="line">        btn.resize(btn.sizeHint())</div><div class="line"></div><div class="line">        self.setGeometry(<span class="number">300</span>, <span class="number">300</span>, <span class="number">300</span>, <span class="number">200</span>)</div><div class="line">        self.setWindowTitle(<span class="string">'Window with Button'</span>)</div><div class="line">        self.show()</div><div class="line"></div><div class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</div><div class="line">    app = QApplication(sys.argv)</div><div class="line">    window = MyWindow()</div><div class="line">    sys.exit(app.exec_())</div></pre></td></tr></table></figure></p><h2 id="使用Event处理事件"><a href="#使用Event处理事件" class="headerlink" title="使用Event处理事件"></a>使用Event处理事件</h2><p>除了上述的信号和槽的处理方式，也可以使用Event相关的类进行处理。下面的代码在关闭窗口时弹出对话框确认是否关闭。根据用户做出的选择，调用<code>event.accept()</code>或<code>ignore()</code>完成对事件的处理。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> sys</div><div class="line"><span class="keyword">from</span> PyQt5.QtWidgets <span class="keyword">import</span> QWidget, QMessageBox, QApplication</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyWindow</span><span class="params">(QWidget)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></div><div class="line">        super(MyWindow, self).__init__()</div><div class="line">        self._init_ui()</div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_init_ui</span><span class="params">(self)</span>:</span></div><div class="line">        self.setGeometry(<span class="number">300</span>, <span class="number">300</span>, <span class="number">300</span>, <span class="number">200</span>)</div><div class="line">        self.show()</div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">closeEvent</span><span class="params">(self, ev)</span>:</span></div><div class="line">        reply = QMessageBox.question(self, <span class="string">'Message'</span>, <span class="string">'Are you sure?'</span>,</div><div class="line">                    QMessageBox.Yes | QMessageBox.No, QMessageBox.No)</div><div class="line">        <span class="keyword">if</span> reply == QMessageBox.Yes:</div><div class="line">            ev.accept()</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            ev.ignore()</div><div class="line"></div><div class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</div><div class="line">    app = QApplication(sys.argv)</div><div class="line">    win = MyWindow()</div><div class="line">    sys.exit(app.exec_())</div></pre></td></tr></table></figure><h2 id="使用Layout组织Widget"><a href="#使用Layout组织Widget" class="headerlink" title="使用Layout组织Widget"></a>使用Layout组织Widget</h2><p>组织Widget的方式可以通过绝对位置调整，但是更推荐使用<code>Layout</code>组织。</p><p>绝对位置是通过指定像素多少来确定widget的大小和位置。这样的话，有以下几个缺点：</p><ul><li>不同平台可能显示效果不统一；</li><li>当parent resize的时候，widget大小和位置并不会自动调整</li><li>编码太麻烦，牵一发而动全身</li></ul><p>下面介绍几种常见的<code>Layout</code>类。</p><h3 id="Box-Layout"><a href="#Box-Layout" class="headerlink" title="Box Layout"></a>Box Layout</h3><p>有<code>QVBoxLayout</code>和<code>QHBoxLayout</code>，用来将widget水平或者竖直排列起来。下面的代码通过这两个layout将按钮放置在窗口的右下角。关键的地方在于使用<code>addSkretch()</code>方法将一个<code>QSpacerItem</code>实例对象插入到了layout中，占据了相应位置。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> sys</div><div class="line"><span class="keyword">from</span> PyQt5.QtWidgets <span class="keyword">import</span> (QWidget, QPushButton,</div><div class="line">    QHBoxLayout, QVBoxLayout, QApplication)</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyWindow</span><span class="params">(QWidget)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></div><div class="line">        super(MyWindow, self).__init__()</div><div class="line">        self._init_ui()</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_init_ui</span><span class="params">(self)</span>:</span></div><div class="line">        okButton = QPushButton(<span class="string">"OK"</span>)</div><div class="line">        cancelButton = QPushButton(<span class="string">"Cancel"</span>)</div><div class="line"></div><div class="line">        hbox = QHBoxLayout()</div><div class="line">        hbox.addStretch(<span class="number">1</span>)</div><div class="line">        hbox.addWidget(okButton)</div><div class="line">        hbox.addWidget(cancelButton)</div><div class="line"></div><div class="line">        vbox = QVBoxLayout()</div><div class="line">        vbox.addStretch(<span class="number">1</span>)</div><div class="line">        vbox.addLayout(hbox)</div><div class="line"></div><div class="line">        self.setLayout(vbox)    </div><div class="line"></div><div class="line">        self.setGeometry(<span class="number">300</span>, <span class="number">300</span>, <span class="number">300</span>, <span class="number">150</span>)</div><div class="line">        self.setWindowTitle(<span class="string">'Buttons'</span>)    </div><div class="line">        self.show()</div><div class="line"></div><div class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</div><div class="line">    app = QApplication(sys.argv)</div><div class="line">    win = MyWindow()</div><div class="line">    sys.exit(app.exec_())</div></pre></td></tr></table></figure><h3 id="Grid-Layout"><a href="#Grid-Layout" class="headerlink" title="Grid Layout"></a>Grid Layout</h3><p><code>QGridLayout</code>将空间划分为行列的grid。在向其中添加item的时候，要指定位置。如下，将5行4列的grid设置为计算器的面板模式。<br><figure class="highlight py"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> sys</div><div class="line"><span class="keyword">from</span> PyQt5.QtWidgets <span class="keyword">import</span> (QWidget, QGridLayout,</div><div class="line">    QPushButton, QApplication)</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyWindow</span><span class="params">(QWidget)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></div><div class="line">        super(MyWindow, self).__init__()</div><div class="line">        self._init_ui()</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_init_ui</span><span class="params">(self)</span>:</span></div><div class="line">        grid = QGridLayout()</div><div class="line">        self.setLayout(grid)</div><div class="line">        names = [<span class="string">'Cls'</span>, <span class="string">'Bck'</span>, <span class="string">''</span>, <span class="string">'Close'</span>,</div><div class="line">                 <span class="string">'7'</span>, <span class="string">'8'</span>, <span class="string">'9'</span>, <span class="string">'/'</span>,</div><div class="line">                <span class="string">'4'</span>, <span class="string">'5'</span>, <span class="string">'6'</span>, <span class="string">'*'</span>,</div><div class="line">                 <span class="string">'1'</span>, <span class="string">'2'</span>, <span class="string">'3'</span>, <span class="string">'-'</span>,</div><div class="line">                <span class="string">'0'</span>, <span class="string">'.'</span>, <span class="string">'='</span>, <span class="string">'+'</span>]</div><div class="line">        positions = [(i,j) <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">5</span>) <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">4</span>)]</div><div class="line">        <span class="keyword">for</span> position, name <span class="keyword">in</span> zip(positions, names):</div><div class="line">            <span class="keyword">if</span> name == <span class="string">''</span>:</div><div class="line">                <span class="keyword">continue</span></div><div class="line">            button = QPushButton(name)</div><div class="line">            grid.addWidget(button, *position)</div><div class="line"></div><div class="line">        self.move(<span class="number">300</span>, <span class="number">150</span>)</div><div class="line">        self.setWindowTitle(<span class="string">'Calculator'</span>)</div><div class="line">        self.show()</div><div class="line"></div><div class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</div><div class="line"></div><div class="line">    app = QApplication(sys.argv)</div><div class="line">    win = MyWindow()</div><div class="line">    sys.exit(app.exec_())</div></pre></td></tr></table></figure></p><p>另外，我们还可以通过<code>setSpacing()</code>方法设置每个单元格之间的间隔。如果某个widget需要占据多个单元格，可以在<code>addWidget()</code>方法中指定要扩展的行列数。</p><h2 id="事件驱动"><a href="#事件驱动" class="headerlink" title="事件驱动"></a>事件驱动</h2><p>PyQt提供了两种事件驱动的处理方式：</p><ul><li>使用<code>event</code>句柄。事件可能是由于UI交互或者定时器等引起，由接收对象进行处理。</li><li>信号和槽。某个widge交互时，释放相应信号，被槽对应的函数捕获进行处理。</li></ul><p>信号和槽可以见上面使用按钮关闭窗口的例子，关键在于调用信号的<code>connect()</code>函数将其绑定到某个槽上。Python中的可调用对象都可以作为槽。</p><p>而使用event句柄处理时，需要重写override原来的处理函数，见上面使用其在关闭窗口时进行弹窗确认的例子。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Qt是一个流行的GUI框架，支持C++/Python。这篇文章是我在这两天通过PyQT制作一个串口通信并画图的小程序的时候，阅读&lt;a href=&quot;http://zetcode.com/gui/pyqt5/introduction/&quot;&gt;PyQT5的一篇教程&lt;/a&gt;时候的记录。&lt;br&gt;
    
    </summary>
    
    
      <category term="python" scheme="https://xmfbit.github.io/tags/python/"/>
    
      <category term="qt" scheme="https://xmfbit.github.io/tags/qt/"/>
    
  </entry>
  
  <entry>
    <title>doc2dash——制作自己的dash文档</title>
    <link href="https://xmfbit.github.io/2017/08/26/doc2dash-usage/"/>
    <id>https://xmfbit.github.io/2017/08/26/doc2dash-usage/</id>
    <published>2017-08-26T11:32:00.000Z</published>
    <updated>2018-03-14T06:04:00.127Z</updated>
    
    <content type="html"><![CDATA[<p>Dash是Mac上一款超棒的应用，提供了诸如C/C++/Python甚至是OpenCV/Vim等软件包或工具软件的参考文档。只要使用App的“Download Docsets”功能，就能轻松下载相应文档。使用的时候只需在Dash的搜索框内输入相应关键词，Dash会在所有文档中进行搜索，给出相关内容的列表。点击我们要寻找的条款，就能够直接在本地阅读文档。在Ubuntu/Windows平台上，Dash也有对应的替代品，例如<a href="https://zealdocs.org" target="_blank" rel="external">zeal</a>就是一款Windows/Linux平台通用的Dash替代软件。</p><p>这样强大的软件，如果只能使用官方提供的文档岂不是有些大材小用？<a href="https://doc2dash.readthedocs.io/en/stable/" target="_blank" rel="external">doc2dash</a>就是一款能够自动生成Dash兼容docset文件的工具。例如，可以使用它为PyTorch生成本地化的docset文件，并导入Dash/zeal中，在本地进行搜索阅读。这不是美滋滋？</p><p>本文章是基于doc2dash的官方介绍，对其使用进行的总结。<br><img src="/img/doc2dash_pytorch_example.jpg" alt="Demo"></p><a id="more"></a><h2 id="安装doc2dash"><a href="#安装doc2dash" class="headerlink" title="安装doc2dash"></a>安装doc2dash</h2><p>doc2dash是基于Python开发的。按照官方网站介绍，为了避免Python包的冲突，最好使用虚拟环境进行安装。我的机器上安装有Anaconda环境，所以首先使用<code>conda create</code>命令新建用于doc2dash的虚拟环境。<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">conda create -n doc2dash</div></pre></td></tr></table></figure></p><p>接下来，激活虚拟环境，并使用<code>pip install</code>命令安装。<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="built_in">source</span> activate doc2dash</div><div class="line">pip install doc2dash</div></pre></td></tr></table></figure></p><p>doc2dash支持的输出格式可以通过sphinx或者pydoctor。其中前者更加常用。下面以PyTorch项目的文档生成为例，介绍doc2dash的具体用法。</p><h2 id="生成PyTorch文档"><a href="#生成PyTorch文档" class="headerlink" title="生成PyTorch文档"></a>生成PyTorch文档</h2><p>doc2dash使用sphinx生成相应的文档。在上述安装doc2dash的过程中，应该已经安装了sphinx包。不过我们还需要手动安装，以便处理rst文档。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">pip install sphinx_rtd_theme</div></pre></td></tr></table></figure><p>进入PyTorch的文档目录<code>docs/</code>，PyTorch已经为我们提供了Makefile，调用sphinx包进行文档处理，可以选择<code>make html</code>命令生成相应的HTML文档，生成的位置为<code>build/html</code>。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"># in directory $PYTORCH/docs, run</div><div class="line">make html</div></pre></td></tr></table></figure><p>接下来，就可以使用doc2dash来继续sphinx的工作，生成Dash可用的文档文件了~使用<code>-n</code>指定生成的文件名称，后面跟source文件夹路径即可。</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># $PYTORCH/docs/build/html即为生成的HTML目录</span></div><div class="line">doc2dash -n pytorch <span class="variable">$PYTORCH</span>/docs/build/html</div></pre></td></tr></table></figure><p>之后，把生成的<code>pytorch.docset</code>导入到Dash中即可。如下图所示，点击“+”找到文件添加即可。<br><img src="/img/doc2dash_how_to_add_docset.jpg" alt="添加docset"></p><h2 id="在Ubuntu上安装zeal"><a href="#在Ubuntu上安装zeal" class="headerlink" title="在Ubuntu上安装zeal"></a>在Ubuntu上安装zeal</h2><p>zeal是Dash在非Mac平台上的替代软件。在Ubuntu上可以使用如下方式轻松安装（见<a href="https://zealdocs.org/download.html#linux" target="_blank" rel="external">官方网站介绍</a>）。</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">sudo add-apt-repository ppa:zeal-developers/ppa</div><div class="line">sudo apt-get update</div><div class="line">sudo apt-get install zeal</div></pre></td></tr></table></figure><p>安装后，可以使用<code>Tool/Docsets</code>下载相应的公开文档。如果想要添加自己生成的文档，只需要将生成的docset文件放到软件的文档库中即可，默认位置应在<code>$HOME/.local/share/Zeal/Zeal/docsets</code>。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Dash是Mac上一款超棒的应用，提供了诸如C/C++/Python甚至是OpenCV/Vim等软件包或工具软件的参考文档。只要使用App的“Download Docsets”功能，就能轻松下载相应文档。使用的时候只需在Dash的搜索框内输入相应关键词，Dash会在所有文档中进行搜索，给出相关内容的列表。点击我们要寻找的条款，就能够直接在本地阅读文档。在Ubuntu/Windows平台上，Dash也有对应的替代品，例如&lt;a href=&quot;https://zealdocs.org&quot;&gt;zeal&lt;/a&gt;就是一款Windows/Linux平台通用的Dash替代软件。&lt;/p&gt;
&lt;p&gt;这样强大的软件，如果只能使用官方提供的文档岂不是有些大材小用？&lt;a href=&quot;https://doc2dash.readthedocs.io/en/stable/&quot;&gt;doc2dash&lt;/a&gt;就是一款能够自动生成Dash兼容docset文件的工具。例如，可以使用它为PyTorch生成本地化的docset文件，并导入Dash/zeal中，在本地进行搜索阅读。这不是美滋滋？&lt;/p&gt;
&lt;p&gt;本文章是基于doc2dash的官方介绍，对其使用进行的总结。&lt;br&gt;&lt;img src=&quot;/img/doc2dash_pytorch_example.jpg&quot; alt=&quot;Demo&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="tool" scheme="https://xmfbit.github.io/tags/tool/"/>
    
  </entry>
  
  <entry>
    <title>使用IPDB调试Python代码</title>
    <link href="https://xmfbit.github.io/2017/08/21/debugging-with-ipdb/"/>
    <id>https://xmfbit.github.io/2017/08/21/debugging-with-ipdb/</id>
    <published>2017-08-21T07:53:16.000Z</published>
    <updated>2018-03-14T06:04:00.126Z</updated>
    
    <content type="html"><![CDATA[<p>IPDB是什么？IPDB（Ipython Debugger），和GDB类似，是一款集成了Ipython的Python代码命令行调试工具，可以看做PDB的升级版。这篇文章总结IPDB的使用方法，主要是若干命令的使用。更多详细的教程或文档还请参考Google。<br><a id="more"></a></p><h2 id="安装与使用"><a href="#安装与使用" class="headerlink" title="安装与使用"></a>安装与使用</h2><p>IPDB以Python第三方库的形式给出，使用<code>pip install ipdb</code>即可轻松安装。</p><p>在使用时，有两种常见方式。</p><h3 id="集成到源代码中"><a href="#集成到源代码中" class="headerlink" title="集成到源代码中"></a>集成到源代码中</h3><p>通过在代码开头导入包，可以直接在代码指定位置插入断点。如下所示：<br><figure class="highlight py"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> ipdb</div><div class="line"><span class="comment"># some code</span></div><div class="line">x = <span class="number">10</span></div><div class="line">ipdb.set_trace()</div><div class="line">y = <span class="number">20</span></div><div class="line"><span class="comment"># other code</span></div></pre></td></tr></table></figure></p><p>则程序会在执行完<code>x = 10</code>这条语句之后停止，展开Ipython环境，就可以自由地调试了。</p><h3 id="命令式"><a href="#命令式" class="headerlink" title="命令式"></a>命令式</h3><p>上面的方法很方便，但是也有不灵活的缺点。对于一段比较棘手的代码，我们可能需要按步执行，边运行边跟踪代码流并进行调试，这时候使用交互式的命令式调试方法更加有效。启动IPDB调试环境的方法也很简单：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">python -m ipdb your_code.py</div></pre></td></tr></table></figure></p><h2 id="常用命令"><a href="#常用命令" class="headerlink" title="常用命令"></a>常用命令</h2><p>IPDB调试环境提供的常见命令有：</p><h3 id="帮助"><a href="#帮助" class="headerlink" title="帮助"></a>帮助</h3><p>帮助文档就是这样一个东西：当你写的时候觉得这TM也要写？当你看别人的东西的时候觉得这TM都没写？</p><p>使用<code>h</code>即可调出IPDB的帮助。可以使用<code>help command</code>的方法查询特定命令的具体用法。</p><h3 id="下一条语句"><a href="#下一条语句" class="headerlink" title="下一条语句"></a>下一条语句</h3><p>使用<code>n</code>(next)执行下一条语句。注意一个函数调用也是一个语句。如何能够实现类似“进入函数内部”的功能呢？</p><h3 id="进入函数内部"><a href="#进入函数内部" class="headerlink" title="进入函数内部"></a>进入函数内部</h3><p>使用<code>s</code>(step into)进入函数调用的内部。</p><h3 id="打断点"><a href="#打断点" class="headerlink" title="打断点"></a>打断点</h3><p>使用<code>b line_number</code>(break)的方式给指定的行号位置加上断点。使用<code>b file_name:line_number</code>的方法给指定的文件（还没执行到的代码可能在外部文件中）中指定行号位置打上断点。</p><p>另外，打断点还支持指定条件下进入，可以查询帮助文档。</p><h3 id="一直执行直到遇到下一个断点"><a href="#一直执行直到遇到下一个断点" class="headerlink" title="一直执行直到遇到下一个断点"></a>一直执行直到遇到下一个断点</h3><p>使用<code>c</code>(continue)执行代码直到遇到某个断点或程序执行完毕。</p><h3 id="一直执行直到返回"><a href="#一直执行直到返回" class="headerlink" title="一直执行直到返回"></a>一直执行直到返回</h3><p>使用<code>r</code>(return)执行代码直到当前所在的这个函数返回。</p><h3 id="跳过某段代码"><a href="#跳过某段代码" class="headerlink" title="跳过某段代码"></a>跳过某段代码</h3><p>使用<code>j line_number</code>(jump)可以跳过某段代码，直接执行指定行号所在的代码。</p><h3 id="更多上下文"><a href="#更多上下文" class="headerlink" title="更多上下文"></a>更多上下文</h3><p>在IPDB调试环境中，默认只显示当前执行的代码行，以及其上下各一行的代码。如果想要看到更多的上下文代码，可以使用<code>l first[, second]</code>(list)命令。</p><p>其中<code>first</code>指示向上最多显示的行号，<code>second</code>指示向下最多显示的行号（可以省略）。当<code>second</code>小于<code>first</code>时，<code>second</code>指的是从<code>first</code>开始的向下的行数（相对值vs绝对值）。</p><p>根据<a href="https://stackoverflow.com/questions/6240887/how-can-i-make-ipdb-show-more-lines-of-context-while-debugging" target="_blank" rel="external">SO上的这个问题</a>，你还可以修改IPDB的源码，一劳永逸地改变上下文的行数。</p><h3 id="我在哪里"><a href="#我在哪里" class="headerlink" title="我在哪里"></a>我在哪里</h3><p>调试兴起，可能你会忘了自己目前所在的行号。例如在打印了若干变量值后，屏幕完全被这些值占据。使用<code>w</code>或者<code>where</code>可以打印出目前所在的行号位置以及上下文信息。</p><h3 id="这是啥"><a href="#这是啥" class="headerlink" title="这是啥"></a>这是啥</h3><p>我们可以使用<code>whatis variable_name</code>的方法，查看变量的类别（感觉有点鸡肋，用<code>type</code>也可以办到）。</p><h3 id="列出当前函数的全部参数"><a href="#列出当前函数的全部参数" class="headerlink" title="列出当前函数的全部参数"></a>列出当前函数的全部参数</h3><p>当你身处一个函数内部的时候，可以使用<code>a</code>(argument)打印出传入函数的所有参数的值。</p><h3 id="打印"><a href="#打印" class="headerlink" title="打印"></a>打印</h3><p>使用<code>p</code>(print)和<code>pp</code>(pretty print)可以打印表达式的值。</p><h3 id="清除断点"><a href="#清除断点" class="headerlink" title="清除断点"></a>清除断点</h3><p>使用<code>cl</code>或者<code>clear file:line_number</code>清除断点。如果没有参数，则清除所有断点。</p><h3 id="再来一次"><a href="#再来一次" class="headerlink" title="再来一次"></a>再来一次</h3><p>使用<code>restart</code>重新启动调试器，断点等信息都会保留。<code>restart</code>实际是<code>run</code>的别名，使用<code>run args</code>的方式传入参数。</p><h3 id="退出"><a href="#退出" class="headerlink" title="退出"></a>退出</h3><p>使用<code>q</code>退出调试，并清除所有信息。</p><p>当然，这并不是IPDB的全部。其他的命令还请参照帮助文档。文档在手，天下我有！</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;IPDB是什么？IPDB（Ipython Debugger），和GDB类似，是一款集成了Ipython的Python代码命令行调试工具，可以看做PDB的升级版。这篇文章总结IPDB的使用方法，主要是若干命令的使用。更多详细的教程或文档还请参考Google。&lt;br&gt;
    
    </summary>
    
    
      <category term="python" scheme="https://xmfbit.github.io/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>Focal Loss论文阅读 - Focal Loss for Dense Object Detection</title>
    <link href="https://xmfbit.github.io/2017/08/14/focal-loss-paper/"/>
    <id>https://xmfbit.github.io/2017/08/14/focal-loss-paper/</id>
    <published>2017-08-14T14:43:55.000Z</published>
    <updated>2018-03-14T06:04:00.129Z</updated>
    
    <content type="html"><![CDATA[<p>Focal Loss这篇文章是He Kaiming和Ross发表在ICCV2017上的文章。关于这篇文章在知乎上有相关的<a href="https://www.zhihu.com/question/63581984" target="_blank" rel="external">讨论</a>。最近一直在做强化学习相关的东西，目标检测方面很长时间不看新的东西了，把自己阅读论文的要点记录如下，也是一次对这方面进展的回顾。</p><p>下图来自于论文，是各种主流模型的比较。其中横轴是前向推断的时间，纵轴是检测器的精度。作者提出的RetinaNet在单独某个维度上都可以吊打其他模型。不过图上没有加入YOLO的对比。YOLO的速度仍然是其一大优势，但是精度和其他方法相比，仍然不高。</p><p><img src="/img/focal_loss_different_model_comparison.jpg" alt="不同模型关于精度和速度的比较"><br><a id="more"></a></p><h2 id="为什么要有Focal-Loss？"><a href="#为什么要有Focal-Loss？" class="headerlink" title="为什么要有Focal Loss？"></a>为什么要有Focal Loss？</h2><p>目前主流的检测算法可以分为两类：one-state和two-stage。前者以YOLO和SSD为代表，后者以RCNN系列为代表。后者的特点是分类器是在一个稀疏的候选目标中进行分类（背景和对应类别），而这是通过前面的proposal过程实现的。例如Seletive Search或者RPN。相对于后者，这种方法是在一个稀疏集合内做分类。与之相反，前者是输出一个稠密的proposal，然后丢进分类器中，直接进行类别分类。后者使用的方法结构一般较为简单，速度较快，但是目前存在的问题是精度不高，普遍不如前者的方法。</p><p>论文作者指出，之所以做稠密分类的后者精度不够高，核心问题（central issus）是稠密proposal中前景和背景的极度不平衡。以我更熟悉的YOLO举例子，比如在PASCAL VOC数据集中，每张图片上标注的目标可能也就几个。但是YOLO V2最后一层的输出是$13 \times 13 \times 5$，也就是$845$个候选目标！大量（简单易区分）的负样本在loss中占据了很大比重，使得有用的loss不能回传回来。</p><p>基于此，作者将经典的交叉熵损失做了变形（见下），给那些易于被分类的简单例子小的权重，给不易区分的难例更大的权重。同时，作者提出了一个新的one-stage的检测器RetinaNet，达到了速度和精度很好地trade-off。</p><script type="math/tex; mode=display">\text{FL}(p_t) = -(1-p_t)^\gamma \log(p_t)</script><h2 id="物体检测的两种主流方法"><a href="#物体检测的两种主流方法" class="headerlink" title="物体检测的两种主流方法"></a>物体检测的两种主流方法</h2><p>在深度学习之前，经典的物体检测方法为滑动窗，并使用人工设计的特征。HoG和DPM等方法是其中比较有名的。</p><p>R-CNN系的方法是目前最为流行的物体检测方法之一，同时也是目前精度最高的方法。在R-CNN系方法中，正负类别不平衡这个问题通过前面的proposal解决了。通过EdgeBoxes，Selective Search，DeepMask，RPN等方法，过滤掉了大多数的背景，实际传给后续网络的proposal的数量是比较少的（1-2K）。</p><p>在YOLO，SSD等方法中，需要直接对feature map的大量proposal（100K）进行检测，而且这些proposal很多都在feature map上重叠。大量的负样本带来两个问题：</p><ul><li>过于简单，有效信息过少，使得训练效率低；</li><li>简单的负样本在训练过程中压倒性优势，使得模型发生退化。</li></ul><p>在Faster-RCNN方法中，Huber Loss被用来降低outlier的影响（较大error的样本，也就是难例，传回来的梯度做了clipping，也只能是$1$）。而FocalLoss是对inner中简单的那些样本对loss的贡献进行限制。即使这些简单样本数量很多，也不让它们在训练中占到优势。</p><h2 id="Focal-Loss"><a href="#Focal-Loss" class="headerlink" title="Focal Loss"></a>Focal Loss</h2><p>Focal Loss从交叉熵损失而来。二分类的交叉熵损失如下：</p><script type="math/tex; mode=display">\text{CE}(p, y) = \begin{cases}-\log(p) \quad &\text{if}\quad y = 1\\ -\log(1-p) &\text{otherwise}\end{cases}</script><p>对应的，多分类的交叉熵损失是这样的：</p><script type="math/tex; mode=display">\text{CE}(p, y) = -\log(p_y)</script><p>如下图所示，蓝色线为交叉熵损失函数随着$p_t$变化的曲线($p_t$意为ground truth，是标注类别所对应的概率)。可以看到，当概率大于$.5$，即认为是易分类的简单样本时，值仍然较大。这样，很多简单样本累加起来，就很可能盖住那些稀少的不易正确分类的类别。<br><img src="/img/focal_loss_vs_ce_loss.jpg" alt="FL vs CELoss"></p><p>为了改善类别样本分布不均衡的问题，已经有人提出了使用加上权重的交叉熵损失，如下（即用参数$\alpha_t$来平衡，这组参数可以是超参数，也可以由类别的比例倒数决定）。作者将其作为比较的baseline。</p><script type="math/tex; mode=display">\text{CE}(p) = -\alpha_t\log(p_t)</script><p>作者提出的则是一个自适应调节的权重，即Focal Loss，定义如下。由上图可以看到$\gamma$取不同值的时候的函数值变化。作者发现，$\gamma=2$时能够获得最佳的效果提升。</p><script type="math/tex; mode=display">\text{FL}(p_t) = -(1-p_t)^\gamma\log(p_t)</script><p>在实际实验中，作者使用的是加权之后的Focal Loss，作者发现这样能够带来些微的性能提升。</p><h2 id="模型初始化"><a href="#模型初始化" class="headerlink" title="模型初始化"></a>模型初始化</h2><p>对于一般的分类网络，初始化之后，往往其输出的预测结果是均等的（随机猜测）。然而作者认为，这种初始化方式在类别极度不均衡的时候是有害的。作者提出，应该初始化模型参数，使得初始化之后，模型输出稀有类别的概率变小（如$0.01$）。作者发现这种初始化方法对于交叉熵损失和Focal Loss的性能提升都有帮助。</p><p>在后续的模型介绍部分，作者较为详细地说明了模型初始化方法。首先，从imagenet预训练得到的base net不做调整，新加入的卷积层权重均初始化为$\sigma=0.01$的高斯分布，偏置项为$0$。对于分类网络的最后一个卷积层，将偏置项置为$b=-\log((1-\pi)/\pi)$。这里的$\pi$参数是一个超参数，其意义是在训练的初始阶段，每个anchor被分类为前景的概率。在实验中，作者实际使用的大小是$0.01$。</p><p>这样进行模型初始化造成的结果就是，在初始阶段，不会产生大量的False Positive，使得训练更加稳定。</p><h2 id="RetinaNet"><a href="#RetinaNet" class="headerlink" title="RetinaNet"></a>RetinaNet</h2><p>作者利用前面介绍的发现和结论，基于ResNet和Feature Pyramid Net（FPN）设计了一种新的one-stage检测框架，命名为RetinaNet。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Focal Loss这篇文章是He Kaiming和Ross发表在ICCV2017上的文章。关于这篇文章在知乎上有相关的&lt;a href=&quot;https://www.zhihu.com/question/63581984&quot;&gt;讨论&lt;/a&gt;。最近一直在做强化学习相关的东西，目标检测方面很长时间不看新的东西了，把自己阅读论文的要点记录如下，也是一次对这方面进展的回顾。&lt;/p&gt;
&lt;p&gt;下图来自于论文，是各种主流模型的比较。其中横轴是前向推断的时间，纵轴是检测器的精度。作者提出的RetinaNet在单独某个维度上都可以吊打其他模型。不过图上没有加入YOLO的对比。YOLO的速度仍然是其一大优势，但是精度和其他方法相比，仍然不高。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/img/focal_loss_different_model_comparison.jpg&quot; alt=&quot;不同模型关于精度和速度的比较&quot;&gt;&lt;br&gt;
    
    </summary>
    
    
      <category term="deep learning" scheme="https://xmfbit.github.io/tags/deep-learning/"/>
    
      <category term="paper" scheme="https://xmfbit.github.io/tags/paper/"/>
    
  </entry>
  
  <entry>
    <title>DELL 游匣7559安装Ubuntu和CUDA记录</title>
    <link href="https://xmfbit.github.io/2017/08/10/install-ubuntu-in-dell/"/>
    <id>https://xmfbit.github.io/2017/08/10/install-ubuntu-in-dell/</id>
    <published>2017-08-10T03:29:38.000Z</published>
    <updated>2018-03-14T06:04:00.131Z</updated>
    
    <content type="html"><![CDATA[<p>虽说开源大法好，但是在我的DELL 游匣7559笔记本上安装Ubuntu+Windows双系统可是耗费了我不少精力。这篇博客是我参考<a href="https://hemenkapadia.github.io/blog/2016/11/11/Ubuntu-with-Nvidia-CUDA-Bumblebee.html" target="_blank" rel="external">这篇文章</a>成功安装Ubuntu16.04和CUDA的记录。感谢上文作者的记录，我才能够最终解决这个问题。基本流程和上文作者相同，只不过没有安装后续的bumblee等工具，所以本文并不是原创，而更多是翻译和备份。<br><img src="/img/install_ubuntu_in_dell_kaiyuandafahao.jpg" alt="开源大法好"><br><a id="more"></a></p><h2 id="蛋疼的过往"><a href="#蛋疼的过往" class="headerlink" title="蛋疼的过往"></a>蛋疼的过往</h2><p>之前我安装过Ubuntu14.04，但是却不支持笔记本的无线网卡，所以一直很不方便。搜索之后才发现，笔记本使用的无线网卡要到Ubuntu15.10以上才有支持，所以想要安装16.04.结果却发现安装界面都进不去。。。</p><h2 id="安装Ubuntu"><a href="#安装Ubuntu" class="headerlink" title="安装Ubuntu"></a>安装Ubuntu</h2><p>我使用的版本号为Ubuntu16.04.3，使用Windows中的UltraISO制作U盘启动盘。在Windows系统中，通过电池计划关闭快速启动功能，之后重启。在开机出现DELL徽标的时候，按下F12进入BIOS，关闭Security Boot选项。按F10保存并重启，选择U盘启动。</p><p>选择“Install Ubuntu”选项，按<code>e</code>，找到包含有<code>quiet splash</code>的那行脚本，将<code>quiet splash</code>替换为以下内容：</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">nomodeset i915.modeset=1 quiet splash</div></pre></td></tr></table></figure><p>之后按F10重启，会进入Ubuntu的安装界面。如何安装Ubuntu这里不再详述。安装完毕之后，重启。出现Ubuntu GRUB引导界面之后，高亮Ubuntu选项（一般来说就是第一个备选项），按<code>e</code>，按照上述方法替换<code>quiet splash</code>。确定可以进入Ubuntu系统并登陆。</p><h2 id="GRUB设置"><a href="#GRUB设置" class="headerlink" title="GRUB设置"></a>GRUB设置</h2><p>下面，修改GRUB设置，避免每次都手动替换。编辑相应配置文件：<code>sudo vi /etc/default/grub</code>，找到包含<code>GRUB_CMDLINE_LINUX_DEFAULT</code>的那一行，将其修改如下（就是将我们上面每次手动输入的内容直接写到了配置里面）：</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">GRUB_CMDLINE_LINUX_DEFAULT=<span class="string">"nomodeset i915.modeset=1 quiet splash"</span></div></pre></td></tr></table></figure><h2 id="更新系统软件"><a href="#更新系统软件" class="headerlink" title="更新系统软件"></a>更新系统软件</h2><p>配置更新源（清华的很好用，非教育网也能轻轻松松上700K），使用如下命令更新，</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">sudo apt-get update</div><div class="line">sudo apt-get upgrade</div><div class="line">sudo apt-get dist-upgrade</div><div class="line">sudo apt-get autoremove</div></pre></td></tr></table></figure><p>参考博客中指出，如果这个过程中让你选GRUB文件，要选择保持原有文件。但是我并没有遇到这个问题。可能是由于我的Ubuntu版本已经是16.04中目前最新的了？</p><p>由于后续有较多的终端文件编辑操作，建议这时候顺便安装Vim。<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sudo apt-get install vim</div></pre></td></tr></table></figure></p><p>更新完之后，重启，确认可以正常登陆系统。</p><h2 id="移除原有的Nvidia和Nouveau驱动"><a href="#移除原有的Nvidia和Nouveau驱动" class="headerlink" title="移除原有的Nvidia和Nouveau驱动"></a>移除原有的Nvidia和Nouveau驱动</h2><p>按下ALT+CTL+F1，进入虚拟终端，首先关闭lightdm服务。这项操作之后会比较经常用到。<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sudo service lightdm stop</div></pre></td></tr></table></figure></p><p>之后，执行卸载操作：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">sudo apt-get remove --purge nvidia*</div><div class="line">sudo apt-get remove --purge bumblebee*</div><div class="line">sudo apt-get --purge remove xserver-xorg-video-nouveau*</div></pre></td></tr></table></figure></p><p>编辑配置文件，<code>/etc/modprobe.d/blacklist.conf</code>，将Nouveau加入到黑名单中：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">blacklist nouveau</div><div class="line">blacklist lbm-nouveau</div><div class="line">alias nouveau off</div><div class="line">alias lbm-nouveau off</div><div class="line">options nouveau modeset=0</div></pre></td></tr></table></figure></p><p>编辑<code>/etc/init/gpu-manager.conf</code>文件，将其前面几行注释掉，改成下面的样子，停止gpu-manager服务：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Comment these start on settings ; GPU Manager ruins our work</span></div><div class="line"><span class="comment">#start on (starting lightdm</span></div><div class="line"><span class="comment">#          or starting kdm</span></div><div class="line"><span class="comment">#          or starting xdm</span></div><div class="line"><span class="comment">#          or starting lxdm)</span></div><div class="line">task</div><div class="line"><span class="built_in">exec</span> gpu-manager --log /var/<span class="built_in">log</span>/gpu-manager.log</div></pre></td></tr></table></figure></p><p>之后，更新initramfs并重启。<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sudo update-initramfs -u -k all</div></pre></td></tr></table></figure></p><p>重启后，确定可以正常登陆系统。并使用下面的命令确定Nouveau被卸载掉了：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 正常情况下，下面的命令应该不产生任何输出</span></div><div class="line">lsmod | grep nouveau</div></pre></td></tr></table></figure></p><p>并确定关闭了gpu-manager服务：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sudo service gpu-manager stop</div></pre></td></tr></table></figure></p><p>至此，Ubuntu系统算是安装完毕了。如果没有使用CUDA的需求，可以从这里开始，安安静静地做一个使用Ubuntu的美男子/小仙女了。<br><img src="/img/install_ubuntu_in_dell_weixiaodaizhepibei.jpg" alt="微笑中带着疲惫"></p><h2 id="安装CUDA"><a href="#安装CUDA" class="headerlink" title="安装CUDA"></a>安装CUDA</h2><p>鉴于国内坑爹的连接资本主义世界的网络环境，建议还是先去Nvidia的官网把CUDA离线安装包下载下来再安装。我使用的是CUDA-8.0-linux.deb安装包。</p><p>按ALT+CTL+F1进入虚拟终端，停止lightdm服务，并安装一些可能要用到的包。<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">sudo service lightdm stop</div><div class="line">sudo apt-get install linux-headers-$(uname -r)</div><div class="line">sudo apt-get install mesa-utils</div></pre></td></tr></table></figure></p><p>安装CUDA包：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">sudo dpkg -i YOUR_CUDA_DEB_PATH</div><div class="line">sudo apt-get update</div><div class="line">sudo apt-get install cuda-8-0</div><div class="line">sudo apt-get autoremove</div></pre></td></tr></table></figure></p><p>安装完毕之后使用<code>sudo reboot</code>重启，确定能够正常登陆系统。</p><p>在这个过程中，作者提到登录界面会出现两次，再次重启之后没有这个问题了。我也遇到了相同的情况。所以，不要慌！</p><h2 id="测试CUDA"><a href="#测试CUDA" class="headerlink" title="测试CUDA"></a>测试CUDA</h2><p>我们来测试一下CUDA。首先，依照你使用shell的不同，将环境变量加入到<code>~/.bashrc</code>或者<code>~/.zshrc</code>中去（不过我相信在经历完这些安装之后应该还没有闲心去搞oh-my-zsh。。。）。<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="built_in">export</span> PATH=<span class="string">"<span class="variable">$PATH</span>:/usr/local/cuda-8.0/bin"</span></div><div class="line"><span class="built_in">export</span> LD_LIBRARY_PATH=<span class="string">"<span class="variable">$LD_LIBRARY_PATH</span>:/usr/local/cuda-8.0/lib"</span></div></pre></td></tr></table></figure></p><p>接下来，我们将使用CUDA自带的example进行测试：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 导入我们刚加入的环境变量</span></div><div class="line"><span class="built_in">source</span> ~/.bashrc</div><div class="line"><span class="built_in">cd</span> /usr/<span class="built_in">local</span>/cuda-8.0/bin</div><div class="line"><span class="comment"># 将CUDA example拷贝到$HOME下</span></div><div class="line">./cuda-install-samples-8.0.sh ~</div><div class="line"><span class="comment"># 进入拷贝到的那个目录 build</span></div><div class="line"><span class="built_in">cd</span> ~/NVIDIA_CUDA-8.0_Samples</div><div class="line">make -j12</div><div class="line"><span class="comment"># 自己挑选几个目录进去运行编译生成的可执行文件测试吧~</span></div></pre></td></tr></table></figure></p><h2 id="Last-But-Not-Least"><a href="#Last-But-Not-Least" class="headerlink" title="Last But Not Least"></a>Last But Not Least</h2><p>安装玩CUDA之后，不要随便更新系统！！！否则可能会损坏你的Kernel和Xserver。<br><img src="/img/install_ubuntu_in_dell_weixiaojiuhao.jpg" alt="微笑就好"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;虽说开源大法好，但是在我的DELL 游匣7559笔记本上安装Ubuntu+Windows双系统可是耗费了我不少精力。这篇博客是我参考&lt;a href=&quot;https://hemenkapadia.github.io/blog/2016/11/11/Ubuntu-with-Nvidia-CUDA-Bumblebee.html&quot;&gt;这篇文章&lt;/a&gt;成功安装Ubuntu16.04和CUDA的记录。感谢上文作者的记录，我才能够最终解决这个问题。基本流程和上文作者相同，只不过没有安装后续的bumblee等工具，所以本文并不是原创，而更多是翻译和备份。&lt;br&gt;&lt;img src=&quot;/img/install_ubuntu_in_dell_kaiyuandafahao.jpg&quot; alt=&quot;开源大法好&quot;&gt;&lt;br&gt;
    
    </summary>
    
    
      <category term="ubuntu" scheme="https://xmfbit.github.io/tags/ubuntu/"/>
    
  </entry>
  
  <entry>
    <title>Effective CPP 阅读 - Chapter 8 定制new和delete</title>
    <link href="https://xmfbit.github.io/2017/07/03/effective-cpp-08/"/>
    <id>https://xmfbit.github.io/2017/07/03/effective-cpp-08/</id>
    <published>2017-07-03T11:47:43.000Z</published>
    <updated>2018-03-14T06:04:00.129Z</updated>
    
    <content type="html"><![CDATA[<p>手动管理内存，这既是C++的优点，也是C++中很容易出问题的地方。本章主要给出分配内存和归还时候的注意事项，主角是<code>operator new</code>和<code>operator delete</code>，配角是new_handler，它在当<code>operator new</code>无法满足客户内存需求时候被调用。</p><p>另外，<code>operator new</code>和<code>operator delete</code>只用于分配单一对象内存。对于数组，应使用<code>operator new[]</code>，并通过<code>operator delete[]</code>归还。除非特别指定，本章中的各项既适用于单一<code>operator new</code>，也适用于<code>operator new[]</code>。</p><p>最后，STL中容器使用的堆内存是由容器拥有的分配器对象（allocator objects）来管理的。本章不讨论。<br><img src="/img/effectivecpp_08_memory_leak_everywherre.jpg" alt="memory_leak_everywhere"><br><a id="more"></a></p><h2 id="49-了解new-handler的行为"><a href="#49-了解new-handler的行为" class="headerlink" title="49 了解new-handler的行为"></a>49 了解new-handler的行为</h2><p>什么是new-handler？当<code>operator new</code>无法满足内存分配需求时，会抛出异常。在抛出异常之前，会先调用一个客户指定的错误处理函数，这就是所谓的new-handler，也就是一个擦屁股的角色。</p><p>为了指定new-handler，必须调用位于标准库<code>&lt;new&gt;</code>的函数<code>set_new_handler</code>。其声明如下：<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">namespace</span> <span class="built_in">std</span> &#123;</div><div class="line">    <span class="function"><span class="keyword">typedef</span> <span class="title">void</span> <span class="params">(*new_handler)</span> <span class="params">()</span></span>;</div><div class="line">    <span class="function">new_handler <span class="title">set_new_handler</span><span class="params">(new_handler p)</span> <span class="title">throw</span><span class="params">()</span></span>;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p><p>其中，传入参数<code>p</code>是你要指定的那个擦屁股函数的指针，返回参数是被取代的那个原始处理函数。<code>throw()</code>表示该函数不抛出任何异常。</p><p>当<code>operator new</code>无法满足内存需求时，会不断调用<code>set_new_handler()</code>，直到找到足够的内存。更加具体的介绍见条款51.</p><p>一个设计良好的new_handler函数可以是以下的设计策略：</p><ul><li>设法找到更多的内存可供使用，以便使得下一次的<code>operator new</code>成功。</li><li>安装另一个new_handler函数。即在其中再次调用<code>set_new_handler</code>，找到其他的擦屁股函数接盘。</li><li>卸载new_handler函数。即将<code>NULL</code>指针传进<code>set_new_handler()</code>中去。这样，<code>operator new</code>会抛出异常。</li><li>抛出<code>bad_alloc</code>（或其派生类）异常。</li><li>不返回（放弃治疗），直接告诉程序exit或abort。</li></ul><p>有的时候想为不同的类定制不同的擦屁股函数。这时候，需要为每个类提供自己的<code>set_new_handler()</code>函数和<code>operator new</code>。如下所示，由于对类的不同对象而言，擦屁股机制都是相同的，所以我们将擦屁股函数声明为类内的静态成员。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">class</span> A &#123;</div><div class="line"><span class="keyword">public</span>:</div><div class="line">    <span class="keyword">static</span> <span class="built_in">std</span>::<span class="function">new_handler <span class="title">set_new_handler</span><span class="params">(<span class="built_in">std</span>::new_handler p)</span> <span class="title">throw</span><span class="params">()</span></span>;</div><div class="line">    <span class="function"><span class="keyword">static</span> <span class="keyword">void</span>* <span class="keyword">operator</span> <span class="title">new</span><span class="params">(<span class="built_in">std</span>::<span class="keyword">size_t</span> size)</span> <span class="title">throw</span><span class="params">(<span class="built_in">std</span>::bad_alloc)</span></span>;</div><div class="line"></div><div class="line"><span class="keyword">private</span>:</div><div class="line">    <span class="keyword">static</span> <span class="built_in">std</span>::new_handler current_handler;</div><div class="line">&#125;;</div><div class="line"></div><div class="line"><span class="comment">// 实现文件</span></div><div class="line"><span class="built_in">std</span>::new_handler A::set_new_handler(<span class="built_in">std</span>::new_handler p) <span class="keyword">throw</span>() &#123;</div><div class="line">    <span class="built_in">std</span>::new_hanlder old = current_handler;</div><div class="line">    current_handler = p;</div><div class="line">    <span class="keyword">return</span> old;</div><div class="line">&#125;</div></pre></td></tr></table></figure><p>静态成员变量必须在类外进行定义（除非是<code>const</code>且为整数型），所以需要在类外定义：<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// 实现文件</span></div><div class="line"><span class="built_in">std</span>::new_handler A::current_handler = <span class="number">0</span>;</div></pre></td></tr></table></figure></p><p>在实现自定义的<code>operator new</code>的时候，首先调用<code>set_new_handler()</code>将自己的擦屁股函数安装为默认，然后调用global的<code>operator new</code>进行内存分配，最后恢复，把原来的擦屁股函数复原回去。书中，作者使用了一个类进行包装，利用类在scope的自动构造与析构，实现了自动化处理：</p><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line">// 这个类实现了自动安装与恢复new_handler</div><div class="line">class Helper &#123;</div><div class="line">public:</div><div class="line">    explicit Helper(std::new_handler p): handler(p) &#123;&#125;</div><div class="line">    ~Helper() &#123;std::set_new_handler(handler); &#125;</div><div class="line">private:</div><div class="line">    std::new_handler handler;</div><div class="line">    // 禁止拷贝构造与赋值</div><div class="line">    Helper(const Helper&amp;);</div><div class="line">    Helper&amp; operator= (const Helper&amp;);</div><div class="line">&#125;;</div><div class="line">// 实现类A自定义的operator new</div><div class="line">void* A::operator new(std::size_t size) throw(std::bad_alloc) &#123;</div><div class="line">    // 存储了函数返回值，也就是原始的 new_handler</div><div class="line">    Helper h(std::set_new_handler(current_handler));</div><div class="line">    return ::operator new(size);</div><div class="line">&#125;</div></pre></td></tr></table></figure><p>新的问题随之而来。如果我们想方便地复用上述代码呢？一个简单的方法是建立一个mixin风格的基类，这种基类用来让派生类继承某个唯一的能力（本例中是设定类的专属new_handler的能力）。而为了让不同的类获得不同的<code>current_handler</code>变量，我们把这个基类做成模板。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt;</div><div class="line"><span class="keyword">class</span> HandlerHelper &#123;</div><div class="line"><span class="keyword">public</span>:</div><div class="line">    <span class="keyword">static</span> <span class="built_in">std</span>::<span class="function">new_handler <span class="title">set_new_handler</span><span class="params">(<span class="built_in">std</span>::new_handler p)</span> <span class="title">throw</span><span class="params">()</span></span>;</div><div class="line">    <span class="function"><span class="keyword">static</span> <span class="keyword">void</span>* <span class="keyword">operator</span> <span class="title">new</span><span class="params">(<span class="built_in">std</span>::<span class="keyword">size_t</span> size)</span> <span class="title">throw</span><span class="params">(<span class="built_in">std</span>::bad_alloc)</span></span>;</div><div class="line">    ... <span class="comment">// 其他的new版本，见条款52</span></div><div class="line"><span class="keyword">private</span>:</div><div class="line">    <span class="keyword">static</span> <span class="built_in">std</span>::new_handler current_handler;</div><div class="line">&#125;;</div><div class="line"><span class="comment">// 实现部分的代码不写了，和上面的Helper和A中的对应内容基本完全一样</span></div></pre></td></tr></table></figure><p>这样，我们只要让类<code>A</code>继承自<code>HandlerHelper&lt;A&gt;</code>即可（看上去很怪异。。。）：<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">class</span> A: <span class="keyword">public</span> HandlerHelper&lt;A&gt; &#123;</div><div class="line">    ...</div><div class="line">&#125;;</div></pre></td></tr></table></figure></p><h2 id="50-了解替换new和delete的合适时机"><a href="#50-了解替换new和delete的合适时机" class="headerlink" title="50 了解替换new和delete的合适时机"></a>50 了解替换<code>new</code>和<code>delete</code>的合适时机</h2><p>最常见的理由（替换之后你能得到什么好处）：</p><ul><li>检测运用上的错误。比如缓冲区越界，我们可以在<code>delete</code>的时候进行检查。</li><li>强化效能。编译器实现的<code>operator new</code>是为了普适性的功能，改成自定义版本可能提升效能。</li><li>收集使用上的统计数据。为了优化程序性能，理当先收集你的软件如何使用动态内存。自定义的<code>operator new</code>和<code>delete</code>能够收集到这些信息。</li></ul><p>但是，写出能正常工作的<code>new</code>却不一定获得很好的性能。（各种细节上的问题，例如内存的对齐。也正因为如此，这里不再重复书上的一个具体实现）例如Boost库中的<code>Pool</code>对分配大量小型对象很有帮助。</p><h2 id="51-编写new和delete时候需要遵守常规"><a href="#51-编写new和delete时候需要遵守常规" class="headerlink" title="51 编写new和delete时候需要遵守常规"></a>51 编写<code>new</code>和<code>delete</code>时候需要遵守常规</h2><p>自定义的<code>operator new</code>需要满足以下几点：</p><ul><li>如果有足够的内存，则返回其指针；否则，遵循条款49的约定。</li><li>具体地，如果内存不足，那么应该循环调用new_handling函数（里面可能会清理出一些内存以供使用）。只有当指向new_handling的指针为<code>NULL</code>时，才抛出异常<code>bad_alloc</code>。</li><li>C++规定，即使用户申请的内存大小为0，也要返回一个合法指针。这个看似诡异的行为是为了简化语言的其他部分。</li><li>还要避免掩盖正常的<code>operator new</code>。</li></ul><p>下面就是一个自定义<code>operator new</code>的例子：<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">void</span>* <span class="keyword">operator</span> <span class="title">new</span><span class="params">(<span class="keyword">size_t</span> size)</span> <span class="title">throw</span><span class="params">(bad_alloc)</span> </span>&#123;</div><div class="line">    <span class="comment">// 你的operator new也可能接受额外参数</span></div><div class="line">    <span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</div><div class="line">    <span class="keyword">if</span>(size == <span class="number">0</span>) &#123;</div><div class="line">        size = <span class="number">1</span>; <span class="comment">// 处理0byte申请</span></div><div class="line">    &#125;</div><div class="line">    <span class="keyword">while</span>(<span class="literal">true</span>) &#123;</div><div class="line">        <span class="comment">// ... try to alloc memory</span></div><div class="line">        <span class="keyword">if</span>(success) &#123;</div><div class="line">            <span class="keyword">return</span> the pointer;</div><div class="line">        &#125;</div><div class="line">        <span class="comment">// 处理分配失败，找出当前的handler</span></div><div class="line">        <span class="comment">// 我们没有诸如get_new_handler()的方法来获取new_handler函数句柄</span></div><div class="line">        <span class="comment">// 所以只能用下面这种方法，利用set_new_handler的返回值获取当前处理函数</span></div><div class="line">        new_handler globalHandler = set_new_handler(<span class="number">0</span>);</div><div class="line">        set_new_handler(globalHandler);</div><div class="line"></div><div class="line">        <span class="keyword">if</span>(globalHandler) &#123;</div><div class="line">            (*globalHandler)();</div><div class="line">        &#125; <span class="keyword">else</span> &#123;</div><div class="line">            <span class="keyword">throw</span> bad_alloc();</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p><p>在自定义<code>operator delete</code>时候，注意处理空指针的情况。C++确保delete NULL pointer是永远安全的。<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">void</span> <span class="keyword">operator</span> <span class="title">delete</span><span class="params">(<span class="keyword">void</span>* memory)</span> <span class="title">throw</span><span class="params">()</span> </span>&#123;</div><div class="line">    <span class="keyword">if</span>(memory == <span class="number">0</span>) <span class="keyword">return</span>;</div><div class="line">    <span class="comment">// ...</span></div><div class="line">&#125;</div></pre></td></tr></table></figure></p><h2 id="52-写了placement-new也要写placement-delete"><a href="#52-写了placement-new也要写placement-delete" class="headerlink" title="52 写了placement new也要写placement delete"></a>52 写了placement new也要写placement delete</h2><p>如果<code>operator new</code>接受的参数除了一定会有的那个<code>size_t</code>之外还有其他参数，那么它就叫做placement new。一个特别有用的placement new的用法是接受一个指针指向对象该被构造之处。声明如下所示：<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">void</span>* <span class="keyword">operator</span> <span class="title">new</span><span class="params">(<span class="keyword">size_t</span> size, <span class="keyword">void</span>* memory)</span> <span class="title">throw</span><span class="params">()</span></span>;</div></pre></td></tr></table></figure></p><p>上述placement new已经被纳入C++规范（可以在头文件<code>&lt;new&gt;</code>中找到它。）这个函数常用来在<code>vector</code>的未使用空间上构造对象。实际上这是placement的得来：特定位置上的new。有的时候，人们谈论placement new时，实际是在专指这个函数。</p><p>本条款主要探讨与placement new使用不当相关的内存泄漏问题。<br>当你写一个<code>new</code>表达式时，共有两个函数被调用：</p><ul><li>分配内存的<code>operator new</code></li><li>该类的构造函数</li></ul><p>假设第一个函数调用成功，第二个函数却抛出异常。这时候我们需要将第一步申请得到的内存返还并恢复旧观，否则就会造成内存泄漏。具体来说，系统会调用和刚才申请内存的<code>operator new</code>对应的delete版本。</p><p>如果目前面对的是正常签名的<code>operator new delete</code>，不会有问题。不过若是当时调用的是修改过签名形式的placement new时，就可能出现问题。例如，我们有下面的placement new，它的功能是在分配内存的时候做一些logging工作。<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// 某个类Wedget内部有自定义的placement new如下</span></div><div class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">void</span>* <span class="keyword">operator</span> <span class="title">new</span><span class="params">(<span class="keyword">size_t</span> size, ostream&amp; logStream)</span> <span class="title">throw</span> <span class="params">(bad_alloc)</span></span>;</div><div class="line"></div><div class="line">Widget* pw = <span class="keyword">new</span> (<span class="built_in">std</span>::<span class="built_in">cerr</span>) Widget;</div></pre></td></tr></table></figure></p><p>如果系统找不到相应的placement delete版本，就会什么都不做。这样，就无法归还已经申请的内存，造成内存泄漏。所以有必要声明一个placement delete，对应那个有logging功能的placement new。<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">void</span> <span class="keyword">operator</span> <span class="title">delete</span><span class="params">(<span class="keyword">void</span>* memory, ostream&amp; logStream)</span> <span class="title">throw</span><span class="params">()</span></span>;</div><div class="line"><span class="comment">// 这样，即使下式抛出异常，也能正确处理</span></div><div class="line">Widget* pw = <span class="keyword">new</span> (<span class="built_in">std</span>::<span class="built_in">cerr</span>) Widget;</div></pre></td></tr></table></figure></p><p>然而，如果什么异常都没有抛出，而客户又使用了下面的表达式返还内存：<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">delete</span> pw;</div></pre></td></tr></table></figure></p><p>那么它调用的是正常版本的delete。所以，除了相对应的placement delete，还有必要同时提供正常版本的delete。前者为了解决构造过程中有异常抛出的情况，后者处理无异常抛出。</p><p>一个比较简单的做法是，建立一个基类，其中有所有正常形式的new和delete。<br><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line">class StdNewDeleteForms &#123;</div><div class="line">public:</div><div class="line">    // 正常的new和delete</div><div class="line">    static void* operator new(std::size_t size) throw std::bad_alloc) &#123;</div><div class="line">        return ::operator new(size);</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    static void operator delete(void* memory) throw() &#123;</div><div class="line">        ::operator delete(memory);</div><div class="line">    &#125;</div><div class="line">    // placement new 和 delete</div><div class="line">    static void* operator new(std::size_t size, void* p) throw() &#123;</div><div class="line">        ::operator new(size, p);</div><div class="line">    &#125;</div><div class="line">    static void operator delete(void* memory, void* p) throw() &#123;</div><div class="line">        ::operator delete(memory, p);</div><div class="line">    &#125;</div><div class="line">    // nothrow new 和 delete</div><div class="line">    static void* operator new(std::size_t size, const std::nothrow_t&amp; nt) throw() &#123;</div><div class="line">        return ::operator new(size, nt);</div><div class="line">    &#125;</div><div class="line">    static void operator delete(void* memory, const std::nothrow_t&amp;) throw() &#123;</div><div class="line">        ::operator delete(mempry);</div><div class="line">    &#125;</div><div class="line">&#125;;</div></pre></td></tr></table></figure></p><p>上面这个类中包含了C++标准中已经规定好的三种形式的new和delete。那么，凡是想以自定义方式扩充标准形式，可利用继承机制和<code>using</code>声明（见条款39），取得标准形式。<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">class</span> Widget: <span class="keyword">public</span> StdNewDeleteForms &#123;</div><div class="line"><span class="keyword">public</span>:</div><div class="line">    <span class="comment">// 使用标准new 和 delete</span></div><div class="line">    <span class="keyword">using</span> StdNewDeleteForms::<span class="keyword">operator</span> <span class="keyword">new</span>;</div><div class="line">    <span class="keyword">using</span> StdNetDeleteForms::<span class="keyword">operator</span> <span class="keyword">delete</span>;</div><div class="line">    <span class="comment">// 添加自定义的placement new 和 delete</span></div><div class="line">    <span class="function"><span class="keyword">static</span> <span class="keyword">void</span>* <span class="keyword">operator</span> <span class="title">new</span><span class="params">(<span class="built_in">std</span>::<span class="keyword">size_t</span> size,</span></span></div><div class="line">        <span class="built_in">std</span>::ostream&amp; logStream) <span class="title">throw</span><span class="params">(<span class="built_in">std</span>::bad_alloc)</span>;</div><div class="line">    <span class="function"><span class="keyword">static</span> <span class="keyword">void</span> <span class="keyword">operator</span> <span class="title">delete</span><span class="params">(<span class="keyword">void</span>* memory, <span class="built_in">std</span>::ostream&amp; logStream)</span> <span class="title">throw</span><span class="params">()</span></span>;</div><div class="line">&#125;;</div></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;手动管理内存，这既是C++的优点，也是C++中很容易出问题的地方。本章主要给出分配内存和归还时候的注意事项，主角是&lt;code&gt;operator new&lt;/code&gt;和&lt;code&gt;operator delete&lt;/code&gt;，配角是new_handler，它在当&lt;code&gt;operator new&lt;/code&gt;无法满足客户内存需求时候被调用。&lt;/p&gt;
&lt;p&gt;另外，&lt;code&gt;operator new&lt;/code&gt;和&lt;code&gt;operator delete&lt;/code&gt;只用于分配单一对象内存。对于数组，应使用&lt;code&gt;operator new[]&lt;/code&gt;，并通过&lt;code&gt;operator delete[]&lt;/code&gt;归还。除非特别指定，本章中的各项既适用于单一&lt;code&gt;operator new&lt;/code&gt;，也适用于&lt;code&gt;operator new[]&lt;/code&gt;。&lt;/p&gt;
&lt;p&gt;最后，STL中容器使用的堆内存是由容器拥有的分配器对象（allocator objects）来管理的。本章不讨论。&lt;br&gt;&lt;img src=&quot;/img/effectivecpp_08_memory_leak_everywherre.jpg&quot; alt=&quot;memory_leak_everywhere&quot;&gt;&lt;br&gt;
    
    </summary>
    
    
      <category term="cpp" scheme="https://xmfbit.github.io/tags/cpp/"/>
    
  </entry>
  
  <entry>
    <title>Neural Network for Machine Learning - Lecture 06 神经网络的“调教”方法</title>
    <link href="https://xmfbit.github.io/2017/06/25/hinton-nnml-06/"/>
    <id>https://xmfbit.github.io/2017/06/25/hinton-nnml-06/</id>
    <published>2017-06-25T05:48:31.000Z</published>
    <updated>2018-03-14T06:04:00.130Z</updated>
    
    <content type="html"><![CDATA[<p>第六周的课程主要讲解了用于神经网络训练的梯度下降方法，首先对比了SGD，full batch GD和mini batch SGD方法，然后给出了几个用于神经网络训练的trick，主要包括输入数据预处理（零均值，单位方差以及PCA解耦），学习率的自适应调节以及网络权重的初始化方法（可以参考各大框架中实现的Xavier初始化方法等）。这篇文章主要记录了后续讲解的几种GD变种方法，如何合理利用梯度信息达到更好的训练效果。由于Hinton这门课确实时间已经很久了，所以文章末尾会结合一篇不错的总结性质的<a href="http://sebastianruder.com/optimizing-gradient-descent/index.html" target="_blank" rel="external">博客</a>和对应的<a href="https://arxiv.org/abs/1609.04747" target="_blank" rel="external">论文</a>以及PyTorch中的相关代码，对目前流行的梯度下降方法做个总结。</p><p>下图即来自上面的这篇博客。</p><p><img src="/img/contours_evaluation_optimizers.gif" alt="几种优化方法的可视化"><br><a id="more"></a></p><h2 id="Momentum"><a href="#Momentum" class="headerlink" title="Momentum"></a>Momentum</h2><p>我们可以把训练过程想象成在权重空间的一个质点（小球），移动到全局最优点的过程。不同于GD，使用梯度信息直接更新权重的位置，momentum方法是将梯度作为速度量。这样做的好处是，当梯度的方向一直不变时，速度可以加快；当梯度方向变化剧烈时，由于符号改变，所以速度减慢，起到了GD中自适应调节学习率的过程。</p><p>具体来说，我们利用新得到的梯度信息，采用滑动平均的方法更新速度。式子中的$\epsilon$为学习率，$\alpha$为momentum系数。</p><script type="math/tex; mode=display">\Delta w_t = v_t = \alpha v_{t-1} - \epsilon g_t = \Delta w_t - \epsilon g_t</script><p>为了说明momentum确实对学习过程有加速作用，假设一个简单的情形，即运动轨迹是一个斜率固定的斜面。那么我们有梯度$g$固定。根据上面的递推公式可以得到通项公式（简单的待定系数法凑出等比数列）：</p><script type="math/tex; mode=display">v_t = \alpha(v_{t-1} + \frac{\epsilon g}{1-\alpha}) - \frac{\epsilon g}{1-\alpha}</script><p>由于$\alpha &lt; 0$，所以当$t = \infty$时，只剩下了后面的常数项，即：</p><script type="math/tex; mode=display">v_\infty = -\frac{\epsilon}{1-\alpha}g</script><p>也就是说，权重更新的幅度变成了原来的$\frac{1}{1-\alpha}$倍。若取$\alpha=0.99$，则加速$100$倍。</p><p>Hinton给出的建议是由于训练开头梯度值比较大，所以momentum系数一开始不要过大，例如可以取$0.5$。当梯度值较小，训练过程被困在一个峡谷的时候，可以适当提升。</p><p>一种改进方法由Nesterov提出。在上面的方法中，我们首先更新了在该处的累积梯度信息，然后向前移动。而Nesterov方法中，我们首先沿着累计梯度信息向前走，然后根据梯度信息进行更正。</p><p><img src="/img/hinton_06_nesterov_momentum.png" alt="Nesterov方法"></p><h2 id="Adaptive-Learning-Rate"><a href="#Adaptive-Learning-Rate" class="headerlink" title="Adaptive Learning Rate"></a>Adaptive Learning Rate</h2><p>这种方法起源于这样的观察：在网络中，不同layer之间的权重更新需要不同的学习率。因为浅层和深层的layer梯度幅值很可能不同。所以，对不同的权重乘上不同的因子是个更加合理的选择。</p><p>例如，我们可以根据梯度是否发生符号变化按照下面的方式调节某个权重$w_{ij}$的增益。注意$0.95$和$0.05$的和是$1$。这样可以使得平衡点在$1$附近。<br><img src="/img/hinton_06_learningrate.png" alt="Different learning rate gain"></p><p>下面是使用这种方法的几个trick，包括限幅，较大的batch size以及和momentum的结合。</p><p><img src="/img/hinton_06_tricks_for_adaptive_lr.png" alt="Tricks for adaptive lr"></p><h2 id="RMSProp"><a href="#RMSProp" class="headerlink" title="RMSProp"></a>RMSProp</h2><p>rprop利用梯度的符号，如果符号保持不变，则相应增大step size；否则减小。但是只能用于full batch GD。RMSProp就是一种可以结合mini batch SGD和rprop的一种方法。</p><p>我们使用滑动平均方法更新梯度的mean square（即RMS中的MS得来）。</p><script type="math/tex; mode=display">\text{MeanSquare}(w, t) = 0.9 \text{MeanSquare}(w, t-1) + 0.1g_t^2</script><p>然后，将梯度除以上面的得到的Mean Square值。</p><p>RMSProp还有一些变种，列举如下：<br><img src="/img/hinton_06_rmsprop_improvement.png" alt="Otehr RMSProp"></p><h2 id="课程总结"><a href="#课程总结" class="headerlink" title="课程总结"></a>课程总结</h2><ul><li>对于小数据集，使用full batch GD（LBFGS或adaptive learning rate如rprop）。</li><li>对于较大数据集，使用mini batch SGD。并可以考虑加上momentmum和RMSProp。</li></ul><p>如何选择学习率是一个较为依赖经验的任务（网络结构不同，任务不同）。<br><img src="/img/hinton_06_summary.png" alt="总结"></p><h2 id="“Modern”-SGD"><a href="#“Modern”-SGD" class="headerlink" title="“Modern” SGD"></a>“Modern” SGD</h2><p>从本部分开始，我将转向总结摘要中提到的那篇博客中的主要内容。首先，给出当前基于梯度的优化方法的一些问题。可以看到，之后人们提出的改进方法就是想办法解决对应问题的。由于与Hinton课程相比，这些方法提出时间（也许称之为流行时间更合适？做数学的那帮人可能很早就知道这些优化方法了吧？）较短，所以这里仿照Modern C++之称呼，就把它们统一叫做Modern SGD吧。。。</p><ul><li>学习率通常很难确定。学习率太大？容易扯到蛋（loss直接爆炸）；学习率太小，训练到天荒地老。。。</li><li>学习率如何在训练中调整。目前常用的方法是退火，要么是固定若干次迭代之后把学习率调小，要么是观察loss到某个阈值后把学习率调小。总之，都是在训练开始前，人工预先定义好的。而这没有考虑到数据集自身的特点。</li><li>学习率对每个网络参数都一样。这点在上面课程中Hinton已经提到，引出了自适应学习率的方法。</li><li>高度非凸函数的优化难题。以前人们多是认为网络很容易收敛到局部极小值。后来有人提出，网络之所以难训练，更多是由于遇到了鞍点。也就是某个方向上它是极小值；而另一个方向却是极大值（高数中介绍过的，马鞍面）</li></ul><p><img src="/img/hinton_06_maanmian.jpg" alt="马鞍面"></p><h3 id="Adagrad"><a href="#Adagrad" class="headerlink" title="Adagrad"></a>Adagrad</h3><p><a href="http://jmlr.org/papers/v12/duchi11a.html" target="_blank" rel="external">Adagrad</a>对不同的参数采用不同的学习率，也是其Ada（Adaptive）的名字得来。我们记时间步$t$时标号为$i$的参数对应的梯度为$g_{i}$，即：</p><script type="math/tex; mode=display">g_{i} = \bigtriangledown_{\theta_i} J(\theta)</script><p>Adagrad使用一个系数来为不同的参数修正学习率，如下：</p><script type="math/tex; mode=display">\hat{g_i} = \frac{1}{\sqrt{G_i+\epsilon}}g_i</script><p>其中，$G_i$是截止到当前时间步$t$时，参数$\theta_i$对应梯度$g_i$的平方和。</p><p>我们可以把上面的式子写成矩阵形式。其中，$\odot$表示逐元素的矩阵相乘（element-wise product）。同时，$G_t = g_t \odot g_t$。</p><script type="math/tex; mode=display">\theta_{t+1} = \theta_t - \frac{\eta}{\sqrt{G_t+\epsilon}}\odot g_t</script><p>我们再来看PyTorch中的相关实现：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># for each gradient of parameters:</span></div><div class="line"><span class="comment"># addcmul(t, alpha, t1, t2): t = t1*t2*alpha + t</span></div><div class="line"><span class="comment"># let epsilon = 1E-10</span></div><div class="line">state[<span class="string">'sum'</span>].addcmul_(<span class="number">1</span>, grad, grad)   <span class="comment"># 计算 G</span></div><div class="line">std = state[<span class="string">'sum'</span>].sqrt().add_(<span class="number">1e-10</span>)  <span class="comment"># 计算 \sqrt(G)</span></div><div class="line">p.data.addcdiv_(-clr, grad, std)       <span class="comment"># 更新</span></div></pre></td></tr></table></figure><p>由于Adagrad对不同的梯度给了不同的学习率修正值，所以使用这种方法时，我们可以不用操心学习率，只是给定一个初始值（如$0.01$）就够了。尤其是对稀疏的数据，Adagrad方法能够自适应调节其梯度更新信息，给那些不常出现（非零）的梯度对应更大的学习率。PyTorch中还为稀疏数据特别优化了更新算法。</p><p>Adagrad的缺点在于由于$G_t$矩阵是平方和，所以分母会越来越大，造成训练后期学习率会变得很小。下面的Adadelta方法针对这个问题进行了改进。</p><h3 id="Adadelta"><a href="#Adadelta" class="headerlink" title="Adadelta"></a>Adadelta</h3><p><a href="https://arxiv.org/abs/1212.5701" target="_blank" rel="external">Adadelta</a>给出的改进方法是不再记录所有的历史时刻的$g$的平方和，而是最近一个有限的观察窗口$w$的累积梯度平方和。在实际使用时，这种方法使用了一个参数$\gamma$（如$0.9$）作为遗忘因子，对$E[g_t^2]$进行统计。</p><script type="math/tex; mode=display">E[g_t^2] = \gamma E[g_{t-1}^2] + (1-\gamma)g_t^2</script><p>由于$\sqrt{E[g_t^2]}$就是$g$的均方根RMS，所以，修正后的梯度如下。注意到，这正是Hinton在课上所讲到的RMSprop的优化方法。</p><script type="math/tex; mode=display">\hat{g}_t = \frac{1}{\text{RMS}[g]}g_t</script><p>作者还观察到，这样更新的话，其实$\theta$和$\Delta \theta$的单位是不一样的（此时$\Delta \theta$是无量纲数）。所以，作者提出再乘上一个$\text{RMS}[\Delta \theta]$来平衡（同时去掉了学习率$\eta$），所以，最终的参数更新如下：</p><script type="math/tex; mode=display">\theta_{t+1} = \theta_t - \frac{\text{RMS}[\Delta \theta]}{\text{RMS}[g]}g_t</script><p>这种方法甚至不再需要学习率。下面是PyTorch中的实现，其中仍然保有学习率<code>lr</code>这一参数设定，默认值为$1.0$。代码注释中，我使用<code>MS</code>来指代$E[x^2]$。即，$\text{RMS}[x] = \sqrt{\text{MS}[x]+\epsilon}$。<br><figure class="highlight py"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># update: MS[g] = MS[g]*\rho + g*g*(1-\rho)</span></div><div class="line">square_avg.mul_(rho).addcmul_(<span class="number">1</span> - rho, grad, grad)</div><div class="line"><span class="comment"># current RMS[g] = sqrt(MS[g] + \epsilon)</span></div><div class="line">std = square_avg.add(eps).sqrt_()</div><div class="line"><span class="comment"># \Delta \theta = RMS[\Delta \theta] / RMS[g]) * g</span></div><div class="line">delta = acc_delta.add(eps).sqrt_().div_(std).mul_(grad)</div><div class="line"><span class="comment"># update parameter: \theta -= lr * \Delta \theta</span></div><div class="line">p.data.add_(-group[<span class="string">'lr'</span>], delta)</div><div class="line"><span class="comment"># update MS[\Delta \theta] = MS[\Delta \theta] * \rho + \Delta \theta^2 * (1-\rho)</span></div><div class="line">acc_delta.mul_(rho).addcmul_(<span class="number">1</span> - rho, delta, delta)</div></pre></td></tr></table></figure></p><h3 id="Adam"><a href="#Adam" class="headerlink" title="Adam"></a>Adam</h3><p><a href="https://arxiv.org/abs/1412.6980" target="_blank" rel="external">Adaptive momen Estimation（Adam，自适应矩估计）</a>，是另一种为不同参数自适应设置不同学习率的方法。Adam方法不止存储过往的梯度平方均值（二阶矩）信息，还存储过往的梯度均值信息（一阶矩）。</p><script type="math/tex; mode=display">\begin{aligned}m_t&=\beta_1 m_{t-1}+(1-\beta_1)g_t\\v_t&=\beta_2 v_{t-1}+(1-\beta_2)g_t^2\end{aligned}</script><p>作者观察到上述估计是有偏的（biase towards $0$），所以给出如下修正：</p><script type="math/tex; mode=display">\begin{aligned}\hat{m} &= \frac{m}{1-\beta_1}\\ \hat{v}&=\frac{v}{1-\beta_2}\end{aligned}</script><p>参数的更新如下：</p><script type="math/tex; mode=display">\theta_{t+1} = \theta_t - \frac{\eta}{\sqrt{\hat{v_t} + \epsilon}}\hat{m_t}</script><p>作者给出$\beta_1 = 0.9$，$\beta_2=0.999$，$\epsilon=10^{-8}$。</p><p>为了更好地理解PyTorch中的实现方式，需要对上式进行变形：</p><script type="math/tex; mode=display">\Delta \theta = \frac{\sqrt{1-\beta_2}}{1-\beta_1}\eta \frac{m_t}{\sqrt{v_t}}</script><p>代码中令$\text{step_size} =  \frac{\sqrt{1-\beta_2}}{1-\beta_1}\eta$。同时，$\beta$也要以指数规律衰减，即：$\beta_t = \beta_0^t$。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># exp_avg is `m`: expected average of g</span></div><div class="line">exp_avg.mul_(beta1).add_(<span class="number">1</span> - beta1, grad)</div><div class="line"><span class="comment"># exp_avg_sq is `v`: expected average of g's square</span></div><div class="line">exp_avg_sq.mul_(beta2).addcmul_(<span class="number">1</span> - beta2, grad, grad)</div><div class="line"></div><div class="line"><span class="comment"># \sqrt&#123;v_t + \epsilon&#125;</span></div><div class="line">denom = exp_avg_sq.sqrt().add_(group[<span class="string">'eps'</span>])</div><div class="line"></div><div class="line"><span class="comment"># 1 - \beta_1^t</span></div><div class="line">bias_correction1 = <span class="number">1</span> - beta1 ** state[<span class="string">'step'</span>]</div><div class="line"><span class="comment"># 1 - \beta_2^t</span></div><div class="line">bias_correction2 = <span class="number">1</span> - beta2 ** state[<span class="string">'step'</span>]</div><div class="line"><span class="comment"># get step_size</span></div><div class="line">step_size = group[<span class="string">'lr'</span>] * math.sqrt(bias_correction2) / bias_correction1</div><div class="line"><span class="comment"># delta = -step_size * m / sqrt(v)</span></div><div class="line">p.data.addcdiv_(-step_size, exp_avg, denom)</div></pre></td></tr></table></figure><h3 id="AdaMax"><a href="#AdaMax" class="headerlink" title="AdaMax"></a>AdaMax</h3><p>上面Adam中，实际上我们是用梯度$g$的$2$范数（$\sqrt{\hat{v_t}}$）去对$g$进行Normalization。那么为什么不用其他形式的范数$p$来试试呢？然而，对于$1$范数和$2$范数，数值是稳定的。对于再大的$p$，数值不稳定。不过，当取无穷范数的时候，又是稳定的了。</p><p>由于无穷范数就是求绝对值最大的分量，所以这种方法叫做<a href="https://arxiv.org/abs/1412.6980" target="_blank" rel="external">AdaMax</a>。其对应的$\hat{v_t}$为（这里为了避免混淆，使用$u_t$指代）：</p><script type="math/tex; mode=display">u_t = \beta_2^\infty u_{t-1} + (1-\beta_2^\infty) g_t^\infty</script><p>我们将$u_t$按照时间展开，可以得到（直接摘自论文的图）。其中最后一步递推式的得来：根据$u_t$把$u_{t-1}$的展开形式也写出来，就不难发现最下面的递推形式。</p><p><img src="/img/hinton_06_adamax.png" alt="Adamax中ut的推导"></p><p>相应的更新权重操作为：</p><script type="math/tex; mode=display">\theta_{t+1} = \theta_t -\frac{\eta}{u_t}\hat{m}_t</script><p>在PyTorch中的实现如下：<br><figure class="highlight py"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Update biased first moment estimate, which is \hat&#123;m&#125;_t</span></div><div class="line">exp_avg.mul_(beta1).add_(<span class="number">1</span> - beta1, grad)</div><div class="line"><span class="comment"># 下面这种用来逐元素求取 max(A, B) 的方法可以学习一个</span></div><div class="line"><span class="comment"># Update the exponentially weighted infinity norm.</span></div><div class="line">norm_buf = torch.cat([</div><div class="line">    exp_inf.mul_(beta2).unsqueeze(<span class="number">0</span>),</div><div class="line">    grad.abs().add_(eps).unsqueeze_(<span class="number">0</span>)</div><div class="line">], <span class="number">0</span>)</div><div class="line"><span class="comment">## 找到 exp_inf 和 g之间的较大者（只需要在刚刚聚合的这个维度上找即可~）</span></div><div class="line">torch.max(norm_buf, <span class="number">0</span>, keepdim=<span class="keyword">False</span>, out=(exp_inf, exp_inf.new().long()))</div><div class="line"></div><div class="line"><span class="comment">## beta1 correction</span></div><div class="line">bias_correction = <span class="number">1</span> - beta1 ** state[<span class="string">'step'</span>]</div><div class="line">clr = group[<span class="string">'lr'</span>] / bias_correction</div><div class="line"></div><div class="line">p.data.addcdiv_(-clr, exp_avg, exp_inf)</div></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;第六周的课程主要讲解了用于神经网络训练的梯度下降方法，首先对比了SGD，full batch GD和mini batch SGD方法，然后给出了几个用于神经网络训练的trick，主要包括输入数据预处理（零均值，单位方差以及PCA解耦），学习率的自适应调节以及网络权重的初始化方法（可以参考各大框架中实现的Xavier初始化方法等）。这篇文章主要记录了后续讲解的几种GD变种方法，如何合理利用梯度信息达到更好的训练效果。由于Hinton这门课确实时间已经很久了，所以文章末尾会结合一篇不错的总结性质的&lt;a href=&quot;http://sebastianruder.com/optimizing-gradient-descent/index.html&quot;&gt;博客&lt;/a&gt;和对应的&lt;a href=&quot;https://arxiv.org/abs/1609.04747&quot;&gt;论文&lt;/a&gt;以及PyTorch中的相关代码，对目前流行的梯度下降方法做个总结。&lt;/p&gt;
&lt;p&gt;下图即来自上面的这篇博客。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/img/contours_evaluation_optimizers.gif&quot; alt=&quot;几种优化方法的可视化&quot;&gt;&lt;br&gt;
    
    </summary>
    
    
      <category term="deep learning" scheme="https://xmfbit.github.io/tags/deep-learning/"/>
    
      <category term="公开课" scheme="https://xmfbit.github.io/tags/%E5%85%AC%E5%BC%80%E8%AF%BE/"/>
    
      <category term="pytorch" scheme="https://xmfbit.github.io/tags/pytorch/"/>
    
  </entry>
  
</feed>
