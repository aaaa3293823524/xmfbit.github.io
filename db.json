{"meta":{"version":1,"warehouse":"2.2.0"},"models":{"Asset":[{"_id":"source/img/camera_skew.png","path":"img/camera_skew.png","modified":0,"renderable":0},{"_id":"source/img/case_1_m.png","path":"img/case_1_m.png","modified":0,"renderable":0},{"_id":"source/img/case_2_m.png","path":"img/case_2_m.png","modified":0,"renderable":0},{"_id":"source/img/case_3_m.png","path":"img/case_3_m.png","modified":0,"renderable":0},{"_id":"source/img/case_4_m.png","path":"img/case_4_m.png","modified":0,"renderable":0},{"_id":"source/img/case_m_5.png","path":"img/case_m_5.png","modified":0,"renderable":0},{"_id":"source/img/corner_type_1.png","path":"img/corner_type_1.png","modified":0,"renderable":0},{"_id":"source/img/gsl_picture.jpg","path":"img/gsl_picture.jpg","modified":0,"renderable":0},{"_id":"source/img/jupyternotebook_logo.png","path":"img/jupyternotebook_logo.png","modified":0,"renderable":0},{"_id":"source/img/helloworld_hexo.png","path":"img/helloworld_hexo.png","modified":0,"renderable":0},{"_id":"source/img/kmeans_data_demo.png","path":"img/kmeans_data_demo.png","modified":0,"renderable":0},{"_id":"source/img/kmeans_success.png","path":"img/kmeans_success.png","modified":0,"renderable":0},{"_id":"source/img/line_fit_demo.png","path":"img/line_fit_demo.png","modified":0,"renderable":0},{"_id":"source/img/meanshift_basics.jpg","path":"img/meanshift_basics.jpg","modified":0,"renderable":0},{"_id":"source/img/meanshift_simple_demo.png","path":"img/meanshift_simple_demo.png","modified":0,"renderable":0},{"_id":"source/img/mnist_example.png","path":"img/mnist_example.png","modified":0,"renderable":0},{"_id":"source/img/pytorch_logo.png","path":"img/pytorch_logo.png","modified":0,"renderable":0},{"_id":"source/img/qicizuobiao.png","path":"img/qicizuobiao.png","modified":0,"renderable":0},{"_id":"source/img/ransac_step.png","path":"img/ransac_step.png","modified":0,"renderable":0},{"_id":"source/img/rotation.png","path":"img/rotation.png","modified":0,"renderable":0},{"_id":"source/img/sift_dominant_orientation.png","path":"img/sift_dominant_orientation.png","modified":0,"renderable":0},{"_id":"source/img/sift_picture.jpg","path":"img/sift_picture.jpg","modified":0,"renderable":0},{"_id":"source/img/svd_ranking.png","path":"img/svd_ranking.png","modified":0,"renderable":0},{"_id":"source/img/svd_picture.jpg","path":"img/svd_picture.jpg","modified":0,"renderable":0},{"_id":"source/img/yolo2_bbox_param.png","path":"img/yolo2_bbox_param.png","modified":0,"renderable":0},{"_id":"source/img/yolo2_cluster_result.png","path":"img/yolo2_cluster_result.png","modified":0,"renderable":0},{"_id":"source/img/yolo2_bbox_location.png","path":"img/yolo2_bbox_location.png","modified":0,"renderable":0},{"_id":"source/img/canny_linking.png","path":"img/canny_linking.png","modified":0,"renderable":0},{"_id":"source/img/canny_nms.png","path":"img/canny_nms.png","modified":0,"renderable":0},{"_id":"source/img/case_6_m.png","path":"img/case_6_m.png","modified":0,"renderable":0},{"_id":"source/img/corner_window_fun.png","path":"img/corner_window_fun.png","modified":0,"renderable":0},{"_id":"source/img/doxygen_picture.png","path":"img/doxygen_picture.png","modified":0,"renderable":0},{"_id":"source/img/edge_camera_man.png","path":"img/edge_camera_man.png","modified":0,"renderable":0},{"_id":"source/img/edge_deriative.png","path":"img/edge_deriative.png","modified":0,"renderable":0},{"_id":"source/img/god_use_vpn.png","path":"img/god_use_vpn.png","modified":0,"renderable":0},{"_id":"source/img/harris_non_scale_constant.png","path":"img/harris_non_scale_constant.png","modified":0,"renderable":0},{"_id":"source/img/meanshift_gradient_of_density.png","path":"img/meanshift_gradient_of_density.png","modified":0,"renderable":0},{"_id":"source/img/meanshift_kernel_function.png","path":"img/meanshift_kernel_function.png","modified":0,"renderable":0},{"_id":"source/img/patch_average_intensity_scale_constant.png","path":"img/patch_average_intensity_scale_constant.png","modified":0,"renderable":0},{"_id":"source/img/qicizuobiao_transform.png","path":"img/qicizuobiao_transform.png","modified":0,"renderable":0},{"_id":"source/img/sift_detection_maximum.png","path":"img/sift_detection_maximum.png","modified":0,"renderable":0},{"_id":"source/img/rotation_matrix.png","path":"img/rotation_matrix.png","modified":0,"renderable":0},{"_id":"source/img/svd_superman.png","path":"img/svd_superman.png","modified":0,"renderable":0},{"_id":"source/img/yolo1_network_arch.png","path":"img/yolo1_network_arch.png","modified":0,"renderable":0},{"_id":"source/img/yolo1_loss_fun.png","path":"img/yolo1_loss_fun.png","modified":0,"renderable":0},{"_id":"source/img/yolo2_different_methods_comparation.png","path":"img/yolo2_different_methods_comparation.png","modified":0,"renderable":0},{"_id":"source/img/yolo2_different_methods_improvement.png","path":"img/yolo2_different_methods_improvement.png","modified":0,"renderable":0},{"_id":"source/img/camera_translation_rotation.png","path":"img/camera_translation_rotation.png","modified":0,"renderable":0},{"_id":"source/img/corner_judge_2.png","path":"img/corner_judge_2.png","modified":0,"renderable":0},{"_id":"source/img/corner_judge.png","path":"img/corner_judge.png","modified":0,"renderable":0},{"_id":"source/img/epipolar_constraint_1.png","path":"img/epipolar_constraint_1.png","modified":0,"renderable":0},{"_id":"source/img/epipolar_fig.png","path":"img/epipolar_fig.png","modified":0,"renderable":0},{"_id":"source/img/fun_noise.png","path":"img/fun_noise.png","modified":0,"renderable":0},{"_id":"source/img/generic_projection_matrix.png","path":"img/generic_projection_matrix.png","modified":0,"renderable":0},{"_id":"source/img/kmeans_sensitive_to_outlier.png","path":"img/kmeans_sensitive_to_outlier.png","modified":0,"renderable":0},{"_id":"source/img/kmeans_object_fun_vs_k.png","path":"img/kmeans_object_fun_vs_k.png","modified":0,"renderable":0},{"_id":"source/img/original_superman.png","path":"img/original_superman.png","modified":0,"renderable":0},{"_id":"source/img/ransac_k.png","path":"img/ransac_k.png","modified":0,"renderable":0},{"_id":"source/img/what_is_corner.png","path":"img/what_is_corner.png","modified":0,"renderable":0},{"_id":"source/img/yolo2_different_methods_comparation_in_table.png","path":"img/yolo2_different_methods_comparation_in_table.png","modified":0,"renderable":0},{"_id":"themes/next/source/css/main.styl","path":"css/main.styl","modified":0,"renderable":1},{"_id":"themes/next/source/images/avatar.gif","path":"images/avatar.gif","modified":0,"renderable":1},{"_id":"themes/next/source/images/algolia_logo.svg","path":"images/algolia_logo.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc-sa.svg","path":"images/cc-by-nc-sa.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc-nd.svg","path":"images/cc-by-nc-nd.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc.svg","path":"images/cc-by-nc.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-sa.svg","path":"images/cc-by-sa.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-nd.svg","path":"images/cc-by-nd.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by.svg","path":"images/cc-by.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-zero.svg","path":"images/cc-zero.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/loading.gif","path":"images/loading.gif","modified":0,"renderable":1},{"_id":"themes/next/source/images/placeholder.gif","path":"images/placeholder.gif","modified":0,"renderable":1},{"_id":"themes/next/source/images/quote-l.svg","path":"images/quote-l.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/quote-r.svg","path":"images/quote-r.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/searchicon.png","path":"images/searchicon.png","modified":0,"renderable":1},{"_id":"source/img/caffe_image.jpg","path":"img/caffe_image.jpg","modified":0,"renderable":0},{"_id":"source/img/kmeans_bigger_demo.png","path":"img/kmeans_bigger_demo.png","modified":0,"renderable":0},{"_id":"source/img/pinhole_camera_model.png","path":"img/pinhole_camera_model.png","modified":0,"renderable":0},{"_id":"source/img/yolo2_dartnet_19_structure.png","path":"img/yolo2_dartnet_19_structure.png","modified":0,"renderable":0},{"_id":"source/avatar/liumengli.jpg","path":"avatar/liumengli.jpg","modified":0,"renderable":0},{"_id":"themes/next/source/js/src/affix.js","path":"js/src/affix.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/algolia-search.js","path":"js/src/algolia-search.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/bootstrap.js","path":"js/src/bootstrap.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/hook-duoshuo.js","path":"js/src/hook-duoshuo.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/motion.js","path":"js/src/motion.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/post-details.js","path":"js/src/post-details.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/scrollspy.js","path":"js/src/scrollspy.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/utils.js","path":"js/src/utils.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.css","path":"lib/algolia-instant-search/instantsearch.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/canvas-nest/canvas-nest.min.js","path":"lib/canvas-nest/canvas-nest.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fastclick/LICENSE","path":"lib/fastclick/LICENSE","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fastclick/README.md","path":"lib/fastclick/README.md","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fastclick/bower.json","path":"lib/fastclick/bower.json","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/HELP-US-OUT.txt","path":"lib/font-awesome/HELP-US-OUT.txt","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/bower.json","path":"lib/font-awesome/bower.json","modified":0,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/CONTRIBUTING.md","path":"lib/jquery_lazyload/CONTRIBUTING.md","modified":0,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/README.md","path":"lib/jquery_lazyload/README.md","modified":0,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/bower.json","path":"lib/jquery_lazyload/bower.json","modified":0,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/jquery.scrollstop.js","path":"lib/jquery_lazyload/jquery.scrollstop.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/jquery.lazyload.js","path":"lib/jquery_lazyload/jquery.lazyload.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/velocity/bower.json","path":"lib/velocity/bower.json","modified":0,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.ui.min.js","path":"lib/velocity/velocity.ui.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.min.js","path":"lib/velocity/velocity.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.ui.js","path":"lib/velocity/velocity.ui.js","modified":0,"renderable":1},{"_id":"source/img/kmeans_demo.png","path":"img/kmeans_demo.png","modified":0,"renderable":0},{"_id":"source/img/svd_flower.png","path":"img/svd_flower.png","modified":0,"renderable":0},{"_id":"source/img/video_linear_alg_essential.png","path":"img/video_linear_alg_essential.png","modified":0,"renderable":0},{"_id":"source/img/dog_x.png","path":"img/dog_x.png","modified":0,"renderable":0},{"_id":"themes/next/source/lib/jquery/index.js","path":"lib/jquery/index.js","modified":0,"renderable":1},{"_id":"source/img/ransac_line_fit.png","path":"img/ransac_line_fit.png","modified":0,"renderable":0},{"_id":"source/img/regex_picture.jpg","path":"img/regex_picture.jpg","modified":0,"renderable":0},{"_id":"source/img/sift_dog.png","path":"img/sift_dog.png","modified":0,"renderable":0},{"_id":"themes/next/source/js/src/schemes/pisces.js","path":"js/src/schemes/pisces.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/blank.gif","path":"lib/fancybox/source/blank.gif","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_loading.gif","path":"lib/fancybox/source/fancybox_loading.gif","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_overlay.png","path":"lib/fancybox/source/fancybox_overlay.png","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_loading@2x.gif","path":"lib/fancybox/source/fancybox_loading@2x.gif","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_sprite.png","path":"lib/fancybox/source/fancybox_sprite.png","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_sprite@2x.png","path":"lib/fancybox/source/fancybox_sprite@2x.png","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.css","path":"lib/fancybox/source/jquery.fancybox.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.pack.js","path":"lib/fancybox/source/jquery.fancybox.pack.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.js","path":"lib/fancybox/source/jquery.fancybox.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fastclick/lib/fastclick.min.js","path":"lib/fastclick/lib/fastclick.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fastclick/lib/fastclick.js","path":"lib/fastclick/lib/fastclick.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css","path":"lib/font-awesome/css/font-awesome.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css.map","path":"lib/font-awesome/css/font-awesome.css.map","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.min.css","path":"lib/font-awesome/css/font-awesome.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.min.js","path":"lib/ua-parser-js/dist/ua-parser.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.pack.js","path":"lib/ua-parser-js/dist/ua-parser.pack.js","modified":0,"renderable":1},{"_id":"source/img/sift_experiment_1.png","path":"img/sift_experiment_1.png","modified":0,"renderable":0},{"_id":"source/img/warpctc_intro.png","path":"img/warpctc_intro.png","modified":0,"renderable":0},{"_id":"source/img/yolo1_detection_system.png","path":"img/yolo1_detection_system.png","modified":0,"renderable":0},{"_id":"themes/next/source/lib/font-awesome/fonts/FontAwesome.otf","path":"lib/font-awesome/fonts/FontAwesome.otf","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.eot","path":"lib/font-awesome/fonts/fontawesome-webfont.eot","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff2","path":"lib/font-awesome/fonts/fontawesome-webfont.woff2","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff","path":"lib/font-awesome/fonts/fontawesome-webfont.woff","modified":0,"renderable":1},{"_id":"source/img/camera_model_things_to_remember.png","path":"img/camera_model_things_to_remember.png","modified":0,"renderable":0},{"_id":"themes/next/source/lib/velocity/velocity.js","path":"lib/velocity/velocity.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/fancybox_buttons.png","path":"lib/fancybox/source/helpers/fancybox_buttons.png","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-buttons.css","path":"lib/fancybox/source/helpers/jquery.fancybox-buttons.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-buttons.js","path":"lib/fancybox/source/helpers/jquery.fancybox-buttons.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-thumbs.css","path":"lib/fancybox/source/helpers/jquery.fancybox-thumbs.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-media.js","path":"lib/fancybox/source/helpers/jquery.fancybox-media.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-thumbs.js","path":"lib/fancybox/source/helpers/jquery.fancybox-thumbs.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.ttf","path":"lib/font-awesome/fonts/fontawesome-webfont.ttf","modified":0,"renderable":1},{"_id":"source/img/cs131_linear_algebra.jpg","path":"img/cs131_linear_algebra.jpg","modified":0,"renderable":0},{"_id":"source/img/yolo2_result.png","path":"img/yolo2_result.png","modified":0,"renderable":0},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.js","path":"lib/algolia-instant-search/instantsearch.min.js","modified":0,"renderable":1},{"_id":"source/img/convolution.png","path":"img/convolution.png","modified":0,"renderable":0},{"_id":"source/img/dog_different_size.png","path":"img/dog_different_size.png","modified":0,"renderable":0},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.svg","path":"lib/font-awesome/fonts/fontawesome-webfont.svg","modified":0,"renderable":1},{"_id":"source/img/kmeans_scaling_up.png","path":"img/kmeans_scaling_up.png","modified":0,"renderable":0},{"_id":"source/img/yolo1_basic_idea.png","path":"img/yolo1_basic_idea.png","modified":0,"renderable":0},{"_id":"source/img/kmeans_algorithm.png","path":"img/kmeans_algorithm.png","modified":0,"renderable":0},{"_id":"source/img/sift_experiment_2.png","path":"img/sift_experiment_2.png","modified":0,"renderable":0},{"_id":"source/img/projective_geometry_property_1.png","path":"img/projective_geometry_property_1.png","modified":0,"renderable":0},{"_id":"source/img/kmeans_image_seg_via_intensity.png","path":"img/kmeans_image_seg_via_intensity.png","modified":0,"renderable":0},{"_id":"source/img/projective_geometry_property_2.png","path":"img/projective_geometry_property_2.png","modified":0,"renderable":0},{"_id":"source/img/camera_geometry_application.png","path":"img/camera_geometry_application.png","modified":0,"renderable":0},{"_id":"source/img/image_matching_hard.png","path":"img/image_matching_hard.png","modified":0,"renderable":0}],"Cache":[{"_id":"source/.DS_Store","hash":"82e765373fe2475242dceaebd47fc2b8175c331e","modified":1486797720000},{"_id":"themes/next/.bowerrc","hash":"3228a58ed0ece9f85e1e3136352094080b8dece1","modified":1488110969000},{"_id":"themes/next/.editorconfig","hash":"792fd2bd8174ece1a75d5fd24ab16594886f3a7f","modified":1488110969000},{"_id":"themes/next/.hound.yml","hash":"b76daa84c9ca3ad292c78412603370a367cc2bc3","modified":1488110969000},{"_id":"themes/next/.gitignore","hash":"5f09fca02e030b7676c1d312cd88ce8fbccf381c","modified":1488110969000},{"_id":"themes/next/.javascript_ignore","hash":"f9ea3c5395f8feb225a24e2c32baa79afda30c16","modified":1488110969000},{"_id":"themes/next/.jshintrc","hash":"9928f81bd822f6a8d67fdbc909b517178533bca9","modified":1488110969000},{"_id":"themes/next/bower.json","hash":"5abc236d9cc2512f5457ed57c1fba76669eb7399","modified":1488110969000},{"_id":"themes/next/README.en.md","hash":"3b0c7998cf17f9cf9e1a5bfcd65679a43a00c817","modified":1488110969000},{"_id":"themes/next/README.md","hash":"500b5606eb6a09c979d16128f8b00f4bf9bc95ac","modified":1488110969000},{"_id":"themes/next/_config.yml","hash":"263059170206812bae848d87c7baa1666cdf9cc2","modified":1488110969000},{"_id":"themes/next/gulpfile.coffee","hash":"61ef0606a8134894d7ac796bc8d0fa4ba6a94483","modified":1488110969000},{"_id":"themes/next/package.json","hash":"877cb98025e59015532c4c9a04a33e2af4ad56f9","modified":1488110969000},{"_id":"source/_posts/build-caffe-ubuntu.md","hash":"057b2c8409a7606d4e30ac81ec7d24191da86925","modified":1488110968000},{"_id":"source/_posts/cs131-edge-detection.md","hash":"a02d1bef37b25b7b0fd159019533bd02637be85b","modified":1488110968000},{"_id":"source/_posts/cs131-camera.md","hash":"9da13519e583f76d96d6bbe8cd37a032b03d7cdb","modified":1488110968000},{"_id":"source/_posts/cs131-filter-svd.md","hash":"d5ce30bc545a605ee347da5a58bea06db82dbe22","modified":1488110968000},{"_id":"source/_posts/cs131-linear-alg.md","hash":"7b56db3f15a3d743b35db4e05e91d057f5d065a4","modified":1488110968000},{"_id":"source/_posts/cs131-mean-shift.md","hash":"9bf359155997eef1e26d8575a0b431ecb1bfb92b","modified":1488110968000},{"_id":"source/_posts/cs131-finding-features.md","hash":"1390fe3ca5f3c751c869dfc4a15602d800b81c09","modified":1488110968000},{"_id":"source/_posts/cs131-kmeans.md","hash":"6bbf310095da3bdf051e605922083e66a33ad17c","modified":1488110968000},{"_id":"source/_posts/cs131-sift.md","hash":"c2d4392bb366299e96cfca061390bf79d8174e3c","modified":1488110968000},{"_id":"source/_posts/digitalocean-shadowsocks.md","hash":"c4a1c5b1d12107a2e21dc83c6cbdeab00fd8a163","modified":1488110968000},{"_id":"source/_posts/jupyternotebook-remote-useage.md","hash":"148c578c28e0c6d42fae5641bf5527e18a262b9e","modified":1488110968000},{"_id":"source/_posts/hello-world.md","hash":"12e5b7342c7683bf5e538412057aac08382bc9f7","modified":1488110968000},{"_id":"source/_posts/gsl-with-vs.md","hash":"73f407dc72fd5b71305fa83717e53bcbda6246e9","modified":1488110968000},{"_id":"source/_posts/pytorch-mnist-example.md","hash":"d448f36e632a45b5b9958811f0a40e2b099503f3","modified":1488640490000},{"_id":"source/_posts/python-reg-exp.md","hash":"f0b75c676805ff0314994b7db3cb11bf0a4091c7","modified":1488110968000},{"_id":"source/_posts/video-linear-alg-essential-property.md","hash":"b662e2be0424da344ef371636772a2bf6a20ad5e","modified":1488110968000},{"_id":"source/_posts/warpctc-caffe.md","hash":"83f759bb5d5c0a11a7646bda45ba0510cedb6508","modified":1488110968000},{"_id":"source/_posts/pytorch-tutor-01.md","hash":"cfe0effb4933e2daba0ae90882f8ca785fecc0a5","modified":1488295801000},{"_id":"source/_posts/use-doxygen.md","hash":"065620944cfe7c382ddd26f7e48cdc8b868f7336","modified":1488110968000},{"_id":"source/tags/index.md","hash":"62ca8832e828cf0af53b2774fe0b4c65af4feed6","modified":1488110968000},{"_id":"source/about/index.md","hash":"7eb7acffb61b7be9cbb41f0b72f409ec4e4be17e","modified":1488110968000},{"_id":"source/_posts/yolo-paper.md","hash":"b8abbcf900749972b77be63683e94ae9499dac09","modified":1488110968000},{"_id":"source/img/camera_skew.png","hash":"c02198d706e8366ffd307fccc931f0400317642c","modified":1488110968000},{"_id":"source/img/case_1_m.png","hash":"6d4b596285b981d003a660d3ce03a95a74946576","modified":1488110968000},{"_id":"source/img/case_2_m.png","hash":"ced9ffe517ff6250f0f6a4e402897592233018ba","modified":1488110968000},{"_id":"source/img/case_3_m.png","hash":"b2d5bc2d469bbf0e4a4534c191feb4a30132e5cc","modified":1488110968000},{"_id":"source/img/case_4_m.png","hash":"85dee8dc61da967d28caa56a10d949f9de5b2e24","modified":1488110968000},{"_id":"source/img/case_m_5.png","hash":"dca097cea37695ed4b237671b017c2f04940de8e","modified":1488110968000},{"_id":"source/img/corner_type_1.png","hash":"4c431c0faf73514d013771932114831969ab3038","modified":1488110968000},{"_id":"source/img/gsl_picture.jpg","hash":"1f4b216b84bae09a72bac183d04503ecf5215a0c","modified":1488110968000},{"_id":"source/img/jupyternotebook_logo.png","hash":"d6e1c6cf1171c28573949ef19f9cadfc19092a07","modified":1488110968000},{"_id":"source/img/helloworld_hexo.png","hash":"f4227d5040c36734b9fb0d5f57eb1f6669199b87","modified":1488110968000},{"_id":"source/img/kmeans_data_demo.png","hash":"f7cf89ccebbfd8d9b3ea5ab2b82f0fe52722755a","modified":1488110968000},{"_id":"source/img/kmeans_success.png","hash":"0fa428daf7f61a07b3d0c1946de4d6aa98fe0647","modified":1488110968000},{"_id":"source/img/line_fit_demo.png","hash":"0916a1148ce54c4b0f9bfc2dde33bd35782bf724","modified":1488110968000},{"_id":"source/img/meanshift_basics.jpg","hash":"6461e3234fbae264333a7c1a9cf74b1faf1fe7b5","modified":1488110968000},{"_id":"source/img/meanshift_simple_demo.png","hash":"201a77852c5d0c90c1fa0369d07599cb5237e2e5","modified":1488110968000},{"_id":"source/img/mnist_example.png","hash":"998011e1ab53d491adec736cebc319511764561e","modified":1488638555000},{"_id":"source/img/pytorch_logo.png","hash":"6fd1856de312e104cc62ad804b6006d205b74c74","modified":1488110968000},{"_id":"source/img/qicizuobiao.png","hash":"2bd2071a724832d96284a6d5a685991ab087f801","modified":1488110968000},{"_id":"source/img/ransac_step.png","hash":"eb9e2d50c4920cfd4516c9e7df2c981d8544b118","modified":1488110968000},{"_id":"source/img/rotation.png","hash":"11557a269f79012091059c15602438a69a211721","modified":1488110968000},{"_id":"source/img/sift_dominant_orientation.png","hash":"9a5a1538b25d9b7edcc18e23ad8a41fdaea341c8","modified":1488110968000},{"_id":"source/img/sift_picture.jpg","hash":"7c6be74bdd2c1ee630ba913e39012e8d515dc920","modified":1488110968000},{"_id":"source/img/svd_ranking.png","hash":"6cae2c6139567934a59a8e93d9672633d1016e1a","modified":1488110968000},{"_id":"source/img/svd_picture.jpg","hash":"10cae458d21313eb9409e87c7a680c79d188f395","modified":1488110968000},{"_id":"source/img/yolo2_bbox_param.png","hash":"5be107b1b67e823d3a2dd05f385dc98d3588dabf","modified":1488110968000},{"_id":"source/img/yolo2_cluster_result.png","hash":"1378adcf8ae8e6db2e7f1f81a9e6d6c8ec849fbb","modified":1488110968000},{"_id":"source/img/yolo2_bbox_location.png","hash":"0c2f9c8b0bf4993abf1596869028fc69c2774dfd","modified":1488110968000},{"_id":"themes/next/.github/ISSUE_TEMPLATE.md","hash":"c2024ded82143807c28a299c5fe6b927ef3525ff","modified":1488110969000},{"_id":"themes/next/.github/CONTRIBUTING.md","hash":"5ab257af816986cd0e53f9527a92d5934ac70ae9","modified":1488110969000},{"_id":"themes/next/languages/default.yml","hash":"767470a80dc257e23e14c3a78e8c52a46c9d6209","modified":1488110969000},{"_id":"themes/next/languages/de.yml","hash":"1fdea1f84b7f691f5b4dd4d2b43eeb27b10fa0c8","modified":1488110969000},{"_id":"themes/next/languages/en.yml","hash":"40057d6608e825d06e0864bac4dcd27ed88ada87","modified":1488110969000},{"_id":"themes/next/languages/fr-FR.yml","hash":"9fca01ef917d33ae2ae6bc04561ec6799dff5351","modified":1488110969000},{"_id":"themes/next/languages/id.yml","hash":"34396bef27c4ab9e9a3c5d3e3aa94b0e3b3a7b0d","modified":1488110969000},{"_id":"themes/next/languages/ko.yml","hash":"b6bc5d6b0c000deb44099b42d3aebb8c49dbfca9","modified":1488110969000},{"_id":"themes/next/languages/ja.yml","hash":"49f12149edcc1892b26a6207328cda64da20116d","modified":1488110969000},{"_id":"themes/next/languages/pt-BR.yml","hash":"7742ba4c0d682cbe1d38305332ebc928abd754b5","modified":1488110969000},{"_id":"themes/next/languages/pt.yml","hash":"6b660b117314cad93f08757601df3adb04c68beb","modified":1488110969000},{"_id":"themes/next/languages/ru.yml","hash":"257d11e626cbe4b9b78785a764190b9278f95c28","modified":1488110969000},{"_id":"themes/next/languages/zh-Hans.yml","hash":"f6c9fafa0f5f0050cd07ca2cf5e38fbae3e28145","modified":1488110969000},{"_id":"themes/next/languages/zh-hk.yml","hash":"34c84c6d04447a25bd5eac576922a13947c000e2","modified":1488110969000},{"_id":"themes/next/languages/zh-tw.yml","hash":"c97a5c41149de9b17f33439b0ecf0eff6fdae50e","modified":1488110969000},{"_id":"themes/next/layout/_layout.swig","hash":"7a1e4443c3ba1e08c20e64ddbf0b8255d034dab0","modified":1488110969000},{"_id":"themes/next/layout/archive.swig","hash":"b5b59d70fc1563f482fa07afd435752774ad5981","modified":1488110969000},{"_id":"themes/next/layout/category.swig","hash":"6422d196ceaff4220d54b8af770e7e957f3364ad","modified":1488110969000},{"_id":"themes/next/layout/index.swig","hash":"427d0b95b854e311ae363088ab39a393bf8fdc8b","modified":1488110969000},{"_id":"themes/next/layout/page.swig","hash":"3727fab9dadb967e9c2204edca787dc72264674a","modified":1488110969000},{"_id":"themes/next/layout/post.swig","hash":"e2e512142961ddfe77eba29eaa88f4a2ee43ae18","modified":1488110969000},{"_id":"themes/next/layout/schedule.swig","hash":"1f1cdc268f4ef773fd3ae693bbdf7d0b2f45c3a3","modified":1488110969000},{"_id":"themes/next/layout/tag.swig","hash":"07cf49c49c39a14dfbe9ce8e7d7eea3d4d0a4911","modified":1488110969000},{"_id":"themes/next/scripts/merge-configs.js","hash":"0c56be2e85c694247cfa327ea6d627b99ca265e8","modified":1488110969000},{"_id":"themes/next/test/.jshintrc","hash":"19f93d13d1689fe033c82eb2d5f3ce30b6543cc0","modified":1488110969000},{"_id":"themes/next/test/helpers.js","hash":"a1f5de25154c3724ffc24a91ddc576cdbd60864f","modified":1488110969000},{"_id":"themes/next/test/intern.js","hash":"11fa8a4f5c3b4119a179ae0a2584c8187f907a73","modified":1488110969000},{"_id":"source/img/canny_linking.png","hash":"8545dcacf874990d7d1b1ae074abbf4eb694f382","modified":1488110968000},{"_id":"source/img/canny_nms.png","hash":"363582d281cb85ba31ebf20dff0b1e7e7c2588cb","modified":1488110968000},{"_id":"source/img/case_6_m.png","hash":"bbd8245d6e3478b0287629f778aa8489489f3a37","modified":1488110968000},{"_id":"source/img/corner_window_fun.png","hash":"81b549c79bbe2bec390aab961ea503d9e80980a5","modified":1488110968000},{"_id":"source/img/doxygen_picture.png","hash":"f4236c94b6b661d6d54343d58e5fefbdc2d98160","modified":1488110968000},{"_id":"source/img/edge_camera_man.png","hash":"99c2057614bdc949df7310fded651a0ea80e4e5e","modified":1488110968000},{"_id":"source/img/edge_deriative.png","hash":"a26ae2c627cdf6b95419c89d52689ad06fdb7d0d","modified":1488110968000},{"_id":"source/img/god_use_vpn.png","hash":"9eac5df14cdc99596cd1872c4f958e60a943d90e","modified":1488110968000},{"_id":"source/img/harris_non_scale_constant.png","hash":"e36afe38ba3f1e428c435b00c12d030c43edb108","modified":1488110968000},{"_id":"source/img/meanshift_gradient_of_density.png","hash":"1a33449962e301dc741e56673579e055f3d41aab","modified":1488110968000},{"_id":"source/img/meanshift_kernel_function.png","hash":"8cababe376a596d11fce090968d0b3046c9fbaa6","modified":1488110968000},{"_id":"source/img/patch_average_intensity_scale_constant.png","hash":"bd0609e8de60de54e12c47ef7416c972f3582a0b","modified":1488110968000},{"_id":"source/img/qicizuobiao_transform.png","hash":"06b6ea9933a6d47c79b700f5c2d3c83e9cd4602f","modified":1488110968000},{"_id":"source/img/sift_detection_maximum.png","hash":"eaa0c3e9bda32792b35008184e794bd3e8966cf4","modified":1488110968000},{"_id":"source/img/rotation_matrix.png","hash":"35aed3946385abfcd448fe99c796db440ea0ddb6","modified":1488110968000},{"_id":"source/img/svd_superman.png","hash":"a4c3b97c4acfac14b978a6a00d163e2436227eda","modified":1488110968000},{"_id":"source/img/yolo1_network_arch.png","hash":"eff2b8461119c0a1f32268ebe8a8bfb3e3a15b99","modified":1488110968000},{"_id":"source/img/yolo1_loss_fun.png","hash":"b3019b1551e05b6fc109dfa788408582d0855aa1","modified":1488110968000},{"_id":"source/img/yolo2_different_methods_comparation.png","hash":"abc4705828afe401a773a2ce29e4d2fc8474237a","modified":1488110968000},{"_id":"source/img/yolo2_different_methods_improvement.png","hash":"05ee7120be804c38a2867508e39aaaaeff7601ed","modified":1488110968000},{"_id":"themes/next/source/fonts/.gitkeep","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1488110969000},{"_id":"source/img/camera_translation_rotation.png","hash":"bd89b487aff4a1c8aa5faff2dd8280557b480478","modified":1488110968000},{"_id":"source/img/corner_judge_2.png","hash":"d6daba0908de0a3686bc925c31f7c34adb4068f2","modified":1488110968000},{"_id":"source/img/corner_judge.png","hash":"4020cd84f4ea052bcc10ce09f25bd1bad8f5c293","modified":1488110968000},{"_id":"source/img/epipolar_constraint_1.png","hash":"25afbc27d8c7d984106b8fd65fff9aa4a97f624b","modified":1488110968000},{"_id":"source/img/epipolar_fig.png","hash":"ae34928aff5f8125b99f237c7e0dd1ee0b006839","modified":1488110968000},{"_id":"source/img/fun_noise.png","hash":"b1e7adbf4ffd2801b0685be5213579bf0f7d16e3","modified":1488110968000},{"_id":"source/img/generic_projection_matrix.png","hash":"4bf8906146b76f3fc74891042de14982eece2346","modified":1488110968000},{"_id":"source/img/kmeans_sensitive_to_outlier.png","hash":"a933295f1867a5684dd984d327f588de1a0f9fe8","modified":1488110968000},{"_id":"source/img/kmeans_object_fun_vs_k.png","hash":"f543e4d170edba374ceea98bb8d3904c4a557a16","modified":1488110968000},{"_id":"source/img/original_superman.png","hash":"310f3412935a64309e53d8e1d8403dd1c52206c8","modified":1488110968000},{"_id":"source/img/ransac_k.png","hash":"abe2b6c3ff381e1c113c96b4839e579a59da8da0","modified":1488110968000},{"_id":"source/img/what_is_corner.png","hash":"18bd784f1845b9252f72cb54e0128157db10a940","modified":1488110968000},{"_id":"source/img/yolo2_different_methods_comparation_in_table.png","hash":"7d0294948cc1680535e3dd04c179583005e07efa","modified":1488110968000},{"_id":"themes/next/layout/_macro/post-collapse.swig","hash":"5864f5567ba5efeabcf6ea355013c0b603ee07f2","modified":1488110969000},{"_id":"themes/next/layout/_custom/header.swig","hash":"adc83b19e793491b1c6ea0fd8b46cd9f32e592fc","modified":1488110969000},{"_id":"themes/next/layout/_custom/sidebar.swig","hash":"adc83b19e793491b1c6ea0fd8b46cd9f32e592fc","modified":1488110969000},{"_id":"themes/next/layout/_macro/post.swig","hash":"e6016def9b512188f4c2725399c9adc7bc41cdae","modified":1488110969000},{"_id":"themes/next/layout/_macro/wechat-subscriber.swig","hash":"14e785adeb0e671ba0ff9a553e6f0d8def6c670c","modified":1488110969000},{"_id":"themes/next/layout/_macro/sidebar.swig","hash":"43d8830bb19da4fc7a5773866be19fa066b62645","modified":1488110969000},{"_id":"themes/next/layout/_macro/reward.swig","hash":"37e5b7c42ec17b9b6b786c5512bcc481a21c974e","modified":1488110969000},{"_id":"themes/next/layout/_partials/comments.swig","hash":"31e3b17af134d05fe293f8a48ebea5f9eb2ec21e","modified":1488110969000},{"_id":"themes/next/layout/_partials/header.swig","hash":"adab5c3f7b173f1b45454787f39dde07aea03483","modified":1488110969000},{"_id":"themes/next/layout/_partials/head.swig","hash":"ca56f92e2fa82b03853869f5073ee1a5626a4796","modified":1488110969000},{"_id":"themes/next/layout/_partials/duoshuo-hot-articles.swig","hash":"5d4638c46aef65bf32a01681495b62416ccc98db","modified":1488110969000},{"_id":"themes/next/layout/_partials/footer.swig","hash":"7172c6053118b7c291a56a7860128a652ae66b83","modified":1488110969000},{"_id":"themes/next/layout/_scripts/baidu-push.swig","hash":"c057b17f79e8261680fbae8dc4e81317a127c799","modified":1488110969000},{"_id":"themes/next/layout/_partials/pagination.swig","hash":"9e8e21d194ef44d271b1cca0bc1448c14d7edf4f","modified":1488110969000},{"_id":"themes/next/layout/_partials/search.swig","hash":"1431719d1dbba3f5ee385eebc46376d1a960b2d5","modified":1488110969000},{"_id":"themes/next/layout/_partials/page-header.swig","hash":"39d613e5a9f8389d4ea52d6082502af8e833b9f2","modified":1488110969000},{"_id":"themes/next/scripts/tags/button.js","hash":"62e6dbeb53d07627a048132c79630b45d9a8f2cc","modified":1488110969000},{"_id":"themes/next/scripts/tags/center-quote.js","hash":"535fc542781021c4326dec24d8495cbb1387634a","modified":1488110969000},{"_id":"themes/next/scripts/tags/full-image.js","hash":"8eeb3fb89540299bdbb799edfdfdac3743b50596","modified":1488110969000},{"_id":"themes/next/scripts/tags/group-pictures.js","hash":"49252824cd53184dc9b97b2f2d87ff28e1b3ef27","modified":1488110969000},{"_id":"themes/next/scripts/tags/note.js","hash":"6752925eedbdb939d8ec4d11bdfb75199f18dd70","modified":1488110969000},{"_id":"themes/next/layout/_scripts/boostrap.swig","hash":"03aaebe9d50f6acb007ec38cc04acd1cfceb404d","modified":1488110969000},{"_id":"themes/next/layout/_scripts/commons.swig","hash":"766b2bdda29523ed6cd8d7aa197f996022f8fd94","modified":1488110969000},{"_id":"themes/next/layout/_scripts/vendors.swig","hash":"4512867d80d9eddfc3a0f5fea3c456f33aa9d522","modified":1488110969000},{"_id":"themes/next/source/css/main.styl","hash":"20702c48d6053c92c5bcdbc68e8d0ef1369848a0","modified":1488110969000},{"_id":"themes/next/source/images/avatar.gif","hash":"264082bb3a1af70d5499c7d22b0902cb454b6d12","modified":1488110969000},{"_id":"themes/next/source/images/algolia_logo.svg","hash":"90035272fa31a3f65b3c0e2cb8a633876ef457dc","modified":1488110969000},{"_id":"themes/next/source/images/cc-by-nc-sa.svg","hash":"3031be41e8753c70508aa88e84ed8f4f653f157e","modified":1488110969000},{"_id":"themes/next/source/images/cc-by-nc-nd.svg","hash":"c6524ece3f8039a5f612feaf865d21ec8a794564","modified":1488110969000},{"_id":"themes/next/source/images/cc-by-nc.svg","hash":"8d39b39d88f8501c0d27f8df9aae47136ebc59b7","modified":1488110969000},{"_id":"themes/next/source/images/cc-by-sa.svg","hash":"aa4742d733c8af8d38d4c183b8adbdcab045872e","modified":1488110969000},{"_id":"themes/next/source/images/cc-by-nd.svg","hash":"c563508ce9ced1e66948024ba1153400ac0e0621","modified":1488110969000},{"_id":"themes/next/source/images/cc-by.svg","hash":"28a0a4fe355a974a5e42f68031652b76798d4f7e","modified":1488110969000},{"_id":"themes/next/source/images/cc-zero.svg","hash":"87669bf8ac268a91d027a0a4802c92a1473e9030","modified":1488110969000},{"_id":"themes/next/source/images/loading.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1488110969000},{"_id":"themes/next/source/images/placeholder.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1488110969000},{"_id":"themes/next/source/images/quote-l.svg","hash":"94e870b4c8c48da61d09522196d4dd40e277a98f","modified":1488110969000},{"_id":"themes/next/source/images/quote-r.svg","hash":"e60ae504f9d99b712c793c3740c6b100d057d4ec","modified":1488110969000},{"_id":"themes/next/source/images/searchicon.png","hash":"67727a6a969be0b2659b908518fa6706eed307b8","modified":1488110969000},{"_id":"source/img/caffe_image.jpg","hash":"a183ae9970b3fddd258210175a9f2f3ac62306be","modified":1488110968000},{"_id":"source/img/kmeans_bigger_demo.png","hash":"926e9588bdb1c1bba243b0702ccfa959a4f46715","modified":1488110968000},{"_id":"source/img/pinhole_camera_model.png","hash":"b72eab7554feea7be265c28d0b11efc3156dae40","modified":1488110968000},{"_id":"source/img/yolo2_dartnet_19_structure.png","hash":"4f2a9c8c3d17fe4dacb97d3964dd568493c45422","modified":1488110968000},{"_id":"themes/next/layout/_scripts/schemes/mist.swig","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1488110969000},{"_id":"themes/next/layout/_scripts/schemes/muse.swig","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1488110969000},{"_id":"themes/next/source/css/_mixins/Mist.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1488110969000},{"_id":"themes/next/source/css/_mixins/Muse.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1488110969000},{"_id":"themes/next/source/css/_mixins/custom.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1488110969000},{"_id":"themes/next/source/css/_variables/Muse.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1488110969000},{"_id":"themes/next/source/css/_variables/custom.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1488110969000},{"_id":"source/avatar/liumengli.jpg","hash":"4069caff1851227a11b74759a75bec586c568cd3","modified":1488110968000},{"_id":"themes/next/layout/_components/algolia-search/assets.swig","hash":"28ff4ed6714c59124569ffcbd10f1173d53ca923","modified":1488110969000},{"_id":"themes/next/layout/_components/algolia-search/dom.swig","hash":"636f1181dd5887a70b4a08ca8f655d4e46635792","modified":1488110969000},{"_id":"themes/next/layout/_partials/head/custom-head.swig","hash":"9e1b9666efa77f4cf8d8261bcfa445a9ac608e53","modified":1488110969000},{"_id":"themes/next/layout/_partials/head/external-fonts.swig","hash":"7ce76358411184482bb0934e70037949dd0da8ca","modified":1488110969000},{"_id":"themes/next/layout/_partials/search/localsearch.swig","hash":"ff5523d5dacaa77a55a24e50e6e6530c3b98bfad","modified":1488110969000},{"_id":"themes/next/layout/_partials/search/swiftype.swig","hash":"959b7e04a96a5596056e4009b73b6489c117597e","modified":1488110969000},{"_id":"themes/next/layout/_partials/search/tinysou.swig","hash":"eefe2388ff3d424694045eda21346989b123977c","modified":1488110969000},{"_id":"themes/next/layout/_partials/share/add-this.swig","hash":"23e23dc0f76ef3c631f24c65277adf7ea517b383","modified":1488110969000},{"_id":"themes/next/layout/_partials/share/baidushare.swig","hash":"1f1107468aaf03f7d0dcd7eb2b653e2813a675b4","modified":1488110969000},{"_id":"themes/next/layout/_partials/share/duoshuo_share.swig","hash":"89c5a5240ecb223acfe1d12377df5562a943fd5d","modified":1488110969000},{"_id":"themes/next/layout/_partials/share/jiathis.swig","hash":"63315fcf210799f894208c9f512737096df84962","modified":1488110969000},{"_id":"themes/next/layout/_scripts/pages/post-details.swig","hash":"069d1357c717572256e5cdee09574ebce529cbae","modified":1488110969000},{"_id":"themes/next/layout/_scripts/schemes/pisces.swig","hash":"a44acf9b0d0f44ef3dfc767376a95c984cc127de","modified":1488110969000},{"_id":"themes/next/layout/_scripts/third-party/analytics.swig","hash":"394d9fff7951287cc90f52acc2d4cbfd1bae079d","modified":1488110969000},{"_id":"themes/next/layout/_scripts/third-party/comments.swig","hash":"4abc01bc870e1d7a783cdbd26166edc782a6a4f4","modified":1488110969000},{"_id":"themes/next/layout/_scripts/third-party/lean-analytics.swig","hash":"92dc60821307fc9769bea9b2d60adaeb798342af","modified":1488110969000},{"_id":"themes/next/layout/_scripts/third-party/tinysou.swig","hash":"cb3a5d36dbe1630bab84e03a52733a46df7c219b","modified":1488110969000},{"_id":"themes/next/layout/_scripts/third-party/mathjax.swig","hash":"6d25596d6a7c57700d37b607f8d9a62d89708683","modified":1488110969000},{"_id":"themes/next/layout/_scripts/third-party/localsearch.swig","hash":"b460e27db3dcd4ab40b17d8926a5c4e624f293a9","modified":1488110969000},{"_id":"themes/next/layout/_scripts/third-party/schedule.swig","hash":"22369026c87fc23893c35a7f250b42f3bb1b60f1","modified":1488110969000},{"_id":"themes/next/source/css/_custom/custom.styl","hash":"328d9a9696cc2ccf59c67d3c26000d569f46344c","modified":1488110969000},{"_id":"themes/next/source/css/_mixins/Pisces.styl","hash":"715d5b40dc52f319fe4bff0325beb874774d9bd9","modified":1488110969000},{"_id":"themes/next/source/css/_mixins/base.styl","hash":"78a83c38f69a8747bb74e420e6c9eeef1ea76525","modified":1488110969000},{"_id":"themes/next/source/css/_variables/Mist.styl","hash":"c8d35a6b9e3bff6d8fdb66de853065af9d37562d","modified":1488110969000},{"_id":"themes/next/source/css/_variables/Pisces.styl","hash":"c459aa6d607d8bcb747544e74f6ad0b8374aa3b1","modified":1488110969000},{"_id":"themes/next/source/css/_variables/base.styl","hash":"fc185c6cec79593775d1c2440dbe2a71cfbe2e99","modified":1488110969000},{"_id":"themes/next/source/js/src/affix.js","hash":"978e0422b5bf1b560236d8d10ebc1adcf66392e3","modified":1488110969000},{"_id":"themes/next/source/js/src/algolia-search.js","hash":"96b29f69b8b916b22f62c9959a117b5a968200a5","modified":1488110969000},{"_id":"themes/next/source/js/src/bootstrap.js","hash":"39bf93769d9080fa01a9a875183b43198f79bc19","modified":1488110969000},{"_id":"themes/next/source/js/src/hook-duoshuo.js","hash":"a6119070c0119f33e08b29da7d2cce2635eb40a0","modified":1488110969000},{"_id":"themes/next/source/js/src/motion.js","hash":"269414e84df544a4ccb88519f6abae4943db3c67","modified":1488110969000},{"_id":"themes/next/source/js/src/post-details.js","hash":"2038f54e289b6da5def09689e69f623187147be5","modified":1488110969000},{"_id":"themes/next/source/js/src/scrollspy.js","hash":"fe4da1b9fe73518226446f5f27d2831e4426fc35","modified":1488110969000},{"_id":"themes/next/source/js/src/utils.js","hash":"384e17ff857f073060f5bf8c6e4f4b7353236331","modified":1488110969000},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.css","hash":"90ef19edc982645b118b095615838d9c5eaba0de","modified":1488110969000},{"_id":"themes/next/source/lib/canvas-nest/canvas-nest.min.js","hash":"0387e75e23b1db108a755073fe52a0d03eb391a7","modified":1488110969000},{"_id":"themes/next/source/lib/fastclick/.bower.json","hash":"93ebd5b35e632f714dcf1753e1f6db77ec74449b","modified":1488110969000},{"_id":"themes/next/source/lib/fastclick/LICENSE","hash":"dcd5b6b43095d9e90353a28b09cb269de8d4838e","modified":1488110969000},{"_id":"themes/next/source/lib/fastclick/README.md","hash":"1decd8e1adad2cd6db0ab50cf56de6035156f4ea","modified":1488110969000},{"_id":"themes/next/source/lib/fastclick/bower.json","hash":"13379463c7463b4b96d13556b46faa4cc38d81e6","modified":1488110969000},{"_id":"themes/next/source/lib/font-awesome/.bower.json","hash":"c1072942459fa0880e8a33a1bd929176b62b4171","modified":1488110969000},{"_id":"themes/next/source/lib/font-awesome/.gitignore","hash":"69d152fa46b517141ec3b1114dd6134724494d83","modified":1488110969000},{"_id":"themes/next/source/lib/font-awesome/.npmignore","hash":"dcf470ab3a358103bb896a539cc03caeda10fa8b","modified":1488110969000},{"_id":"themes/next/source/lib/font-awesome/HELP-US-OUT.txt","hash":"4f7bf961f1bed448f6ba99aeb9219fabf930ba96","modified":1488110969000},{"_id":"themes/next/source/lib/font-awesome/bower.json","hash":"279a8a718ab6c930a67c41237f0aac166c1b9440","modified":1488110969000},{"_id":"themes/next/source/lib/jquery/.bower.json","hash":"91745c2cc6c946c7275f952b2b0760b880cea69e","modified":1488110969000},{"_id":"themes/next/source/lib/jquery_lazyload/.bower.json","hash":"b7638afc93e9cd350d0783565ee9a7da6805ad8e","modified":1488110969000},{"_id":"themes/next/source/lib/jquery_lazyload/CONTRIBUTING.md","hash":"4891864c24c28efecd81a6a8d3f261145190f901","modified":1488110969000},{"_id":"themes/next/source/lib/jquery_lazyload/README.md","hash":"895d50fa29759af7835256522e9dd7dac597765c","modified":1488110969000},{"_id":"themes/next/source/lib/jquery_lazyload/bower.json","hash":"65bc85d12197e71c40a55c0cd7f6823995a05222","modified":1488110969000},{"_id":"themes/next/source/lib/jquery_lazyload/jquery.scrollstop.js","hash":"0e9a81785a011c98be5ea821a8ed7d411818cfd1","modified":1488110969000},{"_id":"themes/next/source/lib/jquery_lazyload/jquery.lazyload.js","hash":"481fd478650e12b67c201a0ea41e92743f8b45a3","modified":1488110969000},{"_id":"themes/next/source/lib/velocity/.bower.json","hash":"05f960846f1c7a93dab1d3f9a1121e86812e8c88","modified":1488110969000},{"_id":"themes/next/source/lib/velocity/bower.json","hash":"2ec99573e84c7117368beccb9e94b6bf35d2db03","modified":1488110969000},{"_id":"themes/next/source/lib/velocity/velocity.ui.min.js","hash":"ed5e534cd680a25d8d14429af824f38a2c7d9908","modified":1488110969000},{"_id":"themes/next/source/lib/velocity/velocity.min.js","hash":"2f1afadc12e4cf59ef3b405308d21baa97e739c6","modified":1488110969000},{"_id":"themes/next/source/lib/velocity/velocity.ui.js","hash":"6a1d101eab3de87527bb54fcc8c7b36b79d8f0df","modified":1488110969000},{"_id":"source/img/kmeans_demo.png","hash":"b10c3c2d5eaf201c0111cde24cc8d3c4533134dd","modified":1488110968000},{"_id":"source/img/svd_flower.png","hash":"e1924f7eaf2a920c769fd63ba27baf210adf59c7","modified":1488110968000},{"_id":"source/img/video_linear_alg_essential.png","hash":"e11c2c04b910a7b9f1a5ccaee3ae9dbb67fca666","modified":1488110968000},{"_id":"source/img/dog_x.png","hash":"5672ad3c046997781703816e4d58d3961fe26eeb","modified":1488110968000},{"_id":"themes/next/source/lib/jquery/index.js","hash":"41b4bfbaa96be6d1440db6e78004ade1c134e276","modified":1488110969000},{"_id":"source/img/ransac_line_fit.png","hash":"f3711368fa15222a40f134c267842999574277e2","modified":1488110968000},{"_id":"source/img/regex_picture.jpg","hash":"1744ddfe9d73a4af202364cbea16ba0f2211dc50","modified":1488110968000},{"_id":"source/img/sift_dog.png","hash":"60371c0dbb5dfae010c84c59227dac9d878d3530","modified":1488110968000},{"_id":"themes/next/layout/_scripts/third-party/analytics/busuanzi-counter.swig","hash":"4fcbf57c4918528ab51d3d042cff92cf5aefb599","modified":1488110969000},{"_id":"themes/next/layout/_scripts/third-party/analytics/application-insights.swig","hash":"60426bf73f8a89ba61fb1be2df3ad5398e32c4ef","modified":1488110969000},{"_id":"themes/next/layout/_scripts/third-party/analytics/cnzz-analytics.swig","hash":"8160b27bee0aa372c7dc7c8476c05bae57f58d0f","modified":1488110969000},{"_id":"themes/next/layout/_scripts/third-party/analytics/baidu-analytics.swig","hash":"deda6a814ed48debc694c4e0c466f06c127163d0","modified":1488110969000},{"_id":"themes/next/layout/_scripts/third-party/analytics/google-analytics.swig","hash":"30a23fa7e816496fdec0e932aa42e2d13098a9c2","modified":1488110969000},{"_id":"themes/next/layout/_scripts/third-party/comments/disqus.swig","hash":"fb1d04ede838b52ca7541973f86c3810f1ad396e","modified":1488110969000},{"_id":"themes/next/layout/_scripts/third-party/analytics/facebook-sdk.swig","hash":"394d008e5e94575280407ad8a1607a028026cbc3","modified":1488110969000},{"_id":"themes/next/layout/_scripts/third-party/analytics/tencent-analytics.swig","hash":"3658414379e0e8a34c45c40feadc3edc8dc55f88","modified":1488110969000},{"_id":"themes/next/layout/_scripts/third-party/comments/duoshuo.swig","hash":"a356b2185d40914447fde817eb3d358ab6b3e4c3","modified":1488110969000},{"_id":"themes/next/layout/_scripts/third-party/comments/hypercomments.swig","hash":"3e8dc5c6c912628a37e3b5f886bec7b2e5ed14ea","modified":1488110969000},{"_id":"themes/next/layout/_scripts/third-party/comments/youyan.swig","hash":"ea8078fa9e10be2bb042749d8b6a97adc38f914c","modified":1488110969000},{"_id":"themes/next/layout/_scripts/third-party/comments/gentie.swig","hash":"03592d1d731592103a41ebb87437fe4b0a4c78ca","modified":1488110969000},{"_id":"themes/next/source/css/_common/components/comments.styl","hash":"471f1627891aca5c0e1973e09fbcb01e1510d193","modified":1488110969000},{"_id":"themes/next/source/css/_common/components/buttons.styl","hash":"0dfb4b3ba3180d7285e66f270e1d3fa0f132c3d2","modified":1488110969000},{"_id":"themes/next/source/css/_common/components/back-to-top.styl","hash":"b49efc66bd055a2d0be7deabfcb02ee72a9a28c8","modified":1488110969000},{"_id":"themes/next/source/css/_common/components/components.styl","hash":"10994990d6e0b4d965a728a22cf7f6ee29cae9f6","modified":1488110969000},{"_id":"themes/next/source/css/_common/components/pagination.styl","hash":"711c8830886619d4f4a0598b0cde5499dce50c62","modified":1488110969000},{"_id":"themes/next/source/css/_common/components/tag-cloud.styl","hash":"dd8a3b22fc2f222ac6e6c05bd8a773fb039169c0","modified":1488110969000},{"_id":"themes/next/source/css/_common/outline/outline.styl","hash":"2186be20e317505cd31886f1291429cc21f76703","modified":1488110969000},{"_id":"themes/next/source/css/_common/scaffolding/helpers.styl","hash":"54c90cf7bdbf5c596179d8dae6e671bad1292662","modified":1488110969000},{"_id":"themes/next/source/css/_common/scaffolding/base.styl","hash":"5304f99581da3a31de3ecec959b7adf9002fde83","modified":1488110969000},{"_id":"themes/next/source/css/_common/scaffolding/normalize.styl","hash":"ece571f38180febaf02ace8187ead8318a300ea7","modified":1488110969000},{"_id":"themes/next/source/css/_common/scaffolding/scaffolding.styl","hash":"013619c472c7e4b08311c464fcbe9fcf5edde603","modified":1488110969000},{"_id":"themes/next/source/css/_common/scaffolding/tables.styl","hash":"64f5d56c08d74a338813df1265580ca0cbf0190b","modified":1488110969000},{"_id":"themes/next/source/css/_schemes/Mist/_base.styl","hash":"c2d079788d6fc2e9a191ccdae94e50d55bf849dc","modified":1488110969000},{"_id":"themes/next/source/css/_schemes/Mist/_header.styl","hash":"5ae7906dc7c1d9468c7f4b4a6feddddc555797a1","modified":1488110969000},{"_id":"themes/next/source/css/_schemes/Mist/_logo.styl","hash":"38e5df90c8689a71c978fd83ba74af3d4e4e5386","modified":1488110969000},{"_id":"themes/next/source/css/_schemes/Mist/_menu.styl","hash":"b0dcca862cd0cc6e732e33d975b476d744911742","modified":1488110969000},{"_id":"themes/next/source/css/_schemes/Mist/_posts-expanded.styl","hash":"fda14bc35be2e1b332809b55b3d07155a833dbf4","modified":1488110969000},{"_id":"themes/next/source/css/_schemes/Mist/_search.styl","hash":"1452cbe674cc1d008e1e9640eb4283841058fc64","modified":1488110969000},{"_id":"themes/next/source/css/_schemes/Mist/index.styl","hash":"9a5581a770af8964064fef7afd3e16963e45547f","modified":1488110969000},{"_id":"themes/next/source/css/_schemes/Muse/_logo.styl","hash":"8829bc556ca38bfec4add4f15a2f028092ac6d46","modified":1488110969000},{"_id":"themes/next/source/css/_schemes/Muse/_layout.styl","hash":"0efa036a15c18f5abb058b7c0fad1dd9ac5eed4c","modified":1488110969000},{"_id":"themes/next/source/css/_schemes/Muse/_menu.styl","hash":"82bbaa6322764779a1ac2e2c8390ce901c7972e2","modified":1488110969000},{"_id":"themes/next/source/css/_schemes/Muse/_search.styl","hash":"1452cbe674cc1d008e1e9640eb4283841058fc64","modified":1488110969000},{"_id":"themes/next/source/css/_schemes/Muse/index.styl","hash":"a0e2030a606c934fb2c5c7373aaae04a1caac4c5","modified":1488110969000},{"_id":"themes/next/source/css/_schemes/Pisces/_brand.styl","hash":"ff9f163bb05c0709577040a875924d36c9ab99d6","modified":1488110969000},{"_id":"themes/next/source/css/_schemes/Pisces/_layout.styl","hash":"dcf9fe43b2ef78b923118ba39efedb38760e76b1","modified":1488110969000},{"_id":"themes/next/source/css/_schemes/Pisces/_menu.styl","hash":"1408209dfb9a22a0982a30bdbd14842c2b53f264","modified":1488110969000},{"_id":"themes/next/source/css/_schemes/Pisces/_posts.styl","hash":"2f878213cb24c5ddc18877f6d15ec5c5f57745ac","modified":1488110969000},{"_id":"themes/next/source/css/_schemes/Pisces/_sidebar.styl","hash":"9b63bd8effc7cf4b96acdea4d73add7df934a222","modified":1488110969000},{"_id":"themes/next/source/css/_schemes/Pisces/index.styl","hash":"69ecd6c97e7cdfd822ac8102b45ad0ede85050db","modified":1488110969000},{"_id":"themes/next/source/js/src/schemes/pisces.js","hash":"9ccee9189c910b8a264802d7b2ec305d12dedcd0","modified":1488110969000},{"_id":"themes/next/source/lib/fancybox/source/blank.gif","hash":"2daeaa8b5f19f0bc209d976c02bd6acb51b00b0a","modified":1488110969000},{"_id":"themes/next/source/lib/fancybox/source/fancybox_loading.gif","hash":"1a755fb2599f3a313cc6cfdb14df043f8c14a99c","modified":1488110969000},{"_id":"themes/next/source/lib/fancybox/source/fancybox_overlay.png","hash":"b3a4ee645ba494f52840ef8412015ba0f465dbe0","modified":1488110969000},{"_id":"themes/next/source/lib/fancybox/source/fancybox_loading@2x.gif","hash":"273b123496a42ba45c3416adb027cd99745058b0","modified":1488110969000},{"_id":"themes/next/source/lib/fancybox/source/fancybox_sprite.png","hash":"17df19f97628e77be09c352bf27425faea248251","modified":1488110969000},{"_id":"themes/next/source/lib/fancybox/source/fancybox_sprite@2x.png","hash":"30c58913f327e28f466a00f4c1ac8001b560aed8","modified":1488110969000},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.css","hash":"5f163444617b6cf267342f06ac166a237bb62df9","modified":1488110969000},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.pack.js","hash":"53360764b429c212f424399384417ccc233bb3be","modified":1488110969000},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.js","hash":"1cf3d47b5ccb7cb6e9019c64f2a88d03a64853e4","modified":1488110969000},{"_id":"themes/next/source/lib/fastclick/lib/fastclick.min.js","hash":"2cae0f5a6c5d6f3cb993015e6863f9483fc4de18","modified":1488110969000},{"_id":"themes/next/source/lib/fastclick/lib/fastclick.js","hash":"06cef196733a710e77ad7e386ced6963f092dc55","modified":1488110969000},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css","hash":"4eda182cbcc046dbf449aef97c02c230cf80a494","modified":1488110969000},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css.map","hash":"0189d278706509412bac4745f96c83984e1d59f4","modified":1488110969000},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.min.css","hash":"fb5b49426dee7f1508500e698d1b3c6b04c8fcce","modified":1488110969000},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.min.js","hash":"38628e75e4412cc6f11074e03e1c6d257aae495b","modified":1488110969000},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.pack.js","hash":"214dad442a92d36af77ed0ca1d9092b16687f02f","modified":1488110969000},{"_id":"source/img/sift_experiment_1.png","hash":"87873fe348c7cb572b8d499730cf2ebc86a1e4ef","modified":1488110968000},{"_id":"source/img/warpctc_intro.png","hash":"2521cbdd73abfb274c8cb3cdd3bb91f86a4d3866","modified":1488110968000},{"_id":"source/img/yolo1_detection_system.png","hash":"3406908556a3d80fc565a94f6e394bfba6c69413","modified":1488110968000},{"_id":"themes/next/source/lib/font-awesome/fonts/FontAwesome.otf","hash":"1b22f17fdc38070de50e6d1ab3a32da71aa2d819","modified":1488110969000},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.eot","hash":"965ce8f688fedbeed504efd498bc9c1622d12362","modified":1488110969000},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff2","hash":"97e438cc545714309882fbceadbf344fcaddcec5","modified":1488110969000},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff","hash":"6d7e6a5fc802b13694d8820fc0138037c0977d2e","modified":1488110969000},{"_id":"source/img/camera_model_things_to_remember.png","hash":"687a27be2aa5a335814315d19421f29fa79fee06","modified":1488110968000},{"_id":"themes/next/source/lib/velocity/velocity.js","hash":"9f08181baea0cc0e906703b7e5df9111b9ef3373","modified":1488110969000},{"_id":"themes/next/source/css/_common/components/highlight/diff.styl","hash":"96f32ea6c3265a3889e6abe57587f6e2a2a40dfb","modified":1488110969000},{"_id":"themes/next/source/css/_common/components/footer/footer.styl","hash":"8994ffcce84deac0471532f270f97c44fea54dc0","modified":1488110969000},{"_id":"themes/next/source/css/_common/components/highlight/highlight.styl","hash":"4b7f81e1006e7acee3d1c840ccba155239f830cc","modified":1488110969000},{"_id":"themes/next/source/css/_common/components/highlight/theme.styl","hash":"b76387934fb6bb75212b23c1a194486892cc495e","modified":1488110969000},{"_id":"themes/next/source/css/_common/components/header/header.styl","hash":"ae1ca14e51de67b07dba8f61ec79ee0e2e344574","modified":1488110969000},{"_id":"themes/next/source/css/_common/components/header/headerband.styl","hash":"d27448f199fc2f9980b601bc22b87f08b5d64dd1","modified":1488110969000},{"_id":"themes/next/source/css/_common/components/header/menu.styl","hash":"c890ce7fe933abad7baf39764a01894924854e92","modified":1488110969000},{"_id":"themes/next/source/css/_common/components/header/site-meta.styl","hash":"6c00f6e0978f4d8f9a846a15579963728aaa6a17","modified":1488110969000},{"_id":"themes/next/source/css/_common/components/header/site-nav.styl","hash":"49c2b2c14a1e7fcc810c6be4b632975d0204c281","modified":1488110969000},{"_id":"themes/next/source/css/_common/components/pages/archive.styl","hash":"7778920dd105fa4de3a7ab206eeba30b1a7bac45","modified":1488110969000},{"_id":"themes/next/source/css/_common/components/pages/categories.styl","hash":"4eff5b252d7b614e500fc7d52c97ce325e57d3ab","modified":1488110969000},{"_id":"themes/next/source/css/_common/components/pages/pages.styl","hash":"2039590632bba3943c39319d80ef630af7928185","modified":1488110969000},{"_id":"themes/next/source/css/_common/components/post/post-button.styl","hash":"fdfadbb4483043c7e0afd541ee9712389e633517","modified":1488110969000},{"_id":"themes/next/source/css/_common/components/pages/post-detail.styl","hash":"9bf4362a4d0ae151ada84b219d39fbe5bb8c790e","modified":1488110969000},{"_id":"themes/next/source/css/_common/components/pages/schedule.styl","hash":"a82afbb72d83ee394aedc7b37ac0008a9823b4f4","modified":1488110969000},{"_id":"themes/next/source/css/_common/components/post/post-collapse.styl","hash":"8fae54591877a73dff0b29b2be2e8935e3c63575","modified":1488110969000},{"_id":"themes/next/source/css/_common/components/post/post-eof.styl","hash":"2cdc094ecf907a02fce25ad4a607cd5c40da0f2b","modified":1488110969000},{"_id":"themes/next/source/css/_common/components/post/post-expand.styl","hash":"b25132fe6a7ad67059a2c3afc60feabb479bdd75","modified":1488110969000},{"_id":"themes/next/source/css/_common/components/post/post-gallery.styl","hash":"387ce23bba52b22a586b2dfb4ec618fe1ffd3926","modified":1488110969000},{"_id":"themes/next/source/css/_common/components/post/post-meta.styl","hash":"7f1aab694caf603809e33cff82beea84cd0128fd","modified":1488110969000},{"_id":"themes/next/source/css/_common/components/post/post-nav.styl","hash":"c6dab7661a6b8c678b21b7eb273cef7100f970f6","modified":1488110969000},{"_id":"themes/next/source/css/_common/components/post/post-reward.styl","hash":"e792c8dc41561c96d128e9b421187f1c3dc978a0","modified":1488110969000},{"_id":"themes/next/source/css/_common/components/post/post-tags.styl","hash":"a352ae5b1f8857393bf770d2e638bf15f0c9585d","modified":1488110969000},{"_id":"themes/next/source/css/_common/components/post/post-title.styl","hash":"963105a531403d7aad6d9e5e23e3bfabb8ec065a","modified":1488110969000},{"_id":"themes/next/source/css/_common/components/post/post-type.styl","hash":"10251257aceecb117233c9554dcf8ecfef8e2104","modified":1488110969000},{"_id":"themes/next/source/css/_common/components/post/post.styl","hash":"bfd806d0a9f21446a22df82ac02e37d0075cc3b5","modified":1488110969000},{"_id":"themes/next/source/css/_common/components/tags/blockquote-center.styl","hash":"c2abe4d87148e23e15d49ee225bc650de60baf46","modified":1488110969000},{"_id":"themes/next/source/css/_common/components/tags/full-image.styl","hash":"b8969e1654eec89a0fd10d88b337fee9cb03cd44","modified":1488110969000},{"_id":"themes/next/source/css/_common/components/tags/group-pictures.styl","hash":"4851b981020c5cbc354a1af9b831a2dcb3cf9d39","modified":1488110969000},{"_id":"themes/next/source/css/_common/components/tags/note.styl","hash":"74d0ba86f698165d13402670382a822c8736a556","modified":1488110969000},{"_id":"themes/next/source/css/_common/components/tags/tags.styl","hash":"3eb73cee103b810fa56901577ecb9c9bb1793cff","modified":1488110969000},{"_id":"themes/next/source/css/_common/components/third-party/algolia-search.styl","hash":"eba491ae624b4c843c8be4c94a044085dad4ba0f","modified":1488110969000},{"_id":"themes/next/source/css/_common/components/third-party/baidushare.styl","hash":"93b08815c4d17e2b96fef8530ec1f1064dede6ef","modified":1488110969000},{"_id":"themes/next/source/css/_common/components/third-party/busuanzi-counter.styl","hash":"b03f891883446f3a5548b7cc90d29c77e62f1053","modified":1488110969000},{"_id":"themes/next/source/css/_common/components/third-party/duoshuo.styl","hash":"2340dd9b3202c61d73cc708b790fac5adddbfc7f","modified":1488110969000},{"_id":"themes/next/source/css/_common/components/third-party/gentie.styl","hash":"586a3ec0f1015e7207cd6a2474362e068c341744","modified":1488110969000},{"_id":"themes/next/source/css/_common/components/third-party/jiathis.styl","hash":"327b5f63d55ec26f7663185c1a778440588d9803","modified":1488110969000},{"_id":"themes/next/source/css/_common/components/third-party/localsearch.styl","hash":"637c6b32c58ecf40041be6e911471cd82671919b","modified":1488110969000},{"_id":"themes/next/source/css/_common/components/third-party/third-party.styl","hash":"42348219db93a85d2ee23cb06cebd4d8ab121726","modified":1488110969000},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-author-links.styl","hash":"2e7ec9aaa3293941106b1bdd09055246aa3c3dc6","modified":1488110969000},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-author.styl","hash":"920343e41c124221a17f050bbb989494d44f7a24","modified":1488110969000},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-blogroll.styl","hash":"c44f6a553ec7ea5508f2054a13be33a62a15d3a9","modified":1488110969000},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-feed-link.styl","hash":"9486ddd2cb255227db102d09a7df4cae0fabad72","modified":1488110969000},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-nav.styl","hash":"45fa7193435a8eae9960267438750b4c9fa9587f","modified":1488110969000},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-toc.styl","hash":"7690b9596ec3a49befbe529a5a2649abec0faf76","modified":1488110969000},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar.styl","hash":"234facd038f144bd0fe09a31ed1357c5d74c517f","modified":1488110969000},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-toggle.styl","hash":"2d3abbc85b979a648e0e579e45f16a6eba49d1e7","modified":1488110969000},{"_id":"themes/next/source/css/_common/components/sidebar/site-state.styl","hash":"3623e7fa4324ec1307370f33d8f287a9e20a5578","modified":1488110969000},{"_id":"themes/next/source/css/_schemes/Mist/outline/outline.styl","hash":"5dc4859c66305f871e56cba78f64bfe3bf1b5f01","modified":1488110969000},{"_id":"themes/next/source/css/_schemes/Mist/sidebar/sidebar-blogroll.styl","hash":"8b8e8cbce98a9296c8fd77f512ae85d945f65d40","modified":1488110969000},{"_id":"themes/next/source/css/_schemes/Muse/sidebar/sidebar-blogroll.styl","hash":"8b8e8cbce98a9296c8fd77f512ae85d945f65d40","modified":1488110969000},{"_id":"themes/next/source/lib/fancybox/source/helpers/fancybox_buttons.png","hash":"e385b139516c6813dcd64b8fc431c364ceafe5f3","modified":1488110969000},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-buttons.css","hash":"1a9d8e5c22b371fcc69d4dbbb823d9c39f04c0c8","modified":1488110969000},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-buttons.js","hash":"91e41741c2e93f732c82aaacec4cfc6e3f3ec876","modified":1488110969000},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-thumbs.css","hash":"4ac329c16a5277592fc12a37cca3d72ca4ec292f","modified":1488110969000},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-media.js","hash":"3bdf69ed2469e4fb57f5a95f17300eef891ff90d","modified":1488110969000},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-thumbs.js","hash":"53e194f4a72e649c04fb586dd57762b8c022800b","modified":1488110969000},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.ttf","hash":"61d8d967807ef12598d81582fa95b9f600c3ee01","modified":1488110969000},{"_id":"source/img/cs131_linear_algebra.jpg","hash":"dd19d6b5b0e026d3c5b1ead3951b6f5073081fa1","modified":1488110968000},{"_id":"source/img/yolo2_result.png","hash":"6241714e33480315ec130b1dcd17db56f098db9b","modified":1488110968000},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.js","hash":"9ccc6f8144f54e86df9a3fd33a18368d81cf3a4f","modified":1488110969000},{"_id":"source/img/convolution.png","hash":"af8e2638d7e484d8267a49ed11d3f3239fa19107","modified":1488110968000},{"_id":"source/img/dog_different_size.png","hash":"c8d097879dcfffa7cf6138b01b22d4792d56522c","modified":1488110968000},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.svg","hash":"c0522272bbaef2acb3d341912754d6ea2d0ecfc0","modified":1488110969000},{"_id":"source/img/kmeans_scaling_up.png","hash":"df16837a020113c80b953765d2b21932dd275d23","modified":1488110968000},{"_id":"source/img/yolo1_basic_idea.png","hash":"831008124b5bd8e25e5b8986cf40fb75767b1d03","modified":1488110968000},{"_id":"source/img/kmeans_algorithm.png","hash":"89598b129b98c9da0eb309911b84eb2e49ff38c6","modified":1488110968000},{"_id":"source/img/sift_experiment_2.png","hash":"c1e482a1e85026664f33ce2dd7e72e3e3a58c0fc","modified":1488110968000},{"_id":"source/img/projective_geometry_property_1.png","hash":"ee1d6866a25eb2822cd8adb7b8666ae653a8bf45","modified":1488110968000},{"_id":"source/img/kmeans_image_seg_via_intensity.png","hash":"9a3c60d288ba39bbe659cf84ed941c53ea501eeb","modified":1488110968000},{"_id":"source/img/projective_geometry_property_2.png","hash":"1d0b53aa8901c0dc46673762820683b3afd4202b","modified":1488110968000},{"_id":"source/img/camera_geometry_application.png","hash":"cb34e60657f460f4cf120b740180740d721256e4","modified":1488110968000},{"_id":"source/img/image_matching_hard.png","hash":"4a09523ad90bb2d41a391edb8fd69bfed2cdf8e9","modified":1488110968000},{"_id":"public/search.xml","hash":"cbd41aab47c0adff5965639b5a1b38f6ee2e965a","modified":1488640617093},{"_id":"public/tags/index.html","hash":"aa30d52a7393d7cf9b5ef5269d69fe2aeeea3e00","modified":1488640618799},{"_id":"public/archives/2014/index.html","hash":"5ab97c05d10c08122c60f0dee98f3031a4f0485a","modified":1488640618848},{"_id":"public/archives/2014/07/index.html","hash":"d8325664cb5ca7a4d80abb44da751d1e959970c4","modified":1488640618849},{"_id":"public/tags/gsl/index.html","hash":"f3756c69e9e2d86faf6e79326ac45a6b01baa5b7","modified":1488640618849},{"_id":"public/tags/python/index.html","hash":"0236cd6c4cf8d68eefb181a564a6a32004a78152","modified":1488640618849},{"_id":"public/tags/math/index.html","hash":"a1e933b5829361e4983de0e3ce72d0e71f457386","modified":1488640618849},{"_id":"public/tags/yolo/index.html","hash":"71967147a0d3f9dc6999099d33e419e73c6a5c9e","modified":1488640618849},{"_id":"public/tags/paper/index.html","hash":"afdc8fc09365b2a9c3acb227fcf09111a3eff550","modified":1488640618849},{"_id":"public/tags/doxygen/index.html","hash":"240ab92c4e74c1bce8b09eb119982d3d37643593","modified":1488640618849},{"_id":"public/2017/03/04/pytorch-mnist-example/index.html","hash":"1d605f4763727888f835b238ef31dc95ade2015a","modified":1488640618849},{"_id":"public/about/index.html","hash":"a69e76724aab68b36231cb85c4d8d2f95d12465a","modified":1488640618849},{"_id":"public/2017/02/26/jupyternotebook-remote-useage/index.html","hash":"6c573c486531c7740119df4af9733654a87fa4d4","modified":1488640618849},{"_id":"public/2017/02/25/pytorch-tutor-01/index.html","hash":"80eb0f3aa661c3c957f149247b5625d9cf11ed45","modified":1488640618849},{"_id":"public/2017/02/12/cs131-mean-shift/index.html","hash":"a4011b68559f487bf3e09d738df711018d742c2c","modified":1488640618849},{"_id":"public/2017/02/22/warpctc-caffe/index.html","hash":"8157aacb1d68f7d5c41b4596098e89fcf431ad2c","modified":1488640618850},{"_id":"public/2017/02/08/digitalocean-shadowsocks/index.html","hash":"54c438d8701ed66bd704b75203a5b54271cff219","modified":1488640618850},{"_id":"public/2017/02/09/build-caffe-ubuntu/index.html","hash":"60557ea2fd1d406be95c7f93fdf6ae7c49e72c86","modified":1488640618850},{"_id":"public/2017/02/05/cs131-kmeans/index.html","hash":"33a38a958e437ea4af819ac7036adef87fa95ad0","modified":1488640618850},{"_id":"public/2017/02/02/cs131-camera/index.html","hash":"5b575042c4a4c96c9609670ba25d75b04428fe96","modified":1488640618850},{"_id":"public/2017/01/30/cs131-sift/index.html","hash":"91c1241226ee7bd0ee4c9f18b7768d3cee99202a","modified":1488640618850},{"_id":"public/2017/02/05/video-linear-alg-essential-property/index.html","hash":"4d6e49edaed0c2b5aff940aad9268761e291139d","modified":1488640618850},{"_id":"public/2017/01/24/cs131-edge-detection/index.html","hash":"7423ae4cb0b22323dbf30fc24a0bf3fd96384d03","modified":1488640618850},{"_id":"public/2017/02/04/yolo-paper/index.html","hash":"c101fe02961964a48ea367a4b0017a9318534fa8","modified":1488640618850},{"_id":"public/2017/01/25/cs131-finding-features/index.html","hash":"c9f900d44a12cecec97dd0176d56d4900843fcd4","modified":1488640618850},{"_id":"public/2017/01/22/cs131-linear-alg/index.html","hash":"68596a561f008e7191ebc932bcecc53f139d0890","modified":1488640618851},{"_id":"public/2016/12/16/hello-world/index.html","hash":"4810b6d2a7e93ebaa17b59bb4c44e31e587c430f","modified":1488640618851},{"_id":"public/2017/01/23/cs131-filter-svd/index.html","hash":"0df08c726cdfdaab5087266b9149896676ce42df","modified":1488640618851},{"_id":"public/2014/07/17/python-reg-exp/index.html","hash":"5bd628bd74f0d3c93fc4f555c3572dfff3ff22fc","modified":1488640618851},{"_id":"public/2016/12/16/use-doxygen/index.html","hash":"a35f272f16f99df7f79252baad0d35c968ad325c","modified":1488640618851},{"_id":"public/archives/index.html","hash":"d02c752fd62472a9f33ac0c47afbfe7992f4dbbb","modified":1488640618851},{"_id":"public/archives/page/2/index.html","hash":"89aa673a8a1b7604700c10f7ecb09b5c5f843de2","modified":1488640618851},{"_id":"public/archives/2016/12/index.html","hash":"38aab11c005b02e0a036a6ac6951566ad38ae5ee","modified":1488640618851},{"_id":"public/archives/2016/index.html","hash":"1d9e984196c7ee484215681eb7d5110f43868725","modified":1488640618851},{"_id":"public/archives/2017/index.html","hash":"d92a2cb32afb8859ae4c5a0b83c7daa09a3a3238","modified":1488640618851},{"_id":"public/archives/2017/page/2/index.html","hash":"6919073bea47732b2100f8962a5c505d4b1347e1","modified":1488640618851},{"_id":"public/archives/2017/02/index.html","hash":"e75a9dca4a79b63861ca2d886e1d7d17cf5aa789","modified":1488640618851},{"_id":"public/archives/2017/01/index.html","hash":"d6536838cb83fc3d1737fe6001fdac14ac2aaa9b","modified":1488640618851},{"_id":"public/page/2/index.html","hash":"e5265254e3c7ebca756ce5bfa5e07114d7d06e2c","modified":1488640618851},{"_id":"public/index.html","hash":"dc35954a4af95b8097e23cdfc7f652b308824923","modified":1488640618851},{"_id":"public/tags/tool/index.html","hash":"b10eeac106051da621d154e9e5db52170889e682","modified":1488640618851},{"_id":"public/tags/公开课/index.html","hash":"defe08c5da7cd8ec19b99339d8331a49d3d77a9e","modified":1488640618851},{"_id":"public/tags/cs131/index.html","hash":"4035900cbf0cb235aaed55dd437a3c773126ce13","modified":1488640618851},{"_id":"public/2016/12/16/gsl-with-vs/index.html","hash":"ea230b825edf5e96a2e479a826bb31ede0f03c9b","modified":1488640618851},{"_id":"public/tags/pytorch/index.html","hash":"bc65dc226396fac0ab465ea5b2328e91097ef84e","modified":1488640618851},{"_id":"public/tags/caffe/index.html","hash":"ad3badf69a88c18fcc399a30b805fdab9676a3ca","modified":1488640618851},{"_id":"public/archives/2017/03/index.html","hash":"38e73a4a2f5bfd48f82798efa918581fbb20b180","modified":1488640618853},{"_id":"public/img/mnist_example.png","hash":"998011e1ab53d491adec736cebc319511764561e","modified":1488640618853}],"Category":[],"Data":[],"Page":[{"title":"标签","date":"2016-12-17T12:10:38.000Z","type":"tags","_content":"","source":"tags/index.md","raw":"---\ntitle: 标签\ndate: 2016-12-17 20:10:38\ntype: tags\n---\n","updated":"2017-02-26T12:09:28.000Z","path":"tags/index.html","comments":1,"layout":"page","_id":"cizve079g0001l61hduol0p7d","content":"","excerpt":"","more":""},{"title":"关于","date":"2016-12-17T12:49:51.000Z","_content":"\nHi, 你好，欢迎来到我的博客。目前，我是北京理工自动化学院组合导航与智能导航实验室的在读研究生，将于2017年4月进入中国科学院计算技术研究所计算机体系结构国家重点实验室工作。\n\n我的个人兴趣主要包括机器人，计算机视觉，嵌入式平台的深度学习框架及应用。我曾经在北京贝虎机器人和地平线机器人有过短暂的实习经历，亲身参与了基于深度学习的物体检测和车道线识别等任务。\n\n我的博客主要是关于个人在工作学习中的点滴心得，既是自己学习过程的整理，也希望能够帮助到有缘之人。本博客将会不定期更新。我是CV DL领域的新兵，如果我的博客中有问题或遗漏，欢迎与我沟通，帮我指正，请联系邮箱xmfbit # gmail.com.\n","source":"about/index.md","raw":"---\ntitle: 关于\ndate: 2016-12-17 20:49:51\n---\n\nHi, 你好，欢迎来到我的博客。目前，我是北京理工自动化学院组合导航与智能导航实验室的在读研究生，将于2017年4月进入中国科学院计算技术研究所计算机体系结构国家重点实验室工作。\n\n我的个人兴趣主要包括机器人，计算机视觉，嵌入式平台的深度学习框架及应用。我曾经在北京贝虎机器人和地平线机器人有过短暂的实习经历，亲身参与了基于深度学习的物体检测和车道线识别等任务。\n\n我的博客主要是关于个人在工作学习中的点滴心得，既是自己学习过程的整理，也希望能够帮助到有缘之人。本博客将会不定期更新。我是CV DL领域的新兵，如果我的博客中有问题或遗漏，欢迎与我沟通，帮我指正，请联系邮箱xmfbit # gmail.com.\n","updated":"2017-02-26T12:09:28.000Z","path":"about/index.html","comments":1,"layout":"page","_id":"cizve079l0003l61h6ap6coy5","content":"<p>Hi, 你好，欢迎来到我的博客。目前，我是北京理工自动化学院组合导航与智能导航实验室的在读研究生，将于2017年4月进入中国科学院计算技术研究所计算机体系结构国家重点实验室工作。</p>\n<p>我的个人兴趣主要包括机器人，计算机视觉，嵌入式平台的深度学习框架及应用。我曾经在北京贝虎机器人和地平线机器人有过短暂的实习经历，亲身参与了基于深度学习的物体检测和车道线识别等任务。</p>\n<p>我的博客主要是关于个人在工作学习中的点滴心得，既是自己学习过程的整理，也希望能够帮助到有缘之人。本博客将会不定期更新。我是CV DL领域的新兵，如果我的博客中有问题或遗漏，欢迎与我沟通，帮我指正，请联系邮箱xmfbit # gmail.com.</p>\n","excerpt":"","more":"<p>Hi, 你好，欢迎来到我的博客。目前，我是北京理工自动化学院组合导航与智能导航实验室的在读研究生，将于2017年4月进入中国科学院计算技术研究所计算机体系结构国家重点实验室工作。</p>\n<p>我的个人兴趣主要包括机器人，计算机视觉，嵌入式平台的深度学习框架及应用。我曾经在北京贝虎机器人和地平线机器人有过短暂的实习经历，亲身参与了基于深度学习的物体检测和车道线识别等任务。</p>\n<p>我的博客主要是关于个人在工作学习中的点滴心得，既是自己学习过程的整理，也希望能够帮助到有缘之人。本博客将会不定期更新。我是CV DL领域的新兵，如果我的博客中有问题或遗漏，欢迎与我沟通，帮我指正，请联系邮箱xmfbit # gmail.com.</p>\n"}],"Post":[{"title":"在Ubuntu14.04构建Caffe","date":"2017-02-09T12:59:05.000Z","_content":"Caffe作为较早的一款深度学习框架，很是流行。然而，由于依赖项众多，而且Jia Yangqing已经毕业，所以留下了不少的坑。这篇博客记录了我在一台操作系统为Ubuntu14.04.3的DELL游匣7559笔记本上编译Caffe的过程，主要是在编译python接口时遇到的import error问题的解决和找不到HDF5链接库的问题。\n\n![caffe](/img/caffe_image.jpg)\n\n<!-- more -->\n## 修改Makefile.config\n当从github上clone下来Caffe的源码之后，我们首先需要修改Makefile.config文件，自定义配置。下面是我的配置文件，主要修改了CUDNN部分和Anaconda Python部分。\n\n``` bash\n## Refer to http://caffe.berkeleyvision.org/installation.html\n# Contributions simplifying and improving our build system are welcome!\n\n# cuDNN acceleration switch (uncomment to build with cuDNN).\nUSE_CUDNN := 1    # 这里我们使用cudnn加速\n\n# CPU-only switch (uncomment to build without GPU support).\n# CPU_ONLY := 1\n\n# uncomment to disable IO dependencies and corresponding data layers\n# USE_OPENCV := 0\n# USE_LEVELDB := 0\n# USE_LMDB := 0\n\n# uncomment to allow MDB_NOLOCK when reading LMDB files (only if necessary)\n#\tYou should not set this flag if you will be reading LMDBs with any\n#\tpossibility of simultaneous read and write\n# ALLOW_LMDB_NOLOCK := 1\n\n# Uncomment if you're using OpenCV 3\n# OPENCV_VERSION := 3\n\n# To customize your choice of compiler, uncomment and set the following.\n# N.B. the default for Linux is g++ and the default for OSX is clang++\n# CUSTOM_CXX := g++\n\n# CUDA directory contains bin/ and lib/ directories that we need.\nCUDA_DIR := /usr/local/cuda\n# On Ubuntu 14.04, if cuda tools are installed via\n# \"sudo apt-get install nvidia-cuda-toolkit\" then use this instead:\n# CUDA_DIR := /usr\n\n# CUDA architecture setting: going with all of them.\n# For CUDA < 6.0, comment the *_50 lines for compatibility.\n# 这里可以去掉sm_20和21，因为实在是已经太老了\n# 如果保留的话，编译时nvcc会给出警告\nCUDA_ARCH := -gencode arch=compute_30,code=sm_30 \\\n\t\t-gencode arch=compute_35,code=sm_35 \\\n\t\t-gencode arch=compute_50,code=sm_50 \\\n\t\t-gencode arch=compute_50,code=compute_50\n\n# BLAS choice:\n# atlas for ATLAS (default)\n# mkl for MKL\n# open for OpenBlas\nBLAS := atlas\n# Custom (MKL/ATLAS/OpenBLAS) include and lib directories.\n# Leave commented to accept the defaults for your choice of BLAS\n# (which should work)!\n# BLAS_INCLUDE := /path/to/your/blas\n# BLAS_LIB := /path/to/your/blas\n\n# Homebrew puts openblas in a directory that is not on the standard search path\n# BLAS_INCLUDE := $(shell brew --prefix openblas)/include\n# BLAS_LIB := $(shell brew --prefix openblas)/lib\n\n# This is required only if you will compile the matlab interface.\n# MATLAB directory should contain the mex binary in /bin.\n# MATLAB_DIR := /usr/local\n# MATLAB_DIR := /Applications/MATLAB_R2012b.app\n\n# NOTE: this is required only if you will compile the python interface.\n# We need to be able to find Python.h and numpy/arrayobject.h.\nPYTHON_INCLUDE := /usr/include/python2.7 \\\n\t\t/usr/lib/python2.7/dist-packages/numpy/core/include\n# Anaconda Python distribution is quite popular. Include path:\n# Verify anaconda location, sometimes it's in root.\n# 这里我们使用Anaconda\nANACONDA_HOME := $(HOME)/anaconda2\n PYTHON_INCLUDE := $(ANACONDA_HOME)/include \\\n\t\t $(ANACONDA_HOME)/include/python2.7 \\\n\t\t $(ANACONDA_HOME)/lib/python2.7/site-packages/numpy/core/include\n\n# Uncomment to use Python 3 (default is Python 2)\n# PYTHON_LIBRARIES := boost_python3 python3.5m\n# PYTHON_INCLUDE := /usr/include/python3.5m \\\n#                 /usr/lib/python3.5/dist-packages/numpy/core/include\n\n# We need to be able to find libpythonX.X.so or .dylib.\n#PYTHON_LIB := /usr/lib\n PYTHON_LIB := $(ANACONDA_HOME)/lib\n\n# Homebrew installs numpy in a non standard path (keg only)\n# PYTHON_INCLUDE += $(dir $(shell python -c 'import numpy.core; print(numpy.core.__file__)'))/include\n# PYTHON_LIB += $(shell brew --prefix numpy)/lib\n\n# Uncomment to support layers written in Python (will link against Python libs)\nWITH_PYTHON_LAYER := 1\n\n# Whatever else you find you need goes here.\nINCLUDE_DIRS := $(PYTHON_INCLUDE) /usr/local/include\nLIBRARY_DIRS := $(PYTHON_LIB) /usr/local/lib /usr/lib\n\n# If Homebrew is installed at a non standard location (for example your home directory) and you use it for general dependencies\n# INCLUDE_DIRS += $(shell brew --prefix)/include\n# LIBRARY_DIRS += $(shell brew --prefix)/lib\n\n# NCCL acceleration switch (uncomment to build with NCCL)\n# https://github.com/NVIDIA/nccl (last tested version: v1.2.3-1+cuda8.0)\n# USE_NCCL := 1\n\n# Uncomment to use `pkg-config` to specify OpenCV library paths.\n# (Usually not necessary -- OpenCV libraries are normally installed in one of the above $LIBRARY_DIRS.)\n# USE_PKG_CONFIG := 1\n\n# N.B. both build and distribute dirs are cleared on `make clean`\nBUILD_DIR := build\nDISTRIBUTE_DIR := distribute\n\n# Uncomment for debugging. Does not work on OSX due to https://github.com/BVLC/caffe/issues/171\n# DEBUG := 1\n\n# The ID of the GPU that 'make runtest' will use to run unit tests.\nTEST_GPUID := 0\n\n# enable pretty build (comment to see full commands)\nQ ?= @\n```\n\n对于CUDNN，从Nvidia官方网站上下载后，可不按照官方给定的方法安装。直接将`include`中的头文件放于`/usr/local/cuda-8.0/include`下，将`lib`中的库文件放于`/usr/loca/cuda-8.0/lib64`文件夹下即可。\n\n## 构建\n使用`make -j8`进行编译，并使用`make pycaffe`生成python接口。并在`.bashrc`中添加内容：\n```\nexport PYTHONPATH=/path_to_caffe/python:$PYTHONPATH\n```\n\n结果在`import caffe`时出现问题如下：\n```\nImportError: libcudnn.so.5: cannot open shared object file: No such file or directory\n```\n解决方法如下，详见GitHub issue[讨论](https://github.com/NVIDIA/DIGITS/issues/8)。\n```\nsudo ldconfig /usr/local/cuda/lib64\n```\n\n然而仍有问题，如下：\n```\nImportError: No module named google.protobuf.internal\n```\n解决方法如下，详见G+ caffe-user group的[帖子](https://groups.google.com/forum/#!topic/caffe-users/9Q10WkpCGxs)。\n```\npip install protobuf\n```\n\n不过仍然存在的问题是远程SSH登录时，不能在`ipython`环境下导入caffe，不知为何。\n\n使用`make test; make runtest`进行测试，结果提示HDF5动态链接库出现问题，怀疑与Anaconda中的HDF5冲突有关。错误信息如下：\n\n```\nerror while loading shared libraries: libhdf5_hl.so.10: cannot open shared object file: No such file or directory\n```\n\n解决方法为手动添加符号链接，详见GitHub[讨论帖](https://github.com/BVLC/caffe/issues/1463)。\n\n```\ncd /usr/lib/x86_64-linux-gnu\nsudo ln -s libhdf5.so.7 libhdf5.so.10\nsudo ln -s libhdf5_hl.so.7 libhdf5_hl.so.10\n```\n\n## 测试\n首先，通过`make runtest`看是否全部test可以通过。其次，可以试运行`example`下的LeNet训练。\n```\ncd $CAFFE_ROOT\n./data/mnist/get_mnist.sh\n./examples/mnist/create_mnist.sh\n./examples/mnist/train_lenet.sh\n```\n","source":"_posts/build-caffe-ubuntu.md","raw":"---\ntitle: 在Ubuntu14.04构建Caffe\ndate: 2017-02-09 20:59:05\ntags:\n    - tool\n    - caffe\n---\nCaffe作为较早的一款深度学习框架，很是流行。然而，由于依赖项众多，而且Jia Yangqing已经毕业，所以留下了不少的坑。这篇博客记录了我在一台操作系统为Ubuntu14.04.3的DELL游匣7559笔记本上编译Caffe的过程，主要是在编译python接口时遇到的import error问题的解决和找不到HDF5链接库的问题。\n\n![caffe](/img/caffe_image.jpg)\n\n<!-- more -->\n## 修改Makefile.config\n当从github上clone下来Caffe的源码之后，我们首先需要修改Makefile.config文件，自定义配置。下面是我的配置文件，主要修改了CUDNN部分和Anaconda Python部分。\n\n``` bash\n## Refer to http://caffe.berkeleyvision.org/installation.html\n# Contributions simplifying and improving our build system are welcome!\n\n# cuDNN acceleration switch (uncomment to build with cuDNN).\nUSE_CUDNN := 1    # 这里我们使用cudnn加速\n\n# CPU-only switch (uncomment to build without GPU support).\n# CPU_ONLY := 1\n\n# uncomment to disable IO dependencies and corresponding data layers\n# USE_OPENCV := 0\n# USE_LEVELDB := 0\n# USE_LMDB := 0\n\n# uncomment to allow MDB_NOLOCK when reading LMDB files (only if necessary)\n#\tYou should not set this flag if you will be reading LMDBs with any\n#\tpossibility of simultaneous read and write\n# ALLOW_LMDB_NOLOCK := 1\n\n# Uncomment if you're using OpenCV 3\n# OPENCV_VERSION := 3\n\n# To customize your choice of compiler, uncomment and set the following.\n# N.B. the default for Linux is g++ and the default for OSX is clang++\n# CUSTOM_CXX := g++\n\n# CUDA directory contains bin/ and lib/ directories that we need.\nCUDA_DIR := /usr/local/cuda\n# On Ubuntu 14.04, if cuda tools are installed via\n# \"sudo apt-get install nvidia-cuda-toolkit\" then use this instead:\n# CUDA_DIR := /usr\n\n# CUDA architecture setting: going with all of them.\n# For CUDA < 6.0, comment the *_50 lines for compatibility.\n# 这里可以去掉sm_20和21，因为实在是已经太老了\n# 如果保留的话，编译时nvcc会给出警告\nCUDA_ARCH := -gencode arch=compute_30,code=sm_30 \\\n\t\t-gencode arch=compute_35,code=sm_35 \\\n\t\t-gencode arch=compute_50,code=sm_50 \\\n\t\t-gencode arch=compute_50,code=compute_50\n\n# BLAS choice:\n# atlas for ATLAS (default)\n# mkl for MKL\n# open for OpenBlas\nBLAS := atlas\n# Custom (MKL/ATLAS/OpenBLAS) include and lib directories.\n# Leave commented to accept the defaults for your choice of BLAS\n# (which should work)!\n# BLAS_INCLUDE := /path/to/your/blas\n# BLAS_LIB := /path/to/your/blas\n\n# Homebrew puts openblas in a directory that is not on the standard search path\n# BLAS_INCLUDE := $(shell brew --prefix openblas)/include\n# BLAS_LIB := $(shell brew --prefix openblas)/lib\n\n# This is required only if you will compile the matlab interface.\n# MATLAB directory should contain the mex binary in /bin.\n# MATLAB_DIR := /usr/local\n# MATLAB_DIR := /Applications/MATLAB_R2012b.app\n\n# NOTE: this is required only if you will compile the python interface.\n# We need to be able to find Python.h and numpy/arrayobject.h.\nPYTHON_INCLUDE := /usr/include/python2.7 \\\n\t\t/usr/lib/python2.7/dist-packages/numpy/core/include\n# Anaconda Python distribution is quite popular. Include path:\n# Verify anaconda location, sometimes it's in root.\n# 这里我们使用Anaconda\nANACONDA_HOME := $(HOME)/anaconda2\n PYTHON_INCLUDE := $(ANACONDA_HOME)/include \\\n\t\t $(ANACONDA_HOME)/include/python2.7 \\\n\t\t $(ANACONDA_HOME)/lib/python2.7/site-packages/numpy/core/include\n\n# Uncomment to use Python 3 (default is Python 2)\n# PYTHON_LIBRARIES := boost_python3 python3.5m\n# PYTHON_INCLUDE := /usr/include/python3.5m \\\n#                 /usr/lib/python3.5/dist-packages/numpy/core/include\n\n# We need to be able to find libpythonX.X.so or .dylib.\n#PYTHON_LIB := /usr/lib\n PYTHON_LIB := $(ANACONDA_HOME)/lib\n\n# Homebrew installs numpy in a non standard path (keg only)\n# PYTHON_INCLUDE += $(dir $(shell python -c 'import numpy.core; print(numpy.core.__file__)'))/include\n# PYTHON_LIB += $(shell brew --prefix numpy)/lib\n\n# Uncomment to support layers written in Python (will link against Python libs)\nWITH_PYTHON_LAYER := 1\n\n# Whatever else you find you need goes here.\nINCLUDE_DIRS := $(PYTHON_INCLUDE) /usr/local/include\nLIBRARY_DIRS := $(PYTHON_LIB) /usr/local/lib /usr/lib\n\n# If Homebrew is installed at a non standard location (for example your home directory) and you use it for general dependencies\n# INCLUDE_DIRS += $(shell brew --prefix)/include\n# LIBRARY_DIRS += $(shell brew --prefix)/lib\n\n# NCCL acceleration switch (uncomment to build with NCCL)\n# https://github.com/NVIDIA/nccl (last tested version: v1.2.3-1+cuda8.0)\n# USE_NCCL := 1\n\n# Uncomment to use `pkg-config` to specify OpenCV library paths.\n# (Usually not necessary -- OpenCV libraries are normally installed in one of the above $LIBRARY_DIRS.)\n# USE_PKG_CONFIG := 1\n\n# N.B. both build and distribute dirs are cleared on `make clean`\nBUILD_DIR := build\nDISTRIBUTE_DIR := distribute\n\n# Uncomment for debugging. Does not work on OSX due to https://github.com/BVLC/caffe/issues/171\n# DEBUG := 1\n\n# The ID of the GPU that 'make runtest' will use to run unit tests.\nTEST_GPUID := 0\n\n# enable pretty build (comment to see full commands)\nQ ?= @\n```\n\n对于CUDNN，从Nvidia官方网站上下载后，可不按照官方给定的方法安装。直接将`include`中的头文件放于`/usr/local/cuda-8.0/include`下，将`lib`中的库文件放于`/usr/loca/cuda-8.0/lib64`文件夹下即可。\n\n## 构建\n使用`make -j8`进行编译，并使用`make pycaffe`生成python接口。并在`.bashrc`中添加内容：\n```\nexport PYTHONPATH=/path_to_caffe/python:$PYTHONPATH\n```\n\n结果在`import caffe`时出现问题如下：\n```\nImportError: libcudnn.so.5: cannot open shared object file: No such file or directory\n```\n解决方法如下，详见GitHub issue[讨论](https://github.com/NVIDIA/DIGITS/issues/8)。\n```\nsudo ldconfig /usr/local/cuda/lib64\n```\n\n然而仍有问题，如下：\n```\nImportError: No module named google.protobuf.internal\n```\n解决方法如下，详见G+ caffe-user group的[帖子](https://groups.google.com/forum/#!topic/caffe-users/9Q10WkpCGxs)。\n```\npip install protobuf\n```\n\n不过仍然存在的问题是远程SSH登录时，不能在`ipython`环境下导入caffe，不知为何。\n\n使用`make test; make runtest`进行测试，结果提示HDF5动态链接库出现问题，怀疑与Anaconda中的HDF5冲突有关。错误信息如下：\n\n```\nerror while loading shared libraries: libhdf5_hl.so.10: cannot open shared object file: No such file or directory\n```\n\n解决方法为手动添加符号链接，详见GitHub[讨论帖](https://github.com/BVLC/caffe/issues/1463)。\n\n```\ncd /usr/lib/x86_64-linux-gnu\nsudo ln -s libhdf5.so.7 libhdf5.so.10\nsudo ln -s libhdf5_hl.so.7 libhdf5_hl.so.10\n```\n\n## 测试\n首先，通过`make runtest`看是否全部test可以通过。其次，可以试运行`example`下的LeNet训练。\n```\ncd $CAFFE_ROOT\n./data/mnist/get_mnist.sh\n./examples/mnist/create_mnist.sh\n./examples/mnist/train_lenet.sh\n```\n","slug":"build-caffe-ubuntu","published":1,"updated":"2017-02-26T12:09:28.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cizve079a0000l61h6214fsk5","content":"<p>Caffe作为较早的一款深度学习框架，很是流行。然而，由于依赖项众多，而且Jia Yangqing已经毕业，所以留下了不少的坑。这篇博客记录了我在一台操作系统为Ubuntu14.04.3的DELL游匣7559笔记本上编译Caffe的过程，主要是在编译python接口时遇到的import error问题的解决和找不到HDF5链接库的问题。</p>\n<p><img src=\"/img/caffe_image.jpg\" alt=\"caffe\"></p>\n<a id=\"more\"></a>\n<h2 id=\"修改Makefile-config\"><a href=\"#修改Makefile-config\" class=\"headerlink\" title=\"修改Makefile.config\"></a>修改Makefile.config</h2><p>当从github上clone下来Caffe的源码之后，我们首先需要修改Makefile.config文件，自定义配置。下面是我的配置文件，主要修改了CUDNN部分和Anaconda Python部分。</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div><div class=\"line\">47</div><div class=\"line\">48</div><div class=\"line\">49</div><div class=\"line\">50</div><div class=\"line\">51</div><div class=\"line\">52</div><div class=\"line\">53</div><div class=\"line\">54</div><div class=\"line\">55</div><div class=\"line\">56</div><div class=\"line\">57</div><div class=\"line\">58</div><div class=\"line\">59</div><div class=\"line\">60</div><div class=\"line\">61</div><div class=\"line\">62</div><div class=\"line\">63</div><div class=\"line\">64</div><div class=\"line\">65</div><div class=\"line\">66</div><div class=\"line\">67</div><div class=\"line\">68</div><div class=\"line\">69</div><div class=\"line\">70</div><div class=\"line\">71</div><div class=\"line\">72</div><div class=\"line\">73</div><div class=\"line\">74</div><div class=\"line\">75</div><div class=\"line\">76</div><div class=\"line\">77</div><div class=\"line\">78</div><div class=\"line\">79</div><div class=\"line\">80</div><div class=\"line\">81</div><div class=\"line\">82</div><div class=\"line\">83</div><div class=\"line\">84</div><div class=\"line\">85</div><div class=\"line\">86</div><div class=\"line\">87</div><div class=\"line\">88</div><div class=\"line\">89</div><div class=\"line\">90</div><div class=\"line\">91</div><div class=\"line\">92</div><div class=\"line\">93</div><div class=\"line\">94</div><div class=\"line\">95</div><div class=\"line\">96</div><div class=\"line\">97</div><div class=\"line\">98</div><div class=\"line\">99</div><div class=\"line\">100</div><div class=\"line\">101</div><div class=\"line\">102</div><div class=\"line\">103</div><div class=\"line\">104</div><div class=\"line\">105</div><div class=\"line\">106</div><div class=\"line\">107</div><div class=\"line\">108</div><div class=\"line\">109</div><div class=\"line\">110</div><div class=\"line\">111</div><div class=\"line\">112</div><div class=\"line\">113</div><div class=\"line\">114</div><div class=\"line\">115</div><div class=\"line\">116</div><div class=\"line\">117</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\">## Refer to http://caffe.berkeleyvision.org/installation.html</span></div><div class=\"line\"><span class=\"comment\"># Contributions simplifying and improving our build system are welcome!</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># cuDNN acceleration switch (uncomment to build with cuDNN).</span></div><div class=\"line\">USE_CUDNN := 1    <span class=\"comment\"># 这里我们使用cudnn加速</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># CPU-only switch (uncomment to build without GPU support).</span></div><div class=\"line\"><span class=\"comment\"># CPU_ONLY := 1</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># uncomment to disable IO dependencies and corresponding data layers</span></div><div class=\"line\"><span class=\"comment\"># USE_OPENCV := 0</span></div><div class=\"line\"><span class=\"comment\"># USE_LEVELDB := 0</span></div><div class=\"line\"><span class=\"comment\"># USE_LMDB := 0</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># uncomment to allow MDB_NOLOCK when reading LMDB files (only if necessary)</span></div><div class=\"line\"><span class=\"comment\">#\tYou should not set this flag if you will be reading LMDBs with any</span></div><div class=\"line\"><span class=\"comment\">#\tpossibility of simultaneous read and write</span></div><div class=\"line\"><span class=\"comment\"># ALLOW_LMDB_NOLOCK := 1</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># Uncomment if you're using OpenCV 3</span></div><div class=\"line\"><span class=\"comment\"># OPENCV_VERSION := 3</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># To customize your choice of compiler, uncomment and set the following.</span></div><div class=\"line\"><span class=\"comment\"># N.B. the default for Linux is g++ and the default for OSX is clang++</span></div><div class=\"line\"><span class=\"comment\"># CUSTOM_CXX := g++</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># CUDA directory contains bin/ and lib/ directories that we need.</span></div><div class=\"line\">CUDA_DIR := /usr/<span class=\"built_in\">local</span>/cuda</div><div class=\"line\"><span class=\"comment\"># On Ubuntu 14.04, if cuda tools are installed via</span></div><div class=\"line\"><span class=\"comment\"># \"sudo apt-get install nvidia-cuda-toolkit\" then use this instead:</span></div><div class=\"line\"><span class=\"comment\"># CUDA_DIR := /usr</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># CUDA architecture setting: going with all of them.</span></div><div class=\"line\"><span class=\"comment\"># For CUDA &lt; 6.0, comment the *_50 lines for compatibility.</span></div><div class=\"line\"><span class=\"comment\"># 这里可以去掉sm_20和21，因为实在是已经太老了</span></div><div class=\"line\"><span class=\"comment\"># 如果保留的话，编译时nvcc会给出警告</span></div><div class=\"line\">CUDA_ARCH := -gencode arch=compute_30,code=sm_30 \\</div><div class=\"line\">\t\t-gencode arch=compute_35,code=sm_35 \\</div><div class=\"line\">\t\t-gencode arch=compute_50,code=sm_50 \\</div><div class=\"line\">\t\t-gencode arch=compute_50,code=compute_50</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># BLAS choice:</span></div><div class=\"line\"><span class=\"comment\"># atlas for ATLAS (default)</span></div><div class=\"line\"><span class=\"comment\"># mkl for MKL</span></div><div class=\"line\"><span class=\"comment\"># open for OpenBlas</span></div><div class=\"line\">BLAS := atlas</div><div class=\"line\"><span class=\"comment\"># Custom (MKL/ATLAS/OpenBLAS) include and lib directories.</span></div><div class=\"line\"><span class=\"comment\"># Leave commented to accept the defaults for your choice of BLAS</span></div><div class=\"line\"><span class=\"comment\"># (which should work)!</span></div><div class=\"line\"><span class=\"comment\"># BLAS_INCLUDE := /path/to/your/blas</span></div><div class=\"line\"><span class=\"comment\"># BLAS_LIB := /path/to/your/blas</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># Homebrew puts openblas in a directory that is not on the standard search path</span></div><div class=\"line\"><span class=\"comment\"># BLAS_INCLUDE := $(shell brew --prefix openblas)/include</span></div><div class=\"line\"><span class=\"comment\"># BLAS_LIB := $(shell brew --prefix openblas)/lib</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># This is required only if you will compile the matlab interface.</span></div><div class=\"line\"><span class=\"comment\"># MATLAB directory should contain the mex binary in /bin.</span></div><div class=\"line\"><span class=\"comment\"># MATLAB_DIR := /usr/local</span></div><div class=\"line\"><span class=\"comment\"># MATLAB_DIR := /Applications/MATLAB_R2012b.app</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># <span class=\"doctag\">NOTE:</span> this is required only if you will compile the python interface.</span></div><div class=\"line\"><span class=\"comment\"># We need to be able to find Python.h and numpy/arrayobject.h.</span></div><div class=\"line\">PYTHON_INCLUDE := /usr/include/python2.7 \\</div><div class=\"line\">\t\t/usr/lib/python2.7/dist-packages/numpy/core/include</div><div class=\"line\"><span class=\"comment\"># Anaconda Python distribution is quite popular. Include path:</span></div><div class=\"line\"><span class=\"comment\"># Verify anaconda location, sometimes it's in root.</span></div><div class=\"line\"><span class=\"comment\"># 这里我们使用Anaconda</span></div><div class=\"line\">ANACONDA_HOME := $(HOME)/anaconda2</div><div class=\"line\"> PYTHON_INCLUDE := $(ANACONDA_HOME)/include \\</div><div class=\"line\">\t\t $(ANACONDA_HOME)/include/python2.7 \\</div><div class=\"line\">\t\t $(ANACONDA_HOME)/lib/python2.7/site-packages/numpy/core/include</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># Uncomment to use Python 3 (default is Python 2)</span></div><div class=\"line\"><span class=\"comment\"># PYTHON_LIBRARIES := boost_python3 python3.5m</span></div><div class=\"line\"><span class=\"comment\"># PYTHON_INCLUDE := /usr/include/python3.5m \\</span></div><div class=\"line\"><span class=\"comment\">#                 /usr/lib/python3.5/dist-packages/numpy/core/include</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># We need to be able to find libpythonX.X.so or .dylib.</span></div><div class=\"line\"><span class=\"comment\">#PYTHON_LIB := /usr/lib</span></div><div class=\"line\"> PYTHON_LIB := $(ANACONDA_HOME)/lib</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># Homebrew installs numpy in a non standard path (keg only)</span></div><div class=\"line\"><span class=\"comment\"># PYTHON_INCLUDE += $(dir $(shell python -c 'import numpy.core; print(numpy.core.__file__)'))/include</span></div><div class=\"line\"><span class=\"comment\"># PYTHON_LIB += $(shell brew --prefix numpy)/lib</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># Uncomment to support layers written in Python (will link against Python libs)</span></div><div class=\"line\">WITH_PYTHON_LAYER := 1</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># Whatever else you find you need goes here.</span></div><div class=\"line\">INCLUDE_DIRS := $(PYTHON_INCLUDE) /usr/<span class=\"built_in\">local</span>/include</div><div class=\"line\">LIBRARY_DIRS := $(PYTHON_LIB) /usr/<span class=\"built_in\">local</span>/lib /usr/lib</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># If Homebrew is installed at a non standard location (for example your home directory) and you use it for general dependencies</span></div><div class=\"line\"><span class=\"comment\"># INCLUDE_DIRS += $(shell brew --prefix)/include</span></div><div class=\"line\"><span class=\"comment\"># LIBRARY_DIRS += $(shell brew --prefix)/lib</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># NCCL acceleration switch (uncomment to build with NCCL)</span></div><div class=\"line\"><span class=\"comment\"># https://github.com/NVIDIA/nccl (last tested version: v1.2.3-1+cuda8.0)</span></div><div class=\"line\"><span class=\"comment\"># USE_NCCL := 1</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># Uncomment to use `pkg-config` to specify OpenCV library paths.</span></div><div class=\"line\"><span class=\"comment\"># (Usually not necessary -- OpenCV libraries are normally installed in one of the above $LIBRARY_DIRS.)</span></div><div class=\"line\"><span class=\"comment\"># USE_PKG_CONFIG := 1</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># N.B. both build and distribute dirs are cleared on `make clean`</span></div><div class=\"line\">BUILD_DIR := build</div><div class=\"line\">DISTRIBUTE_DIR := distribute</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># Uncomment for debugging. Does not work on OSX due to https://github.com/BVLC/caffe/issues/171</span></div><div class=\"line\"><span class=\"comment\"># DEBUG := 1</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># The ID of the GPU that 'make runtest' will use to run unit tests.</span></div><div class=\"line\">TEST_GPUID := 0</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># enable pretty build (comment to see full commands)</span></div><div class=\"line\">Q ?= @</div></pre></td></tr></table></figure>\n<p>对于CUDNN，从Nvidia官方网站上下载后，可不按照官方给定的方法安装。直接将<code>include</code>中的头文件放于<code>/usr/local/cuda-8.0/include</code>下，将<code>lib</code>中的库文件放于<code>/usr/loca/cuda-8.0/lib64</code>文件夹下即可。</p>\n<h2 id=\"构建\"><a href=\"#构建\" class=\"headerlink\" title=\"构建\"></a>构建</h2><p>使用<code>make -j8</code>进行编译，并使用<code>make pycaffe</code>生成python接口。并在<code>.bashrc</code>中添加内容：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">export PYTHONPATH=/path_to_caffe/python:$PYTHONPATH</div></pre></td></tr></table></figure></p>\n<p>结果在<code>import caffe</code>时出现问题如下：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">ImportError: libcudnn.so.5: cannot open shared object file: No such file or directory</div></pre></td></tr></table></figure></p>\n<p>解决方法如下，详见GitHub issue<a href=\"https://github.com/NVIDIA/DIGITS/issues/8\" target=\"_blank\" rel=\"external\">讨论</a>。<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">sudo ldconfig /usr/local/cuda/lib64</div></pre></td></tr></table></figure></p>\n<p>然而仍有问题，如下：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">ImportError: No module named google.protobuf.internal</div></pre></td></tr></table></figure></p>\n<p>解决方法如下，详见G+ caffe-user group的<a href=\"https://groups.google.com/forum/#!topic/caffe-users/9Q10WkpCGxs\" target=\"_blank\" rel=\"external\">帖子</a>。<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">pip install protobuf</div></pre></td></tr></table></figure></p>\n<p>不过仍然存在的问题是远程SSH登录时，不能在<code>ipython</code>环境下导入caffe，不知为何。</p>\n<p>使用<code>make test; make runtest</code>进行测试，结果提示HDF5动态链接库出现问题，怀疑与Anaconda中的HDF5冲突有关。错误信息如下：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">error while loading shared libraries: libhdf5_hl.so.10: cannot open shared object file: No such file or directory</div></pre></td></tr></table></figure>\n<p>解决方法为手动添加符号链接，详见GitHub<a href=\"https://github.com/BVLC/caffe/issues/1463\" target=\"_blank\" rel=\"external\">讨论帖</a>。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\">cd /usr/lib/x86_64-linux-gnu</div><div class=\"line\">sudo ln -s libhdf5.so.7 libhdf5.so.10</div><div class=\"line\">sudo ln -s libhdf5_hl.so.7 libhdf5_hl.so.10</div></pre></td></tr></table></figure>\n<h2 id=\"测试\"><a href=\"#测试\" class=\"headerlink\" title=\"测试\"></a>测试</h2><p>首先，通过<code>make runtest</code>看是否全部test可以通过。其次，可以试运行<code>example</code>下的LeNet训练。<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\">cd $CAFFE_ROOT</div><div class=\"line\">./data/mnist/get_mnist.sh</div><div class=\"line\">./examples/mnist/create_mnist.sh</div><div class=\"line\">./examples/mnist/train_lenet.sh</div></pre></td></tr></table></figure></p>\n","excerpt":"<p>Caffe作为较早的一款深度学习框架，很是流行。然而，由于依赖项众多，而且Jia Yangqing已经毕业，所以留下了不少的坑。这篇博客记录了我在一台操作系统为Ubuntu14.04.3的DELL游匣7559笔记本上编译Caffe的过程，主要是在编译python接口时遇到的import error问题的解决和找不到HDF5链接库的问题。</p>\n<p><img src=\"/img/caffe_image.jpg\" alt=\"caffe\"></p>","more":"<h2 id=\"修改Makefile-config\"><a href=\"#修改Makefile-config\" class=\"headerlink\" title=\"修改Makefile.config\"></a>修改Makefile.config</h2><p>当从github上clone下来Caffe的源码之后，我们首先需要修改Makefile.config文件，自定义配置。下面是我的配置文件，主要修改了CUDNN部分和Anaconda Python部分。</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div><div class=\"line\">47</div><div class=\"line\">48</div><div class=\"line\">49</div><div class=\"line\">50</div><div class=\"line\">51</div><div class=\"line\">52</div><div class=\"line\">53</div><div class=\"line\">54</div><div class=\"line\">55</div><div class=\"line\">56</div><div class=\"line\">57</div><div class=\"line\">58</div><div class=\"line\">59</div><div class=\"line\">60</div><div class=\"line\">61</div><div class=\"line\">62</div><div class=\"line\">63</div><div class=\"line\">64</div><div class=\"line\">65</div><div class=\"line\">66</div><div class=\"line\">67</div><div class=\"line\">68</div><div class=\"line\">69</div><div class=\"line\">70</div><div class=\"line\">71</div><div class=\"line\">72</div><div class=\"line\">73</div><div class=\"line\">74</div><div class=\"line\">75</div><div class=\"line\">76</div><div class=\"line\">77</div><div class=\"line\">78</div><div class=\"line\">79</div><div class=\"line\">80</div><div class=\"line\">81</div><div class=\"line\">82</div><div class=\"line\">83</div><div class=\"line\">84</div><div class=\"line\">85</div><div class=\"line\">86</div><div class=\"line\">87</div><div class=\"line\">88</div><div class=\"line\">89</div><div class=\"line\">90</div><div class=\"line\">91</div><div class=\"line\">92</div><div class=\"line\">93</div><div class=\"line\">94</div><div class=\"line\">95</div><div class=\"line\">96</div><div class=\"line\">97</div><div class=\"line\">98</div><div class=\"line\">99</div><div class=\"line\">100</div><div class=\"line\">101</div><div class=\"line\">102</div><div class=\"line\">103</div><div class=\"line\">104</div><div class=\"line\">105</div><div class=\"line\">106</div><div class=\"line\">107</div><div class=\"line\">108</div><div class=\"line\">109</div><div class=\"line\">110</div><div class=\"line\">111</div><div class=\"line\">112</div><div class=\"line\">113</div><div class=\"line\">114</div><div class=\"line\">115</div><div class=\"line\">116</div><div class=\"line\">117</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\">## Refer to http://caffe.berkeleyvision.org/installation.html</span></div><div class=\"line\"><span class=\"comment\"># Contributions simplifying and improving our build system are welcome!</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># cuDNN acceleration switch (uncomment to build with cuDNN).</span></div><div class=\"line\">USE_CUDNN := 1    <span class=\"comment\"># 这里我们使用cudnn加速</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># CPU-only switch (uncomment to build without GPU support).</span></div><div class=\"line\"><span class=\"comment\"># CPU_ONLY := 1</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># uncomment to disable IO dependencies and corresponding data layers</span></div><div class=\"line\"><span class=\"comment\"># USE_OPENCV := 0</span></div><div class=\"line\"><span class=\"comment\"># USE_LEVELDB := 0</span></div><div class=\"line\"><span class=\"comment\"># USE_LMDB := 0</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># uncomment to allow MDB_NOLOCK when reading LMDB files (only if necessary)</span></div><div class=\"line\"><span class=\"comment\">#\tYou should not set this flag if you will be reading LMDBs with any</span></div><div class=\"line\"><span class=\"comment\">#\tpossibility of simultaneous read and write</span></div><div class=\"line\"><span class=\"comment\"># ALLOW_LMDB_NOLOCK := 1</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># Uncomment if you're using OpenCV 3</span></div><div class=\"line\"><span class=\"comment\"># OPENCV_VERSION := 3</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># To customize your choice of compiler, uncomment and set the following.</span></div><div class=\"line\"><span class=\"comment\"># N.B. the default for Linux is g++ and the default for OSX is clang++</span></div><div class=\"line\"><span class=\"comment\"># CUSTOM_CXX := g++</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># CUDA directory contains bin/ and lib/ directories that we need.</span></div><div class=\"line\">CUDA_DIR := /usr/<span class=\"built_in\">local</span>/cuda</div><div class=\"line\"><span class=\"comment\"># On Ubuntu 14.04, if cuda tools are installed via</span></div><div class=\"line\"><span class=\"comment\"># \"sudo apt-get install nvidia-cuda-toolkit\" then use this instead:</span></div><div class=\"line\"><span class=\"comment\"># CUDA_DIR := /usr</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># CUDA architecture setting: going with all of them.</span></div><div class=\"line\"><span class=\"comment\"># For CUDA &lt; 6.0, comment the *_50 lines for compatibility.</span></div><div class=\"line\"><span class=\"comment\"># 这里可以去掉sm_20和21，因为实在是已经太老了</span></div><div class=\"line\"><span class=\"comment\"># 如果保留的话，编译时nvcc会给出警告</span></div><div class=\"line\">CUDA_ARCH := -gencode arch=compute_30,code=sm_30 \\</div><div class=\"line\">\t\t-gencode arch=compute_35,code=sm_35 \\</div><div class=\"line\">\t\t-gencode arch=compute_50,code=sm_50 \\</div><div class=\"line\">\t\t-gencode arch=compute_50,code=compute_50</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># BLAS choice:</span></div><div class=\"line\"><span class=\"comment\"># atlas for ATLAS (default)</span></div><div class=\"line\"><span class=\"comment\"># mkl for MKL</span></div><div class=\"line\"><span class=\"comment\"># open for OpenBlas</span></div><div class=\"line\">BLAS := atlas</div><div class=\"line\"><span class=\"comment\"># Custom (MKL/ATLAS/OpenBLAS) include and lib directories.</span></div><div class=\"line\"><span class=\"comment\"># Leave commented to accept the defaults for your choice of BLAS</span></div><div class=\"line\"><span class=\"comment\"># (which should work)!</span></div><div class=\"line\"><span class=\"comment\"># BLAS_INCLUDE := /path/to/your/blas</span></div><div class=\"line\"><span class=\"comment\"># BLAS_LIB := /path/to/your/blas</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># Homebrew puts openblas in a directory that is not on the standard search path</span></div><div class=\"line\"><span class=\"comment\"># BLAS_INCLUDE := $(shell brew --prefix openblas)/include</span></div><div class=\"line\"><span class=\"comment\"># BLAS_LIB := $(shell brew --prefix openblas)/lib</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># This is required only if you will compile the matlab interface.</span></div><div class=\"line\"><span class=\"comment\"># MATLAB directory should contain the mex binary in /bin.</span></div><div class=\"line\"><span class=\"comment\"># MATLAB_DIR := /usr/local</span></div><div class=\"line\"><span class=\"comment\"># MATLAB_DIR := /Applications/MATLAB_R2012b.app</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># <span class=\"doctag\">NOTE:</span> this is required only if you will compile the python interface.</span></div><div class=\"line\"><span class=\"comment\"># We need to be able to find Python.h and numpy/arrayobject.h.</span></div><div class=\"line\">PYTHON_INCLUDE := /usr/include/python2.7 \\</div><div class=\"line\">\t\t/usr/lib/python2.7/dist-packages/numpy/core/include</div><div class=\"line\"><span class=\"comment\"># Anaconda Python distribution is quite popular. Include path:</span></div><div class=\"line\"><span class=\"comment\"># Verify anaconda location, sometimes it's in root.</span></div><div class=\"line\"><span class=\"comment\"># 这里我们使用Anaconda</span></div><div class=\"line\">ANACONDA_HOME := $(HOME)/anaconda2</div><div class=\"line\"> PYTHON_INCLUDE := $(ANACONDA_HOME)/include \\</div><div class=\"line\">\t\t $(ANACONDA_HOME)/include/python2.7 \\</div><div class=\"line\">\t\t $(ANACONDA_HOME)/lib/python2.7/site-packages/numpy/core/include</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># Uncomment to use Python 3 (default is Python 2)</span></div><div class=\"line\"><span class=\"comment\"># PYTHON_LIBRARIES := boost_python3 python3.5m</span></div><div class=\"line\"><span class=\"comment\"># PYTHON_INCLUDE := /usr/include/python3.5m \\</span></div><div class=\"line\"><span class=\"comment\">#                 /usr/lib/python3.5/dist-packages/numpy/core/include</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># We need to be able to find libpythonX.X.so or .dylib.</span></div><div class=\"line\"><span class=\"comment\">#PYTHON_LIB := /usr/lib</span></div><div class=\"line\"> PYTHON_LIB := $(ANACONDA_HOME)/lib</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># Homebrew installs numpy in a non standard path (keg only)</span></div><div class=\"line\"><span class=\"comment\"># PYTHON_INCLUDE += $(dir $(shell python -c 'import numpy.core; print(numpy.core.__file__)'))/include</span></div><div class=\"line\"><span class=\"comment\"># PYTHON_LIB += $(shell brew --prefix numpy)/lib</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># Uncomment to support layers written in Python (will link against Python libs)</span></div><div class=\"line\">WITH_PYTHON_LAYER := 1</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># Whatever else you find you need goes here.</span></div><div class=\"line\">INCLUDE_DIRS := $(PYTHON_INCLUDE) /usr/<span class=\"built_in\">local</span>/include</div><div class=\"line\">LIBRARY_DIRS := $(PYTHON_LIB) /usr/<span class=\"built_in\">local</span>/lib /usr/lib</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># If Homebrew is installed at a non standard location (for example your home directory) and you use it for general dependencies</span></div><div class=\"line\"><span class=\"comment\"># INCLUDE_DIRS += $(shell brew --prefix)/include</span></div><div class=\"line\"><span class=\"comment\"># LIBRARY_DIRS += $(shell brew --prefix)/lib</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># NCCL acceleration switch (uncomment to build with NCCL)</span></div><div class=\"line\"><span class=\"comment\"># https://github.com/NVIDIA/nccl (last tested version: v1.2.3-1+cuda8.0)</span></div><div class=\"line\"><span class=\"comment\"># USE_NCCL := 1</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># Uncomment to use `pkg-config` to specify OpenCV library paths.</span></div><div class=\"line\"><span class=\"comment\"># (Usually not necessary -- OpenCV libraries are normally installed in one of the above $LIBRARY_DIRS.)</span></div><div class=\"line\"><span class=\"comment\"># USE_PKG_CONFIG := 1</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># N.B. both build and distribute dirs are cleared on `make clean`</span></div><div class=\"line\">BUILD_DIR := build</div><div class=\"line\">DISTRIBUTE_DIR := distribute</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># Uncomment for debugging. Does not work on OSX due to https://github.com/BVLC/caffe/issues/171</span></div><div class=\"line\"><span class=\"comment\"># DEBUG := 1</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># The ID of the GPU that 'make runtest' will use to run unit tests.</span></div><div class=\"line\">TEST_GPUID := 0</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># enable pretty build (comment to see full commands)</span></div><div class=\"line\">Q ?= @</div></pre></td></tr></table></figure>\n<p>对于CUDNN，从Nvidia官方网站上下载后，可不按照官方给定的方法安装。直接将<code>include</code>中的头文件放于<code>/usr/local/cuda-8.0/include</code>下，将<code>lib</code>中的库文件放于<code>/usr/loca/cuda-8.0/lib64</code>文件夹下即可。</p>\n<h2 id=\"构建\"><a href=\"#构建\" class=\"headerlink\" title=\"构建\"></a>构建</h2><p>使用<code>make -j8</code>进行编译，并使用<code>make pycaffe</code>生成python接口。并在<code>.bashrc</code>中添加内容：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">export PYTHONPATH=/path_to_caffe/python:$PYTHONPATH</div></pre></td></tr></table></figure></p>\n<p>结果在<code>import caffe</code>时出现问题如下：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">ImportError: libcudnn.so.5: cannot open shared object file: No such file or directory</div></pre></td></tr></table></figure></p>\n<p>解决方法如下，详见GitHub issue<a href=\"https://github.com/NVIDIA/DIGITS/issues/8\">讨论</a>。<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">sudo ldconfig /usr/local/cuda/lib64</div></pre></td></tr></table></figure></p>\n<p>然而仍有问题，如下：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">ImportError: No module named google.protobuf.internal</div></pre></td></tr></table></figure></p>\n<p>解决方法如下，详见G+ caffe-user group的<a href=\"https://groups.google.com/forum/#!topic/caffe-users/9Q10WkpCGxs\">帖子</a>。<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">pip install protobuf</div></pre></td></tr></table></figure></p>\n<p>不过仍然存在的问题是远程SSH登录时，不能在<code>ipython</code>环境下导入caffe，不知为何。</p>\n<p>使用<code>make test; make runtest</code>进行测试，结果提示HDF5动态链接库出现问题，怀疑与Anaconda中的HDF5冲突有关。错误信息如下：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">error while loading shared libraries: libhdf5_hl.so.10: cannot open shared object file: No such file or directory</div></pre></td></tr></table></figure>\n<p>解决方法为手动添加符号链接，详见GitHub<a href=\"https://github.com/BVLC/caffe/issues/1463\">讨论帖</a>。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\">cd /usr/lib/x86_64-linux-gnu</div><div class=\"line\">sudo ln -s libhdf5.so.7 libhdf5.so.10</div><div class=\"line\">sudo ln -s libhdf5_hl.so.7 libhdf5_hl.so.10</div></pre></td></tr></table></figure>\n<h2 id=\"测试\"><a href=\"#测试\" class=\"headerlink\" title=\"测试\"></a>测试</h2><p>首先，通过<code>make runtest</code>看是否全部test可以通过。其次，可以试运行<code>example</code>下的LeNet训练。<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\">cd $CAFFE_ROOT</div><div class=\"line\">./data/mnist/get_mnist.sh</div><div class=\"line\">./examples/mnist/create_mnist.sh</div><div class=\"line\">./examples/mnist/train_lenet.sh</div></pre></td></tr></table></figure></p>"},{"title":"CS131-立体视觉基础","date":"2017-02-02T12:38:55.000Z","_content":"\n数字图像是现实世界的物体通过光学成像设备在感光材料上的投影。在3D到2D的转换过程中，深度信息丢失。如何从单幅或者多幅图像中恢复出有用的3D信息，需要使用立体视觉的知识进行分析。如下图所示。这篇博客对课程讲义中立体视觉部分做一整理，分别介绍了针孔相机模型和对极集合的基础知识。\n![立体视觉应用](/img/camera_geometry_application.png)\n<!-- more -->\n## 针孔相机模型（Pinhole Camera）\n\n针孔相机是简化的相机模型。光线沿直线传播。物体反射的光线，通过针孔，在成像面形成倒立的影像。针孔与成像面的距离，称为焦距。一般而言，针孔越小，影像越清晰，但针孔太小，会导致衍射，反而令影像模糊。\n\n### 投影几何的重要性质\n\n在对针孔相机详细说明之前，首先介绍投影几何（Projective Geometry）的几条性质。\n\n在投影变换前后，长度和角度信息丢失，但直线仍然保持是直线。如下图所示。原来垂直和平行的台球桌面的边不再保持原有的角度和相对长度，但是仍然是直线。\n![性质1](/img/projective_geometry_property_1.png)\n\n另一方面，虽然3D世界中的平行线在投影变换后相交（我们将交点称为Vanishing point），但是3D世界中互相平行的线，在2D平面上的Vanishing point为同一点。而所有平行线的Vanishing point又在同一条直线上。如下图所示（竖直的绿色平行线在投影变换后仍然保持平行，实际上并不想交，但我们这里认为其交于无穷远的某点）。\n![性质2](/img/projective_geometry_property_2.png)\n\n### 针孔相机模型\n如下图所示，为针孔相机成像原理示意图。外部世界的一点$P$发出（或反射）的光线，经过小孔$O$，成像在在像平面$\\Pi^\\prime$的点$P^\\prime$处。通过简单的相似三角形知识，可以得到图上的关系式。\n![针孔相机模型示意图](/img/pinhole_camera_model.png)\n\n由上图可以知道，3D空间的一点$P$与成像平面上的对应点$P^\\prime$坐标之间的数量关系为：\n$$\\left\\{\\begin{matrix}\nx^\\prime = fx/z \\\\\ny^\\prime = fy/z\n\\end{matrix}\\right.$$\n\n可是，由于变量$z$位于分母的位置，也就是说如果直接写成变换矩阵的话，这个矩阵是和变量$z$有关的，我们不想这样，而是想得到一个完全由相机本身确定的变换矩阵。所以，我们借鉴齐次矩阵的思想，将原来的点增加一个维度，转换为齐次坐标。如下图的两个例子所示。\n![齐次坐标](/img/qicizuobiao.png)\n\n这样，我们可以将3D和2D之间的变换写成下面的形式。当实际计算的时候，我们首先将3D点转换为4维向量（在末尾补1），然后左乘变换矩阵。最后将得到的结果化为末尾元素为1的形式，这样，前面两个元素就代表了变换后2D点的坐标。这样做，使得变换矩阵$M$完全由相机参数（即焦距$f$）决定。\n![](/img/qicizuobiao_transform.png)\n\n上面的式子只是理想情况下的相机模型，成立的假设条件较强，主要有以下几点假设：\n1. 内假设（和相机本身有关）\n    - 不同方向上焦距相同；\n    - 光学中心在相平面的坐标原点$(0, 0)$\n    - 没有倾斜（no skew）\n2. 外假设（和相机位姿有关，和相机本身参数无关）\n    - 相机没有旋转（坐标轴与世界坐标系方向重合）\n    - 相机没有平移（相机中心与世界坐标系中心重合）\n\n其中，no skew情形我也不知道中文应该如何翻译。这种情况如图所示，由于剪切变形，成像平面的坐标轴并不是严格正交，使得两个轴之间实际存在一个很小的夹角。\n![what is \"skew\"?](/img/camera_skew.png)\n\n下面，按照slide的顺序逐渐将上述假设移去，分析变换矩阵$M$应该如何修正。\n\n#### 理想情况\n理想情况以上假设全部满足，矩阵$M$如下所示。\n![case 1: M](/img/case_1_m.png)\n\n#### 光学中心不在像平面的坐标原点\n假设光学中心位于像平面内坐标为$(u_0, v_0)$的位置。则需要在理想情况计算的坐标基础上加上这个偏移量。所以，矩阵$M$被修正为：\n![case 2: M](/img/case_2_m.png)\n\n#### 像素非正方形\n由于透镜在$x$和$y$方向上的焦距不同，使得两个方向上的比例因子不同，不能再使用同一个$f$计算。修正如下：\n![case 3: M](/img/case_3_m.png)\n\n#### no skew\n这时候$x$方向上其实有$y$轴坐标的分量，所以修正如下：\n![case 4: M](/img/case_4_m.png)\n\n#### 相机的旋转和平移\n相机外部姿态很可能经过了旋转和平移，从而不与世界坐标系完全重合。如图所示。\n![相机旋转和平移](/img/camera_translation_rotation.png)\n\n所以我们需要先对世界坐标系内的3D点做一个齐次变换，将其变换为一个中间坐标系，这个中间坐标系和相机坐标系是重合的。也就是我们的变换矩阵应该是下面这个形式的，其中，$H$是表征旋转和平移的齐次矩阵，$H\\in \\mathbb{R}^{3\\times 4}$。\n$$P^\\prime = MHP$$\n\n首先考虑较为简单的平移。所做的修正如下所示，也就是将原来的$\\mathbb{0}$矩阵变为了一个平移向量。\n![case 5: M](/img/case_m_5.png)\n\n进一步考虑旋转时，我们将旋转分解为绕三个坐标轴的三次旋转，单次旋转矩阵如下所示：\n![绕单轴的旋转矩阵](/img/rotation_matrix.png)\n\n将它们按照旋转顺序左乘起来，可以得到修正后的变换形式如下：\n![case 6: M](/img/case_6_m.png)\n\n#### 最终形式\n综上所示，变换矩阵的最终形式为：\n![最终的投影变换矩阵](/img/generic_projection_matrix.png)\n\n其中，内参数矩阵中有5个自由度，外参数矩阵中有6个自由度（3平移，3旋转）。\n\n上面的内容总结起来，如下图所示。\n![things to remember](/img/camera_model_things_to_remember.png)\n\n## 对极几何\n\n### 基础概念\n如下图所示，给出了对极几何中的几个基本概念。两个相机的中心分别位于$O$点和$O^{'}$点，被观察物体位于$P$点。\n\n![对极几何概念图示](/img/epipolar_fig.png)\n\n- 极点：$e$和$e^\\prime$点分别是$OO^\\prime$连线与两个成像平面的交点，也是相机中心在另一台相机成像平面上对应的像。\n- 极平面：点$O$，$O^\\prime$，$P$点共同确定的平面（灰色）\n- 极线：极平面与两个成像平面的交线，即$pe$和$p^\\prime e^\\prime$（蓝色）\n- 基线：两个相机中心的连线（黄色）\n\n### 极线约束\n从上图可以看出，如果两台相机已经给定，那么给定物体点在左边相机像平面内的成像点，则右边相机成像平面内的点被唯一确定。如何能够找到这个对应点呢？或者换种说法，在对极几何中，如何描述这种不同相机成像点的约束关系呢？\n\n如下图所示，从上面的讨论我们已经知道，物点$P$在左右两个相机成像平面的坐标可以使用由相机内参数矩阵和外参数矩阵确定的齐次变换矩阵得到（下图在标注时，右侧相机的点应被标注为$p^\\prime$）。为了简化问题，在下面的讨论时，我们假定相机都是理想的针孔相机模型，也就是说，变换矩阵只和外参数矩阵（也即是相机的位姿决定）。再做简化，认为世界坐标系与左侧相机的相机坐标系重合。那么，就得到了图上所示的变换矩阵$M$和$M^\\prime$。其中，$R$是左右两个相机之间的旋转矩阵，$T$是$OO^\\prime$有向线段，表明相机中心的位移。\n![极线约束1](/img/epipolar_constraint_1.png)\n\n（下面的推导参考了[博客：计算机视觉基础4——对极几何](http://www.cnblogs.com/gemstone/articles/2294551.html)）。在下面的推导中，我们使用$p^\\prime$表示在相机$O^\\prime$下的向量$O\\prime P$，符号$p$同理。那么，有如下关系成立：\n$R(p-T) = p^\\prime$\n","source":"_posts/cs131-camera.md","raw":"---\ntitle: CS131-立体视觉基础\ndate: 2017-02-02 20:38:55\ntags:\n     - cs131\n     - 公开课\n---\n\n数字图像是现实世界的物体通过光学成像设备在感光材料上的投影。在3D到2D的转换过程中，深度信息丢失。如何从单幅或者多幅图像中恢复出有用的3D信息，需要使用立体视觉的知识进行分析。如下图所示。这篇博客对课程讲义中立体视觉部分做一整理，分别介绍了针孔相机模型和对极集合的基础知识。\n![立体视觉应用](/img/camera_geometry_application.png)\n<!-- more -->\n## 针孔相机模型（Pinhole Camera）\n\n针孔相机是简化的相机模型。光线沿直线传播。物体反射的光线，通过针孔，在成像面形成倒立的影像。针孔与成像面的距离，称为焦距。一般而言，针孔越小，影像越清晰，但针孔太小，会导致衍射，反而令影像模糊。\n\n### 投影几何的重要性质\n\n在对针孔相机详细说明之前，首先介绍投影几何（Projective Geometry）的几条性质。\n\n在投影变换前后，长度和角度信息丢失，但直线仍然保持是直线。如下图所示。原来垂直和平行的台球桌面的边不再保持原有的角度和相对长度，但是仍然是直线。\n![性质1](/img/projective_geometry_property_1.png)\n\n另一方面，虽然3D世界中的平行线在投影变换后相交（我们将交点称为Vanishing point），但是3D世界中互相平行的线，在2D平面上的Vanishing point为同一点。而所有平行线的Vanishing point又在同一条直线上。如下图所示（竖直的绿色平行线在投影变换后仍然保持平行，实际上并不想交，但我们这里认为其交于无穷远的某点）。\n![性质2](/img/projective_geometry_property_2.png)\n\n### 针孔相机模型\n如下图所示，为针孔相机成像原理示意图。外部世界的一点$P$发出（或反射）的光线，经过小孔$O$，成像在在像平面$\\Pi^\\prime$的点$P^\\prime$处。通过简单的相似三角形知识，可以得到图上的关系式。\n![针孔相机模型示意图](/img/pinhole_camera_model.png)\n\n由上图可以知道，3D空间的一点$P$与成像平面上的对应点$P^\\prime$坐标之间的数量关系为：\n$$\\left\\{\\begin{matrix}\nx^\\prime = fx/z \\\\\ny^\\prime = fy/z\n\\end{matrix}\\right.$$\n\n可是，由于变量$z$位于分母的位置，也就是说如果直接写成变换矩阵的话，这个矩阵是和变量$z$有关的，我们不想这样，而是想得到一个完全由相机本身确定的变换矩阵。所以，我们借鉴齐次矩阵的思想，将原来的点增加一个维度，转换为齐次坐标。如下图的两个例子所示。\n![齐次坐标](/img/qicizuobiao.png)\n\n这样，我们可以将3D和2D之间的变换写成下面的形式。当实际计算的时候，我们首先将3D点转换为4维向量（在末尾补1），然后左乘变换矩阵。最后将得到的结果化为末尾元素为1的形式，这样，前面两个元素就代表了变换后2D点的坐标。这样做，使得变换矩阵$M$完全由相机参数（即焦距$f$）决定。\n![](/img/qicizuobiao_transform.png)\n\n上面的式子只是理想情况下的相机模型，成立的假设条件较强，主要有以下几点假设：\n1. 内假设（和相机本身有关）\n    - 不同方向上焦距相同；\n    - 光学中心在相平面的坐标原点$(0, 0)$\n    - 没有倾斜（no skew）\n2. 外假设（和相机位姿有关，和相机本身参数无关）\n    - 相机没有旋转（坐标轴与世界坐标系方向重合）\n    - 相机没有平移（相机中心与世界坐标系中心重合）\n\n其中，no skew情形我也不知道中文应该如何翻译。这种情况如图所示，由于剪切变形，成像平面的坐标轴并不是严格正交，使得两个轴之间实际存在一个很小的夹角。\n![what is \"skew\"?](/img/camera_skew.png)\n\n下面，按照slide的顺序逐渐将上述假设移去，分析变换矩阵$M$应该如何修正。\n\n#### 理想情况\n理想情况以上假设全部满足，矩阵$M$如下所示。\n![case 1: M](/img/case_1_m.png)\n\n#### 光学中心不在像平面的坐标原点\n假设光学中心位于像平面内坐标为$(u_0, v_0)$的位置。则需要在理想情况计算的坐标基础上加上这个偏移量。所以，矩阵$M$被修正为：\n![case 2: M](/img/case_2_m.png)\n\n#### 像素非正方形\n由于透镜在$x$和$y$方向上的焦距不同，使得两个方向上的比例因子不同，不能再使用同一个$f$计算。修正如下：\n![case 3: M](/img/case_3_m.png)\n\n#### no skew\n这时候$x$方向上其实有$y$轴坐标的分量，所以修正如下：\n![case 4: M](/img/case_4_m.png)\n\n#### 相机的旋转和平移\n相机外部姿态很可能经过了旋转和平移，从而不与世界坐标系完全重合。如图所示。\n![相机旋转和平移](/img/camera_translation_rotation.png)\n\n所以我们需要先对世界坐标系内的3D点做一个齐次变换，将其变换为一个中间坐标系，这个中间坐标系和相机坐标系是重合的。也就是我们的变换矩阵应该是下面这个形式的，其中，$H$是表征旋转和平移的齐次矩阵，$H\\in \\mathbb{R}^{3\\times 4}$。\n$$P^\\prime = MHP$$\n\n首先考虑较为简单的平移。所做的修正如下所示，也就是将原来的$\\mathbb{0}$矩阵变为了一个平移向量。\n![case 5: M](/img/case_m_5.png)\n\n进一步考虑旋转时，我们将旋转分解为绕三个坐标轴的三次旋转，单次旋转矩阵如下所示：\n![绕单轴的旋转矩阵](/img/rotation_matrix.png)\n\n将它们按照旋转顺序左乘起来，可以得到修正后的变换形式如下：\n![case 6: M](/img/case_6_m.png)\n\n#### 最终形式\n综上所示，变换矩阵的最终形式为：\n![最终的投影变换矩阵](/img/generic_projection_matrix.png)\n\n其中，内参数矩阵中有5个自由度，外参数矩阵中有6个自由度（3平移，3旋转）。\n\n上面的内容总结起来，如下图所示。\n![things to remember](/img/camera_model_things_to_remember.png)\n\n## 对极几何\n\n### 基础概念\n如下图所示，给出了对极几何中的几个基本概念。两个相机的中心分别位于$O$点和$O^{'}$点，被观察物体位于$P$点。\n\n![对极几何概念图示](/img/epipolar_fig.png)\n\n- 极点：$e$和$e^\\prime$点分别是$OO^\\prime$连线与两个成像平面的交点，也是相机中心在另一台相机成像平面上对应的像。\n- 极平面：点$O$，$O^\\prime$，$P$点共同确定的平面（灰色）\n- 极线：极平面与两个成像平面的交线，即$pe$和$p^\\prime e^\\prime$（蓝色）\n- 基线：两个相机中心的连线（黄色）\n\n### 极线约束\n从上图可以看出，如果两台相机已经给定，那么给定物体点在左边相机像平面内的成像点，则右边相机成像平面内的点被唯一确定。如何能够找到这个对应点呢？或者换种说法，在对极几何中，如何描述这种不同相机成像点的约束关系呢？\n\n如下图所示，从上面的讨论我们已经知道，物点$P$在左右两个相机成像平面的坐标可以使用由相机内参数矩阵和外参数矩阵确定的齐次变换矩阵得到（下图在标注时，右侧相机的点应被标注为$p^\\prime$）。为了简化问题，在下面的讨论时，我们假定相机都是理想的针孔相机模型，也就是说，变换矩阵只和外参数矩阵（也即是相机的位姿决定）。再做简化，认为世界坐标系与左侧相机的相机坐标系重合。那么，就得到了图上所示的变换矩阵$M$和$M^\\prime$。其中，$R$是左右两个相机之间的旋转矩阵，$T$是$OO^\\prime$有向线段，表明相机中心的位移。\n![极线约束1](/img/epipolar_constraint_1.png)\n\n（下面的推导参考了[博客：计算机视觉基础4——对极几何](http://www.cnblogs.com/gemstone/articles/2294551.html)）。在下面的推导中，我们使用$p^\\prime$表示在相机$O^\\prime$下的向量$O\\prime P$，符号$p$同理。那么，有如下关系成立：\n$R(p-T) = p^\\prime$\n","slug":"cs131-camera","published":1,"updated":"2017-02-26T12:09:28.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cizve079j0002l61h8ymnztvv","content":"<p>数字图像是现实世界的物体通过光学成像设备在感光材料上的投影。在3D到2D的转换过程中，深度信息丢失。如何从单幅或者多幅图像中恢复出有用的3D信息，需要使用立体视觉的知识进行分析。如下图所示。这篇博客对课程讲义中立体视觉部分做一整理，分别介绍了针孔相机模型和对极集合的基础知识。<br><img src=\"/img/camera_geometry_application.png\" alt=\"立体视觉应用\"><br><a id=\"more\"></a></p>\n<h2 id=\"针孔相机模型（Pinhole-Camera）\"><a href=\"#针孔相机模型（Pinhole-Camera）\" class=\"headerlink\" title=\"针孔相机模型（Pinhole Camera）\"></a>针孔相机模型（Pinhole Camera）</h2><p>针孔相机是简化的相机模型。光线沿直线传播。物体反射的光线，通过针孔，在成像面形成倒立的影像。针孔与成像面的距离，称为焦距。一般而言，针孔越小，影像越清晰，但针孔太小，会导致衍射，反而令影像模糊。</p>\n<h3 id=\"投影几何的重要性质\"><a href=\"#投影几何的重要性质\" class=\"headerlink\" title=\"投影几何的重要性质\"></a>投影几何的重要性质</h3><p>在对针孔相机详细说明之前，首先介绍投影几何（Projective Geometry）的几条性质。</p>\n<p>在投影变换前后，长度和角度信息丢失，但直线仍然保持是直线。如下图所示。原来垂直和平行的台球桌面的边不再保持原有的角度和相对长度，但是仍然是直线。<br><img src=\"/img/projective_geometry_property_1.png\" alt=\"性质1\"></p>\n<p>另一方面，虽然3D世界中的平行线在投影变换后相交（我们将交点称为Vanishing point），但是3D世界中互相平行的线，在2D平面上的Vanishing point为同一点。而所有平行线的Vanishing point又在同一条直线上。如下图所示（竖直的绿色平行线在投影变换后仍然保持平行，实际上并不想交，但我们这里认为其交于无穷远的某点）。<br><img src=\"/img/projective_geometry_property_2.png\" alt=\"性质2\"></p>\n<h3 id=\"针孔相机模型\"><a href=\"#针孔相机模型\" class=\"headerlink\" title=\"针孔相机模型\"></a>针孔相机模型</h3><p>如下图所示，为针孔相机成像原理示意图。外部世界的一点$P$发出（或反射）的光线，经过小孔$O$，成像在在像平面$\\Pi^\\prime$的点$P^\\prime$处。通过简单的相似三角形知识，可以得到图上的关系式。<br><img src=\"/img/pinhole_camera_model.png\" alt=\"针孔相机模型示意图\"></p>\n<p>由上图可以知道，3D空间的一点$P$与成像平面上的对应点$P^\\prime$坐标之间的数量关系为：</p>\n<script type=\"math/tex; mode=display\">\\left\\{\\begin{matrix}\nx^\\prime = fx/z \\\\\ny^\\prime = fy/z\n\\end{matrix}\\right.</script><p>可是，由于变量$z$位于分母的位置，也就是说如果直接写成变换矩阵的话，这个矩阵是和变量$z$有关的，我们不想这样，而是想得到一个完全由相机本身确定的变换矩阵。所以，我们借鉴齐次矩阵的思想，将原来的点增加一个维度，转换为齐次坐标。如下图的两个例子所示。<br><img src=\"/img/qicizuobiao.png\" alt=\"齐次坐标\"></p>\n<p>这样，我们可以将3D和2D之间的变换写成下面的形式。当实际计算的时候，我们首先将3D点转换为4维向量（在末尾补1），然后左乘变换矩阵。最后将得到的结果化为末尾元素为1的形式，这样，前面两个元素就代表了变换后2D点的坐标。这样做，使得变换矩阵$M$完全由相机参数（即焦距$f$）决定。<br><img src=\"/img/qicizuobiao_transform.png\" alt=\"\"></p>\n<p>上面的式子只是理想情况下的相机模型，成立的假设条件较强，主要有以下几点假设：</p>\n<ol>\n<li>内假设（和相机本身有关）<ul>\n<li>不同方向上焦距相同；</li>\n<li>光学中心在相平面的坐标原点$(0, 0)$</li>\n<li>没有倾斜（no skew）</li>\n</ul>\n</li>\n<li>外假设（和相机位姿有关，和相机本身参数无关）<ul>\n<li>相机没有旋转（坐标轴与世界坐标系方向重合）</li>\n<li>相机没有平移（相机中心与世界坐标系中心重合）</li>\n</ul>\n</li>\n</ol>\n<p>其中，no skew情形我也不知道中文应该如何翻译。这种情况如图所示，由于剪切变形，成像平面的坐标轴并不是严格正交，使得两个轴之间实际存在一个很小的夹角。<br><img src=\"/img/camera_skew.png\" alt=\"what is &quot;skew&quot;?\"></p>\n<p>下面，按照slide的顺序逐渐将上述假设移去，分析变换矩阵$M$应该如何修正。</p>\n<h4 id=\"理想情况\"><a href=\"#理想情况\" class=\"headerlink\" title=\"理想情况\"></a>理想情况</h4><p>理想情况以上假设全部满足，矩阵$M$如下所示。<br><img src=\"/img/case_1_m.png\" alt=\"case 1: M\"></p>\n<h4 id=\"光学中心不在像平面的坐标原点\"><a href=\"#光学中心不在像平面的坐标原点\" class=\"headerlink\" title=\"光学中心不在像平面的坐标原点\"></a>光学中心不在像平面的坐标原点</h4><p>假设光学中心位于像平面内坐标为$(u_0, v_0)$的位置。则需要在理想情况计算的坐标基础上加上这个偏移量。所以，矩阵$M$被修正为：<br><img src=\"/img/case_2_m.png\" alt=\"case 2: M\"></p>\n<h4 id=\"像素非正方形\"><a href=\"#像素非正方形\" class=\"headerlink\" title=\"像素非正方形\"></a>像素非正方形</h4><p>由于透镜在$x$和$y$方向上的焦距不同，使得两个方向上的比例因子不同，不能再使用同一个$f$计算。修正如下：<br><img src=\"/img/case_3_m.png\" alt=\"case 3: M\"></p>\n<h4 id=\"no-skew\"><a href=\"#no-skew\" class=\"headerlink\" title=\"no skew\"></a>no skew</h4><p>这时候$x$方向上其实有$y$轴坐标的分量，所以修正如下：<br><img src=\"/img/case_4_m.png\" alt=\"case 4: M\"></p>\n<h4 id=\"相机的旋转和平移\"><a href=\"#相机的旋转和平移\" class=\"headerlink\" title=\"相机的旋转和平移\"></a>相机的旋转和平移</h4><p>相机外部姿态很可能经过了旋转和平移，从而不与世界坐标系完全重合。如图所示。<br><img src=\"/img/camera_translation_rotation.png\" alt=\"相机旋转和平移\"></p>\n<p>所以我们需要先对世界坐标系内的3D点做一个齐次变换，将其变换为一个中间坐标系，这个中间坐标系和相机坐标系是重合的。也就是我们的变换矩阵应该是下面这个形式的，其中，$H$是表征旋转和平移的齐次矩阵，$H\\in \\mathbb{R}^{3\\times 4}$。</p>\n<script type=\"math/tex; mode=display\">P^\\prime = MHP</script><p>首先考虑较为简单的平移。所做的修正如下所示，也就是将原来的$\\mathbb{0}$矩阵变为了一个平移向量。<br><img src=\"/img/case_m_5.png\" alt=\"case 5: M\"></p>\n<p>进一步考虑旋转时，我们将旋转分解为绕三个坐标轴的三次旋转，单次旋转矩阵如下所示：<br><img src=\"/img/rotation_matrix.png\" alt=\"绕单轴的旋转矩阵\"></p>\n<p>将它们按照旋转顺序左乘起来，可以得到修正后的变换形式如下：<br><img src=\"/img/case_6_m.png\" alt=\"case 6: M\"></p>\n<h4 id=\"最终形式\"><a href=\"#最终形式\" class=\"headerlink\" title=\"最终形式\"></a>最终形式</h4><p>综上所示，变换矩阵的最终形式为：<br><img src=\"/img/generic_projection_matrix.png\" alt=\"最终的投影变换矩阵\"></p>\n<p>其中，内参数矩阵中有5个自由度，外参数矩阵中有6个自由度（3平移，3旋转）。</p>\n<p>上面的内容总结起来，如下图所示。<br><img src=\"/img/camera_model_things_to_remember.png\" alt=\"things to remember\"></p>\n<h2 id=\"对极几何\"><a href=\"#对极几何\" class=\"headerlink\" title=\"对极几何\"></a>对极几何</h2><h3 id=\"基础概念\"><a href=\"#基础概念\" class=\"headerlink\" title=\"基础概念\"></a>基础概念</h3><p>如下图所示，给出了对极几何中的几个基本概念。两个相机的中心分别位于$O$点和$O^{‘}$点，被观察物体位于$P$点。</p>\n<p><img src=\"/img/epipolar_fig.png\" alt=\"对极几何概念图示\"></p>\n<ul>\n<li>极点：$e$和$e^\\prime$点分别是$OO^\\prime$连线与两个成像平面的交点，也是相机中心在另一台相机成像平面上对应的像。</li>\n<li>极平面：点$O$，$O^\\prime$，$P$点共同确定的平面（灰色）</li>\n<li>极线：极平面与两个成像平面的交线，即$pe$和$p^\\prime e^\\prime$（蓝色）</li>\n<li>基线：两个相机中心的连线（黄色）</li>\n</ul>\n<h3 id=\"极线约束\"><a href=\"#极线约束\" class=\"headerlink\" title=\"极线约束\"></a>极线约束</h3><p>从上图可以看出，如果两台相机已经给定，那么给定物体点在左边相机像平面内的成像点，则右边相机成像平面内的点被唯一确定。如何能够找到这个对应点呢？或者换种说法，在对极几何中，如何描述这种不同相机成像点的约束关系呢？</p>\n<p>如下图所示，从上面的讨论我们已经知道，物点$P$在左右两个相机成像平面的坐标可以使用由相机内参数矩阵和外参数矩阵确定的齐次变换矩阵得到（下图在标注时，右侧相机的点应被标注为$p^\\prime$）。为了简化问题，在下面的讨论时，我们假定相机都是理想的针孔相机模型，也就是说，变换矩阵只和外参数矩阵（也即是相机的位姿决定）。再做简化，认为世界坐标系与左侧相机的相机坐标系重合。那么，就得到了图上所示的变换矩阵$M$和$M^\\prime$。其中，$R$是左右两个相机之间的旋转矩阵，$T$是$OO^\\prime$有向线段，表明相机中心的位移。<br><img src=\"/img/epipolar_constraint_1.png\" alt=\"极线约束1\"></p>\n<p>（下面的推导参考了<a href=\"http://www.cnblogs.com/gemstone/articles/2294551.html\" target=\"_blank\" rel=\"external\">博客：计算机视觉基础4——对极几何</a>）。在下面的推导中，我们使用$p^\\prime$表示在相机$O^\\prime$下的向量$O\\prime P$，符号$p$同理。那么，有如下关系成立：<br>$R(p-T) = p^\\prime$</p>\n","excerpt":"<p>数字图像是现实世界的物体通过光学成像设备在感光材料上的投影。在3D到2D的转换过程中，深度信息丢失。如何从单幅或者多幅图像中恢复出有用的3D信息，需要使用立体视觉的知识进行分析。如下图所示。这篇博客对课程讲义中立体视觉部分做一整理，分别介绍了针孔相机模型和对极集合的基础知识。<br><img src=\"/img/camera_geometry_application.png\" alt=\"立体视觉应用\"><br>","more":"</p>\n<h2 id=\"针孔相机模型（Pinhole-Camera）\"><a href=\"#针孔相机模型（Pinhole-Camera）\" class=\"headerlink\" title=\"针孔相机模型（Pinhole Camera）\"></a>针孔相机模型（Pinhole Camera）</h2><p>针孔相机是简化的相机模型。光线沿直线传播。物体反射的光线，通过针孔，在成像面形成倒立的影像。针孔与成像面的距离，称为焦距。一般而言，针孔越小，影像越清晰，但针孔太小，会导致衍射，反而令影像模糊。</p>\n<h3 id=\"投影几何的重要性质\"><a href=\"#投影几何的重要性质\" class=\"headerlink\" title=\"投影几何的重要性质\"></a>投影几何的重要性质</h3><p>在对针孔相机详细说明之前，首先介绍投影几何（Projective Geometry）的几条性质。</p>\n<p>在投影变换前后，长度和角度信息丢失，但直线仍然保持是直线。如下图所示。原来垂直和平行的台球桌面的边不再保持原有的角度和相对长度，但是仍然是直线。<br><img src=\"/img/projective_geometry_property_1.png\" alt=\"性质1\"></p>\n<p>另一方面，虽然3D世界中的平行线在投影变换后相交（我们将交点称为Vanishing point），但是3D世界中互相平行的线，在2D平面上的Vanishing point为同一点。而所有平行线的Vanishing point又在同一条直线上。如下图所示（竖直的绿色平行线在投影变换后仍然保持平行，实际上并不想交，但我们这里认为其交于无穷远的某点）。<br><img src=\"/img/projective_geometry_property_2.png\" alt=\"性质2\"></p>\n<h3 id=\"针孔相机模型\"><a href=\"#针孔相机模型\" class=\"headerlink\" title=\"针孔相机模型\"></a>针孔相机模型</h3><p>如下图所示，为针孔相机成像原理示意图。外部世界的一点$P$发出（或反射）的光线，经过小孔$O$，成像在在像平面$\\Pi^\\prime$的点$P^\\prime$处。通过简单的相似三角形知识，可以得到图上的关系式。<br><img src=\"/img/pinhole_camera_model.png\" alt=\"针孔相机模型示意图\"></p>\n<p>由上图可以知道，3D空间的一点$P$与成像平面上的对应点$P^\\prime$坐标之间的数量关系为：</p>\n<script type=\"math/tex; mode=display\">\\left\\{\\begin{matrix}\nx^\\prime = fx/z \\\\\ny^\\prime = fy/z\n\\end{matrix}\\right.</script><p>可是，由于变量$z$位于分母的位置，也就是说如果直接写成变换矩阵的话，这个矩阵是和变量$z$有关的，我们不想这样，而是想得到一个完全由相机本身确定的变换矩阵。所以，我们借鉴齐次矩阵的思想，将原来的点增加一个维度，转换为齐次坐标。如下图的两个例子所示。<br><img src=\"/img/qicizuobiao.png\" alt=\"齐次坐标\"></p>\n<p>这样，我们可以将3D和2D之间的变换写成下面的形式。当实际计算的时候，我们首先将3D点转换为4维向量（在末尾补1），然后左乘变换矩阵。最后将得到的结果化为末尾元素为1的形式，这样，前面两个元素就代表了变换后2D点的坐标。这样做，使得变换矩阵$M$完全由相机参数（即焦距$f$）决定。<br><img src=\"/img/qicizuobiao_transform.png\" alt=\"\"></p>\n<p>上面的式子只是理想情况下的相机模型，成立的假设条件较强，主要有以下几点假设：</p>\n<ol>\n<li>内假设（和相机本身有关）<ul>\n<li>不同方向上焦距相同；</li>\n<li>光学中心在相平面的坐标原点$(0, 0)$</li>\n<li>没有倾斜（no skew）</li>\n</ul>\n</li>\n<li>外假设（和相机位姿有关，和相机本身参数无关）<ul>\n<li>相机没有旋转（坐标轴与世界坐标系方向重合）</li>\n<li>相机没有平移（相机中心与世界坐标系中心重合）</li>\n</ul>\n</li>\n</ol>\n<p>其中，no skew情形我也不知道中文应该如何翻译。这种情况如图所示，由于剪切变形，成像平面的坐标轴并不是严格正交，使得两个轴之间实际存在一个很小的夹角。<br><img src=\"/img/camera_skew.png\" alt=\"what is &quot;skew&quot;?\"></p>\n<p>下面，按照slide的顺序逐渐将上述假设移去，分析变换矩阵$M$应该如何修正。</p>\n<h4 id=\"理想情况\"><a href=\"#理想情况\" class=\"headerlink\" title=\"理想情况\"></a>理想情况</h4><p>理想情况以上假设全部满足，矩阵$M$如下所示。<br><img src=\"/img/case_1_m.png\" alt=\"case 1: M\"></p>\n<h4 id=\"光学中心不在像平面的坐标原点\"><a href=\"#光学中心不在像平面的坐标原点\" class=\"headerlink\" title=\"光学中心不在像平面的坐标原点\"></a>光学中心不在像平面的坐标原点</h4><p>假设光学中心位于像平面内坐标为$(u_0, v_0)$的位置。则需要在理想情况计算的坐标基础上加上这个偏移量。所以，矩阵$M$被修正为：<br><img src=\"/img/case_2_m.png\" alt=\"case 2: M\"></p>\n<h4 id=\"像素非正方形\"><a href=\"#像素非正方形\" class=\"headerlink\" title=\"像素非正方形\"></a>像素非正方形</h4><p>由于透镜在$x$和$y$方向上的焦距不同，使得两个方向上的比例因子不同，不能再使用同一个$f$计算。修正如下：<br><img src=\"/img/case_3_m.png\" alt=\"case 3: M\"></p>\n<h4 id=\"no-skew\"><a href=\"#no-skew\" class=\"headerlink\" title=\"no skew\"></a>no skew</h4><p>这时候$x$方向上其实有$y$轴坐标的分量，所以修正如下：<br><img src=\"/img/case_4_m.png\" alt=\"case 4: M\"></p>\n<h4 id=\"相机的旋转和平移\"><a href=\"#相机的旋转和平移\" class=\"headerlink\" title=\"相机的旋转和平移\"></a>相机的旋转和平移</h4><p>相机外部姿态很可能经过了旋转和平移，从而不与世界坐标系完全重合。如图所示。<br><img src=\"/img/camera_translation_rotation.png\" alt=\"相机旋转和平移\"></p>\n<p>所以我们需要先对世界坐标系内的3D点做一个齐次变换，将其变换为一个中间坐标系，这个中间坐标系和相机坐标系是重合的。也就是我们的变换矩阵应该是下面这个形式的，其中，$H$是表征旋转和平移的齐次矩阵，$H\\in \\mathbb{R}^{3\\times 4}$。</p>\n<script type=\"math/tex; mode=display\">P^\\prime = MHP</script><p>首先考虑较为简单的平移。所做的修正如下所示，也就是将原来的$\\mathbb{0}$矩阵变为了一个平移向量。<br><img src=\"/img/case_m_5.png\" alt=\"case 5: M\"></p>\n<p>进一步考虑旋转时，我们将旋转分解为绕三个坐标轴的三次旋转，单次旋转矩阵如下所示：<br><img src=\"/img/rotation_matrix.png\" alt=\"绕单轴的旋转矩阵\"></p>\n<p>将它们按照旋转顺序左乘起来，可以得到修正后的变换形式如下：<br><img src=\"/img/case_6_m.png\" alt=\"case 6: M\"></p>\n<h4 id=\"最终形式\"><a href=\"#最终形式\" class=\"headerlink\" title=\"最终形式\"></a>最终形式</h4><p>综上所示，变换矩阵的最终形式为：<br><img src=\"/img/generic_projection_matrix.png\" alt=\"最终的投影变换矩阵\"></p>\n<p>其中，内参数矩阵中有5个自由度，外参数矩阵中有6个自由度（3平移，3旋转）。</p>\n<p>上面的内容总结起来，如下图所示。<br><img src=\"/img/camera_model_things_to_remember.png\" alt=\"things to remember\"></p>\n<h2 id=\"对极几何\"><a href=\"#对极几何\" class=\"headerlink\" title=\"对极几何\"></a>对极几何</h2><h3 id=\"基础概念\"><a href=\"#基础概念\" class=\"headerlink\" title=\"基础概念\"></a>基础概念</h3><p>如下图所示，给出了对极几何中的几个基本概念。两个相机的中心分别位于$O$点和$O^{‘}$点，被观察物体位于$P$点。</p>\n<p><img src=\"/img/epipolar_fig.png\" alt=\"对极几何概念图示\"></p>\n<ul>\n<li>极点：$e$和$e^\\prime$点分别是$OO^\\prime$连线与两个成像平面的交点，也是相机中心在另一台相机成像平面上对应的像。</li>\n<li>极平面：点$O$，$O^\\prime$，$P$点共同确定的平面（灰色）</li>\n<li>极线：极平面与两个成像平面的交线，即$pe$和$p^\\prime e^\\prime$（蓝色）</li>\n<li>基线：两个相机中心的连线（黄色）</li>\n</ul>\n<h3 id=\"极线约束\"><a href=\"#极线约束\" class=\"headerlink\" title=\"极线约束\"></a>极线约束</h3><p>从上图可以看出，如果两台相机已经给定，那么给定物体点在左边相机像平面内的成像点，则右边相机成像平面内的点被唯一确定。如何能够找到这个对应点呢？或者换种说法，在对极几何中，如何描述这种不同相机成像点的约束关系呢？</p>\n<p>如下图所示，从上面的讨论我们已经知道，物点$P$在左右两个相机成像平面的坐标可以使用由相机内参数矩阵和外参数矩阵确定的齐次变换矩阵得到（下图在标注时，右侧相机的点应被标注为$p^\\prime$）。为了简化问题，在下面的讨论时，我们假定相机都是理想的针孔相机模型，也就是说，变换矩阵只和外参数矩阵（也即是相机的位姿决定）。再做简化，认为世界坐标系与左侧相机的相机坐标系重合。那么，就得到了图上所示的变换矩阵$M$和$M^\\prime$。其中，$R$是左右两个相机之间的旋转矩阵，$T$是$OO^\\prime$有向线段，表明相机中心的位移。<br><img src=\"/img/epipolar_constraint_1.png\" alt=\"极线约束1\"></p>\n<p>（下面的推导参考了<a href=\"http://www.cnblogs.com/gemstone/articles/2294551.html\">博客：计算机视觉基础4——对极几何</a>）。在下面的推导中，我们使用$p^\\prime$表示在相机$O^\\prime$下的向量$O\\prime P$，符号$p$同理。那么，有如下关系成立：<br>$R(p-T) = p^\\prime$</p>"},{"title":"CS131-线性滤波器和矩阵的SVD分解","date":"2017-01-23T04:19:05.000Z","_content":"\n数字图像可以看做$\\mathbb{R}^2 \\rightarrow \\mathbb{R}^c$的映射，其中$c$是图像的channel数。使用信号与系统的角度来看，如果单独考察某个channel，可以将图像看做是二维离散系统。\n\n![SVD图示](/img/svd_picture.jpg)\n\n<!-- more -->\n## 卷积\n卷积的概念不再详述，利用不同的kernel与原始图像做卷积，就是对图像进行线性滤波的过程。卷积操作时，以某一个点为中心点，最终结果是这个点以及它的邻域点的线性组合，组合系数由kernel决定。一般kernel的大小取成奇数。如下图所示。（图片来自[博客《图像卷积与滤波的一些知识点》](http://blog.csdn.net/zouxy09/article/details/49080029)）\n![卷积操作示意图](/img/convolution.png)\n\n在卷积操作时，常常需要对图像做padding，常用的padding方法有：\n- zero padding，也就是填充0值。\n- edge replication，也就是复制边缘值进行填充。\n- mirror extension，也就是将图像看做是周期性的，相当于使用对侧像素值进行填充。\n\n## 作业1\n### 调整图像灰度值为0到255\n\n计算相应的k和offset值即可。另外MATLAB中的`uint8`函数可以将结果削顶与截底为0到255之间。\n``` matlab\nscale_ratio = 255.0 / (max_val - min_val);\noffset = -min_val * scale_ratio;\nfixedimg = scale_ratio * dark + offset;\n```\n\n### SVD图像压缩\n\n使用SVD进行图像压缩，下图是将所有奇异值按照从大到小的顺序排列的大小示意图。可以看到，第一个奇异值比其他高出一个数量级。\n![SVD值大小示意图](/img/svd_ranking.png)\n\n#### MATLAB实现\n分别使用10，50， 100个分量进行图像压缩，如下图所示。可以看到，k=10时，已经能够复原出原图像的大致轮廓。当k更大时，更多细节被复原出来。\n![不同分量个数的图像压缩](/img/svd_flower.png)\n\nMATLAB代码如下：\n``` matlab\n%% read image\nim = imread('./flower.bmp');\nim_gray = double(rgb2gray(im));\n[u, s, v] = svd(im_gray);\n%% get sigular value\nsigma = diag(s);\ntop_k = sigma(1:10);\nfigure\nplot(1:length(sigma), sigma, 'r-', 'marker', 's', 'markerfacecolor', 'g');\n\nfigure\nsubplot(2, 2, 1);\nimshow(uint8(im_gray));\ntitle('flower.bmp')\nindex = 2;\nfor k = [10, 50, 100]\n    uk = u(:, 1:k);\n    sk = s(1:k, 1:k);\n    vk = v(:, 1:k);\n    im_rec = uk * sk * vk';\n    subplot(2, 2, index);\n    index = index + 1;\n    imshow(uint8(im_rec));\n    title(sprintf('k = %d', k));\nend\n```\n\n#### 图像SVD压缩中的误差分析\n完全是个人随手推导，不严格的说明：\n\n将矩阵分块。由SVD分解公式$\\mathbf{U}\\mathbf{\\Sigma} \\mathbf{V^\\dagger} = \\mathbf{A}$，把$\\mathbf{U}$按列分块，$\\mathbf{V^\\dagger}$按行分块，有下式成立：\n$$\n\\begin{bmatrix}\nu_1 & u_2 &\\vdots  &u_n\n\\end{bmatrix}\n\\begin{bmatrix}\n\\sigma_1 &  &  & \\\\\\\\\n &  \\sigma_2&  & \\\\\\\\\n &  &  \\ddots& \\\\\\\\\n &  &  &\\sigma_m\n\\end{bmatrix}\n\\begin{bmatrix}\nv_1^\\dagger\\\\\\\\\nv_2^\\dagger\\\\\\\\\n\\dots\\\\\\\\\nv_m^\\dagger\n\\end{bmatrix}=\\mathbf{A}\n$$\n\n由于\n$$\n\\begin{bmatrix}\nu_1 & u_2 &\\vdots  &u_n\n\\end{bmatrix}\n\\begin{bmatrix}\n\\sigma_1 &  &  & \\\\\\\\\n &  \\sigma_2&  & \\\\\\\\\n &  &  \\ddots& \\\\\\\\\n &  &  &\\sigma_m\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n\\sigma_1u_1 & \\sigma_2u_2 &\\vdots  &\\sigma_nu_n\n\\end{bmatrix}\n$$\n\n所以，\n\n$$\\mathbf{A} = \\sum_{i = 1}^{r}\\sigma_iu_iv_i^\\dagger$$\n\n上面的式子和式里面只有$r$项，是因为当$k > r$时，$\\sigma_k = 0$。\n\n所以$$\\mathbf{A} - \\hat{\\mathbf{A}} = \\sum_{i = k+1}^{r}\\sigma_iu_iv_i^\\dagger$$\n\n根绝矩阵范数的[性质](https://zh.wikipedia.org/wiki/矩陣範數)，我们有，\n$$\\left\\lVert\\mathbf{A} - \\hat{\\mathbf{A}}\\right\\rVert \\le \\sum_{i=k+1}^{r}\\sigma_i\\left\\lVert u_i\\right\\rVert\\left\\lVert v_i^\\dagger\\right\\rVert$$\n\n由于$u_i$和$v_i$都是标准正交基，所以范数小于1.故，\n\n$$\\left\\lVert\\mathbf{A} - \\hat{\\mathbf{A}}\\right\\rVert \\le \\sum_{i=k+1}^{r}\\sigma_i$$\n\n取无穷范数，可以知道对于误差矩阵中的任意元素（也就是压缩重建之后任意位置的像素灰度值之差），都有：\n\n$$e \\le \\sum_{i=k+1}^{r}\\sigma_i$$\n\n### SVD与矩阵范数\n\n如果某个函数$f$满足以下的性质，就可以作为矩阵的范数。\n- $f(\\mathbf{A}) = \\mathbf{0} \\Leftrightarrow \\mathbf{A} = \\mathbf{0}$\n- $f(c\\mathbf{A}) = c f(\\mathbf{A}), \\forall c \\in \\mathbb{R}$\n- $f(\\mathbf{A+b}) \\le f(\\mathbf{A}) + f(\\mathbf{B})$\n\n其中，矩阵的2范数可以定义为\n$$\\left\\lVert\\mathbf{A}\\right\\rVert_2 = \\max{\\sqrt{(\\mathbf{A}x)^\\dagger\\mathbf{A}x}}\n$$\n\n其中，$x$是单位向量。上式的意义在于表明矩阵的2范数是对于所有向量，经过该矩阵线性变换后摸长最大的那个变换后向量的长度。\n\n下面，给出不严格的说明，证明矩阵的2范数数值上等于其最大的奇异值。\n\n对于空间内的任意单位向量$x$，利用矩阵的SVD分解，有（为了书写简单，矩阵不再单独加粗）：\n$$(Ax)^\\dagger Ax = x^\\dagger V \\Sigma^\\dagger \\Sigma V^\\dagger x$$\n其中，$U^\\dagger U = I$，已经被消去了。\n\n进一步化简，我们将$V^\\dagger x$看做一个整体，令$\\omega = V\\dagger x$，那么有，\n$$(Ax)^\\dagger Ax = (\\Sigma \\omega)^\\dagger \\Sigma \\omega$$\n\n也就是说，矩阵的2范转换为了$\\Sigma \\omega$的幅值的最大值。由于$\\omega$是酉矩阵和一个单位向量的乘积，所以$\\omega$仍然是单位阵。\n\n由于$\\Sigma$是对角阵，所以$\\omega$与其相乘后，相当于每个分量分别被放大了$\\sigma_i$倍。即\n\n$$\\Sigma \\omega =\n\\begin{bmatrix}\n\\sigma_1 \\omega_1\\\\\\\\\n\\sigma_2 \\omega_2\\\\\\\\\n\\cdots\\\\\\\\\n\\sigma_n \\omega_n\n\\end{bmatrix}\n$$\n\n它的幅值平方为\n\n$$\\left\\lVert \\Sigma \\omega \\right \\rVert ^2 = \\sum_{i=1}^{n}\\sigma_i^2 \\omega_i^2 \\le \\sigma_{1} \\sum_{i=1}^{n}\\omega_i^2 = \\sigma_1^2$$\n\n当且仅当，$\\omega_1 = 1$, $\\omega_k = 0, k > 1$时取得等号。\n\n综上所述，矩阵2范数的值等于其最大的奇异值。\n\n矩阵的另一种范数定义方法Frobenius norm定义如下：\n$$\\left\\lVert A \\right\\rVert_{F} = \\sqrt{\\sum_{i=1}^{m}\\sum_{j=1}^{n}\\left\\vert a_{i,j}\\right\\rvert}$$\n\n如果我们两边平方，可以得到，矩阵的F范数实际等于某个矩阵的迹，见下式：\n\n$$\\left\\lVert A\\right \\rVert_F^2 = \\text{trace}(A^\\dagger A)$$\n\n利用矩阵的SVD分解，可以很容易得出，$\\text{trace}(A^\\dagger A) = \\sum_{i=1}^{r}\\sigma_i^2$\n\n说明如下：\n$$\\text{trace}(A^\\dagger A) = \\text{trace}(V\\Sigma^\\dagger\\Sigma V^\\dagger)$$\n\n由于$V^\\dagger = V^{-1}$，而且$\\text{trace}(BAB^{-1}) = \\text{trace}(A)$，所以，\n$$\\text{trace}(A^\\dagger A) = \\text{trace}(\\Sigma^\\dagger \\Sigma) = \\sum_{i=1}^{r}\\sigma_i^2$$\n\n也就是说，矩阵的F范数等于它的奇异值平方和的平方根。\n\n$$\\left\\lVert A\\right\\rVert_F= \\sqrt{\\sum_{i=1}^{r}\\sigma_i^2}$$\n","source":"_posts/cs131-filter-svd.md","raw":"---\ntitle: CS131-线性滤波器和矩阵的SVD分解\ndate: 2017-01-23 12:19:05\ntags:\n     - cs131\n     - 公开课\n---\n\n数字图像可以看做$\\mathbb{R}^2 \\rightarrow \\mathbb{R}^c$的映射，其中$c$是图像的channel数。使用信号与系统的角度来看，如果单独考察某个channel，可以将图像看做是二维离散系统。\n\n![SVD图示](/img/svd_picture.jpg)\n\n<!-- more -->\n## 卷积\n卷积的概念不再详述，利用不同的kernel与原始图像做卷积，就是对图像进行线性滤波的过程。卷积操作时，以某一个点为中心点，最终结果是这个点以及它的邻域点的线性组合，组合系数由kernel决定。一般kernel的大小取成奇数。如下图所示。（图片来自[博客《图像卷积与滤波的一些知识点》](http://blog.csdn.net/zouxy09/article/details/49080029)）\n![卷积操作示意图](/img/convolution.png)\n\n在卷积操作时，常常需要对图像做padding，常用的padding方法有：\n- zero padding，也就是填充0值。\n- edge replication，也就是复制边缘值进行填充。\n- mirror extension，也就是将图像看做是周期性的，相当于使用对侧像素值进行填充。\n\n## 作业1\n### 调整图像灰度值为0到255\n\n计算相应的k和offset值即可。另外MATLAB中的`uint8`函数可以将结果削顶与截底为0到255之间。\n``` matlab\nscale_ratio = 255.0 / (max_val - min_val);\noffset = -min_val * scale_ratio;\nfixedimg = scale_ratio * dark + offset;\n```\n\n### SVD图像压缩\n\n使用SVD进行图像压缩，下图是将所有奇异值按照从大到小的顺序排列的大小示意图。可以看到，第一个奇异值比其他高出一个数量级。\n![SVD值大小示意图](/img/svd_ranking.png)\n\n#### MATLAB实现\n分别使用10，50， 100个分量进行图像压缩，如下图所示。可以看到，k=10时，已经能够复原出原图像的大致轮廓。当k更大时，更多细节被复原出来。\n![不同分量个数的图像压缩](/img/svd_flower.png)\n\nMATLAB代码如下：\n``` matlab\n%% read image\nim = imread('./flower.bmp');\nim_gray = double(rgb2gray(im));\n[u, s, v] = svd(im_gray);\n%% get sigular value\nsigma = diag(s);\ntop_k = sigma(1:10);\nfigure\nplot(1:length(sigma), sigma, 'r-', 'marker', 's', 'markerfacecolor', 'g');\n\nfigure\nsubplot(2, 2, 1);\nimshow(uint8(im_gray));\ntitle('flower.bmp')\nindex = 2;\nfor k = [10, 50, 100]\n    uk = u(:, 1:k);\n    sk = s(1:k, 1:k);\n    vk = v(:, 1:k);\n    im_rec = uk * sk * vk';\n    subplot(2, 2, index);\n    index = index + 1;\n    imshow(uint8(im_rec));\n    title(sprintf('k = %d', k));\nend\n```\n\n#### 图像SVD压缩中的误差分析\n完全是个人随手推导，不严格的说明：\n\n将矩阵分块。由SVD分解公式$\\mathbf{U}\\mathbf{\\Sigma} \\mathbf{V^\\dagger} = \\mathbf{A}$，把$\\mathbf{U}$按列分块，$\\mathbf{V^\\dagger}$按行分块，有下式成立：\n$$\n\\begin{bmatrix}\nu_1 & u_2 &\\vdots  &u_n\n\\end{bmatrix}\n\\begin{bmatrix}\n\\sigma_1 &  &  & \\\\\\\\\n &  \\sigma_2&  & \\\\\\\\\n &  &  \\ddots& \\\\\\\\\n &  &  &\\sigma_m\n\\end{bmatrix}\n\\begin{bmatrix}\nv_1^\\dagger\\\\\\\\\nv_2^\\dagger\\\\\\\\\n\\dots\\\\\\\\\nv_m^\\dagger\n\\end{bmatrix}=\\mathbf{A}\n$$\n\n由于\n$$\n\\begin{bmatrix}\nu_1 & u_2 &\\vdots  &u_n\n\\end{bmatrix}\n\\begin{bmatrix}\n\\sigma_1 &  &  & \\\\\\\\\n &  \\sigma_2&  & \\\\\\\\\n &  &  \\ddots& \\\\\\\\\n &  &  &\\sigma_m\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n\\sigma_1u_1 & \\sigma_2u_2 &\\vdots  &\\sigma_nu_n\n\\end{bmatrix}\n$$\n\n所以，\n\n$$\\mathbf{A} = \\sum_{i = 1}^{r}\\sigma_iu_iv_i^\\dagger$$\n\n上面的式子和式里面只有$r$项，是因为当$k > r$时，$\\sigma_k = 0$。\n\n所以$$\\mathbf{A} - \\hat{\\mathbf{A}} = \\sum_{i = k+1}^{r}\\sigma_iu_iv_i^\\dagger$$\n\n根绝矩阵范数的[性质](https://zh.wikipedia.org/wiki/矩陣範數)，我们有，\n$$\\left\\lVert\\mathbf{A} - \\hat{\\mathbf{A}}\\right\\rVert \\le \\sum_{i=k+1}^{r}\\sigma_i\\left\\lVert u_i\\right\\rVert\\left\\lVert v_i^\\dagger\\right\\rVert$$\n\n由于$u_i$和$v_i$都是标准正交基，所以范数小于1.故，\n\n$$\\left\\lVert\\mathbf{A} - \\hat{\\mathbf{A}}\\right\\rVert \\le \\sum_{i=k+1}^{r}\\sigma_i$$\n\n取无穷范数，可以知道对于误差矩阵中的任意元素（也就是压缩重建之后任意位置的像素灰度值之差），都有：\n\n$$e \\le \\sum_{i=k+1}^{r}\\sigma_i$$\n\n### SVD与矩阵范数\n\n如果某个函数$f$满足以下的性质，就可以作为矩阵的范数。\n- $f(\\mathbf{A}) = \\mathbf{0} \\Leftrightarrow \\mathbf{A} = \\mathbf{0}$\n- $f(c\\mathbf{A}) = c f(\\mathbf{A}), \\forall c \\in \\mathbb{R}$\n- $f(\\mathbf{A+b}) \\le f(\\mathbf{A}) + f(\\mathbf{B})$\n\n其中，矩阵的2范数可以定义为\n$$\\left\\lVert\\mathbf{A}\\right\\rVert_2 = \\max{\\sqrt{(\\mathbf{A}x)^\\dagger\\mathbf{A}x}}\n$$\n\n其中，$x$是单位向量。上式的意义在于表明矩阵的2范数是对于所有向量，经过该矩阵线性变换后摸长最大的那个变换后向量的长度。\n\n下面，给出不严格的说明，证明矩阵的2范数数值上等于其最大的奇异值。\n\n对于空间内的任意单位向量$x$，利用矩阵的SVD分解，有（为了书写简单，矩阵不再单独加粗）：\n$$(Ax)^\\dagger Ax = x^\\dagger V \\Sigma^\\dagger \\Sigma V^\\dagger x$$\n其中，$U^\\dagger U = I$，已经被消去了。\n\n进一步化简，我们将$V^\\dagger x$看做一个整体，令$\\omega = V\\dagger x$，那么有，\n$$(Ax)^\\dagger Ax = (\\Sigma \\omega)^\\dagger \\Sigma \\omega$$\n\n也就是说，矩阵的2范转换为了$\\Sigma \\omega$的幅值的最大值。由于$\\omega$是酉矩阵和一个单位向量的乘积，所以$\\omega$仍然是单位阵。\n\n由于$\\Sigma$是对角阵，所以$\\omega$与其相乘后，相当于每个分量分别被放大了$\\sigma_i$倍。即\n\n$$\\Sigma \\omega =\n\\begin{bmatrix}\n\\sigma_1 \\omega_1\\\\\\\\\n\\sigma_2 \\omega_2\\\\\\\\\n\\cdots\\\\\\\\\n\\sigma_n \\omega_n\n\\end{bmatrix}\n$$\n\n它的幅值平方为\n\n$$\\left\\lVert \\Sigma \\omega \\right \\rVert ^2 = \\sum_{i=1}^{n}\\sigma_i^2 \\omega_i^2 \\le \\sigma_{1} \\sum_{i=1}^{n}\\omega_i^2 = \\sigma_1^2$$\n\n当且仅当，$\\omega_1 = 1$, $\\omega_k = 0, k > 1$时取得等号。\n\n综上所述，矩阵2范数的值等于其最大的奇异值。\n\n矩阵的另一种范数定义方法Frobenius norm定义如下：\n$$\\left\\lVert A \\right\\rVert_{F} = \\sqrt{\\sum_{i=1}^{m}\\sum_{j=1}^{n}\\left\\vert a_{i,j}\\right\\rvert}$$\n\n如果我们两边平方，可以得到，矩阵的F范数实际等于某个矩阵的迹，见下式：\n\n$$\\left\\lVert A\\right \\rVert_F^2 = \\text{trace}(A^\\dagger A)$$\n\n利用矩阵的SVD分解，可以很容易得出，$\\text{trace}(A^\\dagger A) = \\sum_{i=1}^{r}\\sigma_i^2$\n\n说明如下：\n$$\\text{trace}(A^\\dagger A) = \\text{trace}(V\\Sigma^\\dagger\\Sigma V^\\dagger)$$\n\n由于$V^\\dagger = V^{-1}$，而且$\\text{trace}(BAB^{-1}) = \\text{trace}(A)$，所以，\n$$\\text{trace}(A^\\dagger A) = \\text{trace}(\\Sigma^\\dagger \\Sigma) = \\sum_{i=1}^{r}\\sigma_i^2$$\n\n也就是说，矩阵的F范数等于它的奇异值平方和的平方根。\n\n$$\\left\\lVert A\\right\\rVert_F= \\sqrt{\\sum_{i=1}^{r}\\sigma_i^2}$$\n","slug":"cs131-filter-svd","published":1,"updated":"2017-02-26T12:09:28.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cizve079r0005l61hzvv9pn1w","content":"<p>数字图像可以看做$\\mathbb{R}^2 \\rightarrow \\mathbb{R}^c$的映射，其中$c$是图像的channel数。使用信号与系统的角度来看，如果单独考察某个channel，可以将图像看做是二维离散系统。</p>\n<p><img src=\"/img/svd_picture.jpg\" alt=\"SVD图示\"></p>\n<a id=\"more\"></a>\n<h2 id=\"卷积\"><a href=\"#卷积\" class=\"headerlink\" title=\"卷积\"></a>卷积</h2><p>卷积的概念不再详述，利用不同的kernel与原始图像做卷积，就是对图像进行线性滤波的过程。卷积操作时，以某一个点为中心点，最终结果是这个点以及它的邻域点的线性组合，组合系数由kernel决定。一般kernel的大小取成奇数。如下图所示。（图片来自<a href=\"http://blog.csdn.net/zouxy09/article/details/49080029\" target=\"_blank\" rel=\"external\">博客《图像卷积与滤波的一些知识点》</a>）<br><img src=\"/img/convolution.png\" alt=\"卷积操作示意图\"></p>\n<p>在卷积操作时，常常需要对图像做padding，常用的padding方法有：</p>\n<ul>\n<li>zero padding，也就是填充0值。</li>\n<li>edge replication，也就是复制边缘值进行填充。</li>\n<li>mirror extension，也就是将图像看做是周期性的，相当于使用对侧像素值进行填充。</li>\n</ul>\n<h2 id=\"作业1\"><a href=\"#作业1\" class=\"headerlink\" title=\"作业1\"></a>作业1</h2><h3 id=\"调整图像灰度值为0到255\"><a href=\"#调整图像灰度值为0到255\" class=\"headerlink\" title=\"调整图像灰度值为0到255\"></a>调整图像灰度值为0到255</h3><p>计算相应的k和offset值即可。另外MATLAB中的<code>uint8</code>函数可以将结果削顶与截底为0到255之间。<br><figure class=\"highlight matlab\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\">scale_ratio = <span class=\"number\">255.0</span> / (max_val - min_val);</div><div class=\"line\">offset = -min_val * scale_ratio;</div><div class=\"line\">fixedimg = scale_ratio * dark + offset;</div></pre></td></tr></table></figure></p>\n<h3 id=\"SVD图像压缩\"><a href=\"#SVD图像压缩\" class=\"headerlink\" title=\"SVD图像压缩\"></a>SVD图像压缩</h3><p>使用SVD进行图像压缩，下图是将所有奇异值按照从大到小的顺序排列的大小示意图。可以看到，第一个奇异值比其他高出一个数量级。<br><img src=\"/img/svd_ranking.png\" alt=\"SVD值大小示意图\"></p>\n<h4 id=\"MATLAB实现\"><a href=\"#MATLAB实现\" class=\"headerlink\" title=\"MATLAB实现\"></a>MATLAB实现</h4><p>分别使用10，50， 100个分量进行图像压缩，如下图所示。可以看到，k=10时，已经能够复原出原图像的大致轮廓。当k更大时，更多细节被复原出来。<br><img src=\"/img/svd_flower.png\" alt=\"不同分量个数的图像压缩\"></p>\n<p>MATLAB代码如下：<br><figure class=\"highlight matlab\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\">%% read image</span></div><div class=\"line\">im = imread(<span class=\"string\">'./flower.bmp'</span>);</div><div class=\"line\">im_gray = double(rgb2gray(im));</div><div class=\"line\">[u, s, v] = svd(im_gray);</div><div class=\"line\"><span class=\"comment\">%% get sigular value</span></div><div class=\"line\">sigma = <span class=\"built_in\">diag</span>(s);</div><div class=\"line\">top_k = sigma(<span class=\"number\">1</span>:<span class=\"number\">10</span>);</div><div class=\"line\">figure</div><div class=\"line\">plot(<span class=\"number\">1</span>:<span class=\"built_in\">length</span>(sigma), sigma, <span class=\"string\">'r-'</span>, <span class=\"string\">'marker'</span>, <span class=\"string\">'s'</span>, <span class=\"string\">'markerfacecolor'</span>, <span class=\"string\">'g'</span>);</div><div class=\"line\"></div><div class=\"line\">figure</div><div class=\"line\">subplot(<span class=\"number\">2</span>, <span class=\"number\">2</span>, <span class=\"number\">1</span>);</div><div class=\"line\">imshow(uint8(im_gray));</div><div class=\"line\">title(<span class=\"string\">'flower.bmp'</span>)</div><div class=\"line\">index = <span class=\"number\">2</span>;</div><div class=\"line\"><span class=\"keyword\">for</span> k = [<span class=\"number\">10</span>, <span class=\"number\">50</span>, <span class=\"number\">100</span>]</div><div class=\"line\">    uk = u(:, <span class=\"number\">1</span>:k);</div><div class=\"line\">    sk = s(<span class=\"number\">1</span>:k, <span class=\"number\">1</span>:k);</div><div class=\"line\">    vk = v(:, <span class=\"number\">1</span>:k);</div><div class=\"line\">    im_rec = uk * sk * vk';</div><div class=\"line\">    subplot(<span class=\"number\">2</span>, <span class=\"number\">2</span>, index);</div><div class=\"line\">    index = index + <span class=\"number\">1</span>;</div><div class=\"line\">    imshow(uint8(im_rec));</div><div class=\"line\">    title(sprintf(<span class=\"string\">'k = %d'</span>, k));</div><div class=\"line\"><span class=\"keyword\">end</span></div></pre></td></tr></table></figure></p>\n<h4 id=\"图像SVD压缩中的误差分析\"><a href=\"#图像SVD压缩中的误差分析\" class=\"headerlink\" title=\"图像SVD压缩中的误差分析\"></a>图像SVD压缩中的误差分析</h4><p>完全是个人随手推导，不严格的说明：</p>\n<p>将矩阵分块。由SVD分解公式$\\mathbf{U}\\mathbf{\\Sigma} \\mathbf{V^\\dagger} = \\mathbf{A}$，把$\\mathbf{U}$按列分块，$\\mathbf{V^\\dagger}$按行分块，有下式成立：</p>\n<script type=\"math/tex; mode=display\">\n\\begin{bmatrix}\nu_1 & u_2 &\\vdots  &u_n\n\\end{bmatrix}\n\\begin{bmatrix}\n\\sigma_1 &  &  & \\\\\\\\\n &  \\sigma_2&  & \\\\\\\\\n &  &  \\ddots& \\\\\\\\\n &  &  &\\sigma_m\n\\end{bmatrix}\n\\begin{bmatrix}\nv_1^\\dagger\\\\\\\\\nv_2^\\dagger\\\\\\\\\n\\dots\\\\\\\\\nv_m^\\dagger\n\\end{bmatrix}=\\mathbf{A}</script><p>由于</p>\n<script type=\"math/tex; mode=display\">\n\\begin{bmatrix}\nu_1 & u_2 &\\vdots  &u_n\n\\end{bmatrix}\n\\begin{bmatrix}\n\\sigma_1 &  &  & \\\\\\\\\n &  \\sigma_2&  & \\\\\\\\\n &  &  \\ddots& \\\\\\\\\n &  &  &\\sigma_m\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n\\sigma_1u_1 & \\sigma_2u_2 &\\vdots  &\\sigma_nu_n\n\\end{bmatrix}</script><p>所以，</p>\n<script type=\"math/tex; mode=display\">\\mathbf{A} = \\sum_{i = 1}^{r}\\sigma_iu_iv_i^\\dagger</script><p>上面的式子和式里面只有$r$项，是因为当$k &gt; r$时，$\\sigma_k = 0$。</p>\n<p>所以<script type=\"math/tex\">\\mathbf{A} - \\hat{\\mathbf{A}} = \\sum_{i = k+1}^{r}\\sigma_iu_iv_i^\\dagger</script></p>\n<p>根绝矩阵范数的<a href=\"https://zh.wikipedia.org/wiki/矩陣範數\" target=\"_blank\" rel=\"external\">性质</a>，我们有，</p>\n<script type=\"math/tex; mode=display\">\\left\\lVert\\mathbf{A} - \\hat{\\mathbf{A}}\\right\\rVert \\le \\sum_{i=k+1}^{r}\\sigma_i\\left\\lVert u_i\\right\\rVert\\left\\lVert v_i^\\dagger\\right\\rVert</script><p>由于$u_i$和$v_i$都是标准正交基，所以范数小于1.故，</p>\n<script type=\"math/tex; mode=display\">\\left\\lVert\\mathbf{A} - \\hat{\\mathbf{A}}\\right\\rVert \\le \\sum_{i=k+1}^{r}\\sigma_i</script><p>取无穷范数，可以知道对于误差矩阵中的任意元素（也就是压缩重建之后任意位置的像素灰度值之差），都有：</p>\n<script type=\"math/tex; mode=display\">e \\le \\sum_{i=k+1}^{r}\\sigma_i</script><h3 id=\"SVD与矩阵范数\"><a href=\"#SVD与矩阵范数\" class=\"headerlink\" title=\"SVD与矩阵范数\"></a>SVD与矩阵范数</h3><p>如果某个函数$f$满足以下的性质，就可以作为矩阵的范数。</p>\n<ul>\n<li>$f(\\mathbf{A}) = \\mathbf{0} \\Leftrightarrow \\mathbf{A} = \\mathbf{0}$</li>\n<li>$f(c\\mathbf{A}) = c f(\\mathbf{A}), \\forall c \\in \\mathbb{R}$</li>\n<li>$f(\\mathbf{A+b}) \\le f(\\mathbf{A}) + f(\\mathbf{B})$</li>\n</ul>\n<p>其中，矩阵的2范数可以定义为</p>\n<script type=\"math/tex; mode=display\">\\left\\lVert\\mathbf{A}\\right\\rVert_2 = \\max{\\sqrt{(\\mathbf{A}x)^\\dagger\\mathbf{A}x}}</script><p>其中，$x$是单位向量。上式的意义在于表明矩阵的2范数是对于所有向量，经过该矩阵线性变换后摸长最大的那个变换后向量的长度。</p>\n<p>下面，给出不严格的说明，证明矩阵的2范数数值上等于其最大的奇异值。</p>\n<p>对于空间内的任意单位向量$x$，利用矩阵的SVD分解，有（为了书写简单，矩阵不再单独加粗）：</p>\n<script type=\"math/tex; mode=display\">(Ax)^\\dagger Ax = x^\\dagger V \\Sigma^\\dagger \\Sigma V^\\dagger x</script><p>其中，$U^\\dagger U = I$，已经被消去了。</p>\n<p>进一步化简，我们将$V^\\dagger x$看做一个整体，令$\\omega = V\\dagger x$，那么有，</p>\n<script type=\"math/tex; mode=display\">(Ax)^\\dagger Ax = (\\Sigma \\omega)^\\dagger \\Sigma \\omega</script><p>也就是说，矩阵的2范转换为了$\\Sigma \\omega$的幅值的最大值。由于$\\omega$是酉矩阵和一个单位向量的乘积，所以$\\omega$仍然是单位阵。</p>\n<p>由于$\\Sigma$是对角阵，所以$\\omega$与其相乘后，相当于每个分量分别被放大了$\\sigma_i$倍。即</p>\n<script type=\"math/tex; mode=display\">\\Sigma \\omega =\n\\begin{bmatrix}\n\\sigma_1 \\omega_1\\\\\\\\\n\\sigma_2 \\omega_2\\\\\\\\\n\\cdots\\\\\\\\\n\\sigma_n \\omega_n\n\\end{bmatrix}</script><p>它的幅值平方为</p>\n<script type=\"math/tex; mode=display\">\\left\\lVert \\Sigma \\omega \\right \\rVert ^2 = \\sum_{i=1}^{n}\\sigma_i^2 \\omega_i^2 \\le \\sigma_{1} \\sum_{i=1}^{n}\\omega_i^2 = \\sigma_1^2</script><p>当且仅当，$\\omega_1 = 1$, $\\omega_k = 0, k &gt; 1$时取得等号。</p>\n<p>综上所述，矩阵2范数的值等于其最大的奇异值。</p>\n<p>矩阵的另一种范数定义方法Frobenius norm定义如下：</p>\n<script type=\"math/tex; mode=display\">\\left\\lVert A \\right\\rVert_{F} = \\sqrt{\\sum_{i=1}^{m}\\sum_{j=1}^{n}\\left\\vert a_{i,j}\\right\\rvert}</script><p>如果我们两边平方，可以得到，矩阵的F范数实际等于某个矩阵的迹，见下式：</p>\n<script type=\"math/tex; mode=display\">\\left\\lVert A\\right \\rVert_F^2 = \\text{trace}(A^\\dagger A)</script><p>利用矩阵的SVD分解，可以很容易得出，$\\text{trace}(A^\\dagger A) = \\sum_{i=1}^{r}\\sigma_i^2$</p>\n<p>说明如下：</p>\n<script type=\"math/tex; mode=display\">\\text{trace}(A^\\dagger A) = \\text{trace}(V\\Sigma^\\dagger\\Sigma V^\\dagger)</script><p>由于$V^\\dagger = V^{-1}$，而且$\\text{trace}(BAB^{-1}) = \\text{trace}(A)$，所以，</p>\n<script type=\"math/tex; mode=display\">\\text{trace}(A^\\dagger A) = \\text{trace}(\\Sigma^\\dagger \\Sigma) = \\sum_{i=1}^{r}\\sigma_i^2</script><p>也就是说，矩阵的F范数等于它的奇异值平方和的平方根。</p>\n<script type=\"math/tex; mode=display\">\\left\\lVert A\\right\\rVert_F= \\sqrt{\\sum_{i=1}^{r}\\sigma_i^2}</script>","excerpt":"<p>数字图像可以看做$\\mathbb{R}^2 \\rightarrow \\mathbb{R}^c$的映射，其中$c$是图像的channel数。使用信号与系统的角度来看，如果单独考察某个channel，可以将图像看做是二维离散系统。</p>\n<p><img src=\"/img/svd_picture.jpg\" alt=\"SVD图示\"></p>","more":"<h2 id=\"卷积\"><a href=\"#卷积\" class=\"headerlink\" title=\"卷积\"></a>卷积</h2><p>卷积的概念不再详述，利用不同的kernel与原始图像做卷积，就是对图像进行线性滤波的过程。卷积操作时，以某一个点为中心点，最终结果是这个点以及它的邻域点的线性组合，组合系数由kernel决定。一般kernel的大小取成奇数。如下图所示。（图片来自<a href=\"http://blog.csdn.net/zouxy09/article/details/49080029\">博客《图像卷积与滤波的一些知识点》</a>）<br><img src=\"/img/convolution.png\" alt=\"卷积操作示意图\"></p>\n<p>在卷积操作时，常常需要对图像做padding，常用的padding方法有：</p>\n<ul>\n<li>zero padding，也就是填充0值。</li>\n<li>edge replication，也就是复制边缘值进行填充。</li>\n<li>mirror extension，也就是将图像看做是周期性的，相当于使用对侧像素值进行填充。</li>\n</ul>\n<h2 id=\"作业1\"><a href=\"#作业1\" class=\"headerlink\" title=\"作业1\"></a>作业1</h2><h3 id=\"调整图像灰度值为0到255\"><a href=\"#调整图像灰度值为0到255\" class=\"headerlink\" title=\"调整图像灰度值为0到255\"></a>调整图像灰度值为0到255</h3><p>计算相应的k和offset值即可。另外MATLAB中的<code>uint8</code>函数可以将结果削顶与截底为0到255之间。<br><figure class=\"highlight matlab\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\">scale_ratio = <span class=\"number\">255.0</span> / (max_val - min_val);</div><div class=\"line\">offset = -min_val * scale_ratio;</div><div class=\"line\">fixedimg = scale_ratio * dark + offset;</div></pre></td></tr></table></figure></p>\n<h3 id=\"SVD图像压缩\"><a href=\"#SVD图像压缩\" class=\"headerlink\" title=\"SVD图像压缩\"></a>SVD图像压缩</h3><p>使用SVD进行图像压缩，下图是将所有奇异值按照从大到小的顺序排列的大小示意图。可以看到，第一个奇异值比其他高出一个数量级。<br><img src=\"/img/svd_ranking.png\" alt=\"SVD值大小示意图\"></p>\n<h4 id=\"MATLAB实现\"><a href=\"#MATLAB实现\" class=\"headerlink\" title=\"MATLAB实现\"></a>MATLAB实现</h4><p>分别使用10，50， 100个分量进行图像压缩，如下图所示。可以看到，k=10时，已经能够复原出原图像的大致轮廓。当k更大时，更多细节被复原出来。<br><img src=\"/img/svd_flower.png\" alt=\"不同分量个数的图像压缩\"></p>\n<p>MATLAB代码如下：<br><figure class=\"highlight matlab\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\">%% read image</span></div><div class=\"line\">im = imread(<span class=\"string\">'./flower.bmp'</span>);</div><div class=\"line\">im_gray = double(rgb2gray(im));</div><div class=\"line\">[u, s, v] = svd(im_gray);</div><div class=\"line\"><span class=\"comment\">%% get sigular value</span></div><div class=\"line\">sigma = <span class=\"built_in\">diag</span>(s);</div><div class=\"line\">top_k = sigma(<span class=\"number\">1</span>:<span class=\"number\">10</span>);</div><div class=\"line\">figure</div><div class=\"line\">plot(<span class=\"number\">1</span>:<span class=\"built_in\">length</span>(sigma), sigma, <span class=\"string\">'r-'</span>, <span class=\"string\">'marker'</span>, <span class=\"string\">'s'</span>, <span class=\"string\">'markerfacecolor'</span>, <span class=\"string\">'g'</span>);</div><div class=\"line\"></div><div class=\"line\">figure</div><div class=\"line\">subplot(<span class=\"number\">2</span>, <span class=\"number\">2</span>, <span class=\"number\">1</span>);</div><div class=\"line\">imshow(uint8(im_gray));</div><div class=\"line\">title(<span class=\"string\">'flower.bmp'</span>)</div><div class=\"line\">index = <span class=\"number\">2</span>;</div><div class=\"line\"><span class=\"keyword\">for</span> k = [<span class=\"number\">10</span>, <span class=\"number\">50</span>, <span class=\"number\">100</span>]</div><div class=\"line\">    uk = u(:, <span class=\"number\">1</span>:k);</div><div class=\"line\">    sk = s(<span class=\"number\">1</span>:k, <span class=\"number\">1</span>:k);</div><div class=\"line\">    vk = v(:, <span class=\"number\">1</span>:k);</div><div class=\"line\">    im_rec = uk * sk * vk';</div><div class=\"line\">    subplot(<span class=\"number\">2</span>, <span class=\"number\">2</span>, index);</div><div class=\"line\">    index = index + <span class=\"number\">1</span>;</div><div class=\"line\">    imshow(uint8(im_rec));</div><div class=\"line\">    title(sprintf(<span class=\"string\">'k = %d'</span>, k));</div><div class=\"line\"><span class=\"keyword\">end</span></div></pre></td></tr></table></figure></p>\n<h4 id=\"图像SVD压缩中的误差分析\"><a href=\"#图像SVD压缩中的误差分析\" class=\"headerlink\" title=\"图像SVD压缩中的误差分析\"></a>图像SVD压缩中的误差分析</h4><p>完全是个人随手推导，不严格的说明：</p>\n<p>将矩阵分块。由SVD分解公式$\\mathbf{U}\\mathbf{\\Sigma} \\mathbf{V^\\dagger} = \\mathbf{A}$，把$\\mathbf{U}$按列分块，$\\mathbf{V^\\dagger}$按行分块，有下式成立：</p>\n<script type=\"math/tex; mode=display\">\n\\begin{bmatrix}\nu_1 & u_2 &\\vdots  &u_n\n\\end{bmatrix}\n\\begin{bmatrix}\n\\sigma_1 &  &  & \\\\\\\\\n &  \\sigma_2&  & \\\\\\\\\n &  &  \\ddots& \\\\\\\\\n &  &  &\\sigma_m\n\\end{bmatrix}\n\\begin{bmatrix}\nv_1^\\dagger\\\\\\\\\nv_2^\\dagger\\\\\\\\\n\\dots\\\\\\\\\nv_m^\\dagger\n\\end{bmatrix}=\\mathbf{A}</script><p>由于</p>\n<script type=\"math/tex; mode=display\">\n\\begin{bmatrix}\nu_1 & u_2 &\\vdots  &u_n\n\\end{bmatrix}\n\\begin{bmatrix}\n\\sigma_1 &  &  & \\\\\\\\\n &  \\sigma_2&  & \\\\\\\\\n &  &  \\ddots& \\\\\\\\\n &  &  &\\sigma_m\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n\\sigma_1u_1 & \\sigma_2u_2 &\\vdots  &\\sigma_nu_n\n\\end{bmatrix}</script><p>所以，</p>\n<script type=\"math/tex; mode=display\">\\mathbf{A} = \\sum_{i = 1}^{r}\\sigma_iu_iv_i^\\dagger</script><p>上面的式子和式里面只有$r$项，是因为当$k &gt; r$时，$\\sigma_k = 0$。</p>\n<p>所以<script type=\"math/tex\">\\mathbf{A} - \\hat{\\mathbf{A}} = \\sum_{i = k+1}^{r}\\sigma_iu_iv_i^\\dagger</script></p>\n<p>根绝矩阵范数的<a href=\"https://zh.wikipedia.org/wiki/矩陣範數\">性质</a>，我们有，</p>\n<script type=\"math/tex; mode=display\">\\left\\lVert\\mathbf{A} - \\hat{\\mathbf{A}}\\right\\rVert \\le \\sum_{i=k+1}^{r}\\sigma_i\\left\\lVert u_i\\right\\rVert\\left\\lVert v_i^\\dagger\\right\\rVert</script><p>由于$u_i$和$v_i$都是标准正交基，所以范数小于1.故，</p>\n<script type=\"math/tex; mode=display\">\\left\\lVert\\mathbf{A} - \\hat{\\mathbf{A}}\\right\\rVert \\le \\sum_{i=k+1}^{r}\\sigma_i</script><p>取无穷范数，可以知道对于误差矩阵中的任意元素（也就是压缩重建之后任意位置的像素灰度值之差），都有：</p>\n<script type=\"math/tex; mode=display\">e \\le \\sum_{i=k+1}^{r}\\sigma_i</script><h3 id=\"SVD与矩阵范数\"><a href=\"#SVD与矩阵范数\" class=\"headerlink\" title=\"SVD与矩阵范数\"></a>SVD与矩阵范数</h3><p>如果某个函数$f$满足以下的性质，就可以作为矩阵的范数。</p>\n<ul>\n<li>$f(\\mathbf{A}) = \\mathbf{0} \\Leftrightarrow \\mathbf{A} = \\mathbf{0}$</li>\n<li>$f(c\\mathbf{A}) = c f(\\mathbf{A}), \\forall c \\in \\mathbb{R}$</li>\n<li>$f(\\mathbf{A+b}) \\le f(\\mathbf{A}) + f(\\mathbf{B})$</li>\n</ul>\n<p>其中，矩阵的2范数可以定义为</p>\n<script type=\"math/tex; mode=display\">\\left\\lVert\\mathbf{A}\\right\\rVert_2 = \\max{\\sqrt{(\\mathbf{A}x)^\\dagger\\mathbf{A}x}}</script><p>其中，$x$是单位向量。上式的意义在于表明矩阵的2范数是对于所有向量，经过该矩阵线性变换后摸长最大的那个变换后向量的长度。</p>\n<p>下面，给出不严格的说明，证明矩阵的2范数数值上等于其最大的奇异值。</p>\n<p>对于空间内的任意单位向量$x$，利用矩阵的SVD分解，有（为了书写简单，矩阵不再单独加粗）：</p>\n<script type=\"math/tex; mode=display\">(Ax)^\\dagger Ax = x^\\dagger V \\Sigma^\\dagger \\Sigma V^\\dagger x</script><p>其中，$U^\\dagger U = I$，已经被消去了。</p>\n<p>进一步化简，我们将$V^\\dagger x$看做一个整体，令$\\omega = V\\dagger x$，那么有，</p>\n<script type=\"math/tex; mode=display\">(Ax)^\\dagger Ax = (\\Sigma \\omega)^\\dagger \\Sigma \\omega</script><p>也就是说，矩阵的2范转换为了$\\Sigma \\omega$的幅值的最大值。由于$\\omega$是酉矩阵和一个单位向量的乘积，所以$\\omega$仍然是单位阵。</p>\n<p>由于$\\Sigma$是对角阵，所以$\\omega$与其相乘后，相当于每个分量分别被放大了$\\sigma_i$倍。即</p>\n<script type=\"math/tex; mode=display\">\\Sigma \\omega =\n\\begin{bmatrix}\n\\sigma_1 \\omega_1\\\\\\\\\n\\sigma_2 \\omega_2\\\\\\\\\n\\cdots\\\\\\\\\n\\sigma_n \\omega_n\n\\end{bmatrix}</script><p>它的幅值平方为</p>\n<script type=\"math/tex; mode=display\">\\left\\lVert \\Sigma \\omega \\right \\rVert ^2 = \\sum_{i=1}^{n}\\sigma_i^2 \\omega_i^2 \\le \\sigma_{1} \\sum_{i=1}^{n}\\omega_i^2 = \\sigma_1^2</script><p>当且仅当，$\\omega_1 = 1$, $\\omega_k = 0, k &gt; 1$时取得等号。</p>\n<p>综上所述，矩阵2范数的值等于其最大的奇异值。</p>\n<p>矩阵的另一种范数定义方法Frobenius norm定义如下：</p>\n<script type=\"math/tex; mode=display\">\\left\\lVert A \\right\\rVert_{F} = \\sqrt{\\sum_{i=1}^{m}\\sum_{j=1}^{n}\\left\\vert a_{i,j}\\right\\rvert}</script><p>如果我们两边平方，可以得到，矩阵的F范数实际等于某个矩阵的迹，见下式：</p>\n<script type=\"math/tex; mode=display\">\\left\\lVert A\\right \\rVert_F^2 = \\text{trace}(A^\\dagger A)</script><p>利用矩阵的SVD分解，可以很容易得出，$\\text{trace}(A^\\dagger A) = \\sum_{i=1}^{r}\\sigma_i^2$</p>\n<p>说明如下：</p>\n<script type=\"math/tex; mode=display\">\\text{trace}(A^\\dagger A) = \\text{trace}(V\\Sigma^\\dagger\\Sigma V^\\dagger)</script><p>由于$V^\\dagger = V^{-1}$，而且$\\text{trace}(BAB^{-1}) = \\text{trace}(A)$，所以，</p>\n<script type=\"math/tex; mode=display\">\\text{trace}(A^\\dagger A) = \\text{trace}(\\Sigma^\\dagger \\Sigma) = \\sum_{i=1}^{r}\\sigma_i^2</script><p>也就是说，矩阵的F范数等于它的奇异值平方和的平方根。</p>\n<script type=\"math/tex; mode=display\">\\left\\lVert A\\right\\rVert_F= \\sqrt{\\sum_{i=1}^{r}\\sigma_i^2}</script>"},{"title":"CS131-边缘检测","date":"2017-01-24T02:42:47.000Z","_content":"边缘(Edge)在哺乳动物的视觉中有重要意义。在由若干卷积层构成的深度神经网络中，较低层的卷积层就被训练成为对特定形状的边缘做出响应。边缘检测也是计算机视觉和图像处理领域中一个重要的问题。\n\n![边缘检测图示](/img/edge_camera_man.png)\n<!-- more -->\n## 边缘的产生\n若仍采取将图像视作某一函数的观点，边缘是指这个函数中的不连续点。边缘检测是后续进行目标检测和形状识别的基础，也能够在立体视觉中恢复视角等。边缘的来源主要有以下几点：\n- 物体表面不平造成灰度值的不连续；\n- 深度值不同造成灰度值不连续；\n- 物体表面颜色的突变造成灰度值不连续\n\n## 朴素思想\n利用边缘是图像中不连续点的这一性质，可以通过计算图像的一阶导数，再找到一阶导数的极大值，即认为是边缘点。如下图所示，在白黑交界处，图像一阶导数的值非常大，表明此处灰度值变化剧烈，是边缘。\n![边缘点处导数很大](/img/edge_deriative.png)\n\n问题转换为如何求取图像的一阶导数（或梯度）。由于图像是离散的二元函数，所以下文不再区分求导与差分。\n\n在$x$方向上，令$g_x = \\frac{\\partial f}{\\partial x}$；在$y$方向上，令$g_y = \\frac{\\partial f}{\\partial y}$。梯度的大小和方向为\n$$g = \\lbrack g_x, g_y\\rbrack, \\theta = \\arctan(g_y/g_x)$$\n\n通过和Sobel算子等做卷积，可以求取两个正交方向上图像的一阶导数，并计算梯度，之后检测梯度的局部极大值就能够找出边缘点了。\n\n只是这种方法很容易受到噪声影响。如图所示，真实的边缘点被湮没在了噪声中。\n![噪声影响湮没了边缘点](/img/fun_noise.png)\n\n## 改进1：先平滑\n改进措施1，可以首先对图像进行高斯平滑，再按照上面的方法求取边缘点。根据卷积的性质，有：\n\n$$\\frac{d}{dx}(f\\ast g) = f\\ast\\frac{d}{dx}g$$\n\n所以我们可以先求取高斯核的一阶导数，再和原始图像直接做一次卷积就可以一举两得了。这样，引出了DoG(Deriative of Gaussian)。$x$方向的DoG如图所示。\n![x方向的DoG](/img/dog_x.png)\n\n进行高斯平滑不可避免会使图像中原本的细节部分模糊，所以需要在克服噪声和引入模糊之间做好折中。\n![不同](/img/dog_different_size.png)\n\n## 改进2：Canny检测子\n改进措施2，使用Canny检测子进行检测。Canny检测方法同样基于梯度，其基本原理如下：\n- 使用DoG计算梯度幅值和方向。\n- 非极大值抑制，这个过程需要根据梯度方向做线性插值。如图，沿着点$q$的梯度方向找到了$p$和$r$两个点。这两个点的梯度幅值需要根据其临近的两点做插值得到。\n- 利用梯度方向和边缘线互相垂直这一性质，如图，若已经确定点$p$为边缘点，则向它的梯度方向正交方向上寻找下一个边缘点（点$r$或$s$）。这一步也叫edge linking。\n\n![nms示意图](/img/canny_nms.png)\n![linking示意图](/img/canny_linking.png)\n\n同时，为了提高算法性能，Canny中采用了迟滞阈值的方法，设定`low`和`high`两个阈值，来判定某个点是否属于**强**或**弱**边缘点。在做edge linking的时候，从强边缘点开始，如果遇到了弱边缘点，则继续，直到某点的梯度幅值甚至比`low`还要小，则在此停止。\n\n## 改进3：RANSAC方法\n有的时候，我们并不是想要找到所有的边缘点，可能只是想找到图像中水平方向的某些边缘。这时候可以考虑采用RANSAC方法。\n\nRANSAC方法的思想在于，认为已有的feature大部分都是**好的**。这样，每次随机抽取出若干feature，建立model，再在整个feature集合上进行验证。那么由那些好的feature得到的model一定是得分较高的。（世界上还是好人多啊！）这样就剔除了离群点的影响。\n\n以直线拟合为例，在下图中，给出了使用RANSAC方法拟合直线的步骤。如图1所示，由于离群点的存在，如果直接使用最小二乘法进行拟合，拟合结果效果会很不理想。由于确定一条直线需要两个点，所以从点集中选取两个点，并计算拟合直线。并计算点集中的点在这条直线附近的个数，作为对模型好坏的判定，这些点是新的内点。找出最优的那条直线，使用其所有内点再进行拟合，重复上述操作，直至迭代终止。\n![ransac step](/img/ransac_step.png)\n\n上述RANSAC方法进行直线拟合的过程可以总结如下：\n![ransac line fit alg](/img/ransac_line_fit.png)\n\n按照上述思想，我分别使用最小二乘法和RANSAC方法尝试进行直线拟合。在下面的代码中，我首先产生了正常受到一定高斯噪声污染的数据（图中的红色点），这些点的真值都落在直线$y = 2x+1$上。而后，我随机变化了斜率和截距，以期产生一些离群点（图中的蓝色点）。当然，由于随机性，这种方法生成的点有可能仍然是内点。\n\n而后，我分别使用上述两种方法进行拟合。可以从结果图中看出，RANSAC（绿色线）能够有效避免离群点的干扰，获得更好的拟合效果。在某次实验中，两种方法的拟合结果如下：\n``` bash\nleast square: a = 3.319566, b = -1.446528\nransac method: a = 1.899640, b= 1.298608\n```\n![demo result](/img/line_fit_demo.png)\n\n实验使用的MATLAB代码如下：\n``` matlab\n%% generate data\nx = 0:1:10;\ny_gt = 2*x+1;\ny = y_gt + randn(size(y_gt));\nscatter(x, y, [], [1,0,0]);\nhold on\nout_x = 0:1:10;\nout_y = 5*rand(size(out_x)).*out_x + 4*rand(size(out_x));\nscatter(out_x, out_y, [], [0,0,1]);\nX = [x, out_x]';\nY = [y, out_y]';\nX = [X, ones(length(X), 1)];\n[a, b] = ls_fit(X, Y);\nplot(x, a*x+b, 'linestyle', '--', 'color', 'r');\n\n[ra, rb] = ransac_fit(X, Y, 100, 2, 0.5, 3);\nplot(x, ra*x+rb, 'linestyle', '-.', 'color', 'g');\nfprintf('least square: a = %f, b = %f\\n',a, b);\nfprintf('ransac method: a = %f, b= %f\\n', ra, rb)\nfunction [a, b] = ransac_fit(X, Y, k, n, t ,d)\n% ransac fit\n% k -- maximum iteration number\n% n -- smallest point numer required\n% t -- threshold to identify a point is fit well\n% d -- the number of nearby points to assert a model is fine\ndata = [X, Y];\nN = size(data, 1);\nbest_good_cnt = -1;\nbest_a = 0;\nbest_b = 0;\nfor i = 1:k\n    % sample point\n    idx = randsample(N, n);\n    data_sampled = data(idx, :);\n    % fit with least square\n    [a, b] = ls_fit(data_sampled(:, 1:2), data_sampled(:, 3));\n    % test model\n    not_sampled = ones(N, 1);\n    not_sampled(idx) = 0;\n    not_sampled_data = data(not_sampled == 1, :);\n    distance = abs(not_sampled_data(:, 1:2) * [a; b] - not_sampled_data(:, 3)) / sqrt(a^2+1);\n    inner_flag = distance < t;\n    good_cnt = sum(inner_flag);\n    if good_cnt >= d && good_cnt > best_good_cnt\n        best_good_cnt = good_cnt;\n        data_refine = data(find(inner_flag), :);\n        [a, b] = ls_fit(data_refine(:, 1:2), data_refine(:, 3));\n        best_a = a;\n        best_b = b;\n    end\n    fprintf('iteration %d, best_a = %f, best_b = %f\\n', i, best_a, best_b);\nend\na = best_a;\nb = best_b;\nend\n\nfunction [a, b] = ls_fit(X, Y)\n% least square fit\nA = X'*X\\X'*Y;\na = A(1);\nb = A(2);\nend\n```\n\n我们对RANSAC稍作分析，可以大概了解试验次数$k$的确定方法。\n\n仍然使用上述直线拟合的例子。如果所有点中内点所占的比例为$\\omega$，每次挑选$n$个点尝试（上述demo代码中取$n=2$）。那么每次挑选的两个点全部是内点的概率为$\\omega^n$。当选取的$n$个点全部为内点时，视为有效实验。那么，重复$k$次实验，有效实验次数为0的概率为$(1-\\omega^n)^k$。由于底数小于1，所以我们只需尽量增大$k$，就能够降低这种倒霉的概率。下图是不同$n$和$\\omega$情况下为了使得实验成功的概率大于0.99所需的$k$的分布。\n![k](/img/ransac_k.png)\n\nRANSAC方法的有点在于能够较为鲁棒地估计模型的参数，而且实现简单。缺点在于当离群点比例较大时，为保证实验成功所需的$k$值较大。这时候，可能Hough变换等基于投票的方法更适合用于图像中的直线检测问题。\n","source":"_posts/cs131-edge-detection.md","raw":"---\ntitle: CS131-边缘检测\ndate: 2017-01-24 10:42:47\ntags:\n     - cs131\n     - 公开课\n---\n边缘(Edge)在哺乳动物的视觉中有重要意义。在由若干卷积层构成的深度神经网络中，较低层的卷积层就被训练成为对特定形状的边缘做出响应。边缘检测也是计算机视觉和图像处理领域中一个重要的问题。\n\n![边缘检测图示](/img/edge_camera_man.png)\n<!-- more -->\n## 边缘的产生\n若仍采取将图像视作某一函数的观点，边缘是指这个函数中的不连续点。边缘检测是后续进行目标检测和形状识别的基础，也能够在立体视觉中恢复视角等。边缘的来源主要有以下几点：\n- 物体表面不平造成灰度值的不连续；\n- 深度值不同造成灰度值不连续；\n- 物体表面颜色的突变造成灰度值不连续\n\n## 朴素思想\n利用边缘是图像中不连续点的这一性质，可以通过计算图像的一阶导数，再找到一阶导数的极大值，即认为是边缘点。如下图所示，在白黑交界处，图像一阶导数的值非常大，表明此处灰度值变化剧烈，是边缘。\n![边缘点处导数很大](/img/edge_deriative.png)\n\n问题转换为如何求取图像的一阶导数（或梯度）。由于图像是离散的二元函数，所以下文不再区分求导与差分。\n\n在$x$方向上，令$g_x = \\frac{\\partial f}{\\partial x}$；在$y$方向上，令$g_y = \\frac{\\partial f}{\\partial y}$。梯度的大小和方向为\n$$g = \\lbrack g_x, g_y\\rbrack, \\theta = \\arctan(g_y/g_x)$$\n\n通过和Sobel算子等做卷积，可以求取两个正交方向上图像的一阶导数，并计算梯度，之后检测梯度的局部极大值就能够找出边缘点了。\n\n只是这种方法很容易受到噪声影响。如图所示，真实的边缘点被湮没在了噪声中。\n![噪声影响湮没了边缘点](/img/fun_noise.png)\n\n## 改进1：先平滑\n改进措施1，可以首先对图像进行高斯平滑，再按照上面的方法求取边缘点。根据卷积的性质，有：\n\n$$\\frac{d}{dx}(f\\ast g) = f\\ast\\frac{d}{dx}g$$\n\n所以我们可以先求取高斯核的一阶导数，再和原始图像直接做一次卷积就可以一举两得了。这样，引出了DoG(Deriative of Gaussian)。$x$方向的DoG如图所示。\n![x方向的DoG](/img/dog_x.png)\n\n进行高斯平滑不可避免会使图像中原本的细节部分模糊，所以需要在克服噪声和引入模糊之间做好折中。\n![不同](/img/dog_different_size.png)\n\n## 改进2：Canny检测子\n改进措施2，使用Canny检测子进行检测。Canny检测方法同样基于梯度，其基本原理如下：\n- 使用DoG计算梯度幅值和方向。\n- 非极大值抑制，这个过程需要根据梯度方向做线性插值。如图，沿着点$q$的梯度方向找到了$p$和$r$两个点。这两个点的梯度幅值需要根据其临近的两点做插值得到。\n- 利用梯度方向和边缘线互相垂直这一性质，如图，若已经确定点$p$为边缘点，则向它的梯度方向正交方向上寻找下一个边缘点（点$r$或$s$）。这一步也叫edge linking。\n\n![nms示意图](/img/canny_nms.png)\n![linking示意图](/img/canny_linking.png)\n\n同时，为了提高算法性能，Canny中采用了迟滞阈值的方法，设定`low`和`high`两个阈值，来判定某个点是否属于**强**或**弱**边缘点。在做edge linking的时候，从强边缘点开始，如果遇到了弱边缘点，则继续，直到某点的梯度幅值甚至比`low`还要小，则在此停止。\n\n## 改进3：RANSAC方法\n有的时候，我们并不是想要找到所有的边缘点，可能只是想找到图像中水平方向的某些边缘。这时候可以考虑采用RANSAC方法。\n\nRANSAC方法的思想在于，认为已有的feature大部分都是**好的**。这样，每次随机抽取出若干feature，建立model，再在整个feature集合上进行验证。那么由那些好的feature得到的model一定是得分较高的。（世界上还是好人多啊！）这样就剔除了离群点的影响。\n\n以直线拟合为例，在下图中，给出了使用RANSAC方法拟合直线的步骤。如图1所示，由于离群点的存在，如果直接使用最小二乘法进行拟合，拟合结果效果会很不理想。由于确定一条直线需要两个点，所以从点集中选取两个点，并计算拟合直线。并计算点集中的点在这条直线附近的个数，作为对模型好坏的判定，这些点是新的内点。找出最优的那条直线，使用其所有内点再进行拟合，重复上述操作，直至迭代终止。\n![ransac step](/img/ransac_step.png)\n\n上述RANSAC方法进行直线拟合的过程可以总结如下：\n![ransac line fit alg](/img/ransac_line_fit.png)\n\n按照上述思想，我分别使用最小二乘法和RANSAC方法尝试进行直线拟合。在下面的代码中，我首先产生了正常受到一定高斯噪声污染的数据（图中的红色点），这些点的真值都落在直线$y = 2x+1$上。而后，我随机变化了斜率和截距，以期产生一些离群点（图中的蓝色点）。当然，由于随机性，这种方法生成的点有可能仍然是内点。\n\n而后，我分别使用上述两种方法进行拟合。可以从结果图中看出，RANSAC（绿色线）能够有效避免离群点的干扰，获得更好的拟合效果。在某次实验中，两种方法的拟合结果如下：\n``` bash\nleast square: a = 3.319566, b = -1.446528\nransac method: a = 1.899640, b= 1.298608\n```\n![demo result](/img/line_fit_demo.png)\n\n实验使用的MATLAB代码如下：\n``` matlab\n%% generate data\nx = 0:1:10;\ny_gt = 2*x+1;\ny = y_gt + randn(size(y_gt));\nscatter(x, y, [], [1,0,0]);\nhold on\nout_x = 0:1:10;\nout_y = 5*rand(size(out_x)).*out_x + 4*rand(size(out_x));\nscatter(out_x, out_y, [], [0,0,1]);\nX = [x, out_x]';\nY = [y, out_y]';\nX = [X, ones(length(X), 1)];\n[a, b] = ls_fit(X, Y);\nplot(x, a*x+b, 'linestyle', '--', 'color', 'r');\n\n[ra, rb] = ransac_fit(X, Y, 100, 2, 0.5, 3);\nplot(x, ra*x+rb, 'linestyle', '-.', 'color', 'g');\nfprintf('least square: a = %f, b = %f\\n',a, b);\nfprintf('ransac method: a = %f, b= %f\\n', ra, rb)\nfunction [a, b] = ransac_fit(X, Y, k, n, t ,d)\n% ransac fit\n% k -- maximum iteration number\n% n -- smallest point numer required\n% t -- threshold to identify a point is fit well\n% d -- the number of nearby points to assert a model is fine\ndata = [X, Y];\nN = size(data, 1);\nbest_good_cnt = -1;\nbest_a = 0;\nbest_b = 0;\nfor i = 1:k\n    % sample point\n    idx = randsample(N, n);\n    data_sampled = data(idx, :);\n    % fit with least square\n    [a, b] = ls_fit(data_sampled(:, 1:2), data_sampled(:, 3));\n    % test model\n    not_sampled = ones(N, 1);\n    not_sampled(idx) = 0;\n    not_sampled_data = data(not_sampled == 1, :);\n    distance = abs(not_sampled_data(:, 1:2) * [a; b] - not_sampled_data(:, 3)) / sqrt(a^2+1);\n    inner_flag = distance < t;\n    good_cnt = sum(inner_flag);\n    if good_cnt >= d && good_cnt > best_good_cnt\n        best_good_cnt = good_cnt;\n        data_refine = data(find(inner_flag), :);\n        [a, b] = ls_fit(data_refine(:, 1:2), data_refine(:, 3));\n        best_a = a;\n        best_b = b;\n    end\n    fprintf('iteration %d, best_a = %f, best_b = %f\\n', i, best_a, best_b);\nend\na = best_a;\nb = best_b;\nend\n\nfunction [a, b] = ls_fit(X, Y)\n% least square fit\nA = X'*X\\X'*Y;\na = A(1);\nb = A(2);\nend\n```\n\n我们对RANSAC稍作分析，可以大概了解试验次数$k$的确定方法。\n\n仍然使用上述直线拟合的例子。如果所有点中内点所占的比例为$\\omega$，每次挑选$n$个点尝试（上述demo代码中取$n=2$）。那么每次挑选的两个点全部是内点的概率为$\\omega^n$。当选取的$n$个点全部为内点时，视为有效实验。那么，重复$k$次实验，有效实验次数为0的概率为$(1-\\omega^n)^k$。由于底数小于1，所以我们只需尽量增大$k$，就能够降低这种倒霉的概率。下图是不同$n$和$\\omega$情况下为了使得实验成功的概率大于0.99所需的$k$的分布。\n![k](/img/ransac_k.png)\n\nRANSAC方法的有点在于能够较为鲁棒地估计模型的参数，而且实现简单。缺点在于当离群点比例较大时，为保证实验成功所需的$k$值较大。这时候，可能Hough变换等基于投票的方法更适合用于图像中的直线检测问题。\n","slug":"cs131-edge-detection","published":1,"updated":"2017-02-26T12:09:28.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cizve079v0006l61hbf0puned","content":"<p>边缘(Edge)在哺乳动物的视觉中有重要意义。在由若干卷积层构成的深度神经网络中，较低层的卷积层就被训练成为对特定形状的边缘做出响应。边缘检测也是计算机视觉和图像处理领域中一个重要的问题。</p>\n<p><img src=\"/img/edge_camera_man.png\" alt=\"边缘检测图示\"><br><a id=\"more\"></a></p>\n<h2 id=\"边缘的产生\"><a href=\"#边缘的产生\" class=\"headerlink\" title=\"边缘的产生\"></a>边缘的产生</h2><p>若仍采取将图像视作某一函数的观点，边缘是指这个函数中的不连续点。边缘检测是后续进行目标检测和形状识别的基础，也能够在立体视觉中恢复视角等。边缘的来源主要有以下几点：</p>\n<ul>\n<li>物体表面不平造成灰度值的不连续；</li>\n<li>深度值不同造成灰度值不连续；</li>\n<li>物体表面颜色的突变造成灰度值不连续</li>\n</ul>\n<h2 id=\"朴素思想\"><a href=\"#朴素思想\" class=\"headerlink\" title=\"朴素思想\"></a>朴素思想</h2><p>利用边缘是图像中不连续点的这一性质，可以通过计算图像的一阶导数，再找到一阶导数的极大值，即认为是边缘点。如下图所示，在白黑交界处，图像一阶导数的值非常大，表明此处灰度值变化剧烈，是边缘。<br><img src=\"/img/edge_deriative.png\" alt=\"边缘点处导数很大\"></p>\n<p>问题转换为如何求取图像的一阶导数（或梯度）。由于图像是离散的二元函数，所以下文不再区分求导与差分。</p>\n<p>在$x$方向上，令$g_x = \\frac{\\partial f}{\\partial x}$；在$y$方向上，令$g_y = \\frac{\\partial f}{\\partial y}$。梯度的大小和方向为</p>\n<script type=\"math/tex; mode=display\">g = \\lbrack g_x, g_y\\rbrack, \\theta = \\arctan(g_y/g_x)</script><p>通过和Sobel算子等做卷积，可以求取两个正交方向上图像的一阶导数，并计算梯度，之后检测梯度的局部极大值就能够找出边缘点了。</p>\n<p>只是这种方法很容易受到噪声影响。如图所示，真实的边缘点被湮没在了噪声中。<br><img src=\"/img/fun_noise.png\" alt=\"噪声影响湮没了边缘点\"></p>\n<h2 id=\"改进1：先平滑\"><a href=\"#改进1：先平滑\" class=\"headerlink\" title=\"改进1：先平滑\"></a>改进1：先平滑</h2><p>改进措施1，可以首先对图像进行高斯平滑，再按照上面的方法求取边缘点。根据卷积的性质，有：</p>\n<script type=\"math/tex; mode=display\">\\frac{d}{dx}(f\\ast g) = f\\ast\\frac{d}{dx}g</script><p>所以我们可以先求取高斯核的一阶导数，再和原始图像直接做一次卷积就可以一举两得了。这样，引出了DoG(Deriative of Gaussian)。$x$方向的DoG如图所示。<br><img src=\"/img/dog_x.png\" alt=\"x方向的DoG\"></p>\n<p>进行高斯平滑不可避免会使图像中原本的细节部分模糊，所以需要在克服噪声和引入模糊之间做好折中。<br><img src=\"/img/dog_different_size.png\" alt=\"不同\"></p>\n<h2 id=\"改进2：Canny检测子\"><a href=\"#改进2：Canny检测子\" class=\"headerlink\" title=\"改进2：Canny检测子\"></a>改进2：Canny检测子</h2><p>改进措施2，使用Canny检测子进行检测。Canny检测方法同样基于梯度，其基本原理如下：</p>\n<ul>\n<li>使用DoG计算梯度幅值和方向。</li>\n<li>非极大值抑制，这个过程需要根据梯度方向做线性插值。如图，沿着点$q$的梯度方向找到了$p$和$r$两个点。这两个点的梯度幅值需要根据其临近的两点做插值得到。</li>\n<li>利用梯度方向和边缘线互相垂直这一性质，如图，若已经确定点$p$为边缘点，则向它的梯度方向正交方向上寻找下一个边缘点（点$r$或$s$）。这一步也叫edge linking。</li>\n</ul>\n<p><img src=\"/img/canny_nms.png\" alt=\"nms示意图\"><br><img src=\"/img/canny_linking.png\" alt=\"linking示意图\"></p>\n<p>同时，为了提高算法性能，Canny中采用了迟滞阈值的方法，设定<code>low</code>和<code>high</code>两个阈值，来判定某个点是否属于<strong>强</strong>或<strong>弱</strong>边缘点。在做edge linking的时候，从强边缘点开始，如果遇到了弱边缘点，则继续，直到某点的梯度幅值甚至比<code>low</code>还要小，则在此停止。</p>\n<h2 id=\"改进3：RANSAC方法\"><a href=\"#改进3：RANSAC方法\" class=\"headerlink\" title=\"改进3：RANSAC方法\"></a>改进3：RANSAC方法</h2><p>有的时候，我们并不是想要找到所有的边缘点，可能只是想找到图像中水平方向的某些边缘。这时候可以考虑采用RANSAC方法。</p>\n<p>RANSAC方法的思想在于，认为已有的feature大部分都是<strong>好的</strong>。这样，每次随机抽取出若干feature，建立model，再在整个feature集合上进行验证。那么由那些好的feature得到的model一定是得分较高的。（世界上还是好人多啊！）这样就剔除了离群点的影响。</p>\n<p>以直线拟合为例，在下图中，给出了使用RANSAC方法拟合直线的步骤。如图1所示，由于离群点的存在，如果直接使用最小二乘法进行拟合，拟合结果效果会很不理想。由于确定一条直线需要两个点，所以从点集中选取两个点，并计算拟合直线。并计算点集中的点在这条直线附近的个数，作为对模型好坏的判定，这些点是新的内点。找出最优的那条直线，使用其所有内点再进行拟合，重复上述操作，直至迭代终止。<br><img src=\"/img/ransac_step.png\" alt=\"ransac step\"></p>\n<p>上述RANSAC方法进行直线拟合的过程可以总结如下：<br><img src=\"/img/ransac_line_fit.png\" alt=\"ransac line fit alg\"></p>\n<p>按照上述思想，我分别使用最小二乘法和RANSAC方法尝试进行直线拟合。在下面的代码中，我首先产生了正常受到一定高斯噪声污染的数据（图中的红色点），这些点的真值都落在直线$y = 2x+1$上。而后，我随机变化了斜率和截距，以期产生一些离群点（图中的蓝色点）。当然，由于随机性，这种方法生成的点有可能仍然是内点。</p>\n<p>而后，我分别使用上述两种方法进行拟合。可以从结果图中看出，RANSAC（绿色线）能够有效避免离群点的干扰，获得更好的拟合效果。在某次实验中，两种方法的拟合结果如下：<br><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">least square: a = 3.319566, b = -1.446528</div><div class=\"line\">ransac method: a = 1.899640, b= 1.298608</div></pre></td></tr></table></figure></p>\n<p><img src=\"/img/line_fit_demo.png\" alt=\"demo result\"></p>\n<p>实验使用的MATLAB代码如下：<br><figure class=\"highlight matlab\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div><div class=\"line\">47</div><div class=\"line\">48</div><div class=\"line\">49</div><div class=\"line\">50</div><div class=\"line\">51</div><div class=\"line\">52</div><div class=\"line\">53</div><div class=\"line\">54</div><div class=\"line\">55</div><div class=\"line\">56</div><div class=\"line\">57</div><div class=\"line\">58</div><div class=\"line\">59</div><div class=\"line\">60</div><div class=\"line\">61</div><div class=\"line\">62</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\">%% generate data</span></div><div class=\"line\">x = <span class=\"number\">0</span>:<span class=\"number\">1</span>:<span class=\"number\">10</span>;</div><div class=\"line\">y_gt = <span class=\"number\">2</span>*x+<span class=\"number\">1</span>;</div><div class=\"line\">y = y_gt + <span class=\"built_in\">randn</span>(<span class=\"built_in\">size</span>(y_gt));</div><div class=\"line\">scatter(x, y, [], [<span class=\"number\">1</span>,<span class=\"number\">0</span>,<span class=\"number\">0</span>]);</div><div class=\"line\">hold on</div><div class=\"line\">out_x = <span class=\"number\">0</span>:<span class=\"number\">1</span>:<span class=\"number\">10</span>;</div><div class=\"line\">out_y = <span class=\"number\">5</span>*<span class=\"built_in\">rand</span>(<span class=\"built_in\">size</span>(out_x)).*out_x + <span class=\"number\">4</span>*<span class=\"built_in\">rand</span>(<span class=\"built_in\">size</span>(out_x));</div><div class=\"line\">scatter(out_x, out_y, [], [<span class=\"number\">0</span>,<span class=\"number\">0</span>,<span class=\"number\">1</span>]);</div><div class=\"line\">X = [x, out_x]';</div><div class=\"line\">Y = [y, out_y]';</div><div class=\"line\">X = [X, ones(length(X), <span class=\"number\">1</span>)];</div><div class=\"line\">[a, b] = ls_fit(X, Y);</div><div class=\"line\">plot(x, a*x+b, <span class=\"string\">'linestyle'</span>, <span class=\"string\">'--'</span>, <span class=\"string\">'color'</span>, <span class=\"string\">'r'</span>);</div><div class=\"line\"></div><div class=\"line\">[ra, rb] = ransac_fit(X, Y, <span class=\"number\">100</span>, <span class=\"number\">2</span>, <span class=\"number\">0.5</span>, <span class=\"number\">3</span>);</div><div class=\"line\">plot(x, ra*x+rb, <span class=\"string\">'linestyle'</span>, <span class=\"string\">'-.'</span>, <span class=\"string\">'color'</span>, <span class=\"string\">'g'</span>);</div><div class=\"line\">fprintf(<span class=\"string\">'least square: a = %f, b = %f\\n'</span>,a, b);</div><div class=\"line\">fprintf(<span class=\"string\">'ransac method: a = %f, b= %f\\n'</span>, ra, rb)</div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">function</span> <span class=\"params\">[a, b]</span> = <span class=\"title\">ransac_fit</span><span class=\"params\">(X, Y, k, n, t ,d)</span></span></div><div class=\"line\"><span class=\"comment\">% ransac fit</span></div><div class=\"line\"><span class=\"comment\">% k -- maximum iteration number</span></div><div class=\"line\"><span class=\"comment\">% n -- smallest point numer required</span></div><div class=\"line\"><span class=\"comment\">% t -- threshold to identify a point is fit well</span></div><div class=\"line\"><span class=\"comment\">% d -- the number of nearby points to assert a model is fine</span></div><div class=\"line\">data = [X, Y];</div><div class=\"line\">N = <span class=\"built_in\">size</span>(data, <span class=\"number\">1</span>);</div><div class=\"line\">best_good_cnt = <span class=\"number\">-1</span>;</div><div class=\"line\">best_a = <span class=\"number\">0</span>;</div><div class=\"line\">best_b = <span class=\"number\">0</span>;</div><div class=\"line\"><span class=\"keyword\">for</span> <span class=\"built_in\">i</span> = <span class=\"number\">1</span>:k</div><div class=\"line\">    <span class=\"comment\">% sample point</span></div><div class=\"line\">    idx = randsample(N, n);</div><div class=\"line\">    data_sampled = data(idx, :);</div><div class=\"line\">    <span class=\"comment\">% fit with least square</span></div><div class=\"line\">    [a, b] = ls_fit(data_sampled(:, <span class=\"number\">1</span>:<span class=\"number\">2</span>), data_sampled(:, <span class=\"number\">3</span>));</div><div class=\"line\">    <span class=\"comment\">% test model</span></div><div class=\"line\">    not_sampled = <span class=\"built_in\">ones</span>(N, <span class=\"number\">1</span>);</div><div class=\"line\">    not_sampled(idx) = <span class=\"number\">0</span>;</div><div class=\"line\">    not_sampled_data = data(not_sampled == <span class=\"number\">1</span>, :);</div><div class=\"line\">    distance = <span class=\"built_in\">abs</span>(not_sampled_data(:, <span class=\"number\">1</span>:<span class=\"number\">2</span>) * [a; b] - not_sampled_data(:, <span class=\"number\">3</span>)) / <span class=\"built_in\">sqrt</span>(a^<span class=\"number\">2</span>+<span class=\"number\">1</span>);</div><div class=\"line\">    inner_flag = distance &lt; t;</div><div class=\"line\">    good_cnt = sum(inner_flag);</div><div class=\"line\">    <span class=\"keyword\">if</span> good_cnt &gt;= d &amp;&amp; good_cnt &gt; best_good_cnt</div><div class=\"line\">        best_good_cnt = good_cnt;</div><div class=\"line\">        data_refine = data(<span class=\"built_in\">find</span>(inner_flag), :);</div><div class=\"line\">        [a, b] = ls_fit(data_refine(:, <span class=\"number\">1</span>:<span class=\"number\">2</span>), data_refine(:, <span class=\"number\">3</span>));</div><div class=\"line\">        best_a = a;</div><div class=\"line\">        best_b = b;</div><div class=\"line\">    <span class=\"keyword\">end</span></div><div class=\"line\">    fprintf(<span class=\"string\">'iteration %d, best_a = %f, best_b = %f\\n'</span>, <span class=\"built_in\">i</span>, best_a, best_b);</div><div class=\"line\"><span class=\"keyword\">end</span></div><div class=\"line\">a = best_a;</div><div class=\"line\">b = best_b;</div><div class=\"line\"><span class=\"keyword\">end</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">function</span> <span class=\"params\">[a, b]</span> = <span class=\"title\">ls_fit</span><span class=\"params\">(X, Y)</span></span></div><div class=\"line\"><span class=\"comment\">% least square fit</span></div><div class=\"line\">A = X'*X\\X'*Y;</div><div class=\"line\">a = A(<span class=\"number\">1</span>);</div><div class=\"line\">b = A(<span class=\"number\">2</span>);</div><div class=\"line\"><span class=\"keyword\">end</span></div></pre></td></tr></table></figure></p>\n<p>我们对RANSAC稍作分析，可以大概了解试验次数$k$的确定方法。</p>\n<p>仍然使用上述直线拟合的例子。如果所有点中内点所占的比例为$\\omega$，每次挑选$n$个点尝试（上述demo代码中取$n=2$）。那么每次挑选的两个点全部是内点的概率为$\\omega^n$。当选取的$n$个点全部为内点时，视为有效实验。那么，重复$k$次实验，有效实验次数为0的概率为$(1-\\omega^n)^k$。由于底数小于1，所以我们只需尽量增大$k$，就能够降低这种倒霉的概率。下图是不同$n$和$\\omega$情况下为了使得实验成功的概率大于0.99所需的$k$的分布。<br><img src=\"/img/ransac_k.png\" alt=\"k\"></p>\n<p>RANSAC方法的有点在于能够较为鲁棒地估计模型的参数，而且实现简单。缺点在于当离群点比例较大时，为保证实验成功所需的$k$值较大。这时候，可能Hough变换等基于投票的方法更适合用于图像中的直线检测问题。</p>\n","excerpt":"<p>边缘(Edge)在哺乳动物的视觉中有重要意义。在由若干卷积层构成的深度神经网络中，较低层的卷积层就被训练成为对特定形状的边缘做出响应。边缘检测也是计算机视觉和图像处理领域中一个重要的问题。</p>\n<p><img src=\"/img/edge_camera_man.png\" alt=\"边缘检测图示\"><br>","more":"</p>\n<h2 id=\"边缘的产生\"><a href=\"#边缘的产生\" class=\"headerlink\" title=\"边缘的产生\"></a>边缘的产生</h2><p>若仍采取将图像视作某一函数的观点，边缘是指这个函数中的不连续点。边缘检测是后续进行目标检测和形状识别的基础，也能够在立体视觉中恢复视角等。边缘的来源主要有以下几点：</p>\n<ul>\n<li>物体表面不平造成灰度值的不连续；</li>\n<li>深度值不同造成灰度值不连续；</li>\n<li>物体表面颜色的突变造成灰度值不连续</li>\n</ul>\n<h2 id=\"朴素思想\"><a href=\"#朴素思想\" class=\"headerlink\" title=\"朴素思想\"></a>朴素思想</h2><p>利用边缘是图像中不连续点的这一性质，可以通过计算图像的一阶导数，再找到一阶导数的极大值，即认为是边缘点。如下图所示，在白黑交界处，图像一阶导数的值非常大，表明此处灰度值变化剧烈，是边缘。<br><img src=\"/img/edge_deriative.png\" alt=\"边缘点处导数很大\"></p>\n<p>问题转换为如何求取图像的一阶导数（或梯度）。由于图像是离散的二元函数，所以下文不再区分求导与差分。</p>\n<p>在$x$方向上，令$g_x = \\frac{\\partial f}{\\partial x}$；在$y$方向上，令$g_y = \\frac{\\partial f}{\\partial y}$。梯度的大小和方向为</p>\n<script type=\"math/tex; mode=display\">g = \\lbrack g_x, g_y\\rbrack, \\theta = \\arctan(g_y/g_x)</script><p>通过和Sobel算子等做卷积，可以求取两个正交方向上图像的一阶导数，并计算梯度，之后检测梯度的局部极大值就能够找出边缘点了。</p>\n<p>只是这种方法很容易受到噪声影响。如图所示，真实的边缘点被湮没在了噪声中。<br><img src=\"/img/fun_noise.png\" alt=\"噪声影响湮没了边缘点\"></p>\n<h2 id=\"改进1：先平滑\"><a href=\"#改进1：先平滑\" class=\"headerlink\" title=\"改进1：先平滑\"></a>改进1：先平滑</h2><p>改进措施1，可以首先对图像进行高斯平滑，再按照上面的方法求取边缘点。根据卷积的性质，有：</p>\n<script type=\"math/tex; mode=display\">\\frac{d}{dx}(f\\ast g) = f\\ast\\frac{d}{dx}g</script><p>所以我们可以先求取高斯核的一阶导数，再和原始图像直接做一次卷积就可以一举两得了。这样，引出了DoG(Deriative of Gaussian)。$x$方向的DoG如图所示。<br><img src=\"/img/dog_x.png\" alt=\"x方向的DoG\"></p>\n<p>进行高斯平滑不可避免会使图像中原本的细节部分模糊，所以需要在克服噪声和引入模糊之间做好折中。<br><img src=\"/img/dog_different_size.png\" alt=\"不同\"></p>\n<h2 id=\"改进2：Canny检测子\"><a href=\"#改进2：Canny检测子\" class=\"headerlink\" title=\"改进2：Canny检测子\"></a>改进2：Canny检测子</h2><p>改进措施2，使用Canny检测子进行检测。Canny检测方法同样基于梯度，其基本原理如下：</p>\n<ul>\n<li>使用DoG计算梯度幅值和方向。</li>\n<li>非极大值抑制，这个过程需要根据梯度方向做线性插值。如图，沿着点$q$的梯度方向找到了$p$和$r$两个点。这两个点的梯度幅值需要根据其临近的两点做插值得到。</li>\n<li>利用梯度方向和边缘线互相垂直这一性质，如图，若已经确定点$p$为边缘点，则向它的梯度方向正交方向上寻找下一个边缘点（点$r$或$s$）。这一步也叫edge linking。</li>\n</ul>\n<p><img src=\"/img/canny_nms.png\" alt=\"nms示意图\"><br><img src=\"/img/canny_linking.png\" alt=\"linking示意图\"></p>\n<p>同时，为了提高算法性能，Canny中采用了迟滞阈值的方法，设定<code>low</code>和<code>high</code>两个阈值，来判定某个点是否属于<strong>强</strong>或<strong>弱</strong>边缘点。在做edge linking的时候，从强边缘点开始，如果遇到了弱边缘点，则继续，直到某点的梯度幅值甚至比<code>low</code>还要小，则在此停止。</p>\n<h2 id=\"改进3：RANSAC方法\"><a href=\"#改进3：RANSAC方法\" class=\"headerlink\" title=\"改进3：RANSAC方法\"></a>改进3：RANSAC方法</h2><p>有的时候，我们并不是想要找到所有的边缘点，可能只是想找到图像中水平方向的某些边缘。这时候可以考虑采用RANSAC方法。</p>\n<p>RANSAC方法的思想在于，认为已有的feature大部分都是<strong>好的</strong>。这样，每次随机抽取出若干feature，建立model，再在整个feature集合上进行验证。那么由那些好的feature得到的model一定是得分较高的。（世界上还是好人多啊！）这样就剔除了离群点的影响。</p>\n<p>以直线拟合为例，在下图中，给出了使用RANSAC方法拟合直线的步骤。如图1所示，由于离群点的存在，如果直接使用最小二乘法进行拟合，拟合结果效果会很不理想。由于确定一条直线需要两个点，所以从点集中选取两个点，并计算拟合直线。并计算点集中的点在这条直线附近的个数，作为对模型好坏的判定，这些点是新的内点。找出最优的那条直线，使用其所有内点再进行拟合，重复上述操作，直至迭代终止。<br><img src=\"/img/ransac_step.png\" alt=\"ransac step\"></p>\n<p>上述RANSAC方法进行直线拟合的过程可以总结如下：<br><img src=\"/img/ransac_line_fit.png\" alt=\"ransac line fit alg\"></p>\n<p>按照上述思想，我分别使用最小二乘法和RANSAC方法尝试进行直线拟合。在下面的代码中，我首先产生了正常受到一定高斯噪声污染的数据（图中的红色点），这些点的真值都落在直线$y = 2x+1$上。而后，我随机变化了斜率和截距，以期产生一些离群点（图中的蓝色点）。当然，由于随机性，这种方法生成的点有可能仍然是内点。</p>\n<p>而后，我分别使用上述两种方法进行拟合。可以从结果图中看出，RANSAC（绿色线）能够有效避免离群点的干扰，获得更好的拟合效果。在某次实验中，两种方法的拟合结果如下：<br><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">least square: a = 3.319566, b = -1.446528</div><div class=\"line\">ransac method: a = 1.899640, b= 1.298608</div></pre></td></tr></table></figure></p>\n<p><img src=\"/img/line_fit_demo.png\" alt=\"demo result\"></p>\n<p>实验使用的MATLAB代码如下：<br><figure class=\"highlight matlab\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div><div class=\"line\">47</div><div class=\"line\">48</div><div class=\"line\">49</div><div class=\"line\">50</div><div class=\"line\">51</div><div class=\"line\">52</div><div class=\"line\">53</div><div class=\"line\">54</div><div class=\"line\">55</div><div class=\"line\">56</div><div class=\"line\">57</div><div class=\"line\">58</div><div class=\"line\">59</div><div class=\"line\">60</div><div class=\"line\">61</div><div class=\"line\">62</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\">%% generate data</span></div><div class=\"line\">x = <span class=\"number\">0</span>:<span class=\"number\">1</span>:<span class=\"number\">10</span>;</div><div class=\"line\">y_gt = <span class=\"number\">2</span>*x+<span class=\"number\">1</span>;</div><div class=\"line\">y = y_gt + <span class=\"built_in\">randn</span>(<span class=\"built_in\">size</span>(y_gt));</div><div class=\"line\">scatter(x, y, [], [<span class=\"number\">1</span>,<span class=\"number\">0</span>,<span class=\"number\">0</span>]);</div><div class=\"line\">hold on</div><div class=\"line\">out_x = <span class=\"number\">0</span>:<span class=\"number\">1</span>:<span class=\"number\">10</span>;</div><div class=\"line\">out_y = <span class=\"number\">5</span>*<span class=\"built_in\">rand</span>(<span class=\"built_in\">size</span>(out_x)).*out_x + <span class=\"number\">4</span>*<span class=\"built_in\">rand</span>(<span class=\"built_in\">size</span>(out_x));</div><div class=\"line\">scatter(out_x, out_y, [], [<span class=\"number\">0</span>,<span class=\"number\">0</span>,<span class=\"number\">1</span>]);</div><div class=\"line\">X = [x, out_x]';</div><div class=\"line\">Y = [y, out_y]';</div><div class=\"line\">X = [X, ones(length(X), <span class=\"number\">1</span>)];</div><div class=\"line\">[a, b] = ls_fit(X, Y);</div><div class=\"line\">plot(x, a*x+b, <span class=\"string\">'linestyle'</span>, <span class=\"string\">'--'</span>, <span class=\"string\">'color'</span>, <span class=\"string\">'r'</span>);</div><div class=\"line\"></div><div class=\"line\">[ra, rb] = ransac_fit(X, Y, <span class=\"number\">100</span>, <span class=\"number\">2</span>, <span class=\"number\">0.5</span>, <span class=\"number\">3</span>);</div><div class=\"line\">plot(x, ra*x+rb, <span class=\"string\">'linestyle'</span>, <span class=\"string\">'-.'</span>, <span class=\"string\">'color'</span>, <span class=\"string\">'g'</span>);</div><div class=\"line\">fprintf(<span class=\"string\">'least square: a = %f, b = %f\\n'</span>,a, b);</div><div class=\"line\">fprintf(<span class=\"string\">'ransac method: a = %f, b= %f\\n'</span>, ra, rb)</div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">function</span> <span class=\"params\">[a, b]</span> = <span class=\"title\">ransac_fit</span><span class=\"params\">(X, Y, k, n, t ,d)</span></span></div><div class=\"line\"><span class=\"comment\">% ransac fit</span></div><div class=\"line\"><span class=\"comment\">% k -- maximum iteration number</span></div><div class=\"line\"><span class=\"comment\">% n -- smallest point numer required</span></div><div class=\"line\"><span class=\"comment\">% t -- threshold to identify a point is fit well</span></div><div class=\"line\"><span class=\"comment\">% d -- the number of nearby points to assert a model is fine</span></div><div class=\"line\">data = [X, Y];</div><div class=\"line\">N = <span class=\"built_in\">size</span>(data, <span class=\"number\">1</span>);</div><div class=\"line\">best_good_cnt = <span class=\"number\">-1</span>;</div><div class=\"line\">best_a = <span class=\"number\">0</span>;</div><div class=\"line\">best_b = <span class=\"number\">0</span>;</div><div class=\"line\"><span class=\"keyword\">for</span> <span class=\"built_in\">i</span> = <span class=\"number\">1</span>:k</div><div class=\"line\">    <span class=\"comment\">% sample point</span></div><div class=\"line\">    idx = randsample(N, n);</div><div class=\"line\">    data_sampled = data(idx, :);</div><div class=\"line\">    <span class=\"comment\">% fit with least square</span></div><div class=\"line\">    [a, b] = ls_fit(data_sampled(:, <span class=\"number\">1</span>:<span class=\"number\">2</span>), data_sampled(:, <span class=\"number\">3</span>));</div><div class=\"line\">    <span class=\"comment\">% test model</span></div><div class=\"line\">    not_sampled = <span class=\"built_in\">ones</span>(N, <span class=\"number\">1</span>);</div><div class=\"line\">    not_sampled(idx) = <span class=\"number\">0</span>;</div><div class=\"line\">    not_sampled_data = data(not_sampled == <span class=\"number\">1</span>, :);</div><div class=\"line\">    distance = <span class=\"built_in\">abs</span>(not_sampled_data(:, <span class=\"number\">1</span>:<span class=\"number\">2</span>) * [a; b] - not_sampled_data(:, <span class=\"number\">3</span>)) / <span class=\"built_in\">sqrt</span>(a^<span class=\"number\">2</span>+<span class=\"number\">1</span>);</div><div class=\"line\">    inner_flag = distance &lt; t;</div><div class=\"line\">    good_cnt = sum(inner_flag);</div><div class=\"line\">    <span class=\"keyword\">if</span> good_cnt &gt;= d &amp;&amp; good_cnt &gt; best_good_cnt</div><div class=\"line\">        best_good_cnt = good_cnt;</div><div class=\"line\">        data_refine = data(<span class=\"built_in\">find</span>(inner_flag), :);</div><div class=\"line\">        [a, b] = ls_fit(data_refine(:, <span class=\"number\">1</span>:<span class=\"number\">2</span>), data_refine(:, <span class=\"number\">3</span>));</div><div class=\"line\">        best_a = a;</div><div class=\"line\">        best_b = b;</div><div class=\"line\">    <span class=\"keyword\">end</span></div><div class=\"line\">    fprintf(<span class=\"string\">'iteration %d, best_a = %f, best_b = %f\\n'</span>, <span class=\"built_in\">i</span>, best_a, best_b);</div><div class=\"line\"><span class=\"keyword\">end</span></div><div class=\"line\">a = best_a;</div><div class=\"line\">b = best_b;</div><div class=\"line\"><span class=\"keyword\">end</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">function</span> <span class=\"params\">[a, b]</span> = <span class=\"title\">ls_fit</span><span class=\"params\">(X, Y)</span></span></div><div class=\"line\"><span class=\"comment\">% least square fit</span></div><div class=\"line\">A = X'*X\\X'*Y;</div><div class=\"line\">a = A(<span class=\"number\">1</span>);</div><div class=\"line\">b = A(<span class=\"number\">2</span>);</div><div class=\"line\"><span class=\"keyword\">end</span></div></pre></td></tr></table></figure></p>\n<p>我们对RANSAC稍作分析，可以大概了解试验次数$k$的确定方法。</p>\n<p>仍然使用上述直线拟合的例子。如果所有点中内点所占的比例为$\\omega$，每次挑选$n$个点尝试（上述demo代码中取$n=2$）。那么每次挑选的两个点全部是内点的概率为$\\omega^n$。当选取的$n$个点全部为内点时，视为有效实验。那么，重复$k$次实验，有效实验次数为0的概率为$(1-\\omega^n)^k$。由于底数小于1，所以我们只需尽量增大$k$，就能够降低这种倒霉的概率。下图是不同$n$和$\\omega$情况下为了使得实验成功的概率大于0.99所需的$k$的分布。<br><img src=\"/img/ransac_k.png\" alt=\"k\"></p>\n<p>RANSAC方法的有点在于能够较为鲁棒地估计模型的参数，而且实现简单。缺点在于当离群点比例较大时，为保证实验成功所需的$k$值较大。这时候，可能Hough变换等基于投票的方法更适合用于图像中的直线检测问题。</p>"},{"title":"CS131-线代基础","date":"2017-01-22T07:38:01.000Z","_content":"\nCS131课程(Computer Vision: Foundations and Applications)，是斯坦福大学Li Feifei实验室开设的一门计算机视觉入门基础课程，[该课程](http://vision.stanford.edu/teaching/cs131_fall1617/index.html)目的在于为刚接触计算机视觉领域的学生提供基本原理和应用介绍。目前2016年冬季课程刚刚结束。CS131博客系列主要是关于本课的slide知识点总结与作业重点问题归纳，作为个人学习本门课程的心得体会和复习材料。\n\n由于是个人项目，所以会比较随意，只对个人感兴趣的内容做一总结。这篇文章是对课前[线代基础](http://vision.stanford.edu/teaching/cs131_fall1617/lectures/lecture2_linalg_review_cs131_2016.pdf)的复习与整理。\n![线性代数词云](/img/cs131_linear_algebra.jpg)\n\n<!-- more -->\n## 向量与矩阵\n数字图像可以看做二维矩阵，向量是特殊的矩阵，本课程默认的向量都是列向量。\nslide中给出了一些矩阵行列式和迹的性质，都比较简单，这里不再多说。\n## 矩阵作为线性变换\n通过线代知识，我们知道，在线性空间中，如果给定一组基，线性变换可以通过对应的矩阵来进行描述。\n\n### scale变换\n对角阵可以用来表示放缩变换。\n$$\n\\begin{bmatrix}\ns_x & 0\\\\\\\\\n0 & s_y\n\\end{bmatrix}\\begin{bmatrix}\nx\\\\\\\\\ny\n\\end{bmatrix} = \\begin{bmatrix}\ns_xx\\\\\\\\\ns_yy\n\\end{bmatrix}\n$$\n\n### 旋转变换\n如图所示，逆时针旋转$theta$角度，对应的旋转矩阵为：\n![旋转变换](/img/rotation.png)\n$$\n\\mathbf{R} = \\begin{bmatrix}\n\\cos\\theta &-\\sin\\theta \\\\\\\\\n\\sin\\theta &\\cos\\theta\n\\end{bmatrix}\n$$\n旋转矩阵是[酉矩阵](https://zh.wikipedia.org/wiki/酉矩阵)，矩阵内的各列（或者各行）相互正交。满足如下的关系式：\n$$\n\\mathbf{R}\\mathbf{R^{\\dagger}} = \\mathbf{I}\n$$\n由于$\\det{\\mathbf{R}} = \\det{\\mathbf{R^{\\dagger}}}$，所以，对于酉矩阵，$\\det{\\mathbf{R}} = \\pm 1$\n旋转矩阵是[酉矩阵](https://zh.wikipedia.org/wiki/酉矩阵)，矩阵内的各列（或者各行）相互正交。满足如下的关系式：\n$$\n\\mathbf{R}\\mathbf{R^{\\dagger}} = \\mathbf{I}\n$$\n由于$\\det{\\mathbf{R}} = \\det{\\mathbf{R^{\\dagger}}}$，所以，对于酉矩阵，$\\det{\\mathbf{R}} = \\pm 1$.\n\n### 齐次变换(Homogeneous Transform)\n只用上面的二维矩阵不能表达平移，使用齐次矩阵可以表达放缩，旋转和平移操作。\n$$\n\\mathbf{H} =\\begin{bmatrix}\na & b & t_x\\\\\\\\\nc & d & t_y\\\\\\\\\n0 & 0 & 1\n\\end{bmatrix},\\mathbf{H}\\begin{bmatrix}\nx\\\\\\\\\ny\\\\\\\\\n1\\\\\\\\\n\\end{bmatrix}=\\begin{bmatrix}\nax+by+t_x\\\\\\\\\ncx+dy+t_y\\\\\\\\\n1\n\\end{bmatrix}\n$$\n\n### SVD分解\n可以将矩阵分成若干个矩阵的乘积，叫做矩阵分解，比如QR分解，满秩分解等。SVD分解，即奇异值分解，也是一种特殊的矩阵分解方法。如下式所示，是将矩阵分解成为三个矩阵的乘积：\n$$\\mathbf{U}\\mathbf{\\Sigma}\\mathbf{V^\\dagger} = \\mathbf{A}$$\n其中矩阵$\\mathbf{A}$大小为$m\\times n$，矩阵$\\mathbf{U}$是大小为$m\\times m$的酉矩阵，$\\mathbf{V}$是大小为$n \\times n$的酉矩阵，$\\mathbf{\\Sigma}$是大小为$m \\times n$的旋转矩阵，即只有主对角元素不为0.\n\nSVD分解在主成分分析中年很有用。由于矩阵$\\mathbf{\\Sigma}$一般情况下是将奇异值按照从大到小的顺序摆放，所以矩阵$\\mathbf{U}$中，前面的若干列被视作主成分，后面的列显得相对不这么重要。可以抛弃后面的列，进行图像压缩。\n\n如下图，是使用前10个分量对原图片进行压缩的效果。\n\n``` matlab\nim = imread('./superman.png');\nim_gray = rbg2gray(im);\n[u, s, v] = svd(double(im_gray));\nk = 10;\nuk = u(:, 1:k);\nsigma = diag(s);\nsk = diag(sigma(1:k));\nvk = v(:, 1:k);\nim_k = uk*sk*vk';\nimshow(uint8(im_k))\n```\n\n![原始图像](/img/original_superman.png)\n![压缩图像](/img/svd_superman.png)\n","source":"_posts/cs131-linear-alg.md","raw":"---\ntitle: CS131-线代基础\ndate: 2017-01-22 15:38:01\ntags:\n    - cs131\n    - 公开课\n---\n\nCS131课程(Computer Vision: Foundations and Applications)，是斯坦福大学Li Feifei实验室开设的一门计算机视觉入门基础课程，[该课程](http://vision.stanford.edu/teaching/cs131_fall1617/index.html)目的在于为刚接触计算机视觉领域的学生提供基本原理和应用介绍。目前2016年冬季课程刚刚结束。CS131博客系列主要是关于本课的slide知识点总结与作业重点问题归纳，作为个人学习本门课程的心得体会和复习材料。\n\n由于是个人项目，所以会比较随意，只对个人感兴趣的内容做一总结。这篇文章是对课前[线代基础](http://vision.stanford.edu/teaching/cs131_fall1617/lectures/lecture2_linalg_review_cs131_2016.pdf)的复习与整理。\n![线性代数词云](/img/cs131_linear_algebra.jpg)\n\n<!-- more -->\n## 向量与矩阵\n数字图像可以看做二维矩阵，向量是特殊的矩阵，本课程默认的向量都是列向量。\nslide中给出了一些矩阵行列式和迹的性质，都比较简单，这里不再多说。\n## 矩阵作为线性变换\n通过线代知识，我们知道，在线性空间中，如果给定一组基，线性变换可以通过对应的矩阵来进行描述。\n\n### scale变换\n对角阵可以用来表示放缩变换。\n$$\n\\begin{bmatrix}\ns_x & 0\\\\\\\\\n0 & s_y\n\\end{bmatrix}\\begin{bmatrix}\nx\\\\\\\\\ny\n\\end{bmatrix} = \\begin{bmatrix}\ns_xx\\\\\\\\\ns_yy\n\\end{bmatrix}\n$$\n\n### 旋转变换\n如图所示，逆时针旋转$theta$角度，对应的旋转矩阵为：\n![旋转变换](/img/rotation.png)\n$$\n\\mathbf{R} = \\begin{bmatrix}\n\\cos\\theta &-\\sin\\theta \\\\\\\\\n\\sin\\theta &\\cos\\theta\n\\end{bmatrix}\n$$\n旋转矩阵是[酉矩阵](https://zh.wikipedia.org/wiki/酉矩阵)，矩阵内的各列（或者各行）相互正交。满足如下的关系式：\n$$\n\\mathbf{R}\\mathbf{R^{\\dagger}} = \\mathbf{I}\n$$\n由于$\\det{\\mathbf{R}} = \\det{\\mathbf{R^{\\dagger}}}$，所以，对于酉矩阵，$\\det{\\mathbf{R}} = \\pm 1$\n旋转矩阵是[酉矩阵](https://zh.wikipedia.org/wiki/酉矩阵)，矩阵内的各列（或者各行）相互正交。满足如下的关系式：\n$$\n\\mathbf{R}\\mathbf{R^{\\dagger}} = \\mathbf{I}\n$$\n由于$\\det{\\mathbf{R}} = \\det{\\mathbf{R^{\\dagger}}}$，所以，对于酉矩阵，$\\det{\\mathbf{R}} = \\pm 1$.\n\n### 齐次变换(Homogeneous Transform)\n只用上面的二维矩阵不能表达平移，使用齐次矩阵可以表达放缩，旋转和平移操作。\n$$\n\\mathbf{H} =\\begin{bmatrix}\na & b & t_x\\\\\\\\\nc & d & t_y\\\\\\\\\n0 & 0 & 1\n\\end{bmatrix},\\mathbf{H}\\begin{bmatrix}\nx\\\\\\\\\ny\\\\\\\\\n1\\\\\\\\\n\\end{bmatrix}=\\begin{bmatrix}\nax+by+t_x\\\\\\\\\ncx+dy+t_y\\\\\\\\\n1\n\\end{bmatrix}\n$$\n\n### SVD分解\n可以将矩阵分成若干个矩阵的乘积，叫做矩阵分解，比如QR分解，满秩分解等。SVD分解，即奇异值分解，也是一种特殊的矩阵分解方法。如下式所示，是将矩阵分解成为三个矩阵的乘积：\n$$\\mathbf{U}\\mathbf{\\Sigma}\\mathbf{V^\\dagger} = \\mathbf{A}$$\n其中矩阵$\\mathbf{A}$大小为$m\\times n$，矩阵$\\mathbf{U}$是大小为$m\\times m$的酉矩阵，$\\mathbf{V}$是大小为$n \\times n$的酉矩阵，$\\mathbf{\\Sigma}$是大小为$m \\times n$的旋转矩阵，即只有主对角元素不为0.\n\nSVD分解在主成分分析中年很有用。由于矩阵$\\mathbf{\\Sigma}$一般情况下是将奇异值按照从大到小的顺序摆放，所以矩阵$\\mathbf{U}$中，前面的若干列被视作主成分，后面的列显得相对不这么重要。可以抛弃后面的列，进行图像压缩。\n\n如下图，是使用前10个分量对原图片进行压缩的效果。\n\n``` matlab\nim = imread('./superman.png');\nim_gray = rbg2gray(im);\n[u, s, v] = svd(double(im_gray));\nk = 10;\nuk = u(:, 1:k);\nsigma = diag(s);\nsk = diag(sigma(1:k));\nvk = v(:, 1:k);\nim_k = uk*sk*vk';\nimshow(uint8(im_k))\n```\n\n![原始图像](/img/original_superman.png)\n![压缩图像](/img/svd_superman.png)\n","slug":"cs131-linear-alg","published":1,"updated":"2017-02-26T12:09:28.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cizve079y0007l61h77v2wxan","content":"<p>CS131课程(Computer Vision: Foundations and Applications)，是斯坦福大学Li Feifei实验室开设的一门计算机视觉入门基础课程，<a href=\"http://vision.stanford.edu/teaching/cs131_fall1617/index.html\" target=\"_blank\" rel=\"external\">该课程</a>目的在于为刚接触计算机视觉领域的学生提供基本原理和应用介绍。目前2016年冬季课程刚刚结束。CS131博客系列主要是关于本课的slide知识点总结与作业重点问题归纳，作为个人学习本门课程的心得体会和复习材料。</p>\n<p>由于是个人项目，所以会比较随意，只对个人感兴趣的内容做一总结。这篇文章是对课前<a href=\"http://vision.stanford.edu/teaching/cs131_fall1617/lectures/lecture2_linalg_review_cs131_2016.pdf\" target=\"_blank\" rel=\"external\">线代基础</a>的复习与整理。<br><img src=\"/img/cs131_linear_algebra.jpg\" alt=\"线性代数词云\"></p>\n<a id=\"more\"></a>\n<h2 id=\"向量与矩阵\"><a href=\"#向量与矩阵\" class=\"headerlink\" title=\"向量与矩阵\"></a>向量与矩阵</h2><p>数字图像可以看做二维矩阵，向量是特殊的矩阵，本课程默认的向量都是列向量。<br>slide中给出了一些矩阵行列式和迹的性质，都比较简单，这里不再多说。</p>\n<h2 id=\"矩阵作为线性变换\"><a href=\"#矩阵作为线性变换\" class=\"headerlink\" title=\"矩阵作为线性变换\"></a>矩阵作为线性变换</h2><p>通过线代知识，我们知道，在线性空间中，如果给定一组基，线性变换可以通过对应的矩阵来进行描述。</p>\n<h3 id=\"scale变换\"><a href=\"#scale变换\" class=\"headerlink\" title=\"scale变换\"></a>scale变换</h3><p>对角阵可以用来表示放缩变换。</p>\n<script type=\"math/tex; mode=display\">\n\\begin{bmatrix}\ns_x & 0\\\\\\\\\n0 & s_y\n\\end{bmatrix}\\begin{bmatrix}\nx\\\\\\\\\ny\n\\end{bmatrix} = \\begin{bmatrix}\ns_xx\\\\\\\\\ns_yy\n\\end{bmatrix}</script><h3 id=\"旋转变换\"><a href=\"#旋转变换\" class=\"headerlink\" title=\"旋转变换\"></a>旋转变换</h3><p>如图所示，逆时针旋转$theta$角度，对应的旋转矩阵为：<br><img src=\"/img/rotation.png\" alt=\"旋转变换\"></p>\n<script type=\"math/tex; mode=display\">\n\\mathbf{R} = \\begin{bmatrix}\n\\cos\\theta &-\\sin\\theta \\\\\\\\\n\\sin\\theta &\\cos\\theta\n\\end{bmatrix}</script><p>旋转矩阵是<a href=\"https://zh.wikipedia.org/wiki/酉矩阵\" target=\"_blank\" rel=\"external\">酉矩阵</a>，矩阵内的各列（或者各行）相互正交。满足如下的关系式：</p>\n<script type=\"math/tex; mode=display\">\n\\mathbf{R}\\mathbf{R^{\\dagger}} = \\mathbf{I}</script><p>由于$\\det{\\mathbf{R}} = \\det{\\mathbf{R^{\\dagger}}}$，所以，对于酉矩阵，$\\det{\\mathbf{R}} = \\pm 1$<br>旋转矩阵是<a href=\"https://zh.wikipedia.org/wiki/酉矩阵\" target=\"_blank\" rel=\"external\">酉矩阵</a>，矩阵内的各列（或者各行）相互正交。满足如下的关系式：</p>\n<script type=\"math/tex; mode=display\">\n\\mathbf{R}\\mathbf{R^{\\dagger}} = \\mathbf{I}</script><p>由于$\\det{\\mathbf{R}} = \\det{\\mathbf{R^{\\dagger}}}$，所以，对于酉矩阵，$\\det{\\mathbf{R}} = \\pm 1$.</p>\n<h3 id=\"齐次变换-Homogeneous-Transform\"><a href=\"#齐次变换-Homogeneous-Transform\" class=\"headerlink\" title=\"齐次变换(Homogeneous Transform)\"></a>齐次变换(Homogeneous Transform)</h3><p>只用上面的二维矩阵不能表达平移，使用齐次矩阵可以表达放缩，旋转和平移操作。</p>\n<script type=\"math/tex; mode=display\">\n\\mathbf{H} =\\begin{bmatrix}\na & b & t_x\\\\\\\\\nc & d & t_y\\\\\\\\\n0 & 0 & 1\n\\end{bmatrix},\\mathbf{H}\\begin{bmatrix}\nx\\\\\\\\\ny\\\\\\\\\n1\\\\\\\\\n\\end{bmatrix}=\\begin{bmatrix}\nax+by+t_x\\\\\\\\\ncx+dy+t_y\\\\\\\\\n1\n\\end{bmatrix}</script><h3 id=\"SVD分解\"><a href=\"#SVD分解\" class=\"headerlink\" title=\"SVD分解\"></a>SVD分解</h3><p>可以将矩阵分成若干个矩阵的乘积，叫做矩阵分解，比如QR分解，满秩分解等。SVD分解，即奇异值分解，也是一种特殊的矩阵分解方法。如下式所示，是将矩阵分解成为三个矩阵的乘积：</p>\n<script type=\"math/tex; mode=display\">\\mathbf{U}\\mathbf{\\Sigma}\\mathbf{V^\\dagger} = \\mathbf{A}</script><p>其中矩阵$\\mathbf{A}$大小为$m\\times n$，矩阵$\\mathbf{U}$是大小为$m\\times m$的酉矩阵，$\\mathbf{V}$是大小为$n \\times n$的酉矩阵，$\\mathbf{\\Sigma}$是大小为$m \\times n$的旋转矩阵，即只有主对角元素不为0.</p>\n<p>SVD分解在主成分分析中年很有用。由于矩阵$\\mathbf{\\Sigma}$一般情况下是将奇异值按照从大到小的顺序摆放，所以矩阵$\\mathbf{U}$中，前面的若干列被视作主成分，后面的列显得相对不这么重要。可以抛弃后面的列，进行图像压缩。</p>\n<p>如下图，是使用前10个分量对原图片进行压缩的效果。</p>\n<figure class=\"highlight matlab\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div></pre></td><td class=\"code\"><pre><div class=\"line\">im = imread(<span class=\"string\">'./superman.png'</span>);</div><div class=\"line\">im_gray = rbg2gray(im);</div><div class=\"line\">[u, s, v] = svd(double(im_gray));</div><div class=\"line\">k = <span class=\"number\">10</span>;</div><div class=\"line\">uk = u(:, <span class=\"number\">1</span>:k);</div><div class=\"line\">sigma = <span class=\"built_in\">diag</span>(s);</div><div class=\"line\">sk = <span class=\"built_in\">diag</span>(sigma(<span class=\"number\">1</span>:k));</div><div class=\"line\">vk = v(:, <span class=\"number\">1</span>:k);</div><div class=\"line\">im_k = uk*sk*vk';</div><div class=\"line\">imshow(uint8(im_k))</div></pre></td></tr></table></figure>\n<p><img src=\"/img/original_superman.png\" alt=\"原始图像\"><br><img src=\"/img/svd_superman.png\" alt=\"压缩图像\"></p>\n","excerpt":"<p>CS131课程(Computer Vision: Foundations and Applications)，是斯坦福大学Li Feifei实验室开设的一门计算机视觉入门基础课程，<a href=\"http://vision.stanford.edu/teaching/cs131_fall1617/index.html\">该课程</a>目的在于为刚接触计算机视觉领域的学生提供基本原理和应用介绍。目前2016年冬季课程刚刚结束。CS131博客系列主要是关于本课的slide知识点总结与作业重点问题归纳，作为个人学习本门课程的心得体会和复习材料。</p>\n<p>由于是个人项目，所以会比较随意，只对个人感兴趣的内容做一总结。这篇文章是对课前<a href=\"http://vision.stanford.edu/teaching/cs131_fall1617/lectures/lecture2_linalg_review_cs131_2016.pdf\">线代基础</a>的复习与整理。<br><img src=\"/img/cs131_linear_algebra.jpg\" alt=\"线性代数词云\"></p>","more":"<h2 id=\"向量与矩阵\"><a href=\"#向量与矩阵\" class=\"headerlink\" title=\"向量与矩阵\"></a>向量与矩阵</h2><p>数字图像可以看做二维矩阵，向量是特殊的矩阵，本课程默认的向量都是列向量。<br>slide中给出了一些矩阵行列式和迹的性质，都比较简单，这里不再多说。</p>\n<h2 id=\"矩阵作为线性变换\"><a href=\"#矩阵作为线性变换\" class=\"headerlink\" title=\"矩阵作为线性变换\"></a>矩阵作为线性变换</h2><p>通过线代知识，我们知道，在线性空间中，如果给定一组基，线性变换可以通过对应的矩阵来进行描述。</p>\n<h3 id=\"scale变换\"><a href=\"#scale变换\" class=\"headerlink\" title=\"scale变换\"></a>scale变换</h3><p>对角阵可以用来表示放缩变换。</p>\n<script type=\"math/tex; mode=display\">\n\\begin{bmatrix}\ns_x & 0\\\\\\\\\n0 & s_y\n\\end{bmatrix}\\begin{bmatrix}\nx\\\\\\\\\ny\n\\end{bmatrix} = \\begin{bmatrix}\ns_xx\\\\\\\\\ns_yy\n\\end{bmatrix}</script><h3 id=\"旋转变换\"><a href=\"#旋转变换\" class=\"headerlink\" title=\"旋转变换\"></a>旋转变换</h3><p>如图所示，逆时针旋转$theta$角度，对应的旋转矩阵为：<br><img src=\"/img/rotation.png\" alt=\"旋转变换\"></p>\n<script type=\"math/tex; mode=display\">\n\\mathbf{R} = \\begin{bmatrix}\n\\cos\\theta &-\\sin\\theta \\\\\\\\\n\\sin\\theta &\\cos\\theta\n\\end{bmatrix}</script><p>旋转矩阵是<a href=\"https://zh.wikipedia.org/wiki/酉矩阵\">酉矩阵</a>，矩阵内的各列（或者各行）相互正交。满足如下的关系式：</p>\n<script type=\"math/tex; mode=display\">\n\\mathbf{R}\\mathbf{R^{\\dagger}} = \\mathbf{I}</script><p>由于$\\det{\\mathbf{R}} = \\det{\\mathbf{R^{\\dagger}}}$，所以，对于酉矩阵，$\\det{\\mathbf{R}} = \\pm 1$<br>旋转矩阵是<a href=\"https://zh.wikipedia.org/wiki/酉矩阵\">酉矩阵</a>，矩阵内的各列（或者各行）相互正交。满足如下的关系式：</p>\n<script type=\"math/tex; mode=display\">\n\\mathbf{R}\\mathbf{R^{\\dagger}} = \\mathbf{I}</script><p>由于$\\det{\\mathbf{R}} = \\det{\\mathbf{R^{\\dagger}}}$，所以，对于酉矩阵，$\\det{\\mathbf{R}} = \\pm 1$.</p>\n<h3 id=\"齐次变换-Homogeneous-Transform\"><a href=\"#齐次变换-Homogeneous-Transform\" class=\"headerlink\" title=\"齐次变换(Homogeneous Transform)\"></a>齐次变换(Homogeneous Transform)</h3><p>只用上面的二维矩阵不能表达平移，使用齐次矩阵可以表达放缩，旋转和平移操作。</p>\n<script type=\"math/tex; mode=display\">\n\\mathbf{H} =\\begin{bmatrix}\na & b & t_x\\\\\\\\\nc & d & t_y\\\\\\\\\n0 & 0 & 1\n\\end{bmatrix},\\mathbf{H}\\begin{bmatrix}\nx\\\\\\\\\ny\\\\\\\\\n1\\\\\\\\\n\\end{bmatrix}=\\begin{bmatrix}\nax+by+t_x\\\\\\\\\ncx+dy+t_y\\\\\\\\\n1\n\\end{bmatrix}</script><h3 id=\"SVD分解\"><a href=\"#SVD分解\" class=\"headerlink\" title=\"SVD分解\"></a>SVD分解</h3><p>可以将矩阵分成若干个矩阵的乘积，叫做矩阵分解，比如QR分解，满秩分解等。SVD分解，即奇异值分解，也是一种特殊的矩阵分解方法。如下式所示，是将矩阵分解成为三个矩阵的乘积：</p>\n<script type=\"math/tex; mode=display\">\\mathbf{U}\\mathbf{\\Sigma}\\mathbf{V^\\dagger} = \\mathbf{A}</script><p>其中矩阵$\\mathbf{A}$大小为$m\\times n$，矩阵$\\mathbf{U}$是大小为$m\\times m$的酉矩阵，$\\mathbf{V}$是大小为$n \\times n$的酉矩阵，$\\mathbf{\\Sigma}$是大小为$m \\times n$的旋转矩阵，即只有主对角元素不为0.</p>\n<p>SVD分解在主成分分析中年很有用。由于矩阵$\\mathbf{\\Sigma}$一般情况下是将奇异值按照从大到小的顺序摆放，所以矩阵$\\mathbf{U}$中，前面的若干列被视作主成分，后面的列显得相对不这么重要。可以抛弃后面的列，进行图像压缩。</p>\n<p>如下图，是使用前10个分量对原图片进行压缩的效果。</p>\n<figure class=\"highlight matlab\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div></pre></td><td class=\"code\"><pre><div class=\"line\">im = imread(<span class=\"string\">'./superman.png'</span>);</div><div class=\"line\">im_gray = rbg2gray(im);</div><div class=\"line\">[u, s, v] = svd(double(im_gray));</div><div class=\"line\">k = <span class=\"number\">10</span>;</div><div class=\"line\">uk = u(:, <span class=\"number\">1</span>:k);</div><div class=\"line\">sigma = <span class=\"built_in\">diag</span>(s);</div><div class=\"line\">sk = <span class=\"built_in\">diag</span>(sigma(<span class=\"number\">1</span>:k));</div><div class=\"line\">vk = v(:, <span class=\"number\">1</span>:k);</div><div class=\"line\">im_k = uk*sk*vk';</div><div class=\"line\">imshow(uint8(im_k))</div></pre></td></tr></table></figure>\n<p><img src=\"/img/original_superman.png\" alt=\"原始图像\"><br><img src=\"/img/svd_superman.png\" alt=\"压缩图像\"></p>"},{"title":"CS131-MeanShift","date":"2017-02-12T14:28:15.000Z","_content":"[MeanShift](https://en.wikipedia.org/wiki/Mean_shift)最初由Fukunaga和Hostetler在1975年提出，但是一直到2000左右这篇PAMI的论文[Mean Shift: A Robust Approach Toward Feature Space Analysis](http://courses.csail.mit.edu/6.869/handouts/PAMIMeanshift.pdf)，将它的原理和收敛性等重新整理阐述，并应用于计算机视觉和图像处理领域之后，才逐渐为人熟知。\n\nMeanShift是一种用来寻找特征空间内[模态](https://en.wikipedia.org/wiki/Mode_(statistics))的方法。所谓模态（Mode），就是指数据集中最经常出现的数据。例如，连续随机变量概率密度函数的模态就是指函数的极大值。从概率的角度看，我们可以认为数据集（或者特征空间）内的数据点都是从某个概率分布中随机抽取出来的。这样，数据点越密集的地方就说明这里越有可能是密度函数的极大值。MeanShift就是一种能够从离散的抽样点中估计密度函数局部极大值的方法。\n![MeanShift](/img/meanshift_basics.jpg)\n\n<!-- more -->\n## 核密度估计\n上面提到的PAMI论文篇幅较长，且数学名词较多，我不是很理解。下面的说明过程主要参考了[这篇博客](https://saravananthirumuruganathan.wordpress.com/2010/04/01/introduction-to-mean-shift-algorithm/)和[这篇讲义](https://saravananthirumuruganathan.wordpress.com/2010/04/01/introduction-to-mean-shift-algorithm/)。\n\n注意下面的推导中$x$是一个$d$维的向量。为了书写简单，没有写成向量的加粗形式。首先，先介绍核函数（Kernel Function）的概念。如果某个$\\mathbb{R}^n\\rightarrow \\mathbb{R}$的函数满足以下条件，就能将其作为核函数。\n![kernel](/img/meanshift_kernel_function.png)\n\n比如高斯核函数：\n$$K(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp(-\\frac{x^2}{2\\sigma^2})$$。\n\n核密度估计是一种用来估计随机变量密度函数的非参数化方法。给定核函数$K$，带宽（bandwidth）参数$h$（就是指观察窗口的大小）。那么密度函数可以使用核函数进行估计，如下面的形式所示。其中，$n$是窗口内的数据点的数量。\n$$f(x) = \\frac{1}{nh^d}\\sum_{i=1}^{n}K(\\frac{x-x_i}{h})$$\n\n如果核函数是放射形状且对称的（例如高斯核函数），那么$K(x)$可以写成如下的形式，其中$c_{k,d}$是为了使得核函数满足积分为$1$的正则系数。\n$$K(x) = c_{k,d}k(\\Arrowvert x\\Arrowvert ^2)$$\n\n## mean shift向量\n那么，原来密度函数的极值点就是使得$f(x)$梯度为$0$的点。求取$f(x)$的梯度，可以得到下式。其中$g(s) = -k^\\prime(s)$。\n![密度函数的梯度](/img/meanshift_gradient_of_density.png)\n\n观察后可以发现，乘积第一项连带前面的系数，相当于是使用核函数$G(x) = c_{k,d}g(\\Arrowvert x\\Arrowvert^2)$得到的点$x$处的密度函数估计值（差了常数倍因子）（所以对于给定的某一点$x = x_0$，这一项是定值），后面一项拿出来，定义为$m_h(x)$，称为mean shift向量。\n$$m_h(x) = \\frac{\\sum_{i=1}^{n}x_i g(\\Arrowvert \\frac{x-x_i}{h} \\Arrowvert^2)}{\\sum_{i=1}^{n}g(\\Arrowvert \\frac{x-x_i}{h} \\Arrowvert^2)}-x$$\n\n所以说，$m_h(x)$的指向和$f(x)$的梯度是相同的。另外，从感性的角度出发。$m_h(x)$中的第一项（分式的那一大坨），相当于是在计算$x$周围窗口大小为$h$的这个领域的均值（$g(s)$是加权的系数）。所以$m_h(x)$实际上指示了局部均值和当前点$x$之间的位移。我们想要找到密度函数的局部极大值，不就应该让局部均值向着点密集的方向移动吗？\n\n## 算法流程\n所以，在给定初始位置$x_0$后，我们首先计算此点处的$m_h$，之后，将$x$沿着$m_h$移动即可。下面是一个简单的例子。数据点服从二维高斯分布，均值为$[1, 2]$。其中，红色菱形指示了迭代过程中mean shift的移动。\n\n``` matlab\n%% generate data\nmu = [1 2];\nSigma = [1 0; 0 2]; R = chol(Sigma);\nN = 250;\ndata = repmat(mu, N, 1) + randn(N, 2)*R;\nfigure\nhold on\nscatter(data(:, 1), data(:, 2), 50, 'filled');\n%% meanshift\nmu0 = rand(1,2) * 5;\nmu = mean_shift(mu0, 10, data);\n\nfunction out = gaussian_kernel(x, sigma)\n% gauss kernel, g(x) = \\exp(-x^2/2\\sigma^2)\nout = exp(-x.*x/(2*sigma*sigma));\nend\n\nfunction mu = mean_shift(mu0, h, data)\n% implementation of meanshift algorithm\n% mu_{k+1} = meanshift(mu_{k}) + mu_{k} = \\frac{\\sum_i=1^n xg}{\\sum_i=1^n g}\nmu = mu0;\nsigma = 1;    % parameter for gaussian kernel function\nfor iter = 1:20    \n    fprintf('iter = %d, mu = [%f, %f]\\n', iter, mu(1), mu(2));\n    scatter(mu(1), mu(2), 50, [1,0,0], 'd', 'filled');\n    offset = bsxfun(@minus, mu, data);    % offset = x-x_i\n    dis = sum(offset.^2, 2);              % dis = ||x-x_i||^2\n    x = data(dis < h, :);                 % neighborhood with bandwidth = h\n    g = gaussian_kernel(offset(dis < h), sigma);\n    xg = x.*g;\n    mu_prev = mu;\n    mu = sum(xg, 1) / sum(g, 1);\n    if norm(mu_prev - mu, 2) < 1E-2\n        break;\n    end\n    plot([mu_prev(1) mu(1)], [mu_prev(2), mu(2)], 'b-.', 'linewidth', 2);\nend\nscatter(mu(1), mu(2), 50, [1,0,0], 'd', 'filled');\nend\n```\n![](/img/meanshift_simple_demo.png)\n\n同时，我也试验了其他的kernel函数，如神似logistaic形式，效果也是相似的。\n$$K(x) = \\frac{1}{e^x+e^{-x}+2}$$\n\n``` matlab\nfunction out = logistic_kernel(x)\nout = 1./(exp(x) + exp(-x) + 2);\nend\n```\n","source":"_posts/cs131-mean-shift.md","raw":"---\ntitle: CS131-MeanShift\ndate: 2017-02-12 22:28:15\ntags:\n    - cs131\n    - 公开课\n---\n[MeanShift](https://en.wikipedia.org/wiki/Mean_shift)最初由Fukunaga和Hostetler在1975年提出，但是一直到2000左右这篇PAMI的论文[Mean Shift: A Robust Approach Toward Feature Space Analysis](http://courses.csail.mit.edu/6.869/handouts/PAMIMeanshift.pdf)，将它的原理和收敛性等重新整理阐述，并应用于计算机视觉和图像处理领域之后，才逐渐为人熟知。\n\nMeanShift是一种用来寻找特征空间内[模态](https://en.wikipedia.org/wiki/Mode_(statistics))的方法。所谓模态（Mode），就是指数据集中最经常出现的数据。例如，连续随机变量概率密度函数的模态就是指函数的极大值。从概率的角度看，我们可以认为数据集（或者特征空间）内的数据点都是从某个概率分布中随机抽取出来的。这样，数据点越密集的地方就说明这里越有可能是密度函数的极大值。MeanShift就是一种能够从离散的抽样点中估计密度函数局部极大值的方法。\n![MeanShift](/img/meanshift_basics.jpg)\n\n<!-- more -->\n## 核密度估计\n上面提到的PAMI论文篇幅较长，且数学名词较多，我不是很理解。下面的说明过程主要参考了[这篇博客](https://saravananthirumuruganathan.wordpress.com/2010/04/01/introduction-to-mean-shift-algorithm/)和[这篇讲义](https://saravananthirumuruganathan.wordpress.com/2010/04/01/introduction-to-mean-shift-algorithm/)。\n\n注意下面的推导中$x$是一个$d$维的向量。为了书写简单，没有写成向量的加粗形式。首先，先介绍核函数（Kernel Function）的概念。如果某个$\\mathbb{R}^n\\rightarrow \\mathbb{R}$的函数满足以下条件，就能将其作为核函数。\n![kernel](/img/meanshift_kernel_function.png)\n\n比如高斯核函数：\n$$K(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp(-\\frac{x^2}{2\\sigma^2})$$。\n\n核密度估计是一种用来估计随机变量密度函数的非参数化方法。给定核函数$K$，带宽（bandwidth）参数$h$（就是指观察窗口的大小）。那么密度函数可以使用核函数进行估计，如下面的形式所示。其中，$n$是窗口内的数据点的数量。\n$$f(x) = \\frac{1}{nh^d}\\sum_{i=1}^{n}K(\\frac{x-x_i}{h})$$\n\n如果核函数是放射形状且对称的（例如高斯核函数），那么$K(x)$可以写成如下的形式，其中$c_{k,d}$是为了使得核函数满足积分为$1$的正则系数。\n$$K(x) = c_{k,d}k(\\Arrowvert x\\Arrowvert ^2)$$\n\n## mean shift向量\n那么，原来密度函数的极值点就是使得$f(x)$梯度为$0$的点。求取$f(x)$的梯度，可以得到下式。其中$g(s) = -k^\\prime(s)$。\n![密度函数的梯度](/img/meanshift_gradient_of_density.png)\n\n观察后可以发现，乘积第一项连带前面的系数，相当于是使用核函数$G(x) = c_{k,d}g(\\Arrowvert x\\Arrowvert^2)$得到的点$x$处的密度函数估计值（差了常数倍因子）（所以对于给定的某一点$x = x_0$，这一项是定值），后面一项拿出来，定义为$m_h(x)$，称为mean shift向量。\n$$m_h(x) = \\frac{\\sum_{i=1}^{n}x_i g(\\Arrowvert \\frac{x-x_i}{h} \\Arrowvert^2)}{\\sum_{i=1}^{n}g(\\Arrowvert \\frac{x-x_i}{h} \\Arrowvert^2)}-x$$\n\n所以说，$m_h(x)$的指向和$f(x)$的梯度是相同的。另外，从感性的角度出发。$m_h(x)$中的第一项（分式的那一大坨），相当于是在计算$x$周围窗口大小为$h$的这个领域的均值（$g(s)$是加权的系数）。所以$m_h(x)$实际上指示了局部均值和当前点$x$之间的位移。我们想要找到密度函数的局部极大值，不就应该让局部均值向着点密集的方向移动吗？\n\n## 算法流程\n所以，在给定初始位置$x_0$后，我们首先计算此点处的$m_h$，之后，将$x$沿着$m_h$移动即可。下面是一个简单的例子。数据点服从二维高斯分布，均值为$[1, 2]$。其中，红色菱形指示了迭代过程中mean shift的移动。\n\n``` matlab\n%% generate data\nmu = [1 2];\nSigma = [1 0; 0 2]; R = chol(Sigma);\nN = 250;\ndata = repmat(mu, N, 1) + randn(N, 2)*R;\nfigure\nhold on\nscatter(data(:, 1), data(:, 2), 50, 'filled');\n%% meanshift\nmu0 = rand(1,2) * 5;\nmu = mean_shift(mu0, 10, data);\n\nfunction out = gaussian_kernel(x, sigma)\n% gauss kernel, g(x) = \\exp(-x^2/2\\sigma^2)\nout = exp(-x.*x/(2*sigma*sigma));\nend\n\nfunction mu = mean_shift(mu0, h, data)\n% implementation of meanshift algorithm\n% mu_{k+1} = meanshift(mu_{k}) + mu_{k} = \\frac{\\sum_i=1^n xg}{\\sum_i=1^n g}\nmu = mu0;\nsigma = 1;    % parameter for gaussian kernel function\nfor iter = 1:20    \n    fprintf('iter = %d, mu = [%f, %f]\\n', iter, mu(1), mu(2));\n    scatter(mu(1), mu(2), 50, [1,0,0], 'd', 'filled');\n    offset = bsxfun(@minus, mu, data);    % offset = x-x_i\n    dis = sum(offset.^2, 2);              % dis = ||x-x_i||^2\n    x = data(dis < h, :);                 % neighborhood with bandwidth = h\n    g = gaussian_kernel(offset(dis < h), sigma);\n    xg = x.*g;\n    mu_prev = mu;\n    mu = sum(xg, 1) / sum(g, 1);\n    if norm(mu_prev - mu, 2) < 1E-2\n        break;\n    end\n    plot([mu_prev(1) mu(1)], [mu_prev(2), mu(2)], 'b-.', 'linewidth', 2);\nend\nscatter(mu(1), mu(2), 50, [1,0,0], 'd', 'filled');\nend\n```\n![](/img/meanshift_simple_demo.png)\n\n同时，我也试验了其他的kernel函数，如神似logistaic形式，效果也是相似的。\n$$K(x) = \\frac{1}{e^x+e^{-x}+2}$$\n\n``` matlab\nfunction out = logistic_kernel(x)\nout = 1./(exp(x) + exp(-x) + 2);\nend\n```\n","slug":"cs131-mean-shift","published":1,"updated":"2017-02-26T12:09:28.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cizve07a50009l61h7eg1xkfr","content":"<p><a href=\"https://en.wikipedia.org/wiki/Mean_shift\" target=\"_blank\" rel=\"external\">MeanShift</a>最初由Fukunaga和Hostetler在1975年提出，但是一直到2000左右这篇PAMI的论文<a href=\"http://courses.csail.mit.edu/6.869/handouts/PAMIMeanshift.pdf\" target=\"_blank\" rel=\"external\">Mean Shift: A Robust Approach Toward Feature Space Analysis</a>，将它的原理和收敛性等重新整理阐述，并应用于计算机视觉和图像处理领域之后，才逐渐为人熟知。</p>\n<p>MeanShift是一种用来寻找特征空间内<a href=\"https://en.wikipedia.org/wiki/Mode_(statistics\" target=\"_blank\" rel=\"external\">模态</a>)的方法。所谓模态（Mode），就是指数据集中最经常出现的数据。例如，连续随机变量概率密度函数的模态就是指函数的极大值。从概率的角度看，我们可以认为数据集（或者特征空间）内的数据点都是从某个概率分布中随机抽取出来的。这样，数据点越密集的地方就说明这里越有可能是密度函数的极大值。MeanShift就是一种能够从离散的抽样点中估计密度函数局部极大值的方法。<br><img src=\"/img/meanshift_basics.jpg\" alt=\"MeanShift\"></p>\n<a id=\"more\"></a>\n<h2 id=\"核密度估计\"><a href=\"#核密度估计\" class=\"headerlink\" title=\"核密度估计\"></a>核密度估计</h2><p>上面提到的PAMI论文篇幅较长，且数学名词较多，我不是很理解。下面的说明过程主要参考了<a href=\"https://saravananthirumuruganathan.wordpress.com/2010/04/01/introduction-to-mean-shift-algorithm/\" target=\"_blank\" rel=\"external\">这篇博客</a>和<a href=\"https://saravananthirumuruganathan.wordpress.com/2010/04/01/introduction-to-mean-shift-algorithm/\" target=\"_blank\" rel=\"external\">这篇讲义</a>。</p>\n<p>注意下面的推导中$x$是一个$d$维的向量。为了书写简单，没有写成向量的加粗形式。首先，先介绍核函数（Kernel Function）的概念。如果某个$\\mathbb{R}^n\\rightarrow \\mathbb{R}$的函数满足以下条件，就能将其作为核函数。<br><img src=\"/img/meanshift_kernel_function.png\" alt=\"kernel\"></p>\n<p>比如高斯核函数：</p>\n<script type=\"math/tex; mode=display\">K(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp(-\\frac{x^2}{2\\sigma^2})$$。\n\n核密度估计是一种用来估计随机变量密度函数的非参数化方法。给定核函数$K$，带宽（bandwidth）参数$h$（就是指观察窗口的大小）。那么密度函数可以使用核函数进行估计，如下面的形式所示。其中，$n$是窗口内的数据点的数量。\n$$f(x) = \\frac{1}{nh^d}\\sum_{i=1}^{n}K(\\frac{x-x_i}{h})</script><p>如果核函数是放射形状且对称的（例如高斯核函数），那么$K(x)$可以写成如下的形式，其中$c_{k,d}$是为了使得核函数满足积分为$1$的正则系数。</p>\n<script type=\"math/tex; mode=display\">K(x) = c_{k,d}k(\\Arrowvert x\\Arrowvert ^2)</script><h2 id=\"mean-shift向量\"><a href=\"#mean-shift向量\" class=\"headerlink\" title=\"mean shift向量\"></a>mean shift向量</h2><p>那么，原来密度函数的极值点就是使得$f(x)$梯度为$0$的点。求取$f(x)$的梯度，可以得到下式。其中$g(s) = -k^\\prime(s)$。<br><img src=\"/img/meanshift_gradient_of_density.png\" alt=\"密度函数的梯度\"></p>\n<p>观察后可以发现，乘积第一项连带前面的系数，相当于是使用核函数$G(x) = c_{k,d}g(\\Arrowvert x\\Arrowvert^2)$得到的点$x$处的密度函数估计值（差了常数倍因子）（所以对于给定的某一点$x = x_0$，这一项是定值），后面一项拿出来，定义为$m_h(x)$，称为mean shift向量。</p>\n<script type=\"math/tex; mode=display\">m_h(x) = \\frac{\\sum_{i=1}^{n}x_i g(\\Arrowvert \\frac{x-x_i}{h} \\Arrowvert^2)}{\\sum_{i=1}^{n}g(\\Arrowvert \\frac{x-x_i}{h} \\Arrowvert^2)}-x</script><p>所以说，$m_h(x)$的指向和$f(x)$的梯度是相同的。另外，从感性的角度出发。$m_h(x)$中的第一项（分式的那一大坨），相当于是在计算$x$周围窗口大小为$h$的这个领域的均值（$g(s)$是加权的系数）。所以$m_h(x)$实际上指示了局部均值和当前点$x$之间的位移。我们想要找到密度函数的局部极大值，不就应该让局部均值向着点密集的方向移动吗？</p>\n<h2 id=\"算法流程\"><a href=\"#算法流程\" class=\"headerlink\" title=\"算法流程\"></a>算法流程</h2><p>所以，在给定初始位置$x_0$后，我们首先计算此点处的$m_h$，之后，将$x$沿着$m_h$移动即可。下面是一个简单的例子。数据点服从二维高斯分布，均值为$[1, 2]$。其中，红色菱形指示了迭代过程中mean shift的移动。</p>\n<figure class=\"highlight matlab\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\">%% generate data</span></div><div class=\"line\">mu = [<span class=\"number\">1</span> <span class=\"number\">2</span>];</div><div class=\"line\">Sigma = [<span class=\"number\">1</span> <span class=\"number\">0</span>; <span class=\"number\">0</span> <span class=\"number\">2</span>]; R = chol(Sigma);</div><div class=\"line\">N = <span class=\"number\">250</span>;</div><div class=\"line\">data = <span class=\"built_in\">repmat</span>(mu, N, <span class=\"number\">1</span>) + <span class=\"built_in\">randn</span>(N, <span class=\"number\">2</span>)*R;</div><div class=\"line\">figure</div><div class=\"line\">hold on</div><div class=\"line\">scatter(data(:, <span class=\"number\">1</span>), data(:, <span class=\"number\">2</span>), <span class=\"number\">50</span>, <span class=\"string\">'filled'</span>);</div><div class=\"line\"><span class=\"comment\">%% meanshift</span></div><div class=\"line\">mu0 = <span class=\"built_in\">rand</span>(<span class=\"number\">1</span>,<span class=\"number\">2</span>) * <span class=\"number\">5</span>;</div><div class=\"line\">mu = mean_shift(mu0, <span class=\"number\">10</span>, data);</div><div class=\"line\"></div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">function</span> <span class=\"title\">out</span> = <span class=\"title\">gaussian_kernel</span><span class=\"params\">(x, sigma)</span></span></div><div class=\"line\"><span class=\"comment\">% gauss kernel, g(x) = \\exp(-x^2/2\\sigma^2)</span></div><div class=\"line\">out = <span class=\"built_in\">exp</span>(-x.*x/(<span class=\"number\">2</span>*sigma*sigma));</div><div class=\"line\"><span class=\"keyword\">end</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">function</span> <span class=\"title\">mu</span> = <span class=\"title\">mean_shift</span><span class=\"params\">(mu0, h, data)</span></span></div><div class=\"line\"><span class=\"comment\">% implementation of meanshift algorithm</span></div><div class=\"line\"><span class=\"comment\">% mu_&#123;k+1&#125; = meanshift(mu_&#123;k&#125;) + mu_&#123;k&#125; = \\frac&#123;\\sum_i=1^n xg&#125;&#123;\\sum_i=1^n g&#125;</span></div><div class=\"line\">mu = mu0;</div><div class=\"line\">sigma = <span class=\"number\">1</span>;    <span class=\"comment\">% parameter for gaussian kernel function</span></div><div class=\"line\"><span class=\"keyword\">for</span> iter = <span class=\"number\">1</span>:<span class=\"number\">20</span>    </div><div class=\"line\">    fprintf(<span class=\"string\">'iter = %d, mu = [%f, %f]\\n'</span>, iter, mu(<span class=\"number\">1</span>), mu(<span class=\"number\">2</span>));</div><div class=\"line\">    scatter(mu(<span class=\"number\">1</span>), mu(<span class=\"number\">2</span>), <span class=\"number\">50</span>, [<span class=\"number\">1</span>,<span class=\"number\">0</span>,<span class=\"number\">0</span>], <span class=\"string\">'d'</span>, <span class=\"string\">'filled'</span>);</div><div class=\"line\">    offset = <span class=\"built_in\">bsxfun</span>(@minus, mu, data);    <span class=\"comment\">% offset = x-x_i</span></div><div class=\"line\">    dis = sum(offset.^<span class=\"number\">2</span>, <span class=\"number\">2</span>);              <span class=\"comment\">% dis = ||x-x_i||^2</span></div><div class=\"line\">    x = data(dis &lt; h, :);                 <span class=\"comment\">% neighborhood with bandwidth = h</span></div><div class=\"line\">    g = gaussian_kernel(offset(dis &lt; h), sigma);</div><div class=\"line\">    xg = x.*g;</div><div class=\"line\">    mu_prev = mu;</div><div class=\"line\">    mu = sum(xg, <span class=\"number\">1</span>) / sum(g, <span class=\"number\">1</span>);</div><div class=\"line\">    <span class=\"keyword\">if</span> norm(mu_prev - mu, <span class=\"number\">2</span>) &lt; <span class=\"number\">1E-2</span></div><div class=\"line\">        <span class=\"keyword\">break</span>;</div><div class=\"line\">    <span class=\"keyword\">end</span></div><div class=\"line\">    plot([mu_prev(<span class=\"number\">1</span>) mu(<span class=\"number\">1</span>)], [mu_prev(<span class=\"number\">2</span>), mu(<span class=\"number\">2</span>)], <span class=\"string\">'b-.'</span>, <span class=\"string\">'linewidth'</span>, <span class=\"number\">2</span>);</div><div class=\"line\"><span class=\"keyword\">end</span></div><div class=\"line\">scatter(mu(<span class=\"number\">1</span>), mu(<span class=\"number\">2</span>), <span class=\"number\">50</span>, [<span class=\"number\">1</span>,<span class=\"number\">0</span>,<span class=\"number\">0</span>], <span class=\"string\">'d'</span>, <span class=\"string\">'filled'</span>);</div><div class=\"line\"><span class=\"keyword\">end</span></div></pre></td></tr></table></figure>\n<p><img src=\"/img/meanshift_simple_demo.png\" alt=\"\"></p>\n<p>同时，我也试验了其他的kernel函数，如神似logistaic形式，效果也是相似的。</p>\n<script type=\"math/tex; mode=display\">K(x) = \\frac{1}{e^x+e^{-x}+2}</script><figure class=\"highlight matlab\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"function\"><span class=\"keyword\">function</span> <span class=\"title\">out</span> = <span class=\"title\">logistic_kernel</span><span class=\"params\">(x)</span></span></div><div class=\"line\">out = <span class=\"number\">1.</span>/(<span class=\"built_in\">exp</span>(x) + <span class=\"built_in\">exp</span>(-x) + <span class=\"number\">2</span>);</div><div class=\"line\"><span class=\"keyword\">end</span></div></pre></td></tr></table></figure>\n","excerpt":"<p><a href=\"https://en.wikipedia.org/wiki/Mean_shift\">MeanShift</a>最初由Fukunaga和Hostetler在1975年提出，但是一直到2000左右这篇PAMI的论文<a href=\"http://courses.csail.mit.edu/6.869/handouts/PAMIMeanshift.pdf\">Mean Shift: A Robust Approach Toward Feature Space Analysis</a>，将它的原理和收敛性等重新整理阐述，并应用于计算机视觉和图像处理领域之后，才逐渐为人熟知。</p>\n<p>MeanShift是一种用来寻找特征空间内<a href=\"https://en.wikipedia.org/wiki/Mode_(statistics\">模态</a>)的方法。所谓模态（Mode），就是指数据集中最经常出现的数据。例如，连续随机变量概率密度函数的模态就是指函数的极大值。从概率的角度看，我们可以认为数据集（或者特征空间）内的数据点都是从某个概率分布中随机抽取出来的。这样，数据点越密集的地方就说明这里越有可能是密度函数的极大值。MeanShift就是一种能够从离散的抽样点中估计密度函数局部极大值的方法。<br><img src=\"/img/meanshift_basics.jpg\" alt=\"MeanShift\"></p>","more":"<h2 id=\"核密度估计\"><a href=\"#核密度估计\" class=\"headerlink\" title=\"核密度估计\"></a>核密度估计</h2><p>上面提到的PAMI论文篇幅较长，且数学名词较多，我不是很理解。下面的说明过程主要参考了<a href=\"https://saravananthirumuruganathan.wordpress.com/2010/04/01/introduction-to-mean-shift-algorithm/\">这篇博客</a>和<a href=\"https://saravananthirumuruganathan.wordpress.com/2010/04/01/introduction-to-mean-shift-algorithm/\">这篇讲义</a>。</p>\n<p>注意下面的推导中$x$是一个$d$维的向量。为了书写简单，没有写成向量的加粗形式。首先，先介绍核函数（Kernel Function）的概念。如果某个$\\mathbb{R}^n\\rightarrow \\mathbb{R}$的函数满足以下条件，就能将其作为核函数。<br><img src=\"/img/meanshift_kernel_function.png\" alt=\"kernel\"></p>\n<p>比如高斯核函数：</p>\n<script type=\"math/tex; mode=display\">K(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp(-\\frac{x^2}{2\\sigma^2})$$。\n\n核密度估计是一种用来估计随机变量密度函数的非参数化方法。给定核函数$K$，带宽（bandwidth）参数$h$（就是指观察窗口的大小）。那么密度函数可以使用核函数进行估计，如下面的形式所示。其中，$n$是窗口内的数据点的数量。\n$$f(x) = \\frac{1}{nh^d}\\sum_{i=1}^{n}K(\\frac{x-x_i}{h})</script><p>如果核函数是放射形状且对称的（例如高斯核函数），那么$K(x)$可以写成如下的形式，其中$c_{k,d}$是为了使得核函数满足积分为$1$的正则系数。</p>\n<script type=\"math/tex; mode=display\">K(x) = c_{k,d}k(\\Arrowvert x\\Arrowvert ^2)</script><h2 id=\"mean-shift向量\"><a href=\"#mean-shift向量\" class=\"headerlink\" title=\"mean shift向量\"></a>mean shift向量</h2><p>那么，原来密度函数的极值点就是使得$f(x)$梯度为$0$的点。求取$f(x)$的梯度，可以得到下式。其中$g(s) = -k^\\prime(s)$。<br><img src=\"/img/meanshift_gradient_of_density.png\" alt=\"密度函数的梯度\"></p>\n<p>观察后可以发现，乘积第一项连带前面的系数，相当于是使用核函数$G(x) = c_{k,d}g(\\Arrowvert x\\Arrowvert^2)$得到的点$x$处的密度函数估计值（差了常数倍因子）（所以对于给定的某一点$x = x_0$，这一项是定值），后面一项拿出来，定义为$m_h(x)$，称为mean shift向量。</p>\n<script type=\"math/tex; mode=display\">m_h(x) = \\frac{\\sum_{i=1}^{n}x_i g(\\Arrowvert \\frac{x-x_i}{h} \\Arrowvert^2)}{\\sum_{i=1}^{n}g(\\Arrowvert \\frac{x-x_i}{h} \\Arrowvert^2)}-x</script><p>所以说，$m_h(x)$的指向和$f(x)$的梯度是相同的。另外，从感性的角度出发。$m_h(x)$中的第一项（分式的那一大坨），相当于是在计算$x$周围窗口大小为$h$的这个领域的均值（$g(s)$是加权的系数）。所以$m_h(x)$实际上指示了局部均值和当前点$x$之间的位移。我们想要找到密度函数的局部极大值，不就应该让局部均值向着点密集的方向移动吗？</p>\n<h2 id=\"算法流程\"><a href=\"#算法流程\" class=\"headerlink\" title=\"算法流程\"></a>算法流程</h2><p>所以，在给定初始位置$x_0$后，我们首先计算此点处的$m_h$，之后，将$x$沿着$m_h$移动即可。下面是一个简单的例子。数据点服从二维高斯分布，均值为$[1, 2]$。其中，红色菱形指示了迭代过程中mean shift的移动。</p>\n<figure class=\"highlight matlab\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\">%% generate data</span></div><div class=\"line\">mu = [<span class=\"number\">1</span> <span class=\"number\">2</span>];</div><div class=\"line\">Sigma = [<span class=\"number\">1</span> <span class=\"number\">0</span>; <span class=\"number\">0</span> <span class=\"number\">2</span>]; R = chol(Sigma);</div><div class=\"line\">N = <span class=\"number\">250</span>;</div><div class=\"line\">data = <span class=\"built_in\">repmat</span>(mu, N, <span class=\"number\">1</span>) + <span class=\"built_in\">randn</span>(N, <span class=\"number\">2</span>)*R;</div><div class=\"line\">figure</div><div class=\"line\">hold on</div><div class=\"line\">scatter(data(:, <span class=\"number\">1</span>), data(:, <span class=\"number\">2</span>), <span class=\"number\">50</span>, <span class=\"string\">'filled'</span>);</div><div class=\"line\"><span class=\"comment\">%% meanshift</span></div><div class=\"line\">mu0 = <span class=\"built_in\">rand</span>(<span class=\"number\">1</span>,<span class=\"number\">2</span>) * <span class=\"number\">5</span>;</div><div class=\"line\">mu = mean_shift(mu0, <span class=\"number\">10</span>, data);</div><div class=\"line\"></div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">function</span> <span class=\"title\">out</span> = <span class=\"title\">gaussian_kernel</span><span class=\"params\">(x, sigma)</span></span></div><div class=\"line\"><span class=\"comment\">% gauss kernel, g(x) = \\exp(-x^2/2\\sigma^2)</span></div><div class=\"line\">out = <span class=\"built_in\">exp</span>(-x.*x/(<span class=\"number\">2</span>*sigma*sigma));</div><div class=\"line\"><span class=\"keyword\">end</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">function</span> <span class=\"title\">mu</span> = <span class=\"title\">mean_shift</span><span class=\"params\">(mu0, h, data)</span></span></div><div class=\"line\"><span class=\"comment\">% implementation of meanshift algorithm</span></div><div class=\"line\"><span class=\"comment\">% mu_&#123;k+1&#125; = meanshift(mu_&#123;k&#125;) + mu_&#123;k&#125; = \\frac&#123;\\sum_i=1^n xg&#125;&#123;\\sum_i=1^n g&#125;</span></div><div class=\"line\">mu = mu0;</div><div class=\"line\">sigma = <span class=\"number\">1</span>;    <span class=\"comment\">% parameter for gaussian kernel function</span></div><div class=\"line\"><span class=\"keyword\">for</span> iter = <span class=\"number\">1</span>:<span class=\"number\">20</span>    </div><div class=\"line\">    fprintf(<span class=\"string\">'iter = %d, mu = [%f, %f]\\n'</span>, iter, mu(<span class=\"number\">1</span>), mu(<span class=\"number\">2</span>));</div><div class=\"line\">    scatter(mu(<span class=\"number\">1</span>), mu(<span class=\"number\">2</span>), <span class=\"number\">50</span>, [<span class=\"number\">1</span>,<span class=\"number\">0</span>,<span class=\"number\">0</span>], <span class=\"string\">'d'</span>, <span class=\"string\">'filled'</span>);</div><div class=\"line\">    offset = <span class=\"built_in\">bsxfun</span>(@minus, mu, data);    <span class=\"comment\">% offset = x-x_i</span></div><div class=\"line\">    dis = sum(offset.^<span class=\"number\">2</span>, <span class=\"number\">2</span>);              <span class=\"comment\">% dis = ||x-x_i||^2</span></div><div class=\"line\">    x = data(dis &lt; h, :);                 <span class=\"comment\">% neighborhood with bandwidth = h</span></div><div class=\"line\">    g = gaussian_kernel(offset(dis &lt; h), sigma);</div><div class=\"line\">    xg = x.*g;</div><div class=\"line\">    mu_prev = mu;</div><div class=\"line\">    mu = sum(xg, <span class=\"number\">1</span>) / sum(g, <span class=\"number\">1</span>);</div><div class=\"line\">    <span class=\"keyword\">if</span> norm(mu_prev - mu, <span class=\"number\">2</span>) &lt; <span class=\"number\">1E-2</span></div><div class=\"line\">        <span class=\"keyword\">break</span>;</div><div class=\"line\">    <span class=\"keyword\">end</span></div><div class=\"line\">    plot([mu_prev(<span class=\"number\">1</span>) mu(<span class=\"number\">1</span>)], [mu_prev(<span class=\"number\">2</span>), mu(<span class=\"number\">2</span>)], <span class=\"string\">'b-.'</span>, <span class=\"string\">'linewidth'</span>, <span class=\"number\">2</span>);</div><div class=\"line\"><span class=\"keyword\">end</span></div><div class=\"line\">scatter(mu(<span class=\"number\">1</span>), mu(<span class=\"number\">2</span>), <span class=\"number\">50</span>, [<span class=\"number\">1</span>,<span class=\"number\">0</span>,<span class=\"number\">0</span>], <span class=\"string\">'d'</span>, <span class=\"string\">'filled'</span>);</div><div class=\"line\"><span class=\"keyword\">end</span></div></pre></td></tr></table></figure>\n<p><img src=\"/img/meanshift_simple_demo.png\" alt=\"\"></p>\n<p>同时，我也试验了其他的kernel函数，如神似logistaic形式，效果也是相似的。</p>\n<script type=\"math/tex; mode=display\">K(x) = \\frac{1}{e^x+e^{-x}+2}</script><figure class=\"highlight matlab\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"function\"><span class=\"keyword\">function</span> <span class=\"title\">out</span> = <span class=\"title\">logistic_kernel</span><span class=\"params\">(x)</span></span></div><div class=\"line\">out = <span class=\"number\">1.</span>/(<span class=\"built_in\">exp</span>(x) + <span class=\"built_in\">exp</span>(-x) + <span class=\"number\">2</span>);</div><div class=\"line\"><span class=\"keyword\">end</span></div></pre></td></tr></table></figure>"},{"title":"CS131-描述图像的特征(Harris 角点)","date":"2017-01-25T02:51:47.000Z","_content":"\nfeature是对图像的描述。比如，图像整体灰度值的均值和方差就可以作为feature。如果我们想在一副图像中检测足球时，可以使用滑动窗方法，逐一检查窗口内的灰度值分布是否和一张给定的典型黑白格子足球相似。可想而知，这种方法的性能一定让人捉急。而在image matching问题中，常常需要将不同视角的同一目标物进行matching，进而计算相机转过的角度。这在SLAM问题中很有意义。如下图所示，同一处景观，在不同摄影师的镜头下，不仅视角不同，而且明暗变化也有很多差别，右侧的图暖色调更浓。这也告诉我们，上面提到的只使用全局图像灰度值来做feature的做法有多么不靠谱。\n\n那么，让我们脱离全局特征，转而将注意力集中在局部特征上。这是因为使用局部特征能够更好地处理图像中的遮挡、变形等情况，而且我们的研究对象常常是图像中的部分区域而不是图像整体。更特殊地，在这一讲中，我们主要探究key point作为local feature的描述，并介绍Harris角点检测方法。在下一节中介绍SIFT特征。\n\n![image matching example](/img/image_matching_hard.png)\n\n<!-- more -->\n\n## Harris角点\n角点，即corner，和edge类似，区别在于其在两个方向上都有较为剧烈的灰度变化（而edge只在某一个方向上灰度值变化剧烈）。如图所示。\n![what is corner](/img/what_is_corner.png)\n\n[Harris角点](http://www.bmva.org/bmvc/1988/avc-88-023.pdf)得名于其发明者Harris，是一种常见的角点检测方法。\n给定观察窗口大小，计算平移后窗口内各个像素差值的加权平方和，如下式。\n$$E(u,v) = \\sum_x\\sum_yw(x,y)[I(x+u, y+v) - I(x,y)]^2$$\n\n其中，窗口加权函数$w$可以取做门限函数或gaussian函数。如图所示。\n![window function](/img/corner_window_fun.png)\n\n使用泰勒级数展开，并忽略非线性项，我们有\n$$I(x+u,y+v) = I(x,y) + I_x(x,y)u+I_y(x,y)v$$\n\n所以上式可以写成（线性二次型写成了矩阵形式），\n$$E(u,v) = \\sum_{x,y}w(I_xu+I_yv)^2 = \\begin{bmatrix}u&v\\end{bmatrix}M\\begin{bmatrix}u\\\\\\\\v\\end{bmatrix}$$\n\n其中，\n$$M = w\\begin{bmatrix}I_x^2& I_xI_y\\\\\\\\I_xI_y&I_y^2\\end{bmatrix}$$\n\n当使用门限函数时，权值$w_{i,j} = 1$，则，\n$$M = \\begin{bmatrix}\\sum I_xI_x& \\sum I_xI_y\\\\\\\\\\sum I_xI_y&\\sum I_yI_y\\end{bmatrix} = \\sum \\begin{bmatrix}I_x \\\\\\\\I_y\\end{bmatrix}\\begin{bmatrix}I_x &I_y\\end{bmatrix}$$\n\n当corner与xy坐标轴对齐时候，如下图所示。由于在黑色观察窗口内，只有上侧和左侧存在边缘，且在上边缘，$I_y$很大，而$I_x=0$，在左侧边缘，$I_x$很大而$I_y = 0$，所以，矩阵\n$$M = \\begin{bmatrix}\\lambda_1 & 0 \\\\\\\\ 0&\\lambda_2 \\end{bmatrix}$$\n![M为对角阵](/img/corner_type_1.png)\n\n当corner与坐标轴没有对齐时，经过旋转变换就可以将其转换到与坐标轴对齐的角度，而这种旋转操作可以使用矩阵的相似化来表示（其实是二次型的化简，可以使用合同变换，而旋转变换的矩阵是酉矩阵，转置即为矩阵的逆，所以也是相似变换）。也就是说，矩阵$M$相似于某个对角阵。\n$$M = R^{-1}\\Sigma R, \\text{其中}\\Sigma = \\begin{bmatrix}\\lambda_1&0\\\\\\\\0&\\lambda_2\\end{bmatrix}$$\n\n所以，可以根据下面这张图利用矩阵$M$的特征值来判定角点和边缘点。当两个特征值都较大时为角点；当某个分量近似为0而另一个分量较大时，可以判定为边缘点（因为某个方向的导数为0）；当两个特征值都近似为0时，说明是普通点（flat point）。（课件原图如此，空缺的问号处应分别为$\\lambda_1$和$\\lambda_2$）。\n![使用M矩阵特征值判定](/img/corner_judge.png)\n\n 然而矩阵的特征值计算较为复杂，所以使用下面的方法进行近似计算。\n $$\\theta = \\det(M)-\\alpha\\text{trace}(M)^2 = \\lambda_1\\lambda_2-\\alpha(\\lambda_1+\\lambda_2)^2$$\n![使用theta判定](/img/corner_judge_2.png)\n\n为了减弱噪声的影响，常常使用gaussian窗函数。如下式所示：\n$$w(x,y) = \\exp(-(x^2+y^2)/2\\sigma^2)$$\n","source":"_posts/cs131-finding-features.md","raw":"---\ntitle: CS131-描述图像的特征(Harris 角点)\ndate: 2017-01-25 10:51:47\ntags:\n    - cs131\n    - 公开课\n---\n\nfeature是对图像的描述。比如，图像整体灰度值的均值和方差就可以作为feature。如果我们想在一副图像中检测足球时，可以使用滑动窗方法，逐一检查窗口内的灰度值分布是否和一张给定的典型黑白格子足球相似。可想而知，这种方法的性能一定让人捉急。而在image matching问题中，常常需要将不同视角的同一目标物进行matching，进而计算相机转过的角度。这在SLAM问题中很有意义。如下图所示，同一处景观，在不同摄影师的镜头下，不仅视角不同，而且明暗变化也有很多差别，右侧的图暖色调更浓。这也告诉我们，上面提到的只使用全局图像灰度值来做feature的做法有多么不靠谱。\n\n那么，让我们脱离全局特征，转而将注意力集中在局部特征上。这是因为使用局部特征能够更好地处理图像中的遮挡、变形等情况，而且我们的研究对象常常是图像中的部分区域而不是图像整体。更特殊地，在这一讲中，我们主要探究key point作为local feature的描述，并介绍Harris角点检测方法。在下一节中介绍SIFT特征。\n\n![image matching example](/img/image_matching_hard.png)\n\n<!-- more -->\n\n## Harris角点\n角点，即corner，和edge类似，区别在于其在两个方向上都有较为剧烈的灰度变化（而edge只在某一个方向上灰度值变化剧烈）。如图所示。\n![what is corner](/img/what_is_corner.png)\n\n[Harris角点](http://www.bmva.org/bmvc/1988/avc-88-023.pdf)得名于其发明者Harris，是一种常见的角点检测方法。\n给定观察窗口大小，计算平移后窗口内各个像素差值的加权平方和，如下式。\n$$E(u,v) = \\sum_x\\sum_yw(x,y)[I(x+u, y+v) - I(x,y)]^2$$\n\n其中，窗口加权函数$w$可以取做门限函数或gaussian函数。如图所示。\n![window function](/img/corner_window_fun.png)\n\n使用泰勒级数展开，并忽略非线性项，我们有\n$$I(x+u,y+v) = I(x,y) + I_x(x,y)u+I_y(x,y)v$$\n\n所以上式可以写成（线性二次型写成了矩阵形式），\n$$E(u,v) = \\sum_{x,y}w(I_xu+I_yv)^2 = \\begin{bmatrix}u&v\\end{bmatrix}M\\begin{bmatrix}u\\\\\\\\v\\end{bmatrix}$$\n\n其中，\n$$M = w\\begin{bmatrix}I_x^2& I_xI_y\\\\\\\\I_xI_y&I_y^2\\end{bmatrix}$$\n\n当使用门限函数时，权值$w_{i,j} = 1$，则，\n$$M = \\begin{bmatrix}\\sum I_xI_x& \\sum I_xI_y\\\\\\\\\\sum I_xI_y&\\sum I_yI_y\\end{bmatrix} = \\sum \\begin{bmatrix}I_x \\\\\\\\I_y\\end{bmatrix}\\begin{bmatrix}I_x &I_y\\end{bmatrix}$$\n\n当corner与xy坐标轴对齐时候，如下图所示。由于在黑色观察窗口内，只有上侧和左侧存在边缘，且在上边缘，$I_y$很大，而$I_x=0$，在左侧边缘，$I_x$很大而$I_y = 0$，所以，矩阵\n$$M = \\begin{bmatrix}\\lambda_1 & 0 \\\\\\\\ 0&\\lambda_2 \\end{bmatrix}$$\n![M为对角阵](/img/corner_type_1.png)\n\n当corner与坐标轴没有对齐时，经过旋转变换就可以将其转换到与坐标轴对齐的角度，而这种旋转操作可以使用矩阵的相似化来表示（其实是二次型的化简，可以使用合同变换，而旋转变换的矩阵是酉矩阵，转置即为矩阵的逆，所以也是相似变换）。也就是说，矩阵$M$相似于某个对角阵。\n$$M = R^{-1}\\Sigma R, \\text{其中}\\Sigma = \\begin{bmatrix}\\lambda_1&0\\\\\\\\0&\\lambda_2\\end{bmatrix}$$\n\n所以，可以根据下面这张图利用矩阵$M$的特征值来判定角点和边缘点。当两个特征值都较大时为角点；当某个分量近似为0而另一个分量较大时，可以判定为边缘点（因为某个方向的导数为0）；当两个特征值都近似为0时，说明是普通点（flat point）。（课件原图如此，空缺的问号处应分别为$\\lambda_1$和$\\lambda_2$）。\n![使用M矩阵特征值判定](/img/corner_judge.png)\n\n 然而矩阵的特征值计算较为复杂，所以使用下面的方法进行近似计算。\n $$\\theta = \\det(M)-\\alpha\\text{trace}(M)^2 = \\lambda_1\\lambda_2-\\alpha(\\lambda_1+\\lambda_2)^2$$\n![使用theta判定](/img/corner_judge_2.png)\n\n为了减弱噪声的影响，常常使用gaussian窗函数。如下式所示：\n$$w(x,y) = \\exp(-(x^2+y^2)/2\\sigma^2)$$\n","slug":"cs131-finding-features","published":1,"updated":"2017-02-26T12:09:28.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cizve07a8000al61hz0ftb8g2","content":"<p>feature是对图像的描述。比如，图像整体灰度值的均值和方差就可以作为feature。如果我们想在一副图像中检测足球时，可以使用滑动窗方法，逐一检查窗口内的灰度值分布是否和一张给定的典型黑白格子足球相似。可想而知，这种方法的性能一定让人捉急。而在image matching问题中，常常需要将不同视角的同一目标物进行matching，进而计算相机转过的角度。这在SLAM问题中很有意义。如下图所示，同一处景观，在不同摄影师的镜头下，不仅视角不同，而且明暗变化也有很多差别，右侧的图暖色调更浓。这也告诉我们，上面提到的只使用全局图像灰度值来做feature的做法有多么不靠谱。</p>\n<p>那么，让我们脱离全局特征，转而将注意力集中在局部特征上。这是因为使用局部特征能够更好地处理图像中的遮挡、变形等情况，而且我们的研究对象常常是图像中的部分区域而不是图像整体。更特殊地，在这一讲中，我们主要探究key point作为local feature的描述，并介绍Harris角点检测方法。在下一节中介绍SIFT特征。</p>\n<p><img src=\"/img/image_matching_hard.png\" alt=\"image matching example\"></p>\n<a id=\"more\"></a>\n<h2 id=\"Harris角点\"><a href=\"#Harris角点\" class=\"headerlink\" title=\"Harris角点\"></a>Harris角点</h2><p>角点，即corner，和edge类似，区别在于其在两个方向上都有较为剧烈的灰度变化（而edge只在某一个方向上灰度值变化剧烈）。如图所示。<br><img src=\"/img/what_is_corner.png\" alt=\"what is corner\"></p>\n<p><a href=\"http://www.bmva.org/bmvc/1988/avc-88-023.pdf\" target=\"_blank\" rel=\"external\">Harris角点</a>得名于其发明者Harris，是一种常见的角点检测方法。<br>给定观察窗口大小，计算平移后窗口内各个像素差值的加权平方和，如下式。</p>\n<script type=\"math/tex; mode=display\">E(u,v) = \\sum_x\\sum_yw(x,y)[I(x+u, y+v) - I(x,y)]^2</script><p>其中，窗口加权函数$w$可以取做门限函数或gaussian函数。如图所示。<br><img src=\"/img/corner_window_fun.png\" alt=\"window function\"></p>\n<p>使用泰勒级数展开，并忽略非线性项，我们有</p>\n<script type=\"math/tex; mode=display\">I(x+u,y+v) = I(x,y) + I_x(x,y)u+I_y(x,y)v</script><p>所以上式可以写成（线性二次型写成了矩阵形式），</p>\n<script type=\"math/tex; mode=display\">E(u,v) = \\sum_{x,y}w(I_xu+I_yv)^2 = \\begin{bmatrix}u&v\\end{bmatrix}M\\begin{bmatrix}u\\\\\\\\v\\end{bmatrix}</script><p>其中，</p>\n<script type=\"math/tex; mode=display\">M = w\\begin{bmatrix}I_x^2& I_xI_y\\\\\\\\I_xI_y&I_y^2\\end{bmatrix}</script><p>当使用门限函数时，权值$w_{i,j} = 1$，则，</p>\n<script type=\"math/tex; mode=display\">M = \\begin{bmatrix}\\sum I_xI_x& \\sum I_xI_y\\\\\\\\\\sum I_xI_y&\\sum I_yI_y\\end{bmatrix} = \\sum \\begin{bmatrix}I_x \\\\\\\\I_y\\end{bmatrix}\\begin{bmatrix}I_x &I_y\\end{bmatrix}</script><p>当corner与xy坐标轴对齐时候，如下图所示。由于在黑色观察窗口内，只有上侧和左侧存在边缘，且在上边缘，$I_y$很大，而$I_x=0$，在左侧边缘，$I_x$很大而$I_y = 0$，所以，矩阵</p>\n<script type=\"math/tex; mode=display\">M = \\begin{bmatrix}\\lambda_1 & 0 \\\\\\\\ 0&\\lambda_2 \\end{bmatrix}</script><p><img src=\"/img/corner_type_1.png\" alt=\"M为对角阵\"></p>\n<p>当corner与坐标轴没有对齐时，经过旋转变换就可以将其转换到与坐标轴对齐的角度，而这种旋转操作可以使用矩阵的相似化来表示（其实是二次型的化简，可以使用合同变换，而旋转变换的矩阵是酉矩阵，转置即为矩阵的逆，所以也是相似变换）。也就是说，矩阵$M$相似于某个对角阵。</p>\n<script type=\"math/tex; mode=display\">M = R^{-1}\\Sigma R, \\text{其中}\\Sigma = \\begin{bmatrix}\\lambda_1&0\\\\\\\\0&\\lambda_2\\end{bmatrix}</script><p>所以，可以根据下面这张图利用矩阵$M$的特征值来判定角点和边缘点。当两个特征值都较大时为角点；当某个分量近似为0而另一个分量较大时，可以判定为边缘点（因为某个方向的导数为0）；当两个特征值都近似为0时，说明是普通点（flat point）。（课件原图如此，空缺的问号处应分别为$\\lambda_1$和$\\lambda_2$）。<br><img src=\"/img/corner_judge.png\" alt=\"使用M矩阵特征值判定\"></p>\n<p> 然而矩阵的特征值计算较为复杂，所以使用下面的方法进行近似计算。</p>\n<script type=\"math/tex; mode=display\">\\theta = \\det(M)-\\alpha\\text{trace}(M)^2 = \\lambda_1\\lambda_2-\\alpha(\\lambda_1+\\lambda_2)^2</script><p><img src=\"/img/corner_judge_2.png\" alt=\"使用theta判定\"></p>\n<p>为了减弱噪声的影响，常常使用gaussian窗函数。如下式所示：</p>\n<script type=\"math/tex; mode=display\">w(x,y) = \\exp(-(x^2+y^2)/2\\sigma^2)</script>","excerpt":"<p>feature是对图像的描述。比如，图像整体灰度值的均值和方差就可以作为feature。如果我们想在一副图像中检测足球时，可以使用滑动窗方法，逐一检查窗口内的灰度值分布是否和一张给定的典型黑白格子足球相似。可想而知，这种方法的性能一定让人捉急。而在image matching问题中，常常需要将不同视角的同一目标物进行matching，进而计算相机转过的角度。这在SLAM问题中很有意义。如下图所示，同一处景观，在不同摄影师的镜头下，不仅视角不同，而且明暗变化也有很多差别，右侧的图暖色调更浓。这也告诉我们，上面提到的只使用全局图像灰度值来做feature的做法有多么不靠谱。</p>\n<p>那么，让我们脱离全局特征，转而将注意力集中在局部特征上。这是因为使用局部特征能够更好地处理图像中的遮挡、变形等情况，而且我们的研究对象常常是图像中的部分区域而不是图像整体。更特殊地，在这一讲中，我们主要探究key point作为local feature的描述，并介绍Harris角点检测方法。在下一节中介绍SIFT特征。</p>\n<p><img src=\"/img/image_matching_hard.png\" alt=\"image matching example\"></p>","more":"<h2 id=\"Harris角点\"><a href=\"#Harris角点\" class=\"headerlink\" title=\"Harris角点\"></a>Harris角点</h2><p>角点，即corner，和edge类似，区别在于其在两个方向上都有较为剧烈的灰度变化（而edge只在某一个方向上灰度值变化剧烈）。如图所示。<br><img src=\"/img/what_is_corner.png\" alt=\"what is corner\"></p>\n<p><a href=\"http://www.bmva.org/bmvc/1988/avc-88-023.pdf\">Harris角点</a>得名于其发明者Harris，是一种常见的角点检测方法。<br>给定观察窗口大小，计算平移后窗口内各个像素差值的加权平方和，如下式。</p>\n<script type=\"math/tex; mode=display\">E(u,v) = \\sum_x\\sum_yw(x,y)[I(x+u, y+v) - I(x,y)]^2</script><p>其中，窗口加权函数$w$可以取做门限函数或gaussian函数。如图所示。<br><img src=\"/img/corner_window_fun.png\" alt=\"window function\"></p>\n<p>使用泰勒级数展开，并忽略非线性项，我们有</p>\n<script type=\"math/tex; mode=display\">I(x+u,y+v) = I(x,y) + I_x(x,y)u+I_y(x,y)v</script><p>所以上式可以写成（线性二次型写成了矩阵形式），</p>\n<script type=\"math/tex; mode=display\">E(u,v) = \\sum_{x,y}w(I_xu+I_yv)^2 = \\begin{bmatrix}u&v\\end{bmatrix}M\\begin{bmatrix}u\\\\\\\\v\\end{bmatrix}</script><p>其中，</p>\n<script type=\"math/tex; mode=display\">M = w\\begin{bmatrix}I_x^2& I_xI_y\\\\\\\\I_xI_y&I_y^2\\end{bmatrix}</script><p>当使用门限函数时，权值$w_{i,j} = 1$，则，</p>\n<script type=\"math/tex; mode=display\">M = \\begin{bmatrix}\\sum I_xI_x& \\sum I_xI_y\\\\\\\\\\sum I_xI_y&\\sum I_yI_y\\end{bmatrix} = \\sum \\begin{bmatrix}I_x \\\\\\\\I_y\\end{bmatrix}\\begin{bmatrix}I_x &I_y\\end{bmatrix}</script><p>当corner与xy坐标轴对齐时候，如下图所示。由于在黑色观察窗口内，只有上侧和左侧存在边缘，且在上边缘，$I_y$很大，而$I_x=0$，在左侧边缘，$I_x$很大而$I_y = 0$，所以，矩阵</p>\n<script type=\"math/tex; mode=display\">M = \\begin{bmatrix}\\lambda_1 & 0 \\\\\\\\ 0&\\lambda_2 \\end{bmatrix}</script><p><img src=\"/img/corner_type_1.png\" alt=\"M为对角阵\"></p>\n<p>当corner与坐标轴没有对齐时，经过旋转变换就可以将其转换到与坐标轴对齐的角度，而这种旋转操作可以使用矩阵的相似化来表示（其实是二次型的化简，可以使用合同变换，而旋转变换的矩阵是酉矩阵，转置即为矩阵的逆，所以也是相似变换）。也就是说，矩阵$M$相似于某个对角阵。</p>\n<script type=\"math/tex; mode=display\">M = R^{-1}\\Sigma R, \\text{其中}\\Sigma = \\begin{bmatrix}\\lambda_1&0\\\\\\\\0&\\lambda_2\\end{bmatrix}</script><p>所以，可以根据下面这张图利用矩阵$M$的特征值来判定角点和边缘点。当两个特征值都较大时为角点；当某个分量近似为0而另一个分量较大时，可以判定为边缘点（因为某个方向的导数为0）；当两个特征值都近似为0时，说明是普通点（flat point）。（课件原图如此，空缺的问号处应分别为$\\lambda_1$和$\\lambda_2$）。<br><img src=\"/img/corner_judge.png\" alt=\"使用M矩阵特征值判定\"></p>\n<p> 然而矩阵的特征值计算较为复杂，所以使用下面的方法进行近似计算。</p>\n<script type=\"math/tex; mode=display\">\\theta = \\det(M)-\\alpha\\text{trace}(M)^2 = \\lambda_1\\lambda_2-\\alpha(\\lambda_1+\\lambda_2)^2</script><p><img src=\"/img/corner_judge_2.png\" alt=\"使用theta判定\"></p>\n<p>为了减弱噪声的影响，常常使用gaussian窗函数。如下式所示：</p>\n<script type=\"math/tex; mode=display\">w(x,y) = \\exp(-(x^2+y^2)/2\\sigma^2)</script>"},{"title":"CS131-KMeans聚类","date":"2017-02-05T15:07:00.000Z","_content":"[K-Means聚类](https://zh.wikipedia.org/wiki/K-平均算法)是把n个点（可以是样本的一次观察或一个实例）划分到$k$个聚类中，使得每个点都属于离他最近的均值（此即聚类中心）对应的聚类，以之作为聚类的标准，满足最小化聚类中心和属于该中心的数据点之间的平方距离和（Sum of Square Distance，SSD）。\n$$\\text{SSD} = \\sum_{i=1}^{k}\\sum_{x\\in c_i}(x-c_i)^2$$\n![K-Means Demo](/img/kmeans_demo.png)\n\n<!-- more -->\n## 目标函数\nK-Means方法实际上需要确定两个参数，$c^\\ast$和$\\delta^\\ast$。其中$c\\_{i}^\\ast$代表各个聚类中心的位置，$\\delta\\_{ij}^\\ast$的取值为$\\lbrace 0,1\\rbrace$，代表点$x_j$是否被分到了第$i$个聚类中心。\n\n那么，目标函数可以写成如下的形式。\n$$c^\\ast, \\delta^\\ast = \\arg\\min_{c,\\delta} \\frac{1}{N}\\sum_{j=1}^{N}\\sum_{i=1}^{k}\\delta_{i,j}(c_i-x_j)^2$$\n\n然而这样一来，造成了一种很尴尬的局面。一方面，要想优化$c\\_i$，需要我们给定每个点所属的类；另一方面，优化$\\delta\\_{i,j}$，需要我们给定聚类中心。由于这两个优化变量之间是纠缠在一起的，K-Means算法如何能够达到收敛就变成了一个讨论“先有鸡还是先有蛋”的问题了。\n\n实际使用中，K-Means的迭代过程，实际是EM算法的一个特例。\n\n## 算法流程\nK-Means算法的流程如下所示。\n![K-Means算法流程](/img/kmeans_algorithm.png)\n\n假设我们有$N$个样本点，$\\lbrace x_1, \\dots, x_N\\rbrace, x_i\\in\\mathbb{R}^D$，并给出聚类数目$k$。\n\n首先，随机选取一系列的聚类中心点$\\mu_i, i = 1,\\dots, k$。接着，我们按照距离最近的原则为每个数据点指定相应的聚类中心并计算新的数据点均值更新聚类中心。如此这般，直到收敛。\n\n## 算法细节\n### 初始化\n上面的K-Means方法对初始值很敏感。朴素的初始化方法是直接在数据点中随机抽取$k$个作为聚类初始中心点。不够好的初始值可能造成收敛速度很慢或者聚类失败。下面介绍两种改进方法。\n- kmeans++方法，尽可能地使初始聚类中心分散开（spread-out）。\n  这种方法首先随机产生一个初始聚类中心点，然后按照概率$P = \\omega(x-c_i)^2$选取其他的聚类中心点。其中$\\omega$是归一化系数。\n\n- 多次初始化，保留最好的结果。\n\n### K值的选取\n在上面的讨论中，我们一直把$k$当做已经给定的参数。但是在实际操作中，聚类类别数通常都是未知的。如何确定超参数$k$呢？\n\n我们可以使用不同的$k$值进行试验，并作出不同试验收敛后目标函数随$k$的变化。如下图所示，分别使用$1$到$4$之间的几个数字作为$k$值，曲线在$k=2$处有一个较为明显的弯曲点（elbow point）。故确定$k$取值为2。\n![参数K的确定](/img/kmeans_object_fun_vs_k.png)\n\n### 距离的度量\n目标函数是各个cluster中的点与其中心点的距离均值。其中就涉及到了“距离”的量度。下面是几种较为常用的距离度量方法。\n- 欧几里得距离（最为常用）\n- 余弦距离（向量的夹角）\n- 核函数（[Kernel K-Means](http://www.public.asu.edu/~jye02/CLASSES/Fall-2005/PAPERS/kdd_spectral_kernelkmeans.pdf)）\n\n### 迭代终止条件\n当判断算法已经收敛时，退出迭代。常用的迭代终止条件如下：\n- 达到了预先给定的最大迭代次数\n- 在将数据点assign到不同聚类中心时，和上轮结果没有变化（说明已经收敛）\n- 目标函数（平均的距离）下降小于阈值\n\n## 基于K-Means的图像分割\n图像分割中可以使用K-Means算法。我们首先将一副图像转换为特征空间（Feature Space）。例如使用灰度或者RGB的值作为特征向量，就得到了1D或者3D的特征空间。之后，可以基于Feature Space上各点的相似度，将它们聚类。这样，就完成了对原始图像的分割操作。如下所示。\n![图像分割结果1](/img/kmeans_image_seg_via_intensity.png)\n\n然而，上述方法有一个缺点。那就是在真实图像中，属于同一个物体的像素点常常在空间上是相关的（Spatial Coherent）。而上述方法没有考虑到这一点。我们可以根据颜色亮度和空间上的距离构建Feature Space，获得更加合理的分割效果。\n\n在2012年PAMI上有一篇文章[SLIC Superpixels Compared to State-of-the-art Superpixel Methods](https://infoscience.epfl.ch/record/177415/files/Superpixel_PAMI2011-2.pdf)介绍了使用K-Means进行超像素分割的相关内容，可以参考。这里不再赘述了。\n\n## 优点和不足\n作为非监督学习中的经典算法，K-Means的一大优点便是实现简单，速度较快，最小化条件方差（conditional variance，good represention of data，这里没有具体解释，不太懂）。\n\n它的缺点主要有：\n- 对outlier比较敏感。如下图所示。由于outlier的存在，右边的cluster硬是被拉长了。这方面，K-Medians更有鲁棒性。\n![outlier是个大麻烦](/img/kmeans_sensitive_to_outlier.png)\n- 每个点只能确定地属于或不属于某一个cluster。可以参考高斯混合模型和Fuzzy K-Means的soft assignment。\n- 在round shape情况下性能比较好（想想圆形和方形各点的欧几里得距离）。而且每个cluster最好数据点数目和分布的密度相近。\n- 如果数据点有非凸形状分布，算法表现糟糕。可以参考谱聚类（Spectral Clustering）或者kernel K-Means。\n\n针对K-Means，也有不少相关改进工作，参考下面这幅图吧。\n![K-Means Scaling Up](/img/kmeans_scaling_up.png)\n\n## MATLAB实验\n下面是我自己写的简单的K-Means demo。首先，在数据产生环节，确定$K = 3$个cluster的中心位置，并随机产生$N$个数据点，并使用`scatter`函数做出散点图。\n\n代码中的主要部分为`my_kmeans`函数的实现（为了不与内建的kmeans函数重名，故加上了`my`前缀）。在此函数内部，首先随机产生$K$个聚类中心，并对数据点进行初始的指定。接着，在迭代过程中，不断计算均值来更新聚类中心和assignment，当达到最大迭代次数或者相邻两次迭代中assignment的值不再变化为止，并计算对应的目标函数$J$的值。\n\n注意到该函数初始确定聚类中心时有一段注释的代码，该段代码用于debug时，指定聚类中心，去除随机性，以验证后续K-Means迭代的正确性。\n\n``` matlab\n%% generate data\nK = 3;   % number of clusters\npos = [-5, 5; 0, 1; 3, 6];  % position of cluster centers\nN = 20;    % number of data points\nR = 3;     % radius of clusters\ndata = zeros(N, 2);    % data\nclass = zeros(N, 1);   % index of cluster\n\nfor i = 1:N\n    idx = randi(3, 1);\n    dr = R*rand();\n    data(i, :) = pos(idx, :) + [dr*cos(rand()*2*pi), dr*sin(rand()*2*pi)];\n    class(i) = idx;\nend\n\n%% visualization data points\nfigure\nhold on\ncolor = [1,0,0; 0,1,0; 0,0,1];\nfor i = 1:K\n    x = data(class == i, 1);\n    y = data(class == i, 2);\n    scatter(x, y, 150, repmat(color(i,:), [length(x), 1]), 'filled');\nend\n\n%% K-Means\nbest_J = 1E100;\nbest_idx = 0;\nfor times = 1:5  % 5 times experiments to choose the best result\n    [mu, assignment, J] = my_kmeans(data, K);\n    if best_J > J\n        best_idx = times;\n        best_J = J;\n    end\n    fprintf('%d experiment: J = %f\\n', times, J);\n    disp(mu);\nend\nfprintf('best: %d experiment: J = %f\\n', best_idx, best_J);\n\n%% basic functions\nfunction J = ssd(X, mu, assignment)\n% sum of square distance\n% X -- data, N*D matrix\n% mu -- centers of clusters, K*D matrix\n% assignment -- current assignment of data to clusters\nJ = 0;\nK = size(mu, 1);\nfor k = 1:K\n    x_k = X(assignment == k, :);\n    mu_k = mu(k, :);\n    err2 = bsxfun(@minus, x_k, mu_k).^2;\n    J = J + sum(err2(:));\nend\nJ = J / size(X, 1);\nend\n\nfunction mu = compute_mu(X, assignment, K)\nmu = zeros(K, size(X, 2));\nfor k = 1:K\n    x_k = X(assignment == k, :);\n    mu(k, :) = mean(x_k, 1);\nend\nend\n\nfunction assignment = assign(X, mu)\n% assign data points to clusters\nN = size(X, 1);\nassignment = zeros(N, 1);\nfor i = 1:N\n    x = X(i, :);\n    err2 = bsxfun(@minus, x, mu).^2;\n    dis = sum(err2, 2);\n    [~, idx] = min(dis);\n    assignment(i) = idx;\nend\nend\n\nfunction [mu, assignment, J] = my_kmeans(X, K)\nN = size(X, 1);\nassignment = zeros(N, 1);\nidx = randsample(N, K);\nmu = X(idx, :);\n\n% for i = 1:K\n%     for j = 1:N\n%         if assignment_gt(j) == i\n%             mu(i,:) = X(j,:);\n%             break;\n%         end\n%     end\n% end\nfigure\nhold on\ncolor = [1,0,0; 0,1,0; 0,0,1];\nscatter(mu(:,1), mu(:,2), 200, color, 'd');\nfor iter = 1:20\n    assignment_prev = assignment;\n    assignment = assign(X, mu);\n    if assignment == assignment_prev\n        break;\n    end\n    mu_prev = mu;\n    mu = compute_mu(X, assignment, K);\n    scatter(mu(:, 1), mu(:, 2), 200, color, 'd');\n    MU = zeros(2*K, 2);\n    MU(1:2:end, :) = mu_prev;\n    MU(2:2:end, :) = mu;\n    mu_x = reshape(MU(:, 1), [], K);\n    mu_y = reshape(MU(:, 2), [], K);\n    plot(mu_x, mu_y, 'k-.');\n\nend\nfor i = 1:K\n    x = X(assignment == i, 1);\n    y = X(assignment == i, 2);\n    scatter(x, y, 150, repmat(color(i,:), [length(x), 1]), 'filled');\nend\nJ = ssd(X, mu, assignment);\nend\n```\n\n在demo中，随机选取初始化的聚类中心，重复进行$5$次实验。下图是产生的原始数据的散点图。用不同的颜色标明了cluster。\n![K-Means聚类](/img/kmeans_data_demo.png)\n\n下图是某次的实验结果，其中菱形和它们之间的连线指示了聚类中心的变化。注意，颜色只是用来区别不同的cluster，和上图并没有对应关系。可以看到，初始化时绿色标注的cluster的中心（也即是上图中的红色cluster）虽然偏离了很远，但是在迭代过程中也能实现纠正。\n\n![K-Means聚类](/img/kmeans_success.png)\n\n再换个大点的数据集来做，效果貌似还不错~\n![大一些](/img/kmeans_bigger_demo.png)\n## PS\n这里插一句与本文内容完全无关的一个tip：在Markdown中使用LaTex时，如果在内联形式（也就是行内使用两个美元符号插入公式）时，如果有多个下划线，那么Markdown有时会将下划线之间的部分认为是自己的斜体标记，如下所示：\n```\n$c_{i}^\\ast$ XXX $\\delta_{ij}^\\ast$\n```\n它的显示效果为$c_{i}^\\ast$ XXX $\\delta_{ij}^\\ast$。\n\n这时候，只需在下划线前加入反斜线进行转义即可（虽然略显麻烦）。如下所示：\n```\n$c\\_{i}^\\ast$ XXX $\\delta\\_{ij}^\\ast$\n```\n它的显示效果为$c\\_{i}^\\ast$ XXX $\\delta\\_{ij}^\\ast$。\n\n具体分析可以参见[博客](http://lukang.me/2014/mathjax-for-hexo.html)。\n","source":"_posts/cs131-kmeans.md","raw":"---\ntitle: CS131-KMeans聚类\ndate: 2017-02-05 23:07:00\ntags:\n    - cs131\n    - 公开课\n---\n[K-Means聚类](https://zh.wikipedia.org/wiki/K-平均算法)是把n个点（可以是样本的一次观察或一个实例）划分到$k$个聚类中，使得每个点都属于离他最近的均值（此即聚类中心）对应的聚类，以之作为聚类的标准，满足最小化聚类中心和属于该中心的数据点之间的平方距离和（Sum of Square Distance，SSD）。\n$$\\text{SSD} = \\sum_{i=1}^{k}\\sum_{x\\in c_i}(x-c_i)^2$$\n![K-Means Demo](/img/kmeans_demo.png)\n\n<!-- more -->\n## 目标函数\nK-Means方法实际上需要确定两个参数，$c^\\ast$和$\\delta^\\ast$。其中$c\\_{i}^\\ast$代表各个聚类中心的位置，$\\delta\\_{ij}^\\ast$的取值为$\\lbrace 0,1\\rbrace$，代表点$x_j$是否被分到了第$i$个聚类中心。\n\n那么，目标函数可以写成如下的形式。\n$$c^\\ast, \\delta^\\ast = \\arg\\min_{c,\\delta} \\frac{1}{N}\\sum_{j=1}^{N}\\sum_{i=1}^{k}\\delta_{i,j}(c_i-x_j)^2$$\n\n然而这样一来，造成了一种很尴尬的局面。一方面，要想优化$c\\_i$，需要我们给定每个点所属的类；另一方面，优化$\\delta\\_{i,j}$，需要我们给定聚类中心。由于这两个优化变量之间是纠缠在一起的，K-Means算法如何能够达到收敛就变成了一个讨论“先有鸡还是先有蛋”的问题了。\n\n实际使用中，K-Means的迭代过程，实际是EM算法的一个特例。\n\n## 算法流程\nK-Means算法的流程如下所示。\n![K-Means算法流程](/img/kmeans_algorithm.png)\n\n假设我们有$N$个样本点，$\\lbrace x_1, \\dots, x_N\\rbrace, x_i\\in\\mathbb{R}^D$，并给出聚类数目$k$。\n\n首先，随机选取一系列的聚类中心点$\\mu_i, i = 1,\\dots, k$。接着，我们按照距离最近的原则为每个数据点指定相应的聚类中心并计算新的数据点均值更新聚类中心。如此这般，直到收敛。\n\n## 算法细节\n### 初始化\n上面的K-Means方法对初始值很敏感。朴素的初始化方法是直接在数据点中随机抽取$k$个作为聚类初始中心点。不够好的初始值可能造成收敛速度很慢或者聚类失败。下面介绍两种改进方法。\n- kmeans++方法，尽可能地使初始聚类中心分散开（spread-out）。\n  这种方法首先随机产生一个初始聚类中心点，然后按照概率$P = \\omega(x-c_i)^2$选取其他的聚类中心点。其中$\\omega$是归一化系数。\n\n- 多次初始化，保留最好的结果。\n\n### K值的选取\n在上面的讨论中，我们一直把$k$当做已经给定的参数。但是在实际操作中，聚类类别数通常都是未知的。如何确定超参数$k$呢？\n\n我们可以使用不同的$k$值进行试验，并作出不同试验收敛后目标函数随$k$的变化。如下图所示，分别使用$1$到$4$之间的几个数字作为$k$值，曲线在$k=2$处有一个较为明显的弯曲点（elbow point）。故确定$k$取值为2。\n![参数K的确定](/img/kmeans_object_fun_vs_k.png)\n\n### 距离的度量\n目标函数是各个cluster中的点与其中心点的距离均值。其中就涉及到了“距离”的量度。下面是几种较为常用的距离度量方法。\n- 欧几里得距离（最为常用）\n- 余弦距离（向量的夹角）\n- 核函数（[Kernel K-Means](http://www.public.asu.edu/~jye02/CLASSES/Fall-2005/PAPERS/kdd_spectral_kernelkmeans.pdf)）\n\n### 迭代终止条件\n当判断算法已经收敛时，退出迭代。常用的迭代终止条件如下：\n- 达到了预先给定的最大迭代次数\n- 在将数据点assign到不同聚类中心时，和上轮结果没有变化（说明已经收敛）\n- 目标函数（平均的距离）下降小于阈值\n\n## 基于K-Means的图像分割\n图像分割中可以使用K-Means算法。我们首先将一副图像转换为特征空间（Feature Space）。例如使用灰度或者RGB的值作为特征向量，就得到了1D或者3D的特征空间。之后，可以基于Feature Space上各点的相似度，将它们聚类。这样，就完成了对原始图像的分割操作。如下所示。\n![图像分割结果1](/img/kmeans_image_seg_via_intensity.png)\n\n然而，上述方法有一个缺点。那就是在真实图像中，属于同一个物体的像素点常常在空间上是相关的（Spatial Coherent）。而上述方法没有考虑到这一点。我们可以根据颜色亮度和空间上的距离构建Feature Space，获得更加合理的分割效果。\n\n在2012年PAMI上有一篇文章[SLIC Superpixels Compared to State-of-the-art Superpixel Methods](https://infoscience.epfl.ch/record/177415/files/Superpixel_PAMI2011-2.pdf)介绍了使用K-Means进行超像素分割的相关内容，可以参考。这里不再赘述了。\n\n## 优点和不足\n作为非监督学习中的经典算法，K-Means的一大优点便是实现简单，速度较快，最小化条件方差（conditional variance，good represention of data，这里没有具体解释，不太懂）。\n\n它的缺点主要有：\n- 对outlier比较敏感。如下图所示。由于outlier的存在，右边的cluster硬是被拉长了。这方面，K-Medians更有鲁棒性。\n![outlier是个大麻烦](/img/kmeans_sensitive_to_outlier.png)\n- 每个点只能确定地属于或不属于某一个cluster。可以参考高斯混合模型和Fuzzy K-Means的soft assignment。\n- 在round shape情况下性能比较好（想想圆形和方形各点的欧几里得距离）。而且每个cluster最好数据点数目和分布的密度相近。\n- 如果数据点有非凸形状分布，算法表现糟糕。可以参考谱聚类（Spectral Clustering）或者kernel K-Means。\n\n针对K-Means，也有不少相关改进工作，参考下面这幅图吧。\n![K-Means Scaling Up](/img/kmeans_scaling_up.png)\n\n## MATLAB实验\n下面是我自己写的简单的K-Means demo。首先，在数据产生环节，确定$K = 3$个cluster的中心位置，并随机产生$N$个数据点，并使用`scatter`函数做出散点图。\n\n代码中的主要部分为`my_kmeans`函数的实现（为了不与内建的kmeans函数重名，故加上了`my`前缀）。在此函数内部，首先随机产生$K$个聚类中心，并对数据点进行初始的指定。接着，在迭代过程中，不断计算均值来更新聚类中心和assignment，当达到最大迭代次数或者相邻两次迭代中assignment的值不再变化为止，并计算对应的目标函数$J$的值。\n\n注意到该函数初始确定聚类中心时有一段注释的代码，该段代码用于debug时，指定聚类中心，去除随机性，以验证后续K-Means迭代的正确性。\n\n``` matlab\n%% generate data\nK = 3;   % number of clusters\npos = [-5, 5; 0, 1; 3, 6];  % position of cluster centers\nN = 20;    % number of data points\nR = 3;     % radius of clusters\ndata = zeros(N, 2);    % data\nclass = zeros(N, 1);   % index of cluster\n\nfor i = 1:N\n    idx = randi(3, 1);\n    dr = R*rand();\n    data(i, :) = pos(idx, :) + [dr*cos(rand()*2*pi), dr*sin(rand()*2*pi)];\n    class(i) = idx;\nend\n\n%% visualization data points\nfigure\nhold on\ncolor = [1,0,0; 0,1,0; 0,0,1];\nfor i = 1:K\n    x = data(class == i, 1);\n    y = data(class == i, 2);\n    scatter(x, y, 150, repmat(color(i,:), [length(x), 1]), 'filled');\nend\n\n%% K-Means\nbest_J = 1E100;\nbest_idx = 0;\nfor times = 1:5  % 5 times experiments to choose the best result\n    [mu, assignment, J] = my_kmeans(data, K);\n    if best_J > J\n        best_idx = times;\n        best_J = J;\n    end\n    fprintf('%d experiment: J = %f\\n', times, J);\n    disp(mu);\nend\nfprintf('best: %d experiment: J = %f\\n', best_idx, best_J);\n\n%% basic functions\nfunction J = ssd(X, mu, assignment)\n% sum of square distance\n% X -- data, N*D matrix\n% mu -- centers of clusters, K*D matrix\n% assignment -- current assignment of data to clusters\nJ = 0;\nK = size(mu, 1);\nfor k = 1:K\n    x_k = X(assignment == k, :);\n    mu_k = mu(k, :);\n    err2 = bsxfun(@minus, x_k, mu_k).^2;\n    J = J + sum(err2(:));\nend\nJ = J / size(X, 1);\nend\n\nfunction mu = compute_mu(X, assignment, K)\nmu = zeros(K, size(X, 2));\nfor k = 1:K\n    x_k = X(assignment == k, :);\n    mu(k, :) = mean(x_k, 1);\nend\nend\n\nfunction assignment = assign(X, mu)\n% assign data points to clusters\nN = size(X, 1);\nassignment = zeros(N, 1);\nfor i = 1:N\n    x = X(i, :);\n    err2 = bsxfun(@minus, x, mu).^2;\n    dis = sum(err2, 2);\n    [~, idx] = min(dis);\n    assignment(i) = idx;\nend\nend\n\nfunction [mu, assignment, J] = my_kmeans(X, K)\nN = size(X, 1);\nassignment = zeros(N, 1);\nidx = randsample(N, K);\nmu = X(idx, :);\n\n% for i = 1:K\n%     for j = 1:N\n%         if assignment_gt(j) == i\n%             mu(i,:) = X(j,:);\n%             break;\n%         end\n%     end\n% end\nfigure\nhold on\ncolor = [1,0,0; 0,1,0; 0,0,1];\nscatter(mu(:,1), mu(:,2), 200, color, 'd');\nfor iter = 1:20\n    assignment_prev = assignment;\n    assignment = assign(X, mu);\n    if assignment == assignment_prev\n        break;\n    end\n    mu_prev = mu;\n    mu = compute_mu(X, assignment, K);\n    scatter(mu(:, 1), mu(:, 2), 200, color, 'd');\n    MU = zeros(2*K, 2);\n    MU(1:2:end, :) = mu_prev;\n    MU(2:2:end, :) = mu;\n    mu_x = reshape(MU(:, 1), [], K);\n    mu_y = reshape(MU(:, 2), [], K);\n    plot(mu_x, mu_y, 'k-.');\n\nend\nfor i = 1:K\n    x = X(assignment == i, 1);\n    y = X(assignment == i, 2);\n    scatter(x, y, 150, repmat(color(i,:), [length(x), 1]), 'filled');\nend\nJ = ssd(X, mu, assignment);\nend\n```\n\n在demo中，随机选取初始化的聚类中心，重复进行$5$次实验。下图是产生的原始数据的散点图。用不同的颜色标明了cluster。\n![K-Means聚类](/img/kmeans_data_demo.png)\n\n下图是某次的实验结果，其中菱形和它们之间的连线指示了聚类中心的变化。注意，颜色只是用来区别不同的cluster，和上图并没有对应关系。可以看到，初始化时绿色标注的cluster的中心（也即是上图中的红色cluster）虽然偏离了很远，但是在迭代过程中也能实现纠正。\n\n![K-Means聚类](/img/kmeans_success.png)\n\n再换个大点的数据集来做，效果貌似还不错~\n![大一些](/img/kmeans_bigger_demo.png)\n## PS\n这里插一句与本文内容完全无关的一个tip：在Markdown中使用LaTex时，如果在内联形式（也就是行内使用两个美元符号插入公式）时，如果有多个下划线，那么Markdown有时会将下划线之间的部分认为是自己的斜体标记，如下所示：\n```\n$c_{i}^\\ast$ XXX $\\delta_{ij}^\\ast$\n```\n它的显示效果为$c_{i}^\\ast$ XXX $\\delta_{ij}^\\ast$。\n\n这时候，只需在下划线前加入反斜线进行转义即可（虽然略显麻烦）。如下所示：\n```\n$c\\_{i}^\\ast$ XXX $\\delta\\_{ij}^\\ast$\n```\n它的显示效果为$c\\_{i}^\\ast$ XXX $\\delta\\_{ij}^\\ast$。\n\n具体分析可以参见[博客](http://lukang.me/2014/mathjax-for-hexo.html)。\n","slug":"cs131-kmeans","published":1,"updated":"2017-02-26T12:09:28.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cizve07ab000cl61hbw5419kb","content":"<p><a href=\"https://zh.wikipedia.org/wiki/K-平均算法\" target=\"_blank\" rel=\"external\">K-Means聚类</a>是把n个点（可以是样本的一次观察或一个实例）划分到$k$个聚类中，使得每个点都属于离他最近的均值（此即聚类中心）对应的聚类，以之作为聚类的标准，满足最小化聚类中心和属于该中心的数据点之间的平方距离和（Sum of Square Distance，SSD）。</p>\n<script type=\"math/tex; mode=display\">\\text{SSD} = \\sum_{i=1}^{k}\\sum_{x\\in c_i}(x-c_i)^2</script><p><img src=\"/img/kmeans_demo.png\" alt=\"K-Means Demo\"></p>\n<a id=\"more\"></a>\n<h2 id=\"目标函数\"><a href=\"#目标函数\" class=\"headerlink\" title=\"目标函数\"></a>目标函数</h2><p>K-Means方法实际上需要确定两个参数，$c^\\ast$和$\\delta^\\ast$。其中$c_{i}^\\ast$代表各个聚类中心的位置，$\\delta_{ij}^\\ast$的取值为$\\lbrace 0,1\\rbrace$，代表点$x_j$是否被分到了第$i$个聚类中心。</p>\n<p>那么，目标函数可以写成如下的形式。</p>\n<script type=\"math/tex; mode=display\">c^\\ast, \\delta^\\ast = \\arg\\min_{c,\\delta} \\frac{1}{N}\\sum_{j=1}^{N}\\sum_{i=1}^{k}\\delta_{i,j}(c_i-x_j)^2</script><p>然而这样一来，造成了一种很尴尬的局面。一方面，要想优化$c_i$，需要我们给定每个点所属的类；另一方面，优化$\\delta_{i,j}$，需要我们给定聚类中心。由于这两个优化变量之间是纠缠在一起的，K-Means算法如何能够达到收敛就变成了一个讨论“先有鸡还是先有蛋”的问题了。</p>\n<p>实际使用中，K-Means的迭代过程，实际是EM算法的一个特例。</p>\n<h2 id=\"算法流程\"><a href=\"#算法流程\" class=\"headerlink\" title=\"算法流程\"></a>算法流程</h2><p>K-Means算法的流程如下所示。<br><img src=\"/img/kmeans_algorithm.png\" alt=\"K-Means算法流程\"></p>\n<p>假设我们有$N$个样本点，$\\lbrace x_1, \\dots, x_N\\rbrace, x_i\\in\\mathbb{R}^D$，并给出聚类数目$k$。</p>\n<p>首先，随机选取一系列的聚类中心点$\\mu_i, i = 1,\\dots, k$。接着，我们按照距离最近的原则为每个数据点指定相应的聚类中心并计算新的数据点均值更新聚类中心。如此这般，直到收敛。</p>\n<h2 id=\"算法细节\"><a href=\"#算法细节\" class=\"headerlink\" title=\"算法细节\"></a>算法细节</h2><h3 id=\"初始化\"><a href=\"#初始化\" class=\"headerlink\" title=\"初始化\"></a>初始化</h3><p>上面的K-Means方法对初始值很敏感。朴素的初始化方法是直接在数据点中随机抽取$k$个作为聚类初始中心点。不够好的初始值可能造成收敛速度很慢或者聚类失败。下面介绍两种改进方法。</p>\n<ul>\n<li><p>kmeans++方法，尽可能地使初始聚类中心分散开（spread-out）。<br>这种方法首先随机产生一个初始聚类中心点，然后按照概率$P = \\omega(x-c_i)^2$选取其他的聚类中心点。其中$\\omega$是归一化系数。</p>\n</li>\n<li><p>多次初始化，保留最好的结果。</p>\n</li>\n</ul>\n<h3 id=\"K值的选取\"><a href=\"#K值的选取\" class=\"headerlink\" title=\"K值的选取\"></a>K值的选取</h3><p>在上面的讨论中，我们一直把$k$当做已经给定的参数。但是在实际操作中，聚类类别数通常都是未知的。如何确定超参数$k$呢？</p>\n<p>我们可以使用不同的$k$值进行试验，并作出不同试验收敛后目标函数随$k$的变化。如下图所示，分别使用$1$到$4$之间的几个数字作为$k$值，曲线在$k=2$处有一个较为明显的弯曲点（elbow point）。故确定$k$取值为2。<br><img src=\"/img/kmeans_object_fun_vs_k.png\" alt=\"参数K的确定\"></p>\n<h3 id=\"距离的度量\"><a href=\"#距离的度量\" class=\"headerlink\" title=\"距离的度量\"></a>距离的度量</h3><p>目标函数是各个cluster中的点与其中心点的距离均值。其中就涉及到了“距离”的量度。下面是几种较为常用的距离度量方法。</p>\n<ul>\n<li>欧几里得距离（最为常用）</li>\n<li>余弦距离（向量的夹角）</li>\n<li>核函数（<a href=\"http://www.public.asu.edu/~jye02/CLASSES/Fall-2005/PAPERS/kdd_spectral_kernelkmeans.pdf\" target=\"_blank\" rel=\"external\">Kernel K-Means</a>）</li>\n</ul>\n<h3 id=\"迭代终止条件\"><a href=\"#迭代终止条件\" class=\"headerlink\" title=\"迭代终止条件\"></a>迭代终止条件</h3><p>当判断算法已经收敛时，退出迭代。常用的迭代终止条件如下：</p>\n<ul>\n<li>达到了预先给定的最大迭代次数</li>\n<li>在将数据点assign到不同聚类中心时，和上轮结果没有变化（说明已经收敛）</li>\n<li>目标函数（平均的距离）下降小于阈值</li>\n</ul>\n<h2 id=\"基于K-Means的图像分割\"><a href=\"#基于K-Means的图像分割\" class=\"headerlink\" title=\"基于K-Means的图像分割\"></a>基于K-Means的图像分割</h2><p>图像分割中可以使用K-Means算法。我们首先将一副图像转换为特征空间（Feature Space）。例如使用灰度或者RGB的值作为特征向量，就得到了1D或者3D的特征空间。之后，可以基于Feature Space上各点的相似度，将它们聚类。这样，就完成了对原始图像的分割操作。如下所示。<br><img src=\"/img/kmeans_image_seg_via_intensity.png\" alt=\"图像分割结果1\"></p>\n<p>然而，上述方法有一个缺点。那就是在真实图像中，属于同一个物体的像素点常常在空间上是相关的（Spatial Coherent）。而上述方法没有考虑到这一点。我们可以根据颜色亮度和空间上的距离构建Feature Space，获得更加合理的分割效果。</p>\n<p>在2012年PAMI上有一篇文章<a href=\"https://infoscience.epfl.ch/record/177415/files/Superpixel_PAMI2011-2.pdf\" target=\"_blank\" rel=\"external\">SLIC Superpixels Compared to State-of-the-art Superpixel Methods</a>介绍了使用K-Means进行超像素分割的相关内容，可以参考。这里不再赘述了。</p>\n<h2 id=\"优点和不足\"><a href=\"#优点和不足\" class=\"headerlink\" title=\"优点和不足\"></a>优点和不足</h2><p>作为非监督学习中的经典算法，K-Means的一大优点便是实现简单，速度较快，最小化条件方差（conditional variance，good represention of data，这里没有具体解释，不太懂）。</p>\n<p>它的缺点主要有：</p>\n<ul>\n<li>对outlier比较敏感。如下图所示。由于outlier的存在，右边的cluster硬是被拉长了。这方面，K-Medians更有鲁棒性。<br><img src=\"/img/kmeans_sensitive_to_outlier.png\" alt=\"outlier是个大麻烦\"></li>\n<li>每个点只能确定地属于或不属于某一个cluster。可以参考高斯混合模型和Fuzzy K-Means的soft assignment。</li>\n<li>在round shape情况下性能比较好（想想圆形和方形各点的欧几里得距离）。而且每个cluster最好数据点数目和分布的密度相近。</li>\n<li>如果数据点有非凸形状分布，算法表现糟糕。可以参考谱聚类（Spectral Clustering）或者kernel K-Means。</li>\n</ul>\n<p>针对K-Means，也有不少相关改进工作，参考下面这幅图吧。<br><img src=\"/img/kmeans_scaling_up.png\" alt=\"K-Means Scaling Up\"></p>\n<h2 id=\"MATLAB实验\"><a href=\"#MATLAB实验\" class=\"headerlink\" title=\"MATLAB实验\"></a>MATLAB实验</h2><p>下面是我自己写的简单的K-Means demo。首先，在数据产生环节，确定$K = 3$个cluster的中心位置，并随机产生$N$个数据点，并使用<code>scatter</code>函数做出散点图。</p>\n<p>代码中的主要部分为<code>my_kmeans</code>函数的实现（为了不与内建的kmeans函数重名，故加上了<code>my</code>前缀）。在此函数内部，首先随机产生$K$个聚类中心，并对数据点进行初始的指定。接着，在迭代过程中，不断计算均值来更新聚类中心和assignment，当达到最大迭代次数或者相邻两次迭代中assignment的值不再变化为止，并计算对应的目标函数$J$的值。</p>\n<p>注意到该函数初始确定聚类中心时有一段注释的代码，该段代码用于debug时，指定聚类中心，去除随机性，以验证后续K-Means迭代的正确性。</p>\n<figure class=\"highlight matlab\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div><div class=\"line\">47</div><div class=\"line\">48</div><div class=\"line\">49</div><div class=\"line\">50</div><div class=\"line\">51</div><div class=\"line\">52</div><div class=\"line\">53</div><div class=\"line\">54</div><div class=\"line\">55</div><div class=\"line\">56</div><div class=\"line\">57</div><div class=\"line\">58</div><div class=\"line\">59</div><div class=\"line\">60</div><div class=\"line\">61</div><div class=\"line\">62</div><div class=\"line\">63</div><div class=\"line\">64</div><div class=\"line\">65</div><div class=\"line\">66</div><div class=\"line\">67</div><div class=\"line\">68</div><div class=\"line\">69</div><div class=\"line\">70</div><div class=\"line\">71</div><div class=\"line\">72</div><div class=\"line\">73</div><div class=\"line\">74</div><div class=\"line\">75</div><div class=\"line\">76</div><div class=\"line\">77</div><div class=\"line\">78</div><div class=\"line\">79</div><div class=\"line\">80</div><div class=\"line\">81</div><div class=\"line\">82</div><div class=\"line\">83</div><div class=\"line\">84</div><div class=\"line\">85</div><div class=\"line\">86</div><div class=\"line\">87</div><div class=\"line\">88</div><div class=\"line\">89</div><div class=\"line\">90</div><div class=\"line\">91</div><div class=\"line\">92</div><div class=\"line\">93</div><div class=\"line\">94</div><div class=\"line\">95</div><div class=\"line\">96</div><div class=\"line\">97</div><div class=\"line\">98</div><div class=\"line\">99</div><div class=\"line\">100</div><div class=\"line\">101</div><div class=\"line\">102</div><div class=\"line\">103</div><div class=\"line\">104</div><div class=\"line\">105</div><div class=\"line\">106</div><div class=\"line\">107</div><div class=\"line\">108</div><div class=\"line\">109</div><div class=\"line\">110</div><div class=\"line\">111</div><div class=\"line\">112</div><div class=\"line\">113</div><div class=\"line\">114</div><div class=\"line\">115</div><div class=\"line\">116</div><div class=\"line\">117</div><div class=\"line\">118</div><div class=\"line\">119</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\">%% generate data</span></div><div class=\"line\">K = <span class=\"number\">3</span>;   <span class=\"comment\">% number of clusters</span></div><div class=\"line\">pos = [<span class=\"number\">-5</span>, <span class=\"number\">5</span>; <span class=\"number\">0</span>, <span class=\"number\">1</span>; <span class=\"number\">3</span>, <span class=\"number\">6</span>];  <span class=\"comment\">% position of cluster centers</span></div><div class=\"line\">N = <span class=\"number\">20</span>;    <span class=\"comment\">% number of data points</span></div><div class=\"line\">R = <span class=\"number\">3</span>;     <span class=\"comment\">% radius of clusters</span></div><div class=\"line\">data = <span class=\"built_in\">zeros</span>(N, <span class=\"number\">2</span>);    <span class=\"comment\">% data</span></div><div class=\"line\">class = <span class=\"built_in\">zeros</span>(N, <span class=\"number\">1</span>);   <span class=\"comment\">% index of cluster</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"keyword\">for</span> <span class=\"built_in\">i</span> = <span class=\"number\">1</span>:N</div><div class=\"line\">    idx = randi(<span class=\"number\">3</span>, <span class=\"number\">1</span>);</div><div class=\"line\">    dr = R*<span class=\"built_in\">rand</span>();</div><div class=\"line\">    data(<span class=\"built_in\">i</span>, :) = pos(idx, :) + [dr*cos(rand()*<span class=\"number\">2</span>*pi), dr*sin(rand()*<span class=\"number\">2</span>*pi)];</div><div class=\"line\">    class(<span class=\"built_in\">i</span>) = idx;</div><div class=\"line\"><span class=\"keyword\">end</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">%% visualization data points</span></div><div class=\"line\">figure</div><div class=\"line\">hold on</div><div class=\"line\">color = [<span class=\"number\">1</span>,<span class=\"number\">0</span>,<span class=\"number\">0</span>; <span class=\"number\">0</span>,<span class=\"number\">1</span>,<span class=\"number\">0</span>; <span class=\"number\">0</span>,<span class=\"number\">0</span>,<span class=\"number\">1</span>];</div><div class=\"line\"><span class=\"keyword\">for</span> <span class=\"built_in\">i</span> = <span class=\"number\">1</span>:K</div><div class=\"line\">    x = data(class == <span class=\"built_in\">i</span>, <span class=\"number\">1</span>);</div><div class=\"line\">    y = data(class == <span class=\"built_in\">i</span>, <span class=\"number\">2</span>);</div><div class=\"line\">    scatter(x, y, <span class=\"number\">150</span>, <span class=\"built_in\">repmat</span>(color(<span class=\"built_in\">i</span>,:), [length(x), <span class=\"number\">1</span>]), <span class=\"string\">'filled'</span>);</div><div class=\"line\"><span class=\"keyword\">end</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">%% K-Means</span></div><div class=\"line\">best_J = <span class=\"number\">1E100</span>;</div><div class=\"line\">best_idx = <span class=\"number\">0</span>;</div><div class=\"line\"><span class=\"keyword\">for</span> times = <span class=\"number\">1</span>:<span class=\"number\">5</span>  <span class=\"comment\">% 5 times experiments to choose the best result</span></div><div class=\"line\">    [mu, assignment, J] = my_kmeans(data, K);</div><div class=\"line\">    <span class=\"keyword\">if</span> best_J &gt; J</div><div class=\"line\">        best_idx = times;</div><div class=\"line\">        best_J = J;</div><div class=\"line\">    <span class=\"keyword\">end</span></div><div class=\"line\">    fprintf(<span class=\"string\">'%d experiment: J = %f\\n'</span>, times, J);</div><div class=\"line\">    <span class=\"built_in\">disp</span>(mu);</div><div class=\"line\"><span class=\"keyword\">end</span></div><div class=\"line\">fprintf(<span class=\"string\">'best: %d experiment: J = %f\\n'</span>, best_idx, best_J);</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">%% basic functions</span></div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">function</span> <span class=\"title\">J</span> = <span class=\"title\">ssd</span><span class=\"params\">(X, mu, assignment)</span></span></div><div class=\"line\"><span class=\"comment\">% sum of square distance</span></div><div class=\"line\"><span class=\"comment\">% X -- data, N*D matrix</span></div><div class=\"line\"><span class=\"comment\">% mu -- centers of clusters, K*D matrix</span></div><div class=\"line\"><span class=\"comment\">% assignment -- current assignment of data to clusters</span></div><div class=\"line\">J = <span class=\"number\">0</span>;</div><div class=\"line\">K = <span class=\"built_in\">size</span>(mu, <span class=\"number\">1</span>);</div><div class=\"line\"><span class=\"keyword\">for</span> k = <span class=\"number\">1</span>:K</div><div class=\"line\">    x_k = X(assignment == k, :);</div><div class=\"line\">    mu_k = mu(k, :);</div><div class=\"line\">    err2 = <span class=\"built_in\">bsxfun</span>(@minus, x_k, mu_k).^<span class=\"number\">2</span>;</div><div class=\"line\">    J = J + sum(err2(:));</div><div class=\"line\"><span class=\"keyword\">end</span></div><div class=\"line\">J = J / <span class=\"built_in\">size</span>(X, <span class=\"number\">1</span>);</div><div class=\"line\"><span class=\"keyword\">end</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">function</span> <span class=\"title\">mu</span> = <span class=\"title\">compute_mu</span><span class=\"params\">(X, assignment, K)</span></span></div><div class=\"line\">mu = <span class=\"built_in\">zeros</span>(K, <span class=\"built_in\">size</span>(X, <span class=\"number\">2</span>));</div><div class=\"line\"><span class=\"keyword\">for</span> k = <span class=\"number\">1</span>:K</div><div class=\"line\">    x_k = X(assignment == k, :);</div><div class=\"line\">    mu(k, :) = mean(x_k, <span class=\"number\">1</span>);</div><div class=\"line\"><span class=\"keyword\">end</span></div><div class=\"line\"><span class=\"keyword\">end</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">function</span> <span class=\"title\">assignment</span> = <span class=\"title\">assign</span><span class=\"params\">(X, mu)</span></span></div><div class=\"line\"><span class=\"comment\">% assign data points to clusters</span></div><div class=\"line\">N = <span class=\"built_in\">size</span>(X, <span class=\"number\">1</span>);</div><div class=\"line\">assignment = <span class=\"built_in\">zeros</span>(N, <span class=\"number\">1</span>);</div><div class=\"line\"><span class=\"keyword\">for</span> <span class=\"built_in\">i</span> = <span class=\"number\">1</span>:N</div><div class=\"line\">    x = X(<span class=\"built_in\">i</span>, :);</div><div class=\"line\">    err2 = <span class=\"built_in\">bsxfun</span>(@minus, x, mu).^<span class=\"number\">2</span>;</div><div class=\"line\">    dis = sum(err2, <span class=\"number\">2</span>);</div><div class=\"line\">    [~, idx] = min(dis);</div><div class=\"line\">    assignment(<span class=\"built_in\">i</span>) = idx;</div><div class=\"line\"><span class=\"keyword\">end</span></div><div class=\"line\"><span class=\"keyword\">end</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">function</span> <span class=\"params\">[mu, assignment, J]</span> = <span class=\"title\">my_kmeans</span><span class=\"params\">(X, K)</span></span></div><div class=\"line\">N = <span class=\"built_in\">size</span>(X, <span class=\"number\">1</span>);</div><div class=\"line\">assignment = <span class=\"built_in\">zeros</span>(N, <span class=\"number\">1</span>);</div><div class=\"line\">idx = randsample(N, K);</div><div class=\"line\">mu = X(idx, :);</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">% for i = 1:K</span></div><div class=\"line\"><span class=\"comment\">%     for j = 1:N</span></div><div class=\"line\"><span class=\"comment\">%         if assignment_gt(j) == i</span></div><div class=\"line\"><span class=\"comment\">%             mu(i,:) = X(j,:);</span></div><div class=\"line\"><span class=\"comment\">%             break;</span></div><div class=\"line\"><span class=\"comment\">%         end</span></div><div class=\"line\"><span class=\"comment\">%     end</span></div><div class=\"line\"><span class=\"comment\">% end</span></div><div class=\"line\">figure</div><div class=\"line\">hold on</div><div class=\"line\">color = [<span class=\"number\">1</span>,<span class=\"number\">0</span>,<span class=\"number\">0</span>; <span class=\"number\">0</span>,<span class=\"number\">1</span>,<span class=\"number\">0</span>; <span class=\"number\">0</span>,<span class=\"number\">0</span>,<span class=\"number\">1</span>];</div><div class=\"line\">scatter(mu(:,<span class=\"number\">1</span>), mu(:,<span class=\"number\">2</span>), <span class=\"number\">200</span>, color, <span class=\"string\">'d'</span>);</div><div class=\"line\"><span class=\"keyword\">for</span> iter = <span class=\"number\">1</span>:<span class=\"number\">20</span></div><div class=\"line\">    assignment_prev = assignment;</div><div class=\"line\">    assignment = assign(X, mu);</div><div class=\"line\">    <span class=\"keyword\">if</span> assignment == assignment_prev</div><div class=\"line\">        <span class=\"keyword\">break</span>;</div><div class=\"line\">    <span class=\"keyword\">end</span></div><div class=\"line\">    mu_prev = mu;</div><div class=\"line\">    mu = compute_mu(X, assignment, K);</div><div class=\"line\">    scatter(mu(:, <span class=\"number\">1</span>), mu(:, <span class=\"number\">2</span>), <span class=\"number\">200</span>, color, <span class=\"string\">'d'</span>);</div><div class=\"line\">    MU = <span class=\"built_in\">zeros</span>(<span class=\"number\">2</span>*K, <span class=\"number\">2</span>);</div><div class=\"line\">    MU(<span class=\"number\">1</span>:<span class=\"number\">2</span>:<span class=\"keyword\">end</span>, :) = mu_prev;</div><div class=\"line\">    MU(<span class=\"number\">2</span>:<span class=\"number\">2</span>:<span class=\"keyword\">end</span>, :) = mu;</div><div class=\"line\">    mu_x = <span class=\"built_in\">reshape</span>(MU(:, <span class=\"number\">1</span>), [], K);</div><div class=\"line\">    mu_y = <span class=\"built_in\">reshape</span>(MU(:, <span class=\"number\">2</span>), [], K);</div><div class=\"line\">    plot(mu_x, mu_y, <span class=\"string\">'k-.'</span>);</div><div class=\"line\"></div><div class=\"line\"><span class=\"keyword\">end</span></div><div class=\"line\"><span class=\"keyword\">for</span> <span class=\"built_in\">i</span> = <span class=\"number\">1</span>:K</div><div class=\"line\">    x = X(assignment == <span class=\"built_in\">i</span>, <span class=\"number\">1</span>);</div><div class=\"line\">    y = X(assignment == <span class=\"built_in\">i</span>, <span class=\"number\">2</span>);</div><div class=\"line\">    scatter(x, y, <span class=\"number\">150</span>, <span class=\"built_in\">repmat</span>(color(<span class=\"built_in\">i</span>,:), [length(x), <span class=\"number\">1</span>]), <span class=\"string\">'filled'</span>);</div><div class=\"line\"><span class=\"keyword\">end</span></div><div class=\"line\">J = ssd(X, mu, assignment);</div><div class=\"line\"><span class=\"keyword\">end</span></div></pre></td></tr></table></figure>\n<p>在demo中，随机选取初始化的聚类中心，重复进行$5$次实验。下图是产生的原始数据的散点图。用不同的颜色标明了cluster。<br><img src=\"/img/kmeans_data_demo.png\" alt=\"K-Means聚类\"></p>\n<p>下图是某次的实验结果，其中菱形和它们之间的连线指示了聚类中心的变化。注意，颜色只是用来区别不同的cluster，和上图并没有对应关系。可以看到，初始化时绿色标注的cluster的中心（也即是上图中的红色cluster）虽然偏离了很远，但是在迭代过程中也能实现纠正。</p>\n<p><img src=\"/img/kmeans_success.png\" alt=\"K-Means聚类\"></p>\n<p>再换个大点的数据集来做，效果貌似还不错~<br><img src=\"/img/kmeans_bigger_demo.png\" alt=\"大一些\"></p>\n<h2 id=\"PS\"><a href=\"#PS\" class=\"headerlink\" title=\"PS\"></a>PS</h2><p>这里插一句与本文内容完全无关的一个tip：在Markdown中使用LaTex时，如果在内联形式（也就是行内使用两个美元符号插入公式）时，如果有多个下划线，那么Markdown有时会将下划线之间的部分认为是自己的斜体标记，如下所示：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">$c_&#123;i&#125;^\\ast$ XXX $\\delta_&#123;ij&#125;^\\ast$</div></pre></td></tr></table></figure></p>\n<p>它的显示效果为$c<em>{i}^\\ast$ XXX $\\delta</em>{ij}^\\ast$。</p>\n<p>这时候，只需在下划线前加入反斜线进行转义即可（虽然略显麻烦）。如下所示：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">$c\\_&#123;i&#125;^\\ast$ XXX $\\delta\\_&#123;ij&#125;^\\ast$</div></pre></td></tr></table></figure></p>\n<p>它的显示效果为$c_{i}^\\ast$ XXX $\\delta_{ij}^\\ast$。</p>\n<p>具体分析可以参见<a href=\"http://lukang.me/2014/mathjax-for-hexo.html\" target=\"_blank\" rel=\"external\">博客</a>。</p>\n","excerpt":"<p><a href=\"https://zh.wikipedia.org/wiki/K-平均算法\">K-Means聚类</a>是把n个点（可以是样本的一次观察或一个实例）划分到$k$个聚类中，使得每个点都属于离他最近的均值（此即聚类中心）对应的聚类，以之作为聚类的标准，满足最小化聚类中心和属于该中心的数据点之间的平方距离和（Sum of Square Distance，SSD）。</p>\n<script type=\"math/tex; mode=display\">\\text{SSD} = \\sum_{i=1}^{k}\\sum_{x\\in c_i}(x-c_i)^2</script><p><img src=\"/img/kmeans_demo.png\" alt=\"K-Means Demo\"></p>","more":"<h2 id=\"目标函数\"><a href=\"#目标函数\" class=\"headerlink\" title=\"目标函数\"></a>目标函数</h2><p>K-Means方法实际上需要确定两个参数，$c^\\ast$和$\\delta^\\ast$。其中$c_{i}^\\ast$代表各个聚类中心的位置，$\\delta_{ij}^\\ast$的取值为$\\lbrace 0,1\\rbrace$，代表点$x_j$是否被分到了第$i$个聚类中心。</p>\n<p>那么，目标函数可以写成如下的形式。</p>\n<script type=\"math/tex; mode=display\">c^\\ast, \\delta^\\ast = \\arg\\min_{c,\\delta} \\frac{1}{N}\\sum_{j=1}^{N}\\sum_{i=1}^{k}\\delta_{i,j}(c_i-x_j)^2</script><p>然而这样一来，造成了一种很尴尬的局面。一方面，要想优化$c_i$，需要我们给定每个点所属的类；另一方面，优化$\\delta_{i,j}$，需要我们给定聚类中心。由于这两个优化变量之间是纠缠在一起的，K-Means算法如何能够达到收敛就变成了一个讨论“先有鸡还是先有蛋”的问题了。</p>\n<p>实际使用中，K-Means的迭代过程，实际是EM算法的一个特例。</p>\n<h2 id=\"算法流程\"><a href=\"#算法流程\" class=\"headerlink\" title=\"算法流程\"></a>算法流程</h2><p>K-Means算法的流程如下所示。<br><img src=\"/img/kmeans_algorithm.png\" alt=\"K-Means算法流程\"></p>\n<p>假设我们有$N$个样本点，$\\lbrace x_1, \\dots, x_N\\rbrace, x_i\\in\\mathbb{R}^D$，并给出聚类数目$k$。</p>\n<p>首先，随机选取一系列的聚类中心点$\\mu_i, i = 1,\\dots, k$。接着，我们按照距离最近的原则为每个数据点指定相应的聚类中心并计算新的数据点均值更新聚类中心。如此这般，直到收敛。</p>\n<h2 id=\"算法细节\"><a href=\"#算法细节\" class=\"headerlink\" title=\"算法细节\"></a>算法细节</h2><h3 id=\"初始化\"><a href=\"#初始化\" class=\"headerlink\" title=\"初始化\"></a>初始化</h3><p>上面的K-Means方法对初始值很敏感。朴素的初始化方法是直接在数据点中随机抽取$k$个作为聚类初始中心点。不够好的初始值可能造成收敛速度很慢或者聚类失败。下面介绍两种改进方法。</p>\n<ul>\n<li><p>kmeans++方法，尽可能地使初始聚类中心分散开（spread-out）。<br>这种方法首先随机产生一个初始聚类中心点，然后按照概率$P = \\omega(x-c_i)^2$选取其他的聚类中心点。其中$\\omega$是归一化系数。</p>\n</li>\n<li><p>多次初始化，保留最好的结果。</p>\n</li>\n</ul>\n<h3 id=\"K值的选取\"><a href=\"#K值的选取\" class=\"headerlink\" title=\"K值的选取\"></a>K值的选取</h3><p>在上面的讨论中，我们一直把$k$当做已经给定的参数。但是在实际操作中，聚类类别数通常都是未知的。如何确定超参数$k$呢？</p>\n<p>我们可以使用不同的$k$值进行试验，并作出不同试验收敛后目标函数随$k$的变化。如下图所示，分别使用$1$到$4$之间的几个数字作为$k$值，曲线在$k=2$处有一个较为明显的弯曲点（elbow point）。故确定$k$取值为2。<br><img src=\"/img/kmeans_object_fun_vs_k.png\" alt=\"参数K的确定\"></p>\n<h3 id=\"距离的度量\"><a href=\"#距离的度量\" class=\"headerlink\" title=\"距离的度量\"></a>距离的度量</h3><p>目标函数是各个cluster中的点与其中心点的距离均值。其中就涉及到了“距离”的量度。下面是几种较为常用的距离度量方法。</p>\n<ul>\n<li>欧几里得距离（最为常用）</li>\n<li>余弦距离（向量的夹角）</li>\n<li>核函数（<a href=\"http://www.public.asu.edu/~jye02/CLASSES/Fall-2005/PAPERS/kdd_spectral_kernelkmeans.pdf\">Kernel K-Means</a>）</li>\n</ul>\n<h3 id=\"迭代终止条件\"><a href=\"#迭代终止条件\" class=\"headerlink\" title=\"迭代终止条件\"></a>迭代终止条件</h3><p>当判断算法已经收敛时，退出迭代。常用的迭代终止条件如下：</p>\n<ul>\n<li>达到了预先给定的最大迭代次数</li>\n<li>在将数据点assign到不同聚类中心时，和上轮结果没有变化（说明已经收敛）</li>\n<li>目标函数（平均的距离）下降小于阈值</li>\n</ul>\n<h2 id=\"基于K-Means的图像分割\"><a href=\"#基于K-Means的图像分割\" class=\"headerlink\" title=\"基于K-Means的图像分割\"></a>基于K-Means的图像分割</h2><p>图像分割中可以使用K-Means算法。我们首先将一副图像转换为特征空间（Feature Space）。例如使用灰度或者RGB的值作为特征向量，就得到了1D或者3D的特征空间。之后，可以基于Feature Space上各点的相似度，将它们聚类。这样，就完成了对原始图像的分割操作。如下所示。<br><img src=\"/img/kmeans_image_seg_via_intensity.png\" alt=\"图像分割结果1\"></p>\n<p>然而，上述方法有一个缺点。那就是在真实图像中，属于同一个物体的像素点常常在空间上是相关的（Spatial Coherent）。而上述方法没有考虑到这一点。我们可以根据颜色亮度和空间上的距离构建Feature Space，获得更加合理的分割效果。</p>\n<p>在2012年PAMI上有一篇文章<a href=\"https://infoscience.epfl.ch/record/177415/files/Superpixel_PAMI2011-2.pdf\">SLIC Superpixels Compared to State-of-the-art Superpixel Methods</a>介绍了使用K-Means进行超像素分割的相关内容，可以参考。这里不再赘述了。</p>\n<h2 id=\"优点和不足\"><a href=\"#优点和不足\" class=\"headerlink\" title=\"优点和不足\"></a>优点和不足</h2><p>作为非监督学习中的经典算法，K-Means的一大优点便是实现简单，速度较快，最小化条件方差（conditional variance，good represention of data，这里没有具体解释，不太懂）。</p>\n<p>它的缺点主要有：</p>\n<ul>\n<li>对outlier比较敏感。如下图所示。由于outlier的存在，右边的cluster硬是被拉长了。这方面，K-Medians更有鲁棒性。<br><img src=\"/img/kmeans_sensitive_to_outlier.png\" alt=\"outlier是个大麻烦\"></li>\n<li>每个点只能确定地属于或不属于某一个cluster。可以参考高斯混合模型和Fuzzy K-Means的soft assignment。</li>\n<li>在round shape情况下性能比较好（想想圆形和方形各点的欧几里得距离）。而且每个cluster最好数据点数目和分布的密度相近。</li>\n<li>如果数据点有非凸形状分布，算法表现糟糕。可以参考谱聚类（Spectral Clustering）或者kernel K-Means。</li>\n</ul>\n<p>针对K-Means，也有不少相关改进工作，参考下面这幅图吧。<br><img src=\"/img/kmeans_scaling_up.png\" alt=\"K-Means Scaling Up\"></p>\n<h2 id=\"MATLAB实验\"><a href=\"#MATLAB实验\" class=\"headerlink\" title=\"MATLAB实验\"></a>MATLAB实验</h2><p>下面是我自己写的简单的K-Means demo。首先，在数据产生环节，确定$K = 3$个cluster的中心位置，并随机产生$N$个数据点，并使用<code>scatter</code>函数做出散点图。</p>\n<p>代码中的主要部分为<code>my_kmeans</code>函数的实现（为了不与内建的kmeans函数重名，故加上了<code>my</code>前缀）。在此函数内部，首先随机产生$K$个聚类中心，并对数据点进行初始的指定。接着，在迭代过程中，不断计算均值来更新聚类中心和assignment，当达到最大迭代次数或者相邻两次迭代中assignment的值不再变化为止，并计算对应的目标函数$J$的值。</p>\n<p>注意到该函数初始确定聚类中心时有一段注释的代码，该段代码用于debug时，指定聚类中心，去除随机性，以验证后续K-Means迭代的正确性。</p>\n<figure class=\"highlight matlab\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div><div class=\"line\">47</div><div class=\"line\">48</div><div class=\"line\">49</div><div class=\"line\">50</div><div class=\"line\">51</div><div class=\"line\">52</div><div class=\"line\">53</div><div class=\"line\">54</div><div class=\"line\">55</div><div class=\"line\">56</div><div class=\"line\">57</div><div class=\"line\">58</div><div class=\"line\">59</div><div class=\"line\">60</div><div class=\"line\">61</div><div class=\"line\">62</div><div class=\"line\">63</div><div class=\"line\">64</div><div class=\"line\">65</div><div class=\"line\">66</div><div class=\"line\">67</div><div class=\"line\">68</div><div class=\"line\">69</div><div class=\"line\">70</div><div class=\"line\">71</div><div class=\"line\">72</div><div class=\"line\">73</div><div class=\"line\">74</div><div class=\"line\">75</div><div class=\"line\">76</div><div class=\"line\">77</div><div class=\"line\">78</div><div class=\"line\">79</div><div class=\"line\">80</div><div class=\"line\">81</div><div class=\"line\">82</div><div class=\"line\">83</div><div class=\"line\">84</div><div class=\"line\">85</div><div class=\"line\">86</div><div class=\"line\">87</div><div class=\"line\">88</div><div class=\"line\">89</div><div class=\"line\">90</div><div class=\"line\">91</div><div class=\"line\">92</div><div class=\"line\">93</div><div class=\"line\">94</div><div class=\"line\">95</div><div class=\"line\">96</div><div class=\"line\">97</div><div class=\"line\">98</div><div class=\"line\">99</div><div class=\"line\">100</div><div class=\"line\">101</div><div class=\"line\">102</div><div class=\"line\">103</div><div class=\"line\">104</div><div class=\"line\">105</div><div class=\"line\">106</div><div class=\"line\">107</div><div class=\"line\">108</div><div class=\"line\">109</div><div class=\"line\">110</div><div class=\"line\">111</div><div class=\"line\">112</div><div class=\"line\">113</div><div class=\"line\">114</div><div class=\"line\">115</div><div class=\"line\">116</div><div class=\"line\">117</div><div class=\"line\">118</div><div class=\"line\">119</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\">%% generate data</span></div><div class=\"line\">K = <span class=\"number\">3</span>;   <span class=\"comment\">% number of clusters</span></div><div class=\"line\">pos = [<span class=\"number\">-5</span>, <span class=\"number\">5</span>; <span class=\"number\">0</span>, <span class=\"number\">1</span>; <span class=\"number\">3</span>, <span class=\"number\">6</span>];  <span class=\"comment\">% position of cluster centers</span></div><div class=\"line\">N = <span class=\"number\">20</span>;    <span class=\"comment\">% number of data points</span></div><div class=\"line\">R = <span class=\"number\">3</span>;     <span class=\"comment\">% radius of clusters</span></div><div class=\"line\">data = <span class=\"built_in\">zeros</span>(N, <span class=\"number\">2</span>);    <span class=\"comment\">% data</span></div><div class=\"line\">class = <span class=\"built_in\">zeros</span>(N, <span class=\"number\">1</span>);   <span class=\"comment\">% index of cluster</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"keyword\">for</span> <span class=\"built_in\">i</span> = <span class=\"number\">1</span>:N</div><div class=\"line\">    idx = randi(<span class=\"number\">3</span>, <span class=\"number\">1</span>);</div><div class=\"line\">    dr = R*<span class=\"built_in\">rand</span>();</div><div class=\"line\">    data(<span class=\"built_in\">i</span>, :) = pos(idx, :) + [dr*cos(rand()*<span class=\"number\">2</span>*pi), dr*sin(rand()*<span class=\"number\">2</span>*pi)];</div><div class=\"line\">    class(<span class=\"built_in\">i</span>) = idx;</div><div class=\"line\"><span class=\"keyword\">end</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">%% visualization data points</span></div><div class=\"line\">figure</div><div class=\"line\">hold on</div><div class=\"line\">color = [<span class=\"number\">1</span>,<span class=\"number\">0</span>,<span class=\"number\">0</span>; <span class=\"number\">0</span>,<span class=\"number\">1</span>,<span class=\"number\">0</span>; <span class=\"number\">0</span>,<span class=\"number\">0</span>,<span class=\"number\">1</span>];</div><div class=\"line\"><span class=\"keyword\">for</span> <span class=\"built_in\">i</span> = <span class=\"number\">1</span>:K</div><div class=\"line\">    x = data(class == <span class=\"built_in\">i</span>, <span class=\"number\">1</span>);</div><div class=\"line\">    y = data(class == <span class=\"built_in\">i</span>, <span class=\"number\">2</span>);</div><div class=\"line\">    scatter(x, y, <span class=\"number\">150</span>, <span class=\"built_in\">repmat</span>(color(<span class=\"built_in\">i</span>,:), [length(x), <span class=\"number\">1</span>]), <span class=\"string\">'filled'</span>);</div><div class=\"line\"><span class=\"keyword\">end</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">%% K-Means</span></div><div class=\"line\">best_J = <span class=\"number\">1E100</span>;</div><div class=\"line\">best_idx = <span class=\"number\">0</span>;</div><div class=\"line\"><span class=\"keyword\">for</span> times = <span class=\"number\">1</span>:<span class=\"number\">5</span>  <span class=\"comment\">% 5 times experiments to choose the best result</span></div><div class=\"line\">    [mu, assignment, J] = my_kmeans(data, K);</div><div class=\"line\">    <span class=\"keyword\">if</span> best_J &gt; J</div><div class=\"line\">        best_idx = times;</div><div class=\"line\">        best_J = J;</div><div class=\"line\">    <span class=\"keyword\">end</span></div><div class=\"line\">    fprintf(<span class=\"string\">'%d experiment: J = %f\\n'</span>, times, J);</div><div class=\"line\">    <span class=\"built_in\">disp</span>(mu);</div><div class=\"line\"><span class=\"keyword\">end</span></div><div class=\"line\">fprintf(<span class=\"string\">'best: %d experiment: J = %f\\n'</span>, best_idx, best_J);</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">%% basic functions</span></div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">function</span> <span class=\"title\">J</span> = <span class=\"title\">ssd</span><span class=\"params\">(X, mu, assignment)</span></span></div><div class=\"line\"><span class=\"comment\">% sum of square distance</span></div><div class=\"line\"><span class=\"comment\">% X -- data, N*D matrix</span></div><div class=\"line\"><span class=\"comment\">% mu -- centers of clusters, K*D matrix</span></div><div class=\"line\"><span class=\"comment\">% assignment -- current assignment of data to clusters</span></div><div class=\"line\">J = <span class=\"number\">0</span>;</div><div class=\"line\">K = <span class=\"built_in\">size</span>(mu, <span class=\"number\">1</span>);</div><div class=\"line\"><span class=\"keyword\">for</span> k = <span class=\"number\">1</span>:K</div><div class=\"line\">    x_k = X(assignment == k, :);</div><div class=\"line\">    mu_k = mu(k, :);</div><div class=\"line\">    err2 = <span class=\"built_in\">bsxfun</span>(@minus, x_k, mu_k).^<span class=\"number\">2</span>;</div><div class=\"line\">    J = J + sum(err2(:));</div><div class=\"line\"><span class=\"keyword\">end</span></div><div class=\"line\">J = J / <span class=\"built_in\">size</span>(X, <span class=\"number\">1</span>);</div><div class=\"line\"><span class=\"keyword\">end</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">function</span> <span class=\"title\">mu</span> = <span class=\"title\">compute_mu</span><span class=\"params\">(X, assignment, K)</span></span></div><div class=\"line\">mu = <span class=\"built_in\">zeros</span>(K, <span class=\"built_in\">size</span>(X, <span class=\"number\">2</span>));</div><div class=\"line\"><span class=\"keyword\">for</span> k = <span class=\"number\">1</span>:K</div><div class=\"line\">    x_k = X(assignment == k, :);</div><div class=\"line\">    mu(k, :) = mean(x_k, <span class=\"number\">1</span>);</div><div class=\"line\"><span class=\"keyword\">end</span></div><div class=\"line\"><span class=\"keyword\">end</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">function</span> <span class=\"title\">assignment</span> = <span class=\"title\">assign</span><span class=\"params\">(X, mu)</span></span></div><div class=\"line\"><span class=\"comment\">% assign data points to clusters</span></div><div class=\"line\">N = <span class=\"built_in\">size</span>(X, <span class=\"number\">1</span>);</div><div class=\"line\">assignment = <span class=\"built_in\">zeros</span>(N, <span class=\"number\">1</span>);</div><div class=\"line\"><span class=\"keyword\">for</span> <span class=\"built_in\">i</span> = <span class=\"number\">1</span>:N</div><div class=\"line\">    x = X(<span class=\"built_in\">i</span>, :);</div><div class=\"line\">    err2 = <span class=\"built_in\">bsxfun</span>(@minus, x, mu).^<span class=\"number\">2</span>;</div><div class=\"line\">    dis = sum(err2, <span class=\"number\">2</span>);</div><div class=\"line\">    [~, idx] = min(dis);</div><div class=\"line\">    assignment(<span class=\"built_in\">i</span>) = idx;</div><div class=\"line\"><span class=\"keyword\">end</span></div><div class=\"line\"><span class=\"keyword\">end</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">function</span> <span class=\"params\">[mu, assignment, J]</span> = <span class=\"title\">my_kmeans</span><span class=\"params\">(X, K)</span></span></div><div class=\"line\">N = <span class=\"built_in\">size</span>(X, <span class=\"number\">1</span>);</div><div class=\"line\">assignment = <span class=\"built_in\">zeros</span>(N, <span class=\"number\">1</span>);</div><div class=\"line\">idx = randsample(N, K);</div><div class=\"line\">mu = X(idx, :);</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">% for i = 1:K</span></div><div class=\"line\"><span class=\"comment\">%     for j = 1:N</span></div><div class=\"line\"><span class=\"comment\">%         if assignment_gt(j) == i</span></div><div class=\"line\"><span class=\"comment\">%             mu(i,:) = X(j,:);</span></div><div class=\"line\"><span class=\"comment\">%             break;</span></div><div class=\"line\"><span class=\"comment\">%         end</span></div><div class=\"line\"><span class=\"comment\">%     end</span></div><div class=\"line\"><span class=\"comment\">% end</span></div><div class=\"line\">figure</div><div class=\"line\">hold on</div><div class=\"line\">color = [<span class=\"number\">1</span>,<span class=\"number\">0</span>,<span class=\"number\">0</span>; <span class=\"number\">0</span>,<span class=\"number\">1</span>,<span class=\"number\">0</span>; <span class=\"number\">0</span>,<span class=\"number\">0</span>,<span class=\"number\">1</span>];</div><div class=\"line\">scatter(mu(:,<span class=\"number\">1</span>), mu(:,<span class=\"number\">2</span>), <span class=\"number\">200</span>, color, <span class=\"string\">'d'</span>);</div><div class=\"line\"><span class=\"keyword\">for</span> iter = <span class=\"number\">1</span>:<span class=\"number\">20</span></div><div class=\"line\">    assignment_prev = assignment;</div><div class=\"line\">    assignment = assign(X, mu);</div><div class=\"line\">    <span class=\"keyword\">if</span> assignment == assignment_prev</div><div class=\"line\">        <span class=\"keyword\">break</span>;</div><div class=\"line\">    <span class=\"keyword\">end</span></div><div class=\"line\">    mu_prev = mu;</div><div class=\"line\">    mu = compute_mu(X, assignment, K);</div><div class=\"line\">    scatter(mu(:, <span class=\"number\">1</span>), mu(:, <span class=\"number\">2</span>), <span class=\"number\">200</span>, color, <span class=\"string\">'d'</span>);</div><div class=\"line\">    MU = <span class=\"built_in\">zeros</span>(<span class=\"number\">2</span>*K, <span class=\"number\">2</span>);</div><div class=\"line\">    MU(<span class=\"number\">1</span>:<span class=\"number\">2</span>:<span class=\"keyword\">end</span>, :) = mu_prev;</div><div class=\"line\">    MU(<span class=\"number\">2</span>:<span class=\"number\">2</span>:<span class=\"keyword\">end</span>, :) = mu;</div><div class=\"line\">    mu_x = <span class=\"built_in\">reshape</span>(MU(:, <span class=\"number\">1</span>), [], K);</div><div class=\"line\">    mu_y = <span class=\"built_in\">reshape</span>(MU(:, <span class=\"number\">2</span>), [], K);</div><div class=\"line\">    plot(mu_x, mu_y, <span class=\"string\">'k-.'</span>);</div><div class=\"line\"></div><div class=\"line\"><span class=\"keyword\">end</span></div><div class=\"line\"><span class=\"keyword\">for</span> <span class=\"built_in\">i</span> = <span class=\"number\">1</span>:K</div><div class=\"line\">    x = X(assignment == <span class=\"built_in\">i</span>, <span class=\"number\">1</span>);</div><div class=\"line\">    y = X(assignment == <span class=\"built_in\">i</span>, <span class=\"number\">2</span>);</div><div class=\"line\">    scatter(x, y, <span class=\"number\">150</span>, <span class=\"built_in\">repmat</span>(color(<span class=\"built_in\">i</span>,:), [length(x), <span class=\"number\">1</span>]), <span class=\"string\">'filled'</span>);</div><div class=\"line\"><span class=\"keyword\">end</span></div><div class=\"line\">J = ssd(X, mu, assignment);</div><div class=\"line\"><span class=\"keyword\">end</span></div></pre></td></tr></table></figure>\n<p>在demo中，随机选取初始化的聚类中心，重复进行$5$次实验。下图是产生的原始数据的散点图。用不同的颜色标明了cluster。<br><img src=\"/img/kmeans_data_demo.png\" alt=\"K-Means聚类\"></p>\n<p>下图是某次的实验结果，其中菱形和它们之间的连线指示了聚类中心的变化。注意，颜色只是用来区别不同的cluster，和上图并没有对应关系。可以看到，初始化时绿色标注的cluster的中心（也即是上图中的红色cluster）虽然偏离了很远，但是在迭代过程中也能实现纠正。</p>\n<p><img src=\"/img/kmeans_success.png\" alt=\"K-Means聚类\"></p>\n<p>再换个大点的数据集来做，效果貌似还不错~<br><img src=\"/img/kmeans_bigger_demo.png\" alt=\"大一些\"></p>\n<h2 id=\"PS\"><a href=\"#PS\" class=\"headerlink\" title=\"PS\"></a>PS</h2><p>这里插一句与本文内容完全无关的一个tip：在Markdown中使用LaTex时，如果在内联形式（也就是行内使用两个美元符号插入公式）时，如果有多个下划线，那么Markdown有时会将下划线之间的部分认为是自己的斜体标记，如下所示：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">$c_&#123;i&#125;^\\ast$ XXX $\\delta_&#123;ij&#125;^\\ast$</div></pre></td></tr></table></figure></p>\n<p>它的显示效果为$c<em>{i}^\\ast$ XXX $\\delta</em>{ij}^\\ast$。</p>\n<p>这时候，只需在下划线前加入反斜线进行转义即可（虽然略显麻烦）。如下所示：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">$c\\_&#123;i&#125;^\\ast$ XXX $\\delta\\_&#123;ij&#125;^\\ast$</div></pre></td></tr></table></figure></p>\n<p>它的显示效果为$c_{i}^\\ast$ XXX $\\delta_{ij}^\\ast$。</p>\n<p>具体分析可以参见<a href=\"http://lukang.me/2014/mathjax-for-hexo.html\">博客</a>。</p>"},{"title":"CS131-描述图像的特征(SIFT)","date":"2017-01-30T14:16:18.000Z","_content":"\n[SIFT(尺度不变特征变换，Scale Invariant Feature Transform)](https://en.wikipedia.org/wiki/Scale-invariant_feature_transform),最早由Lowe提出，目的是为了解决目标检测（Object Detection）问题中提取图像特征的问题。从名字可以看出，SIFT的优势就在于对尺度变换的不变性，同时SIFT还满足平移变换和旋转变换下的不变性，并对光照变化和3D视角变换有一定的不变性。它的主要步骤如下：\n- scale space上的极值检测。使用DoG在不同的scaleast和image position下找到interest point。\n- interest point的localization。对上面的interest point进行稳定度检测，并确定其所在的scale和position。\n- 确定方向。通过计算图像的梯度图，确定key point的方向，下一步的feature operation就是在这个方向，scale和position上进行的。\n- 确定key point的描述子。使用图像的局部梯度作为key point的描述子，最终构成SIFT特征向量。\n\n![SIFT图示](/img/sift_picture.jpg)\n\n<!-- more -->\n## SIFT介绍\n上讲中介绍的Harris角点方法计算简便，并具有平移不变性和旋转不变性。特征$f$对某种变换$\\mathcal{T}$具有不变性，是指在经过变换后原特征保持不变，也就是$f(I) = f(\\mathcal{T}(I))$。但是Harris角点不具有尺度变换不变性，如下图所示。当图像被放大后，原图的角点被判定为了边缘点。\n![harris的尺度变换不满足尺度不变性](/img/harris_non_scale_constant.png)\n\n而我们想要得到一种对尺度变换保持不变性的特征计算方法。例如图像patch的像素平均亮度，如下图所示。region size缩小为原始的一半后，亮度直方图的形状不变，即平均亮度不变。\n![平均亮度满足尺度变化呢不变性](/img/patch_average_intensity_scale_constant.png)\n\n而Lowe想到了使用局部极值来作为feature来保证对scale变换的不变性。在算法的具体实现中，他使用DoG来获得局部极值。\n\n[Lowe的论文](http://www.cs.ubc.ca/~lowe/papers/ijcv04.pdf)中也提到了SIFT特征的应用。SIFT特征可以产生稠密（dense）的key point，例如一张500x500像素大小的图像，一般可以产生~2000个稳定的SIFT特征。在进行image matching和recognition时，可以将ref image的SIFT特征提前计算出来保存在数据库中，并计算待处理图像的SIFT特征，根据特征向量的欧氏距离进行匹配。\n\n这篇博客主要是[Lowe上述论文](http://www.cs.ubc.ca/~lowe/papers/ijcv04.pdf)的读书笔记，按照SIFT特征的计算步骤进行组织。\n\n## 尺度空间极值的检测方法\n前人研究已经指出，在一系列合理假设下，高斯核是唯一满足尺度不变性的。所谓的尺度空间，就是指原始图像$I(x,y)$和可变尺度的高斯核$G(x,y,\\sigma)$的卷积结果。如下式所示：\n$$L(x,y,\\sigma) = G(x,y,\\sigma)\\ast I(x,y)$$\n\n其中，$G(x,y, \\sigma) = \\frac{1}{2\\pi\\sigma^2}\\exp(-(x^2+y^2)/2\\sigma^2)$。不同的$\\sigma$代表不同的尺度。\n\nDoG(difference of Gaussian)函数定义为不同尺度的高斯核与图像卷积结果之差，即，\n$$D(x,y,\\sigma) = L(x,y,k\\sigma) - L(x,y,\\sigma)$$\n\n如下图所示，输入的图像重复地与高斯核进行卷积得到结果图像$L$（左侧），这样构成了一个octave。相邻的$L$之间作差得到DoG图像（右侧）。当一个octave处理完之后，将当前octave的最后一张高斯卷积结果图像降采样两倍，按照前述方法构造下一个octave。在图中，我们将一个octave划分为$s$个间隔（即$s+1$个图像）时，设$\\sigma$最终变成了2倍（即$\\sigma$加倍）。那么，显然有相邻两层之间的$k = 2^{1/s}$。不过为了保证首尾两张图像也能够计算差值，我们实际上需要再补上两张（做一个padding），也就是说一个octave内的总图像为$s+3$（下图中的$s=2$）。\n![DoG的计算](/img/sift_dog.png)\n\n为何我们要费力气得到DoG呢？论文中作者给出的解释是：DoG对scale-normalized后的Guassian Laplace函数$\\sigma^2\\Delta G$提供了足够的近似。其中前面的$\\sigma^2$系数正是为了保证尺度不变性。而前人的研究则指出，$\\sigma \\Delta G$函数的极值，和其他一大票其他function比较时，能够提供最稳定的feature。\n\n对于高斯核函数，有以下性质：\n$$\\frac{\\partial G}{\\partial \\sigma} = \\sigma \\Delta G$$\n\n我们将式子左侧的微分变成差分，得到了下式：\n$$\\sigma\\Delta G \\approx \\frac{G(x,y,k\\sigma)-G(x,y,\\sigma)}{k\\sigma - \\sigma}$$\n\n也就是：\n$$G(x,y,k\\sigma)-G(x,y,\\sigma) \\approx (k-1)\\sigma^2 \\Delta G$$\n当$k=1$时，上式的近似误差为0（即上面的$s=\\infty$，也就是说octave的划分是连续的）。但是当$k$较大时，如$\\sqrt{2}$（这时$s=2$，octave划分十分粗糙了），仍然保证了稳定性。同时，由于相邻两层的比例$k$是定值，所以$k-1$这个因子对于检测极值没有影响。我们在计算时，无需除以$k-1$。\n\n构造完了DoG图像金字塔，下面我们的任务就是检测其中的极值。如下图，对于每个点，我们将它与金字塔中相邻的26个点作比较，找到局部极值。\n![检测极值](/img/sift_detection_maximum.png)\n\n另外，我们将输入图像行列预先使用线性插值的方法resize为原来的2倍，获取更多的key point。\n\n此外，在Lowe的论文中还提到了使用DoG的泰勒展开和海森矩阵进行key point的精细确定方法，并对key point进行过滤。这里也不再赘述了。\n\n## 128维feature的获取\n我们需要在每个key point处提取128维的特征向量。这里结合CS131课程作业2的相关练习作出说明。对于每个key point，我们关注它位于图像金字塔中的具体层数以及像素点位置，也就是说通过索引`pyramid{scale}(y, x)`就可以获取key point。我们遍历key point，为它们赋予一个128维的特征向量。\n\n我们选取point周围16x16大小的区域，称为一个patch。将其分为4x4共计16个cell。这样，每个cell内共有像素点16个。对于每个cell，我们计算它的局部梯度方向直方图，直方图共有8个bin。也就是说每个cell可以得到一个8维的特征向量。将16个cell的特征向量首尾相接，就得到了128维的SIFT特征向量。在下面的代码中，使用变量`patch_mag`和`patch_theta`分别代表patch的梯度幅值和角度，它们可以很简单地使用卷积和数学运算得到。\n\n``` matlab\npatch_mag = sqrt(patch_dx.^2 + patch_dy.^2);\npatch_theta = atan2(patch_dy, patch_dx);  % atan2的返回结果在区间[-pi, pi]上。\npatch_theta = mod(patch_theta, 2*pi);   % 这里我们要将其转换为[0, 2pi]\n```\n\n之后，我们需要获取key point的主方向。其定义可见slide，即为key point扩展出来的patch的梯度方向直方图的峰值对应的角度。\n![何为主方向](/img/sift_dominant_orientation.png)\n\n所以我们首先应该设计如何构建局部梯度方向直方图。我们只要将`[0, 2pi]`区间划分为若干个`bin`，并将patch内的每个点使用其梯度大小向对应的`bin`内投票即可。如下所示：\n\n``` matlab\nfunction [histogram, angles] = ComputeGradientHistogram(num_bins, gradient_magnitudes, gradient_angles)\n% Compute a gradient histogram using gradient magnitudes and directions.\n% Each point is assigned to one of num_bins depending on its gradient\n% direction; the gradient magnitude of that point is added to its bin.\n%\n% INPUT\n% num_bins: The number of bins to which points should be assigned.\n% gradient_magnitudes, gradient angles:\n%       Two arrays of the same shape where gradient_magnitudes(i) and\n%       gradient_angles(i) give the magnitude and direction of the gradient\n%       for the ith point. gradient_angles ranges from 0 to 2*pi\n%                                      \n% OUTPUT\n% histogram: A 1 x num_bins array containing the gradient histogram. Entry 1 is\n%       the sum of entries in gradient_magnitudes whose corresponding\n%       gradient_angles lie between 0 and angle_step. Similarly, entry 2 is for\n%       angles between angle_step and 2*angle_step. Angle_step is calculated as\n%       2*pi/num_bins.\n\n% angles: A 1 x num_bins array which holds the histogram bin lower bounds.\n%       In other words, histogram(i) contains the sum of the\n%       gradient magnitudes of all points whose gradient directions fall\n%       in the range [angles(i), angles(i + 1))\n\n    angle_step = 2 * pi / num_bins;\n    angles = 0 : angle_step : (2*pi-angle_step);\n\n    histogram = zeros(1, num_bins);\n    num = numel(gradient_angles);\n    for n = 1:num\n        index = floor(gradient_angles(n) / angle_step) + 1;\n        histogram(index) = histogram(index) + gradient_magnitudes(n);\n    end    \nend\n\n```\n\nLowe论文中推荐的`bin`数目为36个，计算主方向的函数如下：\n\n``` matlab\nfunction direction = ComputeDominantDirection(gradient_magnitudes, gradient_angles)\n% Computes the dominant gradient direction for the region around a keypoint\n% given the scale of the keypoint and the gradient magnitudes and gradient\n% angles of the pixels in the region surrounding the keypoint.\n%\n% INPUT\n% gradient_magnitudes, gradient_angles:\n%   Two arrays of the same shape where gradient_magnitudes(i) and\n%   gradient_angles(i) give the magnitude and direction of the gradient for\n%   the ith point.\n\n    % Compute a gradient histogram using the weighted gradient magnitudes.\n    % In David Lowe's paper he suggests using 36 bins for this histogram.\n    num_bins = 36;\n    % Step 1:\n    % compute the 36-bin histogram of angles using ComputeGradientHistogram()\n    [histogram, angle_bound] = ComputeGradientHistogram(num_bins, gradient_magnitudes, gradient_angles);\n    % Step 2:\n    % Find the maximum value of the gradient histogram, and set \"direction\"\n    % to the angle corresponding to the maximum. (To match our solutions,\n    % just use the lower-bound angle of the max histogram bin. (E.g. return\n    % 0 radians if it's bin 1.)\n    [~, max_index] = max(histogram);\n    direction = angle_bound(max_index);\nend\n```\n\n之后，我们更新patch内各点的梯度方向，计算其与主方向的夹角，作为新的方向。并将梯度进行高斯平滑。\n\n``` matlab\npatch_theta = patch_theta - ComputeDominantDirection(patch_mag, patch_theta);;\npatch_theta = mod(patch_theta, 2*pi);\npatch_mag = patch_mag .* fspecial('gaussian', patch_size, patch_size / 2); % patch_size = 16\n```\n\n遍历cell，计算feature如下：\n\n``` matlab\nfeature = [];\nrow_iter = 1;\nfor y = 1:num_histograms\n    col_iter = 1;\n    for x = 1:num_histograms\n        cell_mag = patch_mag(row_iter: row_iter + pixelsPerHistogram - 1, ...\n                             col_iter: col_iter + pixelsPerHistogram - 1);\n        cell_theta = patch_theta(row_iter: row_iter + pixelsPerHistogram - 1, ...\n                             col_iter: col_iter + pixelsPerHistogram - 1);\n        [histogram, ~] = ComputeGradientHistogram(num_angles, cell_mag, cell_theta);\n        feature = [feature, histogram];\n        col_iter = col_iter + pixelsPerHistogram;\n    end\n    row_iter = row_iter + pixelsPerHistogram;\nend\n```\n\n最后，对feature做normalization。首先将feature化为单位长度，并将其中太大的分量进行限幅（如0.2的阈值），之后再重新将其转换为单位长度。\n\n这样，就完成了SIFT特征的计算。在Lowe的论文中，有更多对实现细节的讨论，这里只是跟随课程slide和作业走完了算法流程，不再赘述。\n\n## 应用：图像特征点匹配\n和Harris角点一样，SIFT特征可以用作两幅图像的特征点匹配，并且具有多种变换不变性的优点。对于两幅图像分别计算得到的特征点SIFT特征向量，可以使用下面简单的方法搜寻匹配点。计算每组点对之间的欧式距离，如果最近的距离比第二近的距离小得多，那么可以认为有一对成功匹配的特征点。其MATLAB代码如下，其中`descriptor`是两幅图像的SIFT特征向量。阈值默认为取做0.7。\n\n``` matlab\nfunction match = SIFTSimpleMatcher(descriptor1, descriptor2, thresh)\n% SIFTSimpleMatcher\n%   Match one set of SIFT descriptors (descriptor1) to another set of\n%   descriptors (decriptor2). Each descriptor from descriptor1 can at\n%   most be matched to one member of descriptor2, but descriptors from\n%   descriptor2 can be matched more than once.\n%   \n%   Matches are determined as follows:\n%   For each descriptor vector in descriptor1, find the Euclidean distance\n%   between it and each descriptor vector in descriptor2. If the smallest\n%   distance is less than thresh*(the next smallest distance), we say that\n%   the two vectors are a match, and we add the row [d1 index, d2 index] to\n%   the \"match\" array.\n%   \n% INPUT:\n%   descriptor1: N1 * 128 matrix, each row is a SIFT descriptor.\n%   descriptor2: N2 * 128 matrix, each row is a SIFT descriptor.\n%   thresh: a given threshold of ratio. Typically 0.7\n%\n% OUTPUT:\n%   Match: N * 2 matrix, each row is a match.\n%          For example, Match(k, :) = [i, j] means i-th descriptor in\n%          descriptor1 is matched to j-th descriptor in descriptor2.\n    if ~exist('thresh', 'var'),\n        thresh = 0.7;\n    end\n\n    match = [];\n    [N1, ~] = size(descriptor1);\n    for i = 1:N1\n        fea = descriptor1(i, :);\n        err = bsxfun(@minus, fea, descriptor2);\n        dis = sqrt(sum(err.^2, 2));\n        [sorted_dis, ind] = sort(dis, 1);\n        if sorted_dis(1) < thresh * sorted_dis(2)\n            match = [match; [i, ind(1)]];\n        end\n    end\nend\n\n```\n\n接下来，我们可以使用最小二乘法计算两幅图像之间的仿射变换矩阵（齐次变换矩阵）$H$。其中$H$满足：\n$$Hp_{\\text{before}} = p_{\\text{after}}$$\n\n其中\n$$p = \\begin{bmatrix}x \\\\\\\\ y \\\\\\\\ 1\\end{bmatrix}$$\n\n对上式稍作变形，有\n$$p_{\\text{before}}^\\dagger H^\\dagger = p_{\\text{after}}\\dagger$$\n\n就可以使用标准的最小二乘正则方程进行求解了。代码如下：\n\n``` matlab\nfunction H = ComputeAffineMatrix( Pt1, Pt2 )\n%ComputeAffineMatrix\n%   Computes the transformation matrix that transforms a point from\n%   coordinate frame 1 to coordinate frame 2\n%Input:\n%   Pt1: N * 2 matrix, each row is a point in image 1\n%       (N must be at least 3)\n%   Pt2: N * 2 matrix, each row is the point in image 2 that\n%       matches the same point in image 1 (N should be more than 3)\n%Output:\n%   H: 3 * 3 affine transformation matrix,\n%       such that H*pt1(i,:) = pt2(i,:)\n\n    N = size(Pt1,1);\n    if size(Pt1, 1) ~= size(Pt2, 1),\n        error('Dimensions unmatched.');\n    elseif N<3\n        error('At least 3 points are required.');\n    end\n\n    % Convert the input points to homogeneous coordintes.\n    P1 = [Pt1';ones(1,N)];\n    P2 = [Pt2';ones(1,N)];\n\n    H = P1*P1'\\P1*P2';\n    H = H';\n\n    % Sometimes numerical issues cause least-squares to produce a bottom\n    % row which is not exactly [0 0 1], which confuses some of the later\n    % code. So we'll ensure the bottom row is exactly [0 0 1].\n    H(3,:) = [0 0 1];\nend\n```\n\n作业中的其他例子也很有趣，这里不再多说了。贴上两张图像拼接的实验结果吧~\n![result 1](/img/sift_experiment_1.png)\n![result 2](/img/sift_experiment_2.png)\n","source":"_posts/cs131-sift.md","raw":"---\ntitle: CS131-描述图像的特征(SIFT)\ndate: 2017-01-30 22:16:18\ntags:\n     - cs131\n     - 公开课\n---\n\n[SIFT(尺度不变特征变换，Scale Invariant Feature Transform)](https://en.wikipedia.org/wiki/Scale-invariant_feature_transform),最早由Lowe提出，目的是为了解决目标检测（Object Detection）问题中提取图像特征的问题。从名字可以看出，SIFT的优势就在于对尺度变换的不变性，同时SIFT还满足平移变换和旋转变换下的不变性，并对光照变化和3D视角变换有一定的不变性。它的主要步骤如下：\n- scale space上的极值检测。使用DoG在不同的scaleast和image position下找到interest point。\n- interest point的localization。对上面的interest point进行稳定度检测，并确定其所在的scale和position。\n- 确定方向。通过计算图像的梯度图，确定key point的方向，下一步的feature operation就是在这个方向，scale和position上进行的。\n- 确定key point的描述子。使用图像的局部梯度作为key point的描述子，最终构成SIFT特征向量。\n\n![SIFT图示](/img/sift_picture.jpg)\n\n<!-- more -->\n## SIFT介绍\n上讲中介绍的Harris角点方法计算简便，并具有平移不变性和旋转不变性。特征$f$对某种变换$\\mathcal{T}$具有不变性，是指在经过变换后原特征保持不变，也就是$f(I) = f(\\mathcal{T}(I))$。但是Harris角点不具有尺度变换不变性，如下图所示。当图像被放大后，原图的角点被判定为了边缘点。\n![harris的尺度变换不满足尺度不变性](/img/harris_non_scale_constant.png)\n\n而我们想要得到一种对尺度变换保持不变性的特征计算方法。例如图像patch的像素平均亮度，如下图所示。region size缩小为原始的一半后，亮度直方图的形状不变，即平均亮度不变。\n![平均亮度满足尺度变化呢不变性](/img/patch_average_intensity_scale_constant.png)\n\n而Lowe想到了使用局部极值来作为feature来保证对scale变换的不变性。在算法的具体实现中，他使用DoG来获得局部极值。\n\n[Lowe的论文](http://www.cs.ubc.ca/~lowe/papers/ijcv04.pdf)中也提到了SIFT特征的应用。SIFT特征可以产生稠密（dense）的key point，例如一张500x500像素大小的图像，一般可以产生~2000个稳定的SIFT特征。在进行image matching和recognition时，可以将ref image的SIFT特征提前计算出来保存在数据库中，并计算待处理图像的SIFT特征，根据特征向量的欧氏距离进行匹配。\n\n这篇博客主要是[Lowe上述论文](http://www.cs.ubc.ca/~lowe/papers/ijcv04.pdf)的读书笔记，按照SIFT特征的计算步骤进行组织。\n\n## 尺度空间极值的检测方法\n前人研究已经指出，在一系列合理假设下，高斯核是唯一满足尺度不变性的。所谓的尺度空间，就是指原始图像$I(x,y)$和可变尺度的高斯核$G(x,y,\\sigma)$的卷积结果。如下式所示：\n$$L(x,y,\\sigma) = G(x,y,\\sigma)\\ast I(x,y)$$\n\n其中，$G(x,y, \\sigma) = \\frac{1}{2\\pi\\sigma^2}\\exp(-(x^2+y^2)/2\\sigma^2)$。不同的$\\sigma$代表不同的尺度。\n\nDoG(difference of Gaussian)函数定义为不同尺度的高斯核与图像卷积结果之差，即，\n$$D(x,y,\\sigma) = L(x,y,k\\sigma) - L(x,y,\\sigma)$$\n\n如下图所示，输入的图像重复地与高斯核进行卷积得到结果图像$L$（左侧），这样构成了一个octave。相邻的$L$之间作差得到DoG图像（右侧）。当一个octave处理完之后，将当前octave的最后一张高斯卷积结果图像降采样两倍，按照前述方法构造下一个octave。在图中，我们将一个octave划分为$s$个间隔（即$s+1$个图像）时，设$\\sigma$最终变成了2倍（即$\\sigma$加倍）。那么，显然有相邻两层之间的$k = 2^{1/s}$。不过为了保证首尾两张图像也能够计算差值，我们实际上需要再补上两张（做一个padding），也就是说一个octave内的总图像为$s+3$（下图中的$s=2$）。\n![DoG的计算](/img/sift_dog.png)\n\n为何我们要费力气得到DoG呢？论文中作者给出的解释是：DoG对scale-normalized后的Guassian Laplace函数$\\sigma^2\\Delta G$提供了足够的近似。其中前面的$\\sigma^2$系数正是为了保证尺度不变性。而前人的研究则指出，$\\sigma \\Delta G$函数的极值，和其他一大票其他function比较时，能够提供最稳定的feature。\n\n对于高斯核函数，有以下性质：\n$$\\frac{\\partial G}{\\partial \\sigma} = \\sigma \\Delta G$$\n\n我们将式子左侧的微分变成差分，得到了下式：\n$$\\sigma\\Delta G \\approx \\frac{G(x,y,k\\sigma)-G(x,y,\\sigma)}{k\\sigma - \\sigma}$$\n\n也就是：\n$$G(x,y,k\\sigma)-G(x,y,\\sigma) \\approx (k-1)\\sigma^2 \\Delta G$$\n当$k=1$时，上式的近似误差为0（即上面的$s=\\infty$，也就是说octave的划分是连续的）。但是当$k$较大时，如$\\sqrt{2}$（这时$s=2$，octave划分十分粗糙了），仍然保证了稳定性。同时，由于相邻两层的比例$k$是定值，所以$k-1$这个因子对于检测极值没有影响。我们在计算时，无需除以$k-1$。\n\n构造完了DoG图像金字塔，下面我们的任务就是检测其中的极值。如下图，对于每个点，我们将它与金字塔中相邻的26个点作比较，找到局部极值。\n![检测极值](/img/sift_detection_maximum.png)\n\n另外，我们将输入图像行列预先使用线性插值的方法resize为原来的2倍，获取更多的key point。\n\n此外，在Lowe的论文中还提到了使用DoG的泰勒展开和海森矩阵进行key point的精细确定方法，并对key point进行过滤。这里也不再赘述了。\n\n## 128维feature的获取\n我们需要在每个key point处提取128维的特征向量。这里结合CS131课程作业2的相关练习作出说明。对于每个key point，我们关注它位于图像金字塔中的具体层数以及像素点位置，也就是说通过索引`pyramid{scale}(y, x)`就可以获取key point。我们遍历key point，为它们赋予一个128维的特征向量。\n\n我们选取point周围16x16大小的区域，称为一个patch。将其分为4x4共计16个cell。这样，每个cell内共有像素点16个。对于每个cell，我们计算它的局部梯度方向直方图，直方图共有8个bin。也就是说每个cell可以得到一个8维的特征向量。将16个cell的特征向量首尾相接，就得到了128维的SIFT特征向量。在下面的代码中，使用变量`patch_mag`和`patch_theta`分别代表patch的梯度幅值和角度，它们可以很简单地使用卷积和数学运算得到。\n\n``` matlab\npatch_mag = sqrt(patch_dx.^2 + patch_dy.^2);\npatch_theta = atan2(patch_dy, patch_dx);  % atan2的返回结果在区间[-pi, pi]上。\npatch_theta = mod(patch_theta, 2*pi);   % 这里我们要将其转换为[0, 2pi]\n```\n\n之后，我们需要获取key point的主方向。其定义可见slide，即为key point扩展出来的patch的梯度方向直方图的峰值对应的角度。\n![何为主方向](/img/sift_dominant_orientation.png)\n\n所以我们首先应该设计如何构建局部梯度方向直方图。我们只要将`[0, 2pi]`区间划分为若干个`bin`，并将patch内的每个点使用其梯度大小向对应的`bin`内投票即可。如下所示：\n\n``` matlab\nfunction [histogram, angles] = ComputeGradientHistogram(num_bins, gradient_magnitudes, gradient_angles)\n% Compute a gradient histogram using gradient magnitudes and directions.\n% Each point is assigned to one of num_bins depending on its gradient\n% direction; the gradient magnitude of that point is added to its bin.\n%\n% INPUT\n% num_bins: The number of bins to which points should be assigned.\n% gradient_magnitudes, gradient angles:\n%       Two arrays of the same shape where gradient_magnitudes(i) and\n%       gradient_angles(i) give the magnitude and direction of the gradient\n%       for the ith point. gradient_angles ranges from 0 to 2*pi\n%                                      \n% OUTPUT\n% histogram: A 1 x num_bins array containing the gradient histogram. Entry 1 is\n%       the sum of entries in gradient_magnitudes whose corresponding\n%       gradient_angles lie between 0 and angle_step. Similarly, entry 2 is for\n%       angles between angle_step and 2*angle_step. Angle_step is calculated as\n%       2*pi/num_bins.\n\n% angles: A 1 x num_bins array which holds the histogram bin lower bounds.\n%       In other words, histogram(i) contains the sum of the\n%       gradient magnitudes of all points whose gradient directions fall\n%       in the range [angles(i), angles(i + 1))\n\n    angle_step = 2 * pi / num_bins;\n    angles = 0 : angle_step : (2*pi-angle_step);\n\n    histogram = zeros(1, num_bins);\n    num = numel(gradient_angles);\n    for n = 1:num\n        index = floor(gradient_angles(n) / angle_step) + 1;\n        histogram(index) = histogram(index) + gradient_magnitudes(n);\n    end    \nend\n\n```\n\nLowe论文中推荐的`bin`数目为36个，计算主方向的函数如下：\n\n``` matlab\nfunction direction = ComputeDominantDirection(gradient_magnitudes, gradient_angles)\n% Computes the dominant gradient direction for the region around a keypoint\n% given the scale of the keypoint and the gradient magnitudes and gradient\n% angles of the pixels in the region surrounding the keypoint.\n%\n% INPUT\n% gradient_magnitudes, gradient_angles:\n%   Two arrays of the same shape where gradient_magnitudes(i) and\n%   gradient_angles(i) give the magnitude and direction of the gradient for\n%   the ith point.\n\n    % Compute a gradient histogram using the weighted gradient magnitudes.\n    % In David Lowe's paper he suggests using 36 bins for this histogram.\n    num_bins = 36;\n    % Step 1:\n    % compute the 36-bin histogram of angles using ComputeGradientHistogram()\n    [histogram, angle_bound] = ComputeGradientHistogram(num_bins, gradient_magnitudes, gradient_angles);\n    % Step 2:\n    % Find the maximum value of the gradient histogram, and set \"direction\"\n    % to the angle corresponding to the maximum. (To match our solutions,\n    % just use the lower-bound angle of the max histogram bin. (E.g. return\n    % 0 radians if it's bin 1.)\n    [~, max_index] = max(histogram);\n    direction = angle_bound(max_index);\nend\n```\n\n之后，我们更新patch内各点的梯度方向，计算其与主方向的夹角，作为新的方向。并将梯度进行高斯平滑。\n\n``` matlab\npatch_theta = patch_theta - ComputeDominantDirection(patch_mag, patch_theta);;\npatch_theta = mod(patch_theta, 2*pi);\npatch_mag = patch_mag .* fspecial('gaussian', patch_size, patch_size / 2); % patch_size = 16\n```\n\n遍历cell，计算feature如下：\n\n``` matlab\nfeature = [];\nrow_iter = 1;\nfor y = 1:num_histograms\n    col_iter = 1;\n    for x = 1:num_histograms\n        cell_mag = patch_mag(row_iter: row_iter + pixelsPerHistogram - 1, ...\n                             col_iter: col_iter + pixelsPerHistogram - 1);\n        cell_theta = patch_theta(row_iter: row_iter + pixelsPerHistogram - 1, ...\n                             col_iter: col_iter + pixelsPerHistogram - 1);\n        [histogram, ~] = ComputeGradientHistogram(num_angles, cell_mag, cell_theta);\n        feature = [feature, histogram];\n        col_iter = col_iter + pixelsPerHistogram;\n    end\n    row_iter = row_iter + pixelsPerHistogram;\nend\n```\n\n最后，对feature做normalization。首先将feature化为单位长度，并将其中太大的分量进行限幅（如0.2的阈值），之后再重新将其转换为单位长度。\n\n这样，就完成了SIFT特征的计算。在Lowe的论文中，有更多对实现细节的讨论，这里只是跟随课程slide和作业走完了算法流程，不再赘述。\n\n## 应用：图像特征点匹配\n和Harris角点一样，SIFT特征可以用作两幅图像的特征点匹配，并且具有多种变换不变性的优点。对于两幅图像分别计算得到的特征点SIFT特征向量，可以使用下面简单的方法搜寻匹配点。计算每组点对之间的欧式距离，如果最近的距离比第二近的距离小得多，那么可以认为有一对成功匹配的特征点。其MATLAB代码如下，其中`descriptor`是两幅图像的SIFT特征向量。阈值默认为取做0.7。\n\n``` matlab\nfunction match = SIFTSimpleMatcher(descriptor1, descriptor2, thresh)\n% SIFTSimpleMatcher\n%   Match one set of SIFT descriptors (descriptor1) to another set of\n%   descriptors (decriptor2). Each descriptor from descriptor1 can at\n%   most be matched to one member of descriptor2, but descriptors from\n%   descriptor2 can be matched more than once.\n%   \n%   Matches are determined as follows:\n%   For each descriptor vector in descriptor1, find the Euclidean distance\n%   between it and each descriptor vector in descriptor2. If the smallest\n%   distance is less than thresh*(the next smallest distance), we say that\n%   the two vectors are a match, and we add the row [d1 index, d2 index] to\n%   the \"match\" array.\n%   \n% INPUT:\n%   descriptor1: N1 * 128 matrix, each row is a SIFT descriptor.\n%   descriptor2: N2 * 128 matrix, each row is a SIFT descriptor.\n%   thresh: a given threshold of ratio. Typically 0.7\n%\n% OUTPUT:\n%   Match: N * 2 matrix, each row is a match.\n%          For example, Match(k, :) = [i, j] means i-th descriptor in\n%          descriptor1 is matched to j-th descriptor in descriptor2.\n    if ~exist('thresh', 'var'),\n        thresh = 0.7;\n    end\n\n    match = [];\n    [N1, ~] = size(descriptor1);\n    for i = 1:N1\n        fea = descriptor1(i, :);\n        err = bsxfun(@minus, fea, descriptor2);\n        dis = sqrt(sum(err.^2, 2));\n        [sorted_dis, ind] = sort(dis, 1);\n        if sorted_dis(1) < thresh * sorted_dis(2)\n            match = [match; [i, ind(1)]];\n        end\n    end\nend\n\n```\n\n接下来，我们可以使用最小二乘法计算两幅图像之间的仿射变换矩阵（齐次变换矩阵）$H$。其中$H$满足：\n$$Hp_{\\text{before}} = p_{\\text{after}}$$\n\n其中\n$$p = \\begin{bmatrix}x \\\\\\\\ y \\\\\\\\ 1\\end{bmatrix}$$\n\n对上式稍作变形，有\n$$p_{\\text{before}}^\\dagger H^\\dagger = p_{\\text{after}}\\dagger$$\n\n就可以使用标准的最小二乘正则方程进行求解了。代码如下：\n\n``` matlab\nfunction H = ComputeAffineMatrix( Pt1, Pt2 )\n%ComputeAffineMatrix\n%   Computes the transformation matrix that transforms a point from\n%   coordinate frame 1 to coordinate frame 2\n%Input:\n%   Pt1: N * 2 matrix, each row is a point in image 1\n%       (N must be at least 3)\n%   Pt2: N * 2 matrix, each row is the point in image 2 that\n%       matches the same point in image 1 (N should be more than 3)\n%Output:\n%   H: 3 * 3 affine transformation matrix,\n%       such that H*pt1(i,:) = pt2(i,:)\n\n    N = size(Pt1,1);\n    if size(Pt1, 1) ~= size(Pt2, 1),\n        error('Dimensions unmatched.');\n    elseif N<3\n        error('At least 3 points are required.');\n    end\n\n    % Convert the input points to homogeneous coordintes.\n    P1 = [Pt1';ones(1,N)];\n    P2 = [Pt2';ones(1,N)];\n\n    H = P1*P1'\\P1*P2';\n    H = H';\n\n    % Sometimes numerical issues cause least-squares to produce a bottom\n    % row which is not exactly [0 0 1], which confuses some of the later\n    % code. So we'll ensure the bottom row is exactly [0 0 1].\n    H(3,:) = [0 0 1];\nend\n```\n\n作业中的其他例子也很有趣，这里不再多说了。贴上两张图像拼接的实验结果吧~\n![result 1](/img/sift_experiment_1.png)\n![result 2](/img/sift_experiment_2.png)\n","slug":"cs131-sift","published":1,"updated":"2017-02-26T12:09:28.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cizve07af000el61h25osqi6t","content":"<p><a href=\"https://en.wikipedia.org/wiki/Scale-invariant_feature_transform\" target=\"_blank\" rel=\"external\">SIFT(尺度不变特征变换，Scale Invariant Feature Transform)</a>,最早由Lowe提出，目的是为了解决目标检测（Object Detection）问题中提取图像特征的问题。从名字可以看出，SIFT的优势就在于对尺度变换的不变性，同时SIFT还满足平移变换和旋转变换下的不变性，并对光照变化和3D视角变换有一定的不变性。它的主要步骤如下：</p>\n<ul>\n<li>scale space上的极值检测。使用DoG在不同的scaleast和image position下找到interest point。</li>\n<li>interest point的localization。对上面的interest point进行稳定度检测，并确定其所在的scale和position。</li>\n<li>确定方向。通过计算图像的梯度图，确定key point的方向，下一步的feature operation就是在这个方向，scale和position上进行的。</li>\n<li>确定key point的描述子。使用图像的局部梯度作为key point的描述子，最终构成SIFT特征向量。</li>\n</ul>\n<p><img src=\"/img/sift_picture.jpg\" alt=\"SIFT图示\"></p>\n<a id=\"more\"></a>\n<h2 id=\"SIFT介绍\"><a href=\"#SIFT介绍\" class=\"headerlink\" title=\"SIFT介绍\"></a>SIFT介绍</h2><p>上讲中介绍的Harris角点方法计算简便，并具有平移不变性和旋转不变性。特征$f$对某种变换$\\mathcal{T}$具有不变性，是指在经过变换后原特征保持不变，也就是$f(I) = f(\\mathcal{T}(I))$。但是Harris角点不具有尺度变换不变性，如下图所示。当图像被放大后，原图的角点被判定为了边缘点。<br><img src=\"/img/harris_non_scale_constant.png\" alt=\"harris的尺度变换不满足尺度不变性\"></p>\n<p>而我们想要得到一种对尺度变换保持不变性的特征计算方法。例如图像patch的像素平均亮度，如下图所示。region size缩小为原始的一半后，亮度直方图的形状不变，即平均亮度不变。<br><img src=\"/img/patch_average_intensity_scale_constant.png\" alt=\"平均亮度满足尺度变化呢不变性\"></p>\n<p>而Lowe想到了使用局部极值来作为feature来保证对scale变换的不变性。在算法的具体实现中，他使用DoG来获得局部极值。</p>\n<p><a href=\"http://www.cs.ubc.ca/~lowe/papers/ijcv04.pdf\" target=\"_blank\" rel=\"external\">Lowe的论文</a>中也提到了SIFT特征的应用。SIFT特征可以产生稠密（dense）的key point，例如一张500x500像素大小的图像，一般可以产生~2000个稳定的SIFT特征。在进行image matching和recognition时，可以将ref image的SIFT特征提前计算出来保存在数据库中，并计算待处理图像的SIFT特征，根据特征向量的欧氏距离进行匹配。</p>\n<p>这篇博客主要是<a href=\"http://www.cs.ubc.ca/~lowe/papers/ijcv04.pdf\" target=\"_blank\" rel=\"external\">Lowe上述论文</a>的读书笔记，按照SIFT特征的计算步骤进行组织。</p>\n<h2 id=\"尺度空间极值的检测方法\"><a href=\"#尺度空间极值的检测方法\" class=\"headerlink\" title=\"尺度空间极值的检测方法\"></a>尺度空间极值的检测方法</h2><p>前人研究已经指出，在一系列合理假设下，高斯核是唯一满足尺度不变性的。所谓的尺度空间，就是指原始图像$I(x,y)$和可变尺度的高斯核$G(x,y,\\sigma)$的卷积结果。如下式所示：</p>\n<script type=\"math/tex; mode=display\">L(x,y,\\sigma) = G(x,y,\\sigma)\\ast I(x,y)</script><p>其中，$G(x,y, \\sigma) = \\frac{1}{2\\pi\\sigma^2}\\exp(-(x^2+y^2)/2\\sigma^2)$。不同的$\\sigma$代表不同的尺度。</p>\n<p>DoG(difference of Gaussian)函数定义为不同尺度的高斯核与图像卷积结果之差，即，</p>\n<script type=\"math/tex; mode=display\">D(x,y,\\sigma) = L(x,y,k\\sigma) - L(x,y,\\sigma)</script><p>如下图所示，输入的图像重复地与高斯核进行卷积得到结果图像$L$（左侧），这样构成了一个octave。相邻的$L$之间作差得到DoG图像（右侧）。当一个octave处理完之后，将当前octave的最后一张高斯卷积结果图像降采样两倍，按照前述方法构造下一个octave。在图中，我们将一个octave划分为$s$个间隔（即$s+1$个图像）时，设$\\sigma$最终变成了2倍（即$\\sigma$加倍）。那么，显然有相邻两层之间的$k = 2^{1/s}$。不过为了保证首尾两张图像也能够计算差值，我们实际上需要再补上两张（做一个padding），也就是说一个octave内的总图像为$s+3$（下图中的$s=2$）。<br><img src=\"/img/sift_dog.png\" alt=\"DoG的计算\"></p>\n<p>为何我们要费力气得到DoG呢？论文中作者给出的解释是：DoG对scale-normalized后的Guassian Laplace函数$\\sigma^2\\Delta G$提供了足够的近似。其中前面的$\\sigma^2$系数正是为了保证尺度不变性。而前人的研究则指出，$\\sigma \\Delta G$函数的极值，和其他一大票其他function比较时，能够提供最稳定的feature。</p>\n<p>对于高斯核函数，有以下性质：</p>\n<script type=\"math/tex; mode=display\">\\frac{\\partial G}{\\partial \\sigma} = \\sigma \\Delta G</script><p>我们将式子左侧的微分变成差分，得到了下式：</p>\n<script type=\"math/tex; mode=display\">\\sigma\\Delta G \\approx \\frac{G(x,y,k\\sigma)-G(x,y,\\sigma)}{k\\sigma - \\sigma}</script><p>也就是：</p>\n<script type=\"math/tex; mode=display\">G(x,y,k\\sigma)-G(x,y,\\sigma) \\approx (k-1)\\sigma^2 \\Delta G</script><p>当$k=1$时，上式的近似误差为0（即上面的$s=\\infty$，也就是说octave的划分是连续的）。但是当$k$较大时，如$\\sqrt{2}$（这时$s=2$，octave划分十分粗糙了），仍然保证了稳定性。同时，由于相邻两层的比例$k$是定值，所以$k-1$这个因子对于检测极值没有影响。我们在计算时，无需除以$k-1$。</p>\n<p>构造完了DoG图像金字塔，下面我们的任务就是检测其中的极值。如下图，对于每个点，我们将它与金字塔中相邻的26个点作比较，找到局部极值。<br><img src=\"/img/sift_detection_maximum.png\" alt=\"检测极值\"></p>\n<p>另外，我们将输入图像行列预先使用线性插值的方法resize为原来的2倍，获取更多的key point。</p>\n<p>此外，在Lowe的论文中还提到了使用DoG的泰勒展开和海森矩阵进行key point的精细确定方法，并对key point进行过滤。这里也不再赘述了。</p>\n<h2 id=\"128维feature的获取\"><a href=\"#128维feature的获取\" class=\"headerlink\" title=\"128维feature的获取\"></a>128维feature的获取</h2><p>我们需要在每个key point处提取128维的特征向量。这里结合CS131课程作业2的相关练习作出说明。对于每个key point，我们关注它位于图像金字塔中的具体层数以及像素点位置，也就是说通过索引<code>pyramid{scale}(y, x)</code>就可以获取key point。我们遍历key point，为它们赋予一个128维的特征向量。</p>\n<p>我们选取point周围16x16大小的区域，称为一个patch。将其分为4x4共计16个cell。这样，每个cell内共有像素点16个。对于每个cell，我们计算它的局部梯度方向直方图，直方图共有8个bin。也就是说每个cell可以得到一个8维的特征向量。将16个cell的特征向量首尾相接，就得到了128维的SIFT特征向量。在下面的代码中，使用变量<code>patch_mag</code>和<code>patch_theta</code>分别代表patch的梯度幅值和角度，它们可以很简单地使用卷积和数学运算得到。</p>\n<figure class=\"highlight matlab\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\">patch_mag = <span class=\"built_in\">sqrt</span>(patch_dx.^<span class=\"number\">2</span> + patch_dy.^<span class=\"number\">2</span>);</div><div class=\"line\">patch_theta = <span class=\"built_in\">atan2</span>(patch_dy, patch_dx);  <span class=\"comment\">% atan2的返回结果在区间[-pi, pi]上。</span></div><div class=\"line\">patch_theta = <span class=\"built_in\">mod</span>(patch_theta, <span class=\"number\">2</span>*<span class=\"built_in\">pi</span>);   <span class=\"comment\">% 这里我们要将其转换为[0, 2pi]</span></div></pre></td></tr></table></figure>\n<p>之后，我们需要获取key point的主方向。其定义可见slide，即为key point扩展出来的patch的梯度方向直方图的峰值对应的角度。<br><img src=\"/img/sift_dominant_orientation.png\" alt=\"何为主方向\"></p>\n<p>所以我们首先应该设计如何构建局部梯度方向直方图。我们只要将<code>[0, 2pi]</code>区间划分为若干个<code>bin</code>，并将patch内的每个点使用其梯度大小向对应的<code>bin</code>内投票即可。如下所示：</p>\n<figure class=\"highlight matlab\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"function\"><span class=\"keyword\">function</span> <span class=\"params\">[histogram, angles]</span> = <span class=\"title\">ComputeGradientHistogram</span><span class=\"params\">(num_bins, gradient_magnitudes, gradient_angles)</span></span></div><div class=\"line\"><span class=\"comment\">% Compute a gradient histogram using gradient magnitudes and directions.</span></div><div class=\"line\"><span class=\"comment\">% Each point is assigned to one of num_bins depending on its gradient</span></div><div class=\"line\"><span class=\"comment\">% direction; the gradient magnitude of that point is added to its bin.</span></div><div class=\"line\"><span class=\"comment\">%</span></div><div class=\"line\"><span class=\"comment\">% INPUT</span></div><div class=\"line\"><span class=\"comment\">% num_bins: The number of bins to which points should be assigned.</span></div><div class=\"line\"><span class=\"comment\">% gradient_magnitudes, gradient angles:</span></div><div class=\"line\"><span class=\"comment\">%       Two arrays of the same shape where gradient_magnitudes(i) and</span></div><div class=\"line\"><span class=\"comment\">%       gradient_angles(i) give the magnitude and direction of the gradient</span></div><div class=\"line\"><span class=\"comment\">%       for the ith point. gradient_angles ranges from 0 to 2*pi</span></div><div class=\"line\"><span class=\"comment\">%                                      </span></div><div class=\"line\"><span class=\"comment\">% OUTPUT</span></div><div class=\"line\"><span class=\"comment\">% histogram: A 1 x num_bins array containing the gradient histogram. Entry 1 is</span></div><div class=\"line\"><span class=\"comment\">%       the sum of entries in gradient_magnitudes whose corresponding</span></div><div class=\"line\"><span class=\"comment\">%       gradient_angles lie between 0 and angle_step. Similarly, entry 2 is for</span></div><div class=\"line\"><span class=\"comment\">%       angles between angle_step and 2*angle_step. Angle_step is calculated as</span></div><div class=\"line\"><span class=\"comment\">%       2*pi/num_bins.</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">% angles: A 1 x num_bins array which holds the histogram bin lower bounds.</span></div><div class=\"line\"><span class=\"comment\">%       In other words, histogram(i) contains the sum of the</span></div><div class=\"line\"><span class=\"comment\">%       gradient magnitudes of all points whose gradient directions fall</span></div><div class=\"line\"><span class=\"comment\">%       in the range [angles(i), angles(i + 1))</span></div><div class=\"line\"></div><div class=\"line\">    angle_step = <span class=\"number\">2</span> * <span class=\"built_in\">pi</span> / num_bins;</div><div class=\"line\">    angles = <span class=\"number\">0</span> : angle_step : (<span class=\"number\">2</span>*<span class=\"built_in\">pi</span>-angle_step);</div><div class=\"line\"></div><div class=\"line\">    histogram = <span class=\"built_in\">zeros</span>(<span class=\"number\">1</span>, num_bins);</div><div class=\"line\">    num = <span class=\"built_in\">numel</span>(gradient_angles);</div><div class=\"line\">    <span class=\"keyword\">for</span> n = <span class=\"number\">1</span>:num</div><div class=\"line\">        index = <span class=\"built_in\">floor</span>(gradient_angles(n) / angle_step) + <span class=\"number\">1</span>;</div><div class=\"line\">        histogram(index) = histogram(index) + gradient_magnitudes(n);</div><div class=\"line\">    <span class=\"keyword\">end</span>    </div><div class=\"line\"><span class=\"keyword\">end</span></div></pre></td></tr></table></figure>\n<p>Lowe论文中推荐的<code>bin</code>数目为36个，计算主方向的函数如下：</p>\n<figure class=\"highlight matlab\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"function\"><span class=\"keyword\">function</span> <span class=\"title\">direction</span> = <span class=\"title\">ComputeDominantDirection</span><span class=\"params\">(gradient_magnitudes, gradient_angles)</span></span></div><div class=\"line\"><span class=\"comment\">% Computes the dominant gradient direction for the region around a keypoint</span></div><div class=\"line\"><span class=\"comment\">% given the scale of the keypoint and the gradient magnitudes and gradient</span></div><div class=\"line\"><span class=\"comment\">% angles of the pixels in the region surrounding the keypoint.</span></div><div class=\"line\"><span class=\"comment\">%</span></div><div class=\"line\"><span class=\"comment\">% INPUT</span></div><div class=\"line\"><span class=\"comment\">% gradient_magnitudes, gradient_angles:</span></div><div class=\"line\"><span class=\"comment\">%   Two arrays of the same shape where gradient_magnitudes(i) and</span></div><div class=\"line\"><span class=\"comment\">%   gradient_angles(i) give the magnitude and direction of the gradient for</span></div><div class=\"line\"><span class=\"comment\">%   the ith point.</span></div><div class=\"line\"></div><div class=\"line\">    <span class=\"comment\">% Compute a gradient histogram using the weighted gradient magnitudes.</span></div><div class=\"line\">    <span class=\"comment\">% In David Lowe's paper he suggests using 36 bins for this histogram.</span></div><div class=\"line\">    num_bins = <span class=\"number\">36</span>;</div><div class=\"line\">    <span class=\"comment\">% Step 1:</span></div><div class=\"line\">    <span class=\"comment\">% compute the 36-bin histogram of angles using ComputeGradientHistogram()</span></div><div class=\"line\">    [histogram, angle_bound] = ComputeGradientHistogram(num_bins, gradient_magnitudes, gradient_angles);</div><div class=\"line\">    <span class=\"comment\">% Step 2:</span></div><div class=\"line\">    <span class=\"comment\">% Find the maximum value of the gradient histogram, and set \"direction\"</span></div><div class=\"line\">    <span class=\"comment\">% to the angle corresponding to the maximum. (To match our solutions,</span></div><div class=\"line\">    <span class=\"comment\">% just use the lower-bound angle of the max histogram bin. (E.g. return</span></div><div class=\"line\">    <span class=\"comment\">% 0 radians if it's bin 1.)</span></div><div class=\"line\">    [~, max_index] = max(histogram);</div><div class=\"line\">    direction = angle_bound(max_index);</div><div class=\"line\"><span class=\"keyword\">end</span></div></pre></td></tr></table></figure>\n<p>之后，我们更新patch内各点的梯度方向，计算其与主方向的夹角，作为新的方向。并将梯度进行高斯平滑。</p>\n<figure class=\"highlight matlab\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\">patch_theta = patch_theta - ComputeDominantDirection(patch_mag, patch_theta);;</div><div class=\"line\">patch_theta = <span class=\"built_in\">mod</span>(patch_theta, <span class=\"number\">2</span>*<span class=\"built_in\">pi</span>);</div><div class=\"line\">patch_mag = patch_mag .* fspecial(<span class=\"string\">'gaussian'</span>, patch_size, patch_size / <span class=\"number\">2</span>); <span class=\"comment\">% patch_size = 16</span></div></pre></td></tr></table></figure>\n<p>遍历cell，计算feature如下：</p>\n<figure class=\"highlight matlab\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div></pre></td><td class=\"code\"><pre><div class=\"line\">feature = [];</div><div class=\"line\">row_iter = <span class=\"number\">1</span>;</div><div class=\"line\"><span class=\"keyword\">for</span> y = <span class=\"number\">1</span>:num_histograms</div><div class=\"line\">    col_iter = <span class=\"number\">1</span>;</div><div class=\"line\">    <span class=\"keyword\">for</span> x = <span class=\"number\">1</span>:num_histograms</div><div class=\"line\">        cell_mag = patch_mag(row_iter: row_iter + pixelsPerHistogram - <span class=\"number\">1</span>, ...</div><div class=\"line\">                             col_iter: col_iter + pixelsPerHistogram - <span class=\"number\">1</span>);</div><div class=\"line\">        cell_theta = patch_theta(row_iter: row_iter + pixelsPerHistogram - <span class=\"number\">1</span>, ...</div><div class=\"line\">                             col_iter: col_iter + pixelsPerHistogram - <span class=\"number\">1</span>);</div><div class=\"line\">        [histogram, ~] = ComputeGradientHistogram(num_angles, cell_mag, cell_theta);</div><div class=\"line\">        feature = [feature, histogram];</div><div class=\"line\">        col_iter = col_iter + pixelsPerHistogram;</div><div class=\"line\">    <span class=\"keyword\">end</span></div><div class=\"line\">    row_iter = row_iter + pixelsPerHistogram;</div><div class=\"line\"><span class=\"keyword\">end</span></div></pre></td></tr></table></figure>\n<p>最后，对feature做normalization。首先将feature化为单位长度，并将其中太大的分量进行限幅（如0.2的阈值），之后再重新将其转换为单位长度。</p>\n<p>这样，就完成了SIFT特征的计算。在Lowe的论文中，有更多对实现细节的讨论，这里只是跟随课程slide和作业走完了算法流程，不再赘述。</p>\n<h2 id=\"应用：图像特征点匹配\"><a href=\"#应用：图像特征点匹配\" class=\"headerlink\" title=\"应用：图像特征点匹配\"></a>应用：图像特征点匹配</h2><p>和Harris角点一样，SIFT特征可以用作两幅图像的特征点匹配，并且具有多种变换不变性的优点。对于两幅图像分别计算得到的特征点SIFT特征向量，可以使用下面简单的方法搜寻匹配点。计算每组点对之间的欧式距离，如果最近的距离比第二近的距离小得多，那么可以认为有一对成功匹配的特征点。其MATLAB代码如下，其中<code>descriptor</code>是两幅图像的SIFT特征向量。阈值默认为取做0.7。</p>\n<figure class=\"highlight matlab\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"function\"><span class=\"keyword\">function</span> <span class=\"title\">match</span> = <span class=\"title\">SIFTSimpleMatcher</span><span class=\"params\">(descriptor1, descriptor2, thresh)</span></span></div><div class=\"line\"><span class=\"comment\">% SIFTSimpleMatcher</span></div><div class=\"line\"><span class=\"comment\">%   Match one set of SIFT descriptors (descriptor1) to another set of</span></div><div class=\"line\"><span class=\"comment\">%   descriptors (decriptor2). Each descriptor from descriptor1 can at</span></div><div class=\"line\"><span class=\"comment\">%   most be matched to one member of descriptor2, but descriptors from</span></div><div class=\"line\"><span class=\"comment\">%   descriptor2 can be matched more than once.</span></div><div class=\"line\"><span class=\"comment\">%   </span></div><div class=\"line\"><span class=\"comment\">%   Matches are determined as follows:</span></div><div class=\"line\"><span class=\"comment\">%   For each descriptor vector in descriptor1, find the Euclidean distance</span></div><div class=\"line\"><span class=\"comment\">%   between it and each descriptor vector in descriptor2. If the smallest</span></div><div class=\"line\"><span class=\"comment\">%   distance is less than thresh*(the next smallest distance), we say that</span></div><div class=\"line\"><span class=\"comment\">%   the two vectors are a match, and we add the row [d1 index, d2 index] to</span></div><div class=\"line\"><span class=\"comment\">%   the \"match\" array.</span></div><div class=\"line\"><span class=\"comment\">%   </span></div><div class=\"line\"><span class=\"comment\">% INPUT:</span></div><div class=\"line\"><span class=\"comment\">%   descriptor1: N1 * 128 matrix, each row is a SIFT descriptor.</span></div><div class=\"line\"><span class=\"comment\">%   descriptor2: N2 * 128 matrix, each row is a SIFT descriptor.</span></div><div class=\"line\"><span class=\"comment\">%   thresh: a given threshold of ratio. Typically 0.7</span></div><div class=\"line\"><span class=\"comment\">%</span></div><div class=\"line\"><span class=\"comment\">% OUTPUT:</span></div><div class=\"line\"><span class=\"comment\">%   Match: N * 2 matrix, each row is a match.</span></div><div class=\"line\"><span class=\"comment\">%          For example, Match(k, :) = [i, j] means i-th descriptor in</span></div><div class=\"line\"><span class=\"comment\">%          descriptor1 is matched to j-th descriptor in descriptor2.</span></div><div class=\"line\">    <span class=\"keyword\">if</span> ~exist(<span class=\"string\">'thresh'</span>, <span class=\"string\">'var'</span>),</div><div class=\"line\">        thresh = <span class=\"number\">0.7</span>;</div><div class=\"line\">    <span class=\"keyword\">end</span></div><div class=\"line\"></div><div class=\"line\">    match = [];</div><div class=\"line\">    [N1, ~] = <span class=\"built_in\">size</span>(descriptor1);</div><div class=\"line\">    <span class=\"keyword\">for</span> <span class=\"built_in\">i</span> = <span class=\"number\">1</span>:N1</div><div class=\"line\">        fea = descriptor1(<span class=\"built_in\">i</span>, :);</div><div class=\"line\">        err = <span class=\"built_in\">bsxfun</span>(@minus, fea, descriptor2);</div><div class=\"line\">        dis = <span class=\"built_in\">sqrt</span>(sum(err.^<span class=\"number\">2</span>, <span class=\"number\">2</span>));</div><div class=\"line\">        [sorted_dis, ind] = sort(dis, <span class=\"number\">1</span>);</div><div class=\"line\">        <span class=\"keyword\">if</span> sorted_dis(<span class=\"number\">1</span>) &lt; thresh * sorted_dis(<span class=\"number\">2</span>)</div><div class=\"line\">            match = [match; [i, ind(<span class=\"number\">1</span>)]];</div><div class=\"line\">        <span class=\"keyword\">end</span></div><div class=\"line\">    <span class=\"keyword\">end</span></div><div class=\"line\"><span class=\"keyword\">end</span></div></pre></td></tr></table></figure>\n<p>接下来，我们可以使用最小二乘法计算两幅图像之间的仿射变换矩阵（齐次变换矩阵）$H$。其中$H$满足：</p>\n<script type=\"math/tex; mode=display\">Hp_{\\text{before}} = p_{\\text{after}}</script><p>其中</p>\n<script type=\"math/tex; mode=display\">p = \\begin{bmatrix}x \\\\\\\\ y \\\\\\\\ 1\\end{bmatrix}</script><p>对上式稍作变形，有</p>\n<script type=\"math/tex; mode=display\">p_{\\text{before}}^\\dagger H^\\dagger = p_{\\text{after}}\\dagger</script><p>就可以使用标准的最小二乘正则方程进行求解了。代码如下：</p>\n<figure class=\"highlight matlab\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"function\"><span class=\"keyword\">function</span> <span class=\"title\">H</span> = <span class=\"title\">ComputeAffineMatrix</span><span class=\"params\">( Pt1, Pt2 )</span></span></div><div class=\"line\"><span class=\"comment\">%ComputeAffineMatrix</span></div><div class=\"line\"><span class=\"comment\">%   Computes the transformation matrix that transforms a point from</span></div><div class=\"line\"><span class=\"comment\">%   coordinate frame 1 to coordinate frame 2</span></div><div class=\"line\"><span class=\"comment\">%Input:</span></div><div class=\"line\"><span class=\"comment\">%   Pt1: N * 2 matrix, each row is a point in image 1</span></div><div class=\"line\"><span class=\"comment\">%       (N must be at least 3)</span></div><div class=\"line\"><span class=\"comment\">%   Pt2: N * 2 matrix, each row is the point in image 2 that</span></div><div class=\"line\"><span class=\"comment\">%       matches the same point in image 1 (N should be more than 3)</span></div><div class=\"line\"><span class=\"comment\">%Output:</span></div><div class=\"line\"><span class=\"comment\">%   H: 3 * 3 affine transformation matrix,</span></div><div class=\"line\"><span class=\"comment\">%       such that H*pt1(i,:) = pt2(i,:)</span></div><div class=\"line\"></div><div class=\"line\">    N = <span class=\"built_in\">size</span>(Pt1,<span class=\"number\">1</span>);</div><div class=\"line\">    <span class=\"keyword\">if</span> <span class=\"built_in\">size</span>(Pt1, <span class=\"number\">1</span>) ~= <span class=\"built_in\">size</span>(Pt2, <span class=\"number\">1</span>),</div><div class=\"line\">        error(<span class=\"string\">'Dimensions unmatched.'</span>);</div><div class=\"line\">    <span class=\"keyword\">elseif</span> N&lt;<span class=\"number\">3</span></div><div class=\"line\">        error(<span class=\"string\">'At least 3 points are required.'</span>);</div><div class=\"line\">    <span class=\"keyword\">end</span></div><div class=\"line\"></div><div class=\"line\">    <span class=\"comment\">% Convert the input points to homogeneous coordintes.</span></div><div class=\"line\">    P1 = [Pt1<span class=\"string\">';ones(1,N)];</span></div><div class=\"line\">    P2 = [Pt2';ones(<span class=\"number\">1</span>,N)];</div><div class=\"line\"></div><div class=\"line\">    H = P1*P1'\\P1*P2';</div><div class=\"line\">    H = H';</div><div class=\"line\"></div><div class=\"line\">    <span class=\"comment\">% Sometimes numerical issues cause least-squares to produce a bottom</span></div><div class=\"line\">    <span class=\"comment\">% row which is not exactly [0 0 1], which confuses some of the later</span></div><div class=\"line\">    <span class=\"comment\">% code. So we'll ensure the bottom row is exactly [0 0 1].</span></div><div class=\"line\">    H(<span class=\"number\">3</span>,:) = [<span class=\"number\">0</span> <span class=\"number\">0</span> <span class=\"number\">1</span>];</div><div class=\"line\"><span class=\"keyword\">end</span></div></pre></td></tr></table></figure>\n<p>作业中的其他例子也很有趣，这里不再多说了。贴上两张图像拼接的实验结果吧~<br><img src=\"/img/sift_experiment_1.png\" alt=\"result 1\"><br><img src=\"/img/sift_experiment_2.png\" alt=\"result 2\"></p>\n","excerpt":"<p><a href=\"https://en.wikipedia.org/wiki/Scale-invariant_feature_transform\">SIFT(尺度不变特征变换，Scale Invariant Feature Transform)</a>,最早由Lowe提出，目的是为了解决目标检测（Object Detection）问题中提取图像特征的问题。从名字可以看出，SIFT的优势就在于对尺度变换的不变性，同时SIFT还满足平移变换和旋转变换下的不变性，并对光照变化和3D视角变换有一定的不变性。它的主要步骤如下：</p>\n<ul>\n<li>scale space上的极值检测。使用DoG在不同的scaleast和image position下找到interest point。</li>\n<li>interest point的localization。对上面的interest point进行稳定度检测，并确定其所在的scale和position。</li>\n<li>确定方向。通过计算图像的梯度图，确定key point的方向，下一步的feature operation就是在这个方向，scale和position上进行的。</li>\n<li>确定key point的描述子。使用图像的局部梯度作为key point的描述子，最终构成SIFT特征向量。</li>\n</ul>\n<p><img src=\"/img/sift_picture.jpg\" alt=\"SIFT图示\"></p>","more":"<h2 id=\"SIFT介绍\"><a href=\"#SIFT介绍\" class=\"headerlink\" title=\"SIFT介绍\"></a>SIFT介绍</h2><p>上讲中介绍的Harris角点方法计算简便，并具有平移不变性和旋转不变性。特征$f$对某种变换$\\mathcal{T}$具有不变性，是指在经过变换后原特征保持不变，也就是$f(I) = f(\\mathcal{T}(I))$。但是Harris角点不具有尺度变换不变性，如下图所示。当图像被放大后，原图的角点被判定为了边缘点。<br><img src=\"/img/harris_non_scale_constant.png\" alt=\"harris的尺度变换不满足尺度不变性\"></p>\n<p>而我们想要得到一种对尺度变换保持不变性的特征计算方法。例如图像patch的像素平均亮度，如下图所示。region size缩小为原始的一半后，亮度直方图的形状不变，即平均亮度不变。<br><img src=\"/img/patch_average_intensity_scale_constant.png\" alt=\"平均亮度满足尺度变化呢不变性\"></p>\n<p>而Lowe想到了使用局部极值来作为feature来保证对scale变换的不变性。在算法的具体实现中，他使用DoG来获得局部极值。</p>\n<p><a href=\"http://www.cs.ubc.ca/~lowe/papers/ijcv04.pdf\">Lowe的论文</a>中也提到了SIFT特征的应用。SIFT特征可以产生稠密（dense）的key point，例如一张500x500像素大小的图像，一般可以产生~2000个稳定的SIFT特征。在进行image matching和recognition时，可以将ref image的SIFT特征提前计算出来保存在数据库中，并计算待处理图像的SIFT特征，根据特征向量的欧氏距离进行匹配。</p>\n<p>这篇博客主要是<a href=\"http://www.cs.ubc.ca/~lowe/papers/ijcv04.pdf\">Lowe上述论文</a>的读书笔记，按照SIFT特征的计算步骤进行组织。</p>\n<h2 id=\"尺度空间极值的检测方法\"><a href=\"#尺度空间极值的检测方法\" class=\"headerlink\" title=\"尺度空间极值的检测方法\"></a>尺度空间极值的检测方法</h2><p>前人研究已经指出，在一系列合理假设下，高斯核是唯一满足尺度不变性的。所谓的尺度空间，就是指原始图像$I(x,y)$和可变尺度的高斯核$G(x,y,\\sigma)$的卷积结果。如下式所示：</p>\n<script type=\"math/tex; mode=display\">L(x,y,\\sigma) = G(x,y,\\sigma)\\ast I(x,y)</script><p>其中，$G(x,y, \\sigma) = \\frac{1}{2\\pi\\sigma^2}\\exp(-(x^2+y^2)/2\\sigma^2)$。不同的$\\sigma$代表不同的尺度。</p>\n<p>DoG(difference of Gaussian)函数定义为不同尺度的高斯核与图像卷积结果之差，即，</p>\n<script type=\"math/tex; mode=display\">D(x,y,\\sigma) = L(x,y,k\\sigma) - L(x,y,\\sigma)</script><p>如下图所示，输入的图像重复地与高斯核进行卷积得到结果图像$L$（左侧），这样构成了一个octave。相邻的$L$之间作差得到DoG图像（右侧）。当一个octave处理完之后，将当前octave的最后一张高斯卷积结果图像降采样两倍，按照前述方法构造下一个octave。在图中，我们将一个octave划分为$s$个间隔（即$s+1$个图像）时，设$\\sigma$最终变成了2倍（即$\\sigma$加倍）。那么，显然有相邻两层之间的$k = 2^{1/s}$。不过为了保证首尾两张图像也能够计算差值，我们实际上需要再补上两张（做一个padding），也就是说一个octave内的总图像为$s+3$（下图中的$s=2$）。<br><img src=\"/img/sift_dog.png\" alt=\"DoG的计算\"></p>\n<p>为何我们要费力气得到DoG呢？论文中作者给出的解释是：DoG对scale-normalized后的Guassian Laplace函数$\\sigma^2\\Delta G$提供了足够的近似。其中前面的$\\sigma^2$系数正是为了保证尺度不变性。而前人的研究则指出，$\\sigma \\Delta G$函数的极值，和其他一大票其他function比较时，能够提供最稳定的feature。</p>\n<p>对于高斯核函数，有以下性质：</p>\n<script type=\"math/tex; mode=display\">\\frac{\\partial G}{\\partial \\sigma} = \\sigma \\Delta G</script><p>我们将式子左侧的微分变成差分，得到了下式：</p>\n<script type=\"math/tex; mode=display\">\\sigma\\Delta G \\approx \\frac{G(x,y,k\\sigma)-G(x,y,\\sigma)}{k\\sigma - \\sigma}</script><p>也就是：</p>\n<script type=\"math/tex; mode=display\">G(x,y,k\\sigma)-G(x,y,\\sigma) \\approx (k-1)\\sigma^2 \\Delta G</script><p>当$k=1$时，上式的近似误差为0（即上面的$s=\\infty$，也就是说octave的划分是连续的）。但是当$k$较大时，如$\\sqrt{2}$（这时$s=2$，octave划分十分粗糙了），仍然保证了稳定性。同时，由于相邻两层的比例$k$是定值，所以$k-1$这个因子对于检测极值没有影响。我们在计算时，无需除以$k-1$。</p>\n<p>构造完了DoG图像金字塔，下面我们的任务就是检测其中的极值。如下图，对于每个点，我们将它与金字塔中相邻的26个点作比较，找到局部极值。<br><img src=\"/img/sift_detection_maximum.png\" alt=\"检测极值\"></p>\n<p>另外，我们将输入图像行列预先使用线性插值的方法resize为原来的2倍，获取更多的key point。</p>\n<p>此外，在Lowe的论文中还提到了使用DoG的泰勒展开和海森矩阵进行key point的精细确定方法，并对key point进行过滤。这里也不再赘述了。</p>\n<h2 id=\"128维feature的获取\"><a href=\"#128维feature的获取\" class=\"headerlink\" title=\"128维feature的获取\"></a>128维feature的获取</h2><p>我们需要在每个key point处提取128维的特征向量。这里结合CS131课程作业2的相关练习作出说明。对于每个key point，我们关注它位于图像金字塔中的具体层数以及像素点位置，也就是说通过索引<code>pyramid{scale}(y, x)</code>就可以获取key point。我们遍历key point，为它们赋予一个128维的特征向量。</p>\n<p>我们选取point周围16x16大小的区域，称为一个patch。将其分为4x4共计16个cell。这样，每个cell内共有像素点16个。对于每个cell，我们计算它的局部梯度方向直方图，直方图共有8个bin。也就是说每个cell可以得到一个8维的特征向量。将16个cell的特征向量首尾相接，就得到了128维的SIFT特征向量。在下面的代码中，使用变量<code>patch_mag</code>和<code>patch_theta</code>分别代表patch的梯度幅值和角度，它们可以很简单地使用卷积和数学运算得到。</p>\n<figure class=\"highlight matlab\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\">patch_mag = <span class=\"built_in\">sqrt</span>(patch_dx.^<span class=\"number\">2</span> + patch_dy.^<span class=\"number\">2</span>);</div><div class=\"line\">patch_theta = <span class=\"built_in\">atan2</span>(patch_dy, patch_dx);  <span class=\"comment\">% atan2的返回结果在区间[-pi, pi]上。</span></div><div class=\"line\">patch_theta = <span class=\"built_in\">mod</span>(patch_theta, <span class=\"number\">2</span>*<span class=\"built_in\">pi</span>);   <span class=\"comment\">% 这里我们要将其转换为[0, 2pi]</span></div></pre></td></tr></table></figure>\n<p>之后，我们需要获取key point的主方向。其定义可见slide，即为key point扩展出来的patch的梯度方向直方图的峰值对应的角度。<br><img src=\"/img/sift_dominant_orientation.png\" alt=\"何为主方向\"></p>\n<p>所以我们首先应该设计如何构建局部梯度方向直方图。我们只要将<code>[0, 2pi]</code>区间划分为若干个<code>bin</code>，并将patch内的每个点使用其梯度大小向对应的<code>bin</code>内投票即可。如下所示：</p>\n<figure class=\"highlight matlab\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"function\"><span class=\"keyword\">function</span> <span class=\"params\">[histogram, angles]</span> = <span class=\"title\">ComputeGradientHistogram</span><span class=\"params\">(num_bins, gradient_magnitudes, gradient_angles)</span></span></div><div class=\"line\"><span class=\"comment\">% Compute a gradient histogram using gradient magnitudes and directions.</span></div><div class=\"line\"><span class=\"comment\">% Each point is assigned to one of num_bins depending on its gradient</span></div><div class=\"line\"><span class=\"comment\">% direction; the gradient magnitude of that point is added to its bin.</span></div><div class=\"line\"><span class=\"comment\">%</span></div><div class=\"line\"><span class=\"comment\">% INPUT</span></div><div class=\"line\"><span class=\"comment\">% num_bins: The number of bins to which points should be assigned.</span></div><div class=\"line\"><span class=\"comment\">% gradient_magnitudes, gradient angles:</span></div><div class=\"line\"><span class=\"comment\">%       Two arrays of the same shape where gradient_magnitudes(i) and</span></div><div class=\"line\"><span class=\"comment\">%       gradient_angles(i) give the magnitude and direction of the gradient</span></div><div class=\"line\"><span class=\"comment\">%       for the ith point. gradient_angles ranges from 0 to 2*pi</span></div><div class=\"line\"><span class=\"comment\">%                                      </span></div><div class=\"line\"><span class=\"comment\">% OUTPUT</span></div><div class=\"line\"><span class=\"comment\">% histogram: A 1 x num_bins array containing the gradient histogram. Entry 1 is</span></div><div class=\"line\"><span class=\"comment\">%       the sum of entries in gradient_magnitudes whose corresponding</span></div><div class=\"line\"><span class=\"comment\">%       gradient_angles lie between 0 and angle_step. Similarly, entry 2 is for</span></div><div class=\"line\"><span class=\"comment\">%       angles between angle_step and 2*angle_step. Angle_step is calculated as</span></div><div class=\"line\"><span class=\"comment\">%       2*pi/num_bins.</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">% angles: A 1 x num_bins array which holds the histogram bin lower bounds.</span></div><div class=\"line\"><span class=\"comment\">%       In other words, histogram(i) contains the sum of the</span></div><div class=\"line\"><span class=\"comment\">%       gradient magnitudes of all points whose gradient directions fall</span></div><div class=\"line\"><span class=\"comment\">%       in the range [angles(i), angles(i + 1))</span></div><div class=\"line\"></div><div class=\"line\">    angle_step = <span class=\"number\">2</span> * <span class=\"built_in\">pi</span> / num_bins;</div><div class=\"line\">    angles = <span class=\"number\">0</span> : angle_step : (<span class=\"number\">2</span>*<span class=\"built_in\">pi</span>-angle_step);</div><div class=\"line\"></div><div class=\"line\">    histogram = <span class=\"built_in\">zeros</span>(<span class=\"number\">1</span>, num_bins);</div><div class=\"line\">    num = <span class=\"built_in\">numel</span>(gradient_angles);</div><div class=\"line\">    <span class=\"keyword\">for</span> n = <span class=\"number\">1</span>:num</div><div class=\"line\">        index = <span class=\"built_in\">floor</span>(gradient_angles(n) / angle_step) + <span class=\"number\">1</span>;</div><div class=\"line\">        histogram(index) = histogram(index) + gradient_magnitudes(n);</div><div class=\"line\">    <span class=\"keyword\">end</span>    </div><div class=\"line\"><span class=\"keyword\">end</span></div></pre></td></tr></table></figure>\n<p>Lowe论文中推荐的<code>bin</code>数目为36个，计算主方向的函数如下：</p>\n<figure class=\"highlight matlab\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"function\"><span class=\"keyword\">function</span> <span class=\"title\">direction</span> = <span class=\"title\">ComputeDominantDirection</span><span class=\"params\">(gradient_magnitudes, gradient_angles)</span></span></div><div class=\"line\"><span class=\"comment\">% Computes the dominant gradient direction for the region around a keypoint</span></div><div class=\"line\"><span class=\"comment\">% given the scale of the keypoint and the gradient magnitudes and gradient</span></div><div class=\"line\"><span class=\"comment\">% angles of the pixels in the region surrounding the keypoint.</span></div><div class=\"line\"><span class=\"comment\">%</span></div><div class=\"line\"><span class=\"comment\">% INPUT</span></div><div class=\"line\"><span class=\"comment\">% gradient_magnitudes, gradient_angles:</span></div><div class=\"line\"><span class=\"comment\">%   Two arrays of the same shape where gradient_magnitudes(i) and</span></div><div class=\"line\"><span class=\"comment\">%   gradient_angles(i) give the magnitude and direction of the gradient for</span></div><div class=\"line\"><span class=\"comment\">%   the ith point.</span></div><div class=\"line\"></div><div class=\"line\">    <span class=\"comment\">% Compute a gradient histogram using the weighted gradient magnitudes.</span></div><div class=\"line\">    <span class=\"comment\">% In David Lowe's paper he suggests using 36 bins for this histogram.</span></div><div class=\"line\">    num_bins = <span class=\"number\">36</span>;</div><div class=\"line\">    <span class=\"comment\">% Step 1:</span></div><div class=\"line\">    <span class=\"comment\">% compute the 36-bin histogram of angles using ComputeGradientHistogram()</span></div><div class=\"line\">    [histogram, angle_bound] = ComputeGradientHistogram(num_bins, gradient_magnitudes, gradient_angles);</div><div class=\"line\">    <span class=\"comment\">% Step 2:</span></div><div class=\"line\">    <span class=\"comment\">% Find the maximum value of the gradient histogram, and set \"direction\"</span></div><div class=\"line\">    <span class=\"comment\">% to the angle corresponding to the maximum. (To match our solutions,</span></div><div class=\"line\">    <span class=\"comment\">% just use the lower-bound angle of the max histogram bin. (E.g. return</span></div><div class=\"line\">    <span class=\"comment\">% 0 radians if it's bin 1.)</span></div><div class=\"line\">    [~, max_index] = max(histogram);</div><div class=\"line\">    direction = angle_bound(max_index);</div><div class=\"line\"><span class=\"keyword\">end</span></div></pre></td></tr></table></figure>\n<p>之后，我们更新patch内各点的梯度方向，计算其与主方向的夹角，作为新的方向。并将梯度进行高斯平滑。</p>\n<figure class=\"highlight matlab\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\">patch_theta = patch_theta - ComputeDominantDirection(patch_mag, patch_theta);;</div><div class=\"line\">patch_theta = <span class=\"built_in\">mod</span>(patch_theta, <span class=\"number\">2</span>*<span class=\"built_in\">pi</span>);</div><div class=\"line\">patch_mag = patch_mag .* fspecial(<span class=\"string\">'gaussian'</span>, patch_size, patch_size / <span class=\"number\">2</span>); <span class=\"comment\">% patch_size = 16</span></div></pre></td></tr></table></figure>\n<p>遍历cell，计算feature如下：</p>\n<figure class=\"highlight matlab\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div></pre></td><td class=\"code\"><pre><div class=\"line\">feature = [];</div><div class=\"line\">row_iter = <span class=\"number\">1</span>;</div><div class=\"line\"><span class=\"keyword\">for</span> y = <span class=\"number\">1</span>:num_histograms</div><div class=\"line\">    col_iter = <span class=\"number\">1</span>;</div><div class=\"line\">    <span class=\"keyword\">for</span> x = <span class=\"number\">1</span>:num_histograms</div><div class=\"line\">        cell_mag = patch_mag(row_iter: row_iter + pixelsPerHistogram - <span class=\"number\">1</span>, ...</div><div class=\"line\">                             col_iter: col_iter + pixelsPerHistogram - <span class=\"number\">1</span>);</div><div class=\"line\">        cell_theta = patch_theta(row_iter: row_iter + pixelsPerHistogram - <span class=\"number\">1</span>, ...</div><div class=\"line\">                             col_iter: col_iter + pixelsPerHistogram - <span class=\"number\">1</span>);</div><div class=\"line\">        [histogram, ~] = ComputeGradientHistogram(num_angles, cell_mag, cell_theta);</div><div class=\"line\">        feature = [feature, histogram];</div><div class=\"line\">        col_iter = col_iter + pixelsPerHistogram;</div><div class=\"line\">    <span class=\"keyword\">end</span></div><div class=\"line\">    row_iter = row_iter + pixelsPerHistogram;</div><div class=\"line\"><span class=\"keyword\">end</span></div></pre></td></tr></table></figure>\n<p>最后，对feature做normalization。首先将feature化为单位长度，并将其中太大的分量进行限幅（如0.2的阈值），之后再重新将其转换为单位长度。</p>\n<p>这样，就完成了SIFT特征的计算。在Lowe的论文中，有更多对实现细节的讨论，这里只是跟随课程slide和作业走完了算法流程，不再赘述。</p>\n<h2 id=\"应用：图像特征点匹配\"><a href=\"#应用：图像特征点匹配\" class=\"headerlink\" title=\"应用：图像特征点匹配\"></a>应用：图像特征点匹配</h2><p>和Harris角点一样，SIFT特征可以用作两幅图像的特征点匹配，并且具有多种变换不变性的优点。对于两幅图像分别计算得到的特征点SIFT特征向量，可以使用下面简单的方法搜寻匹配点。计算每组点对之间的欧式距离，如果最近的距离比第二近的距离小得多，那么可以认为有一对成功匹配的特征点。其MATLAB代码如下，其中<code>descriptor</code>是两幅图像的SIFT特征向量。阈值默认为取做0.7。</p>\n<figure class=\"highlight matlab\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"function\"><span class=\"keyword\">function</span> <span class=\"title\">match</span> = <span class=\"title\">SIFTSimpleMatcher</span><span class=\"params\">(descriptor1, descriptor2, thresh)</span></span></div><div class=\"line\"><span class=\"comment\">% SIFTSimpleMatcher</span></div><div class=\"line\"><span class=\"comment\">%   Match one set of SIFT descriptors (descriptor1) to another set of</span></div><div class=\"line\"><span class=\"comment\">%   descriptors (decriptor2). Each descriptor from descriptor1 can at</span></div><div class=\"line\"><span class=\"comment\">%   most be matched to one member of descriptor2, but descriptors from</span></div><div class=\"line\"><span class=\"comment\">%   descriptor2 can be matched more than once.</span></div><div class=\"line\"><span class=\"comment\">%   </span></div><div class=\"line\"><span class=\"comment\">%   Matches are determined as follows:</span></div><div class=\"line\"><span class=\"comment\">%   For each descriptor vector in descriptor1, find the Euclidean distance</span></div><div class=\"line\"><span class=\"comment\">%   between it and each descriptor vector in descriptor2. If the smallest</span></div><div class=\"line\"><span class=\"comment\">%   distance is less than thresh*(the next smallest distance), we say that</span></div><div class=\"line\"><span class=\"comment\">%   the two vectors are a match, and we add the row [d1 index, d2 index] to</span></div><div class=\"line\"><span class=\"comment\">%   the \"match\" array.</span></div><div class=\"line\"><span class=\"comment\">%   </span></div><div class=\"line\"><span class=\"comment\">% INPUT:</span></div><div class=\"line\"><span class=\"comment\">%   descriptor1: N1 * 128 matrix, each row is a SIFT descriptor.</span></div><div class=\"line\"><span class=\"comment\">%   descriptor2: N2 * 128 matrix, each row is a SIFT descriptor.</span></div><div class=\"line\"><span class=\"comment\">%   thresh: a given threshold of ratio. Typically 0.7</span></div><div class=\"line\"><span class=\"comment\">%</span></div><div class=\"line\"><span class=\"comment\">% OUTPUT:</span></div><div class=\"line\"><span class=\"comment\">%   Match: N * 2 matrix, each row is a match.</span></div><div class=\"line\"><span class=\"comment\">%          For example, Match(k, :) = [i, j] means i-th descriptor in</span></div><div class=\"line\"><span class=\"comment\">%          descriptor1 is matched to j-th descriptor in descriptor2.</span></div><div class=\"line\">    <span class=\"keyword\">if</span> ~exist(<span class=\"string\">'thresh'</span>, <span class=\"string\">'var'</span>),</div><div class=\"line\">        thresh = <span class=\"number\">0.7</span>;</div><div class=\"line\">    <span class=\"keyword\">end</span></div><div class=\"line\"></div><div class=\"line\">    match = [];</div><div class=\"line\">    [N1, ~] = <span class=\"built_in\">size</span>(descriptor1);</div><div class=\"line\">    <span class=\"keyword\">for</span> <span class=\"built_in\">i</span> = <span class=\"number\">1</span>:N1</div><div class=\"line\">        fea = descriptor1(<span class=\"built_in\">i</span>, :);</div><div class=\"line\">        err = <span class=\"built_in\">bsxfun</span>(@minus, fea, descriptor2);</div><div class=\"line\">        dis = <span class=\"built_in\">sqrt</span>(sum(err.^<span class=\"number\">2</span>, <span class=\"number\">2</span>));</div><div class=\"line\">        [sorted_dis, ind] = sort(dis, <span class=\"number\">1</span>);</div><div class=\"line\">        <span class=\"keyword\">if</span> sorted_dis(<span class=\"number\">1</span>) &lt; thresh * sorted_dis(<span class=\"number\">2</span>)</div><div class=\"line\">            match = [match; [i, ind(<span class=\"number\">1</span>)]];</div><div class=\"line\">        <span class=\"keyword\">end</span></div><div class=\"line\">    <span class=\"keyword\">end</span></div><div class=\"line\"><span class=\"keyword\">end</span></div></pre></td></tr></table></figure>\n<p>接下来，我们可以使用最小二乘法计算两幅图像之间的仿射变换矩阵（齐次变换矩阵）$H$。其中$H$满足：</p>\n<script type=\"math/tex; mode=display\">Hp_{\\text{before}} = p_{\\text{after}}</script><p>其中</p>\n<script type=\"math/tex; mode=display\">p = \\begin{bmatrix}x \\\\\\\\ y \\\\\\\\ 1\\end{bmatrix}</script><p>对上式稍作变形，有</p>\n<script type=\"math/tex; mode=display\">p_{\\text{before}}^\\dagger H^\\dagger = p_{\\text{after}}\\dagger</script><p>就可以使用标准的最小二乘正则方程进行求解了。代码如下：</p>\n<figure class=\"highlight matlab\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"function\"><span class=\"keyword\">function</span> <span class=\"title\">H</span> = <span class=\"title\">ComputeAffineMatrix</span><span class=\"params\">( Pt1, Pt2 )</span></span></div><div class=\"line\"><span class=\"comment\">%ComputeAffineMatrix</span></div><div class=\"line\"><span class=\"comment\">%   Computes the transformation matrix that transforms a point from</span></div><div class=\"line\"><span class=\"comment\">%   coordinate frame 1 to coordinate frame 2</span></div><div class=\"line\"><span class=\"comment\">%Input:</span></div><div class=\"line\"><span class=\"comment\">%   Pt1: N * 2 matrix, each row is a point in image 1</span></div><div class=\"line\"><span class=\"comment\">%       (N must be at least 3)</span></div><div class=\"line\"><span class=\"comment\">%   Pt2: N * 2 matrix, each row is the point in image 2 that</span></div><div class=\"line\"><span class=\"comment\">%       matches the same point in image 1 (N should be more than 3)</span></div><div class=\"line\"><span class=\"comment\">%Output:</span></div><div class=\"line\"><span class=\"comment\">%   H: 3 * 3 affine transformation matrix,</span></div><div class=\"line\"><span class=\"comment\">%       such that H*pt1(i,:) = pt2(i,:)</span></div><div class=\"line\"></div><div class=\"line\">    N = <span class=\"built_in\">size</span>(Pt1,<span class=\"number\">1</span>);</div><div class=\"line\">    <span class=\"keyword\">if</span> <span class=\"built_in\">size</span>(Pt1, <span class=\"number\">1</span>) ~= <span class=\"built_in\">size</span>(Pt2, <span class=\"number\">1</span>),</div><div class=\"line\">        error(<span class=\"string\">'Dimensions unmatched.'</span>);</div><div class=\"line\">    <span class=\"keyword\">elseif</span> N&lt;<span class=\"number\">3</span></div><div class=\"line\">        error(<span class=\"string\">'At least 3 points are required.'</span>);</div><div class=\"line\">    <span class=\"keyword\">end</span></div><div class=\"line\"></div><div class=\"line\">    <span class=\"comment\">% Convert the input points to homogeneous coordintes.</span></div><div class=\"line\">    P1 = [Pt1<span class=\"string\">';ones(1,N)];</div><div class=\"line\">    P2 = [Pt2'</span>;ones(<span class=\"number\">1</span>,N)];</div><div class=\"line\"></div><div class=\"line\">    H = P1*P1'\\P1*P2';</div><div class=\"line\">    H = H';</div><div class=\"line\"></div><div class=\"line\">    <span class=\"comment\">% Sometimes numerical issues cause least-squares to produce a bottom</span></div><div class=\"line\">    <span class=\"comment\">% row which is not exactly [0 0 1], which confuses some of the later</span></div><div class=\"line\">    <span class=\"comment\">% code. So we'll ensure the bottom row is exactly [0 0 1].</span></div><div class=\"line\">    H(<span class=\"number\">3</span>,:) = [<span class=\"number\">0</span> <span class=\"number\">0</span> <span class=\"number\">1</span>];</div><div class=\"line\"><span class=\"keyword\">end</span></div></pre></td></tr></table></figure>\n<p>作业中的其他例子也很有趣，这里不再多说了。贴上两张图像拼接的实验结果吧~<br><img src=\"/img/sift_experiment_1.png\" alt=\"result 1\"><br><img src=\"/img/sift_experiment_2.png\" alt=\"result 2\"></p>"},{"title":"在DigitalOcean上配置Shadowsocks实现IPV4/IPV6翻墙","date":"2017-02-08T09:31:35.000Z","_content":"身在天朝，GFW是一个很蛋疼的东西。有人说GFW挡住了天朝与世界其他地方的交流，尤其给科研造成了很多不便；也有人认为GFW挡住了美利坚的“坚船利炮”（Google，Facebook，Tweet等），让中国的互联网企业高速发展。这两种思路都很有道理，然而GFW对计算机相关行业人员造成不便倒也是千真万确，让人在无数次404之后，脱口而出F***。也有人打趣，会不会FQ，已经成为了CS入行的第一道关卡，正好用来刷掉一批人。之前我使用GoAgent来FQ，后来又使用了一段时间的免费SS服务，后来机缘巧合从一个印度佬那里挣了一些美元，所以搬到了DigitalOcean上。DO的机器，对我一个学生来说，说实话并不算便宜了，不过好在手里还有一些美元，也在GitHub那里进行了学生认证，算是也可以应付了。\n\n之前我已经在DO的机器上配置过shadowsocks，还顺手给iPad解决了FQ的问题。然而当时没有记录，这次我换了一台机器，机房位于NY，把ss重新配置了一遍，再不做些记录，下次恐怕又要东翻西找。\n![佛跳墙](/img/god_use_vpn.png)\n\n<!-- more -->\n## 申请机器\n在进行GitHub学生认证后，可以使用它发给的一个优惠码注册DO，不过仍然要绑定自己的PayPal账户，先交上五美金。之后，DO会赠送50刀。\n\n申请机器时，直接选择Ubuntu 16.04操作系统，并勾选IPV6 Enable，省去后面的麻烦。\n\n## 安装ss\n远程登录后，我们需要安装ss。安装命令很简单。\n``` bash\napt-get install python-pip\npip install shadowsocks\n```\n然而，在安装时，我遇到了一个奇怪的问题，提示我`unsupported locale setting`，后来搜索得知，是语言配置的问题，见[这篇博文](http://www.linfuyan.com/locale_error_unsupported_locale_setting/)，解决办法如下：\n``` bash\nexport LC_ALL=C\n```\n\n## 编辑配置文件\n之后，进入`/etc`目录，建立一个名叫`shadowsocks.json`的文件（文件名任意，一会对应即可），文件配置内容如下：\n```\n{\n\"server\":\"::\",  \n\"server_port\":8388,\n\"local_address\": \"127.0.0.1\",\n\"local_port\": 1080,\n\"password\":\"your_password（任写）\",\n\"timeout\":600,\n\"method\":\"aes-256-cfb\"\n}\n```\n其中第一行写成`::`即是为了IPV6连接。\n\n## 编辑启动项，设置自动启动\n之后，我们编辑启动项配置文件，使得ss服务能够开机自动启动。\n\n编辑`/etc/rc.local`文件，在`exit 0`之前添加如下命令。\n```\nssserver -c /etc/shadowsocks.json -d start  # 这里的json文件名要相对应\n```\n\n之后，使用`reboot`命令重启即可。\n\n## 客户端配置\n客户端使用ss，编辑服务器，按照json文件中的内容填写即可。注意密码相对应。\n","source":"_posts/digitalocean-shadowsocks.md","raw":"---\ntitle: 在DigitalOcean上配置Shadowsocks实现IPV4/IPV6翻墙\ndate: 2017-02-08 17:31:35\ntags:\n    - tool\n---\n身在天朝，GFW是一个很蛋疼的东西。有人说GFW挡住了天朝与世界其他地方的交流，尤其给科研造成了很多不便；也有人认为GFW挡住了美利坚的“坚船利炮”（Google，Facebook，Tweet等），让中国的互联网企业高速发展。这两种思路都很有道理，然而GFW对计算机相关行业人员造成不便倒也是千真万确，让人在无数次404之后，脱口而出F***。也有人打趣，会不会FQ，已经成为了CS入行的第一道关卡，正好用来刷掉一批人。之前我使用GoAgent来FQ，后来又使用了一段时间的免费SS服务，后来机缘巧合从一个印度佬那里挣了一些美元，所以搬到了DigitalOcean上。DO的机器，对我一个学生来说，说实话并不算便宜了，不过好在手里还有一些美元，也在GitHub那里进行了学生认证，算是也可以应付了。\n\n之前我已经在DO的机器上配置过shadowsocks，还顺手给iPad解决了FQ的问题。然而当时没有记录，这次我换了一台机器，机房位于NY，把ss重新配置了一遍，再不做些记录，下次恐怕又要东翻西找。\n![佛跳墙](/img/god_use_vpn.png)\n\n<!-- more -->\n## 申请机器\n在进行GitHub学生认证后，可以使用它发给的一个优惠码注册DO，不过仍然要绑定自己的PayPal账户，先交上五美金。之后，DO会赠送50刀。\n\n申请机器时，直接选择Ubuntu 16.04操作系统，并勾选IPV6 Enable，省去后面的麻烦。\n\n## 安装ss\n远程登录后，我们需要安装ss。安装命令很简单。\n``` bash\napt-get install python-pip\npip install shadowsocks\n```\n然而，在安装时，我遇到了一个奇怪的问题，提示我`unsupported locale setting`，后来搜索得知，是语言配置的问题，见[这篇博文](http://www.linfuyan.com/locale_error_unsupported_locale_setting/)，解决办法如下：\n``` bash\nexport LC_ALL=C\n```\n\n## 编辑配置文件\n之后，进入`/etc`目录，建立一个名叫`shadowsocks.json`的文件（文件名任意，一会对应即可），文件配置内容如下：\n```\n{\n\"server\":\"::\",  \n\"server_port\":8388,\n\"local_address\": \"127.0.0.1\",\n\"local_port\": 1080,\n\"password\":\"your_password（任写）\",\n\"timeout\":600,\n\"method\":\"aes-256-cfb\"\n}\n```\n其中第一行写成`::`即是为了IPV6连接。\n\n## 编辑启动项，设置自动启动\n之后，我们编辑启动项配置文件，使得ss服务能够开机自动启动。\n\n编辑`/etc/rc.local`文件，在`exit 0`之前添加如下命令。\n```\nssserver -c /etc/shadowsocks.json -d start  # 这里的json文件名要相对应\n```\n\n之后，使用`reboot`命令重启即可。\n\n## 客户端配置\n客户端使用ss，编辑服务器，按照json文件中的内容填写即可。注意密码相对应。\n","slug":"digitalocean-shadowsocks","published":1,"updated":"2017-02-26T12:09:28.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cizve07ah000hl61hcbllf1kr","content":"<p>身在天朝，GFW是一个很蛋疼的东西。有人说GFW挡住了天朝与世界其他地方的交流，尤其给科研造成了很多不便；也有人认为GFW挡住了美利坚的“坚船利炮”（Google，Facebook，Tweet等），让中国的互联网企业高速发展。这两种思路都很有道理，然而GFW对计算机相关行业人员造成不便倒也是千真万确，让人在无数次404之后，脱口而出F<em>*</em>。也有人打趣，会不会FQ，已经成为了CS入行的第一道关卡，正好用来刷掉一批人。之前我使用GoAgent来FQ，后来又使用了一段时间的免费SS服务，后来机缘巧合从一个印度佬那里挣了一些美元，所以搬到了DigitalOcean上。DO的机器，对我一个学生来说，说实话并不算便宜了，不过好在手里还有一些美元，也在GitHub那里进行了学生认证，算是也可以应付了。</p>\n<p>之前我已经在DO的机器上配置过shadowsocks，还顺手给iPad解决了FQ的问题。然而当时没有记录，这次我换了一台机器，机房位于NY，把ss重新配置了一遍，再不做些记录，下次恐怕又要东翻西找。<br><img src=\"/img/god_use_vpn.png\" alt=\"佛跳墙\"></p>\n<a id=\"more\"></a>\n<h2 id=\"申请机器\"><a href=\"#申请机器\" class=\"headerlink\" title=\"申请机器\"></a>申请机器</h2><p>在进行GitHub学生认证后，可以使用它发给的一个优惠码注册DO，不过仍然要绑定自己的PayPal账户，先交上五美金。之后，DO会赠送50刀。</p>\n<p>申请机器时，直接选择Ubuntu 16.04操作系统，并勾选IPV6 Enable，省去后面的麻烦。</p>\n<h2 id=\"安装ss\"><a href=\"#安装ss\" class=\"headerlink\" title=\"安装ss\"></a>安装ss</h2><p>远程登录后，我们需要安装ss。安装命令很简单。<br><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">apt-get install python-pip</div><div class=\"line\">pip install shadowsocks</div></pre></td></tr></table></figure></p>\n<p>然而，在安装时，我遇到了一个奇怪的问题，提示我<code>unsupported locale setting</code>，后来搜索得知，是语言配置的问题，见<a href=\"http://www.linfuyan.com/locale_error_unsupported_locale_setting/\" target=\"_blank\" rel=\"external\">这篇博文</a>，解决办法如下：<br><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"built_in\">export</span> LC_ALL=C</div></pre></td></tr></table></figure></p>\n<h2 id=\"编辑配置文件\"><a href=\"#编辑配置文件\" class=\"headerlink\" title=\"编辑配置文件\"></a>编辑配置文件</h2><p>之后，进入<code>/etc</code>目录，建立一个名叫<code>shadowsocks.json</code>的文件（文件名任意，一会对应即可），文件配置内容如下：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div></pre></td><td class=\"code\"><pre><div class=\"line\">&#123;</div><div class=\"line\">&quot;server&quot;:&quot;::&quot;,  </div><div class=\"line\">&quot;server_port&quot;:8388,</div><div class=\"line\">&quot;local_address&quot;: &quot;127.0.0.1&quot;,</div><div class=\"line\">&quot;local_port&quot;: 1080,</div><div class=\"line\">&quot;password&quot;:&quot;your_password（任写）&quot;,</div><div class=\"line\">&quot;timeout&quot;:600,</div><div class=\"line\">&quot;method&quot;:&quot;aes-256-cfb&quot;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure></p>\n<p>其中第一行写成<code>::</code>即是为了IPV6连接。</p>\n<h2 id=\"编辑启动项，设置自动启动\"><a href=\"#编辑启动项，设置自动启动\" class=\"headerlink\" title=\"编辑启动项，设置自动启动\"></a>编辑启动项，设置自动启动</h2><p>之后，我们编辑启动项配置文件，使得ss服务能够开机自动启动。</p>\n<p>编辑<code>/etc/rc.local</code>文件，在<code>exit 0</code>之前添加如下命令。<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">ssserver -c /etc/shadowsocks.json -d start  # 这里的json文件名要相对应</div></pre></td></tr></table></figure></p>\n<p>之后，使用<code>reboot</code>命令重启即可。</p>\n<h2 id=\"客户端配置\"><a href=\"#客户端配置\" class=\"headerlink\" title=\"客户端配置\"></a>客户端配置</h2><p>客户端使用ss，编辑服务器，按照json文件中的内容填写即可。注意密码相对应。</p>\n","excerpt":"<p>身在天朝，GFW是一个很蛋疼的东西。有人说GFW挡住了天朝与世界其他地方的交流，尤其给科研造成了很多不便；也有人认为GFW挡住了美利坚的“坚船利炮”（Google，Facebook，Tweet等），让中国的互联网企业高速发展。这两种思路都很有道理，然而GFW对计算机相关行业人员造成不便倒也是千真万确，让人在无数次404之后，脱口而出F<em>*</em>。也有人打趣，会不会FQ，已经成为了CS入行的第一道关卡，正好用来刷掉一批人。之前我使用GoAgent来FQ，后来又使用了一段时间的免费SS服务，后来机缘巧合从一个印度佬那里挣了一些美元，所以搬到了DigitalOcean上。DO的机器，对我一个学生来说，说实话并不算便宜了，不过好在手里还有一些美元，也在GitHub那里进行了学生认证，算是也可以应付了。</p>\n<p>之前我已经在DO的机器上配置过shadowsocks，还顺手给iPad解决了FQ的问题。然而当时没有记录，这次我换了一台机器，机房位于NY，把ss重新配置了一遍，再不做些记录，下次恐怕又要东翻西找。<br><img src=\"/img/god_use_vpn.png\" alt=\"佛跳墙\"></p>","more":"<h2 id=\"申请机器\"><a href=\"#申请机器\" class=\"headerlink\" title=\"申请机器\"></a>申请机器</h2><p>在进行GitHub学生认证后，可以使用它发给的一个优惠码注册DO，不过仍然要绑定自己的PayPal账户，先交上五美金。之后，DO会赠送50刀。</p>\n<p>申请机器时，直接选择Ubuntu 16.04操作系统，并勾选IPV6 Enable，省去后面的麻烦。</p>\n<h2 id=\"安装ss\"><a href=\"#安装ss\" class=\"headerlink\" title=\"安装ss\"></a>安装ss</h2><p>远程登录后，我们需要安装ss。安装命令很简单。<br><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">apt-get install python-pip</div><div class=\"line\">pip install shadowsocks</div></pre></td></tr></table></figure></p>\n<p>然而，在安装时，我遇到了一个奇怪的问题，提示我<code>unsupported locale setting</code>，后来搜索得知，是语言配置的问题，见<a href=\"http://www.linfuyan.com/locale_error_unsupported_locale_setting/\">这篇博文</a>，解决办法如下：<br><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"built_in\">export</span> LC_ALL=C</div></pre></td></tr></table></figure></p>\n<h2 id=\"编辑配置文件\"><a href=\"#编辑配置文件\" class=\"headerlink\" title=\"编辑配置文件\"></a>编辑配置文件</h2><p>之后，进入<code>/etc</code>目录，建立一个名叫<code>shadowsocks.json</code>的文件（文件名任意，一会对应即可），文件配置内容如下：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div></pre></td><td class=\"code\"><pre><div class=\"line\">&#123;</div><div class=\"line\">&quot;server&quot;:&quot;::&quot;,  </div><div class=\"line\">&quot;server_port&quot;:8388,</div><div class=\"line\">&quot;local_address&quot;: &quot;127.0.0.1&quot;,</div><div class=\"line\">&quot;local_port&quot;: 1080,</div><div class=\"line\">&quot;password&quot;:&quot;your_password（任写）&quot;,</div><div class=\"line\">&quot;timeout&quot;:600,</div><div class=\"line\">&quot;method&quot;:&quot;aes-256-cfb&quot;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure></p>\n<p>其中第一行写成<code>::</code>即是为了IPV6连接。</p>\n<h2 id=\"编辑启动项，设置自动启动\"><a href=\"#编辑启动项，设置自动启动\" class=\"headerlink\" title=\"编辑启动项，设置自动启动\"></a>编辑启动项，设置自动启动</h2><p>之后，我们编辑启动项配置文件，使得ss服务能够开机自动启动。</p>\n<p>编辑<code>/etc/rc.local</code>文件，在<code>exit 0</code>之前添加如下命令。<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">ssserver -c /etc/shadowsocks.json -d start  # 这里的json文件名要相对应</div></pre></td></tr></table></figure></p>\n<p>之后，使用<code>reboot</code>命令重启即可。</p>\n<h2 id=\"客户端配置\"><a href=\"#客户端配置\" class=\"headerlink\" title=\"客户端配置\"></a>客户端配置</h2><p>客户端使用ss，编辑服务器，按照json文件中的内容填写即可。注意密码相对应。</p>"},{"title":"远程登录Jupyter笔记本","date":"2017-02-26T11:53:11.000Z","_content":"Jupyter Notebook可以很方便地记录代码和内容，很适合边写笔记边写示例代码进行学习总结。在本机使用时，只需在相应文件夹下使用`jupyter notebook`命令即可在浏览器中打开笔记页面，进行编辑。而本篇文章记述了如何在远端登录并使用Jupyter笔记本。这样，就可以利用服务器较强的运算能力来搞事情了。\n![jupyternotebook](/img/jupyternotebook_logo.png)\n<!-- more -->\n\n## 配置jupter notebook\n登录远程服务器后，使用如下命令生成配置文件。\n\n``` bash\njupyter notebook --generate-config\n```\n\n并对其内容进行修改。我主要修改了两处地方：\n\n- `c.NotebookApp.ip='*'`，即不限制ip访问\n- `c.NotebookApp.password = u'hash_value'`\n\n上面的`hash_value`是由用户给定的密码生成的。可以使用`ipython`中的命令轻松搞定。\n\n``` python\nfrom notebook.auth import passwd\npasswd()\n\"\"\"\n这里会要求用户输入密码并确认，之后生成的hash值就是要填写到上面的\n\"\"\"\n```\n\n## 启动notebook\n之后，在远程服务器上启动笔记本`jupyter notebook`。接着，在本地机器上访问`远程服务器ip:8888`（默认端口为`8888`，也可以在配置文件中修改），输入密码即可访问远程笔记本。\n\n本篇内容参考自博客[远程访问jupyter notebook](http://blog.leanote.com/post/jevonswang/远程访问jupyter-notebook)。\n","source":"_posts/jupyternotebook-remote-useage.md","raw":"---\ntitle: 远程登录Jupyter笔记本\ndate: 2017-02-26 19:53:11\ntags:\n    - tool\n---\nJupyter Notebook可以很方便地记录代码和内容，很适合边写笔记边写示例代码进行学习总结。在本机使用时，只需在相应文件夹下使用`jupyter notebook`命令即可在浏览器中打开笔记页面，进行编辑。而本篇文章记述了如何在远端登录并使用Jupyter笔记本。这样，就可以利用服务器较强的运算能力来搞事情了。\n![jupyternotebook](/img/jupyternotebook_logo.png)\n<!-- more -->\n\n## 配置jupter notebook\n登录远程服务器后，使用如下命令生成配置文件。\n\n``` bash\njupyter notebook --generate-config\n```\n\n并对其内容进行修改。我主要修改了两处地方：\n\n- `c.NotebookApp.ip='*'`，即不限制ip访问\n- `c.NotebookApp.password = u'hash_value'`\n\n上面的`hash_value`是由用户给定的密码生成的。可以使用`ipython`中的命令轻松搞定。\n\n``` python\nfrom notebook.auth import passwd\npasswd()\n\"\"\"\n这里会要求用户输入密码并确认，之后生成的hash值就是要填写到上面的\n\"\"\"\n```\n\n## 启动notebook\n之后，在远程服务器上启动笔记本`jupyter notebook`。接着，在本地机器上访问`远程服务器ip:8888`（默认端口为`8888`，也可以在配置文件中修改），输入密码即可访问远程笔记本。\n\n本篇内容参考自博客[远程访问jupyter notebook](http://blog.leanote.com/post/jevonswang/远程访问jupyter-notebook)。\n","slug":"jupyternotebook-remote-useage","published":1,"updated":"2017-02-26T12:09:28.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cizve07al000il61h965n49u4","content":"<p>Jupyter Notebook可以很方便地记录代码和内容，很适合边写笔记边写示例代码进行学习总结。在本机使用时，只需在相应文件夹下使用<code>jupyter notebook</code>命令即可在浏览器中打开笔记页面，进行编辑。而本篇文章记述了如何在远端登录并使用Jupyter笔记本。这样，就可以利用服务器较强的运算能力来搞事情了。<br><img src=\"/img/jupyternotebook_logo.png\" alt=\"jupyternotebook\"><br><a id=\"more\"></a></p>\n<h2 id=\"配置jupter-notebook\"><a href=\"#配置jupter-notebook\" class=\"headerlink\" title=\"配置jupter notebook\"></a>配置jupter notebook</h2><p>登录远程服务器后，使用如下命令生成配置文件。</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">jupyter notebook --generate-config</div></pre></td></tr></table></figure>\n<p>并对其内容进行修改。我主要修改了两处地方：</p>\n<ul>\n<li><code>c.NotebookApp.ip=&#39;*&#39;</code>，即不限制ip访问</li>\n<li><code>c.NotebookApp.password = u&#39;hash_value&#39;</code></li>\n</ul>\n<p>上面的<code>hash_value</code>是由用户给定的密码生成的。可以使用<code>ipython</code>中的命令轻松搞定。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">from</span> notebook.auth <span class=\"keyword\">import</span> passwd</div><div class=\"line\">passwd()</div><div class=\"line\"><span class=\"string\">\"\"\"</span></div><div class=\"line\">这里会要求用户输入密码并确认，之后生成的hash值就是要填写到上面的</div><div class=\"line\">\"\"\"</div></pre></td></tr></table></figure>\n<h2 id=\"启动notebook\"><a href=\"#启动notebook\" class=\"headerlink\" title=\"启动notebook\"></a>启动notebook</h2><p>之后，在远程服务器上启动笔记本<code>jupyter notebook</code>。接着，在本地机器上访问<code>远程服务器ip:8888</code>（默认端口为<code>8888</code>，也可以在配置文件中修改），输入密码即可访问远程笔记本。</p>\n<p>本篇内容参考自博客<a href=\"http://blog.leanote.com/post/jevonswang/远程访问jupyter-notebook\" target=\"_blank\" rel=\"external\">远程访问jupyter notebook</a>。</p>\n","excerpt":"<p>Jupyter Notebook可以很方便地记录代码和内容，很适合边写笔记边写示例代码进行学习总结。在本机使用时，只需在相应文件夹下使用<code>jupyter notebook</code>命令即可在浏览器中打开笔记页面，进行编辑。而本篇文章记述了如何在远端登录并使用Jupyter笔记本。这样，就可以利用服务器较强的运算能力来搞事情了。<br><img src=\"/img/jupyternotebook_logo.png\" alt=\"jupyternotebook\"><br>","more":"</p>\n<h2 id=\"配置jupter-notebook\"><a href=\"#配置jupter-notebook\" class=\"headerlink\" title=\"配置jupter notebook\"></a>配置jupter notebook</h2><p>登录远程服务器后，使用如下命令生成配置文件。</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">jupyter notebook --generate-config</div></pre></td></tr></table></figure>\n<p>并对其内容进行修改。我主要修改了两处地方：</p>\n<ul>\n<li><code>c.NotebookApp.ip=&#39;*&#39;</code>，即不限制ip访问</li>\n<li><code>c.NotebookApp.password = u&#39;hash_value&#39;</code></li>\n</ul>\n<p>上面的<code>hash_value</code>是由用户给定的密码生成的。可以使用<code>ipython</code>中的命令轻松搞定。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">from</span> notebook.auth <span class=\"keyword\">import</span> passwd</div><div class=\"line\">passwd()</div><div class=\"line\"><span class=\"string\">\"\"\"</div><div class=\"line\">这里会要求用户输入密码并确认，之后生成的hash值就是要填写到上面的</div><div class=\"line\">\"\"\"</span></div></pre></td></tr></table></figure>\n<h2 id=\"启动notebook\"><a href=\"#启动notebook\" class=\"headerlink\" title=\"启动notebook\"></a>启动notebook</h2><p>之后，在远程服务器上启动笔记本<code>jupyter notebook</code>。接着，在本地机器上访问<code>远程服务器ip:8888</code>（默认端口为<code>8888</code>，也可以在配置文件中修改），输入密码即可访问远程笔记本。</p>\n<p>本篇内容参考自博客<a href=\"http://blog.leanote.com/post/jevonswang/远程访问jupyter-notebook\">远程访问jupyter notebook</a>。</p>"},{"title":"Hello World","date":"2016-12-16T11:00:00.000Z","_content":"Welcome to [Hexo](https://hexo.io/)! This is your very first post. Check [documentation](https://hexo.io/docs/) for more info. If you get any problems when using Hexo, you can find the answer in [troubleshooting](https://hexo.io/docs/troubleshooting.html) or you can ask me on [GitHub](https://github.com/hexojs/hexo/issues).\n\n![Hexo](/img/helloworld_hexo.png)\n<!-- more -->\n## Quick Start\n\n### Create a new post\n\n``` bash\n$ hexo new \"My New Post\"\n```\n\nMore info: [Writing](https://hexo.io/docs/writing.html)\n\n### Run server\n\n``` bash\n$ hexo server\n```\n\nMore info: [Server](https://hexo.io/docs/server.html)\n\n### Generate static files\n\n``` bash\n$ hexo generate\n```\n\nMore info: [Generating](https://hexo.io/docs/generating.html)\n\n### Deploy to remote sites\n\n``` bash\n$ hexo deploy\n```\n\nMore info: [Deployment](https://hexo.io/docs/deployment.html)\n\n### Code highlight\n\nHello World!\n\n``` cpp\n#include <iostream>\nint main() {\n    std::cout << \"HelloWorld\\n\";\n}\n```\n\n``` py\nprint 'HelloWorld'\n```\n\n### Latex Support by Mathjax\n\nMass-energy equation by Einstein: $E = mc^2$\n\na linear equation:\n    $$\\mathbf{A}\\mathbf{v} = \\mathbf{y}$$\n","source":"_posts/hello-world.md","raw":"---\ntitle: Hello World\ndate: 2016-12-16 19:00:00\n---\nWelcome to [Hexo](https://hexo.io/)! This is your very first post. Check [documentation](https://hexo.io/docs/) for more info. If you get any problems when using Hexo, you can find the answer in [troubleshooting](https://hexo.io/docs/troubleshooting.html) or you can ask me on [GitHub](https://github.com/hexojs/hexo/issues).\n\n![Hexo](/img/helloworld_hexo.png)\n<!-- more -->\n## Quick Start\n\n### Create a new post\n\n``` bash\n$ hexo new \"My New Post\"\n```\n\nMore info: [Writing](https://hexo.io/docs/writing.html)\n\n### Run server\n\n``` bash\n$ hexo server\n```\n\nMore info: [Server](https://hexo.io/docs/server.html)\n\n### Generate static files\n\n``` bash\n$ hexo generate\n```\n\nMore info: [Generating](https://hexo.io/docs/generating.html)\n\n### Deploy to remote sites\n\n``` bash\n$ hexo deploy\n```\n\nMore info: [Deployment](https://hexo.io/docs/deployment.html)\n\n### Code highlight\n\nHello World!\n\n``` cpp\n#include <iostream>\nint main() {\n    std::cout << \"HelloWorld\\n\";\n}\n```\n\n``` py\nprint 'HelloWorld'\n```\n\n### Latex Support by Mathjax\n\nMass-energy equation by Einstein: $E = mc^2$\n\na linear equation:\n    $$\\mathbf{A}\\mathbf{v} = \\mathbf{y}$$\n","slug":"hello-world","published":1,"updated":"2017-02-26T12:09:28.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cizve07am000ll61h5knoo3wa","content":"<p>Welcome to <a href=\"https://hexo.io/\" target=\"_blank\" rel=\"external\">Hexo</a>! This is your very first post. Check <a href=\"https://hexo.io/docs/\" target=\"_blank\" rel=\"external\">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href=\"https://hexo.io/docs/troubleshooting.html\" target=\"_blank\" rel=\"external\">troubleshooting</a> or you can ask me on <a href=\"https://github.com/hexojs/hexo/issues\" target=\"_blank\" rel=\"external\">GitHub</a>.</p>\n<p><img src=\"/img/helloworld_hexo.png\" alt=\"Hexo\"><br><a id=\"more\"></a></p>\n<h2 id=\"Quick-Start\"><a href=\"#Quick-Start\" class=\"headerlink\" title=\"Quick Start\"></a>Quick Start</h2><h3 id=\"Create-a-new-post\"><a href=\"#Create-a-new-post\" class=\"headerlink\" title=\"Create a new post\"></a>Create a new post</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">$ hexo new <span class=\"string\">\"My New Post\"</span></div></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/writing.html\" target=\"_blank\" rel=\"external\">Writing</a></p>\n<h3 id=\"Run-server\"><a href=\"#Run-server\" class=\"headerlink\" title=\"Run server\"></a>Run server</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">$ hexo server</div></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/server.html\" target=\"_blank\" rel=\"external\">Server</a></p>\n<h3 id=\"Generate-static-files\"><a href=\"#Generate-static-files\" class=\"headerlink\" title=\"Generate static files\"></a>Generate static files</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">$ hexo generate</div></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/generating.html\" target=\"_blank\" rel=\"external\">Generating</a></p>\n<h3 id=\"Deploy-to-remote-sites\"><a href=\"#Deploy-to-remote-sites\" class=\"headerlink\" title=\"Deploy to remote sites\"></a>Deploy to remote sites</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">$ hexo deploy</div></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/deployment.html\" target=\"_blank\" rel=\"external\">Deployment</a></p>\n<h3 id=\"Code-highlight\"><a href=\"#Code-highlight\" class=\"headerlink\" title=\"Code highlight\"></a>Code highlight</h3><p>Hello World!</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">include</span> <span class=\"meta-string\">&lt;iostream&gt;</span></span></div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">int</span> <span class=\"title\">main</span><span class=\"params\">()</span> </span>&#123;</div><div class=\"line\">    <span class=\"built_in\">std</span>::<span class=\"built_in\">cout</span> &lt;&lt; <span class=\"string\">\"HelloWorld\\n\"</span>;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">print</span> <span class=\"string\">'HelloWorld'</span></div></pre></td></tr></table></figure>\n<h3 id=\"Latex-Support-by-Mathjax\"><a href=\"#Latex-Support-by-Mathjax\" class=\"headerlink\" title=\"Latex Support by Mathjax\"></a>Latex Support by Mathjax</h3><p>Mass-energy equation by Einstein: $E = mc^2$</p>\n<p>a linear equation:</p>\n<pre><code>$$\\mathbf{A}\\mathbf{v} = \\mathbf{y}$$\n</code></pre>","excerpt":"<p>Welcome to <a href=\"https://hexo.io/\">Hexo</a>! This is your very first post. Check <a href=\"https://hexo.io/docs/\">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href=\"https://hexo.io/docs/troubleshooting.html\">troubleshooting</a> or you can ask me on <a href=\"https://github.com/hexojs/hexo/issues\">GitHub</a>.</p>\n<p><img src=\"/img/helloworld_hexo.png\" alt=\"Hexo\"><br>","more":"</p>\n<h2 id=\"Quick-Start\"><a href=\"#Quick-Start\" class=\"headerlink\" title=\"Quick Start\"></a>Quick Start</h2><h3 id=\"Create-a-new-post\"><a href=\"#Create-a-new-post\" class=\"headerlink\" title=\"Create a new post\"></a>Create a new post</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">$ hexo new <span class=\"string\">\"My New Post\"</span></div></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/writing.html\">Writing</a></p>\n<h3 id=\"Run-server\"><a href=\"#Run-server\" class=\"headerlink\" title=\"Run server\"></a>Run server</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">$ hexo server</div></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/server.html\">Server</a></p>\n<h3 id=\"Generate-static-files\"><a href=\"#Generate-static-files\" class=\"headerlink\" title=\"Generate static files\"></a>Generate static files</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">$ hexo generate</div></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/generating.html\">Generating</a></p>\n<h3 id=\"Deploy-to-remote-sites\"><a href=\"#Deploy-to-remote-sites\" class=\"headerlink\" title=\"Deploy to remote sites\"></a>Deploy to remote sites</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">$ hexo deploy</div></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/deployment.html\">Deployment</a></p>\n<h3 id=\"Code-highlight\"><a href=\"#Code-highlight\" class=\"headerlink\" title=\"Code highlight\"></a>Code highlight</h3><p>Hello World!</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">include</span> <span class=\"meta-string\">&lt;iostream&gt;</span></span></div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">int</span> <span class=\"title\">main</span><span class=\"params\">()</span> </span>&#123;</div><div class=\"line\">    <span class=\"built_in\">std</span>::<span class=\"built_in\">cout</span> &lt;&lt; <span class=\"string\">\"HelloWorld\\n\"</span>;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">print</span> <span class=\"string\">'HelloWorld'</span></div></pre></td></tr></table></figure>\n<h3 id=\"Latex-Support-by-Mathjax\"><a href=\"#Latex-Support-by-Mathjax\" class=\"headerlink\" title=\"Latex Support by Mathjax\"></a>Latex Support by Mathjax</h3><p>Mass-energy equation by Einstein: $E = mc^2$</p>\n<p>a linear equation:</p>\n<pre><code>$$\\mathbf{A}\\mathbf{v} = \\mathbf{y}$$\n</code></pre>"},{"title":"使用 Visual Studio 编译 GSL 科学计算库","date":"2016-12-16T11:00:00.000Z","_content":"\nGSL是一个GNU支持的科学计算库，提供了很丰富的数值计算方法。本文介绍了GSL库在Windows环境下使用VisualStudio进行编译构建的过程。\n\n![GSL is GNU Sentific Library](/img/gsl_picture.jpg)\n<!-- more -->\n[GSL 的项目主页](http://www.gnu.org/software/gsl/)提供的说明来看，GSL支持如下的科学计算：\n\n\n（下面的这张表格的HTML使用的是[No-Cruft Excel to HTML Table Converter](http://pressbin.com/tools/excel_to_html_table/index.html)生成的）\n{% raw %}\n<table>\n   <tr>\n      <td>Complex Numbers </td>\n      <td>Roots of Polynomials</td>\n   </tr>\n   <tr>\n      <td>Special Functions </td>\n      <td>Vectors and Matrices</td>\n   </tr>\n   <tr>\n      <td>Permutations </td>\n      <td>Sorting</td>\n   </tr>\n   <tr>\n      <td>BLAS Support </td>\n      <td>Linear Algebra</td>\n   </tr>\n   <tr>\n      <td>Eigensystems </td>\n      <td>Fast Fourier Transforms</td>\n   </tr>\n   <tr>\n      <td>Quadrature </td>\n      <td>Random Numbers</td>\n   </tr>\n   <tr>\n      <td>Quasi-Random Sequences </td>\n      <td>Random Distributions</td>\n   </tr>\n   <tr>\n      <td>Statistics </td>\n      <td>Histograms</td>\n   </tr>\n   <tr>\n      <td>N-Tuples </td>\n      <td>Monte Carlo Integration</td>\n   </tr>\n   <tr>\n      <td>Simulated Annealing </td>\n      <td>Differential Equations</td>\n   </tr>\n   <tr>\n      <td>Interpolation </td>\n      <td>Numerical Differentiation</td>\n   </tr>\n   <tr>\n      <td>Chebyshev Approximation </td>\n      <td>Series Acceleration</td>\n   </tr>\n   <tr>\n      <td>Discrete Hankel Transforms </td>\n      <td>Root-Finding</td>\n   </tr>\n   <tr>\n      <td>Minimization </td>\n      <td>Least-Squares Fitting</td>\n   </tr>\n   <tr>\n      <td>Physical Constants </td>\n      <td>IEEE Floating-Point</td>\n   </tr>\n   <tr>\n      <td>Discrete Wavelet Transforms </td>\n      <td>Basis splines</td>\n   </tr>\n</table>\n{% endraw %}\n\nGSL的Linux下的配置很简单，照着它的INSTALL文件一步一步来就可以了。CMAKE大法HAO!\n\n``` bash\n./configure\nmake\nmake install\nmake clean\n```\n\n同样的，GSL也可以在Windows环境下配置，下面记录了如何在Windows环境下使用 Visual Studio 和 CMakeGUI 编译测试GSL。\n\n## 使用CMAKE编译成.SLN文件\n\n打开CMAKEGUI，将输入代码路径选为GSL源代码地址，输出路径设为自己想要的输出路径。点击 “Configure“，选择Visual Studio2013为编译器，点击Finish后会进行必要的配置。然后将表格里面的选项都打上勾，再次点击”Configure“，等待完成之后点击”Generate“。完成之后，就可以在输出路径下看到GSL.sln文件了。\n\n## 使用Visual Studio生成解决方案\n\n使用 Visual Studio 打开刚才生成的.SLN文件，分别在Debug和Release模式下生成解决方案，等待完成即可。\n\n当完成后，你应该可以在路径下看到这样一张图，我们主要关注的文件夹是\\bin，\\gsl，\\Debug和\\Release。\n\n\n## 加入环境变量\n\n修改环境变量的Path，将\\GSL_Build_Path\\bin\\Debug加入，这主要是为了\\Debug文件夹下面的gsl.dll文件。如果不进行这一步的话，一会虽然可以编译，但是却不能运行。\n\n这里顺便注释一句，当使用第三方库的时候，如果需要动态链接库的支持，其中一种方法就是将DLL文件的路径加入到Path中去。\n\n## 建立Visual Studio属性表\n\nVisual Studio可以通过建立工程属性表的方法来配置工程选项，一个OpenCV的例子可以参见Yuanbo She的这篇博文 [Opencv 完美配置攻略 2014 (Win8.1 + Opencv 2.4.8 + VS 2013)](http://my.phirobot.com/blog/2014-02-opencv_configuration_in_vs.html)。\n\n配置文件中主要是包含文件和静态链接库LIB的路径设置。下面把我的贴出来，只需要根据GSL的生成路径做相应修改即可。注意我的属性表中保留了OpenCV的内容，如果不需要的话，尽可以删掉。上面的博文对这张属性表如何配置讲得很清楚，有问题可以去参考。\n\n``` html\n<?xml version=\"1.0\" encoding=\"utf-8\"?>\n<Project ToolsVersion=\"4.0\" xmlns=\"http://schemas.microsoft.com/developer/msbuild/2003\">\n  <ImportGroup Label=\"PropertySheets\" />\n  <PropertyGroup Label=\"UserMacros\" />\n  <PropertyGroup>\n        <IncludePath>$(OPENCV249)\\include;E:\\GSLCode\\gsl-build\\;$(IncludePath)</IncludePath>\n        <LibraryPath Condition=\"'$(Platform)'=='Win32'\">$(OPENCV249)\\x86\\vc12\\lib;E:\\GSLCode\\gsl-build\\Debug;$(LibraryPath)</LibraryPath>\n        <LibraryPath Condition=\"'$(Platform)'=='X64'\">$(OPENCV249)\\x64\\vc12\\lib;E:\\GSLCode\\gsl-build\\Debug;$(LibraryPath)</LibraryPath>\n  </PropertyGroup>\n  <ItemDefinitionGroup>\n        <Link Condition=\"'$(Configuration)'=='Debug'\">\n          <AdditionalDependencies>opencv_calib3d249d.lib;opencv_contrib249d.lib;opencv_core249d.lib;opencv_features2d249d.lib;opencv_flann249d.lib;opencv_gpu249d.lib;opencv_highgui249d.lib;opencv_imgproc249d.lib;opencv_legacy249d.lib;opencv_ml249d.lib;opencv_nonfree249d.lib;opencv_objdetect249d.lib;opencv_ocl249d.lib;opencv_photo249d.lib;opencv_stitching249d.lib;opencv_superres249d.lib;opencv_ts249d.lib;opencv_video249d.lib;opencv_videostab249d.lib;gsl.lib;gslcblas.lib;%(AdditionalDependencies)</AdditionalDependencies>\n        </Link>\n        <Link Condition=\"'$(Configuration)'=='Release'\">\n          <AdditionalDependencies>opencv_calib3d249.lib;opencv_contrib249.lib;opencv_core249.lib;opencv_features2d249.lib;opencv_flann249.lib;opencv_gpu249.lib;opencv_highgui249.lib;opencv_imgproc249.lib;opencv_legacy249.lib;opencv_ml249.lib;opencv_nonfree249.lib;opencv_objdetect249.lib;opencv_ocl249.lib;opencv_photo249.lib;opencv_stitching249.lib;opencv_superres249.lib;opencv_ts249.lib;opencv_video249.lib;opencv_videostab249.lib;gsl.lib;gslcblas.lib;%(AdditionalDependencies)</AdditionalDependencies>\n        </Link>\n  </ItemDefinitionGroup>\n  <ItemGroup />\n</Project>\n```\n\n在以后建立Visual Studio工程的时候，在属性窗口直接添加现有属性表就可以了！\n\n## 测试\n\n在项目网站的教程上直接找到一段代码，进行测试，输出贝塞尔函数的值。\n\n\n``` cpp\n#include <stdio.h>\n#include <gsl/gsl_sf_bessel.h>\nint main(void)\n{\n\tdouble x = 5.0;\n\tdouble y = gsl_sf_bessel_J0(x);\n\tprintf(\"J0(%g) = %.18e\\n\", x, y);\n\treturn 0;\n}\n```\n\n控制台输出正确：\n{% raw %}\n<p><img src=\"http://i.imgur.com/uXhVvwS.jpg\" width=\"600\" height=\"200\"></p>\n{% endraw %}\n","source":"_posts/gsl-with-vs.md","raw":"---\ntitle: 使用 Visual Studio 编译 GSL 科学计算库\ndate: 2016-12-16 19:00:00\ntags:\n    - tool\n    - gsl\n---\n\nGSL是一个GNU支持的科学计算库，提供了很丰富的数值计算方法。本文介绍了GSL库在Windows环境下使用VisualStudio进行编译构建的过程。\n\n![GSL is GNU Sentific Library](/img/gsl_picture.jpg)\n<!-- more -->\n[GSL 的项目主页](http://www.gnu.org/software/gsl/)提供的说明来看，GSL支持如下的科学计算：\n\n\n（下面的这张表格的HTML使用的是[No-Cruft Excel to HTML Table Converter](http://pressbin.com/tools/excel_to_html_table/index.html)生成的）\n{% raw %}\n<table>\n   <tr>\n      <td>Complex Numbers </td>\n      <td>Roots of Polynomials</td>\n   </tr>\n   <tr>\n      <td>Special Functions </td>\n      <td>Vectors and Matrices</td>\n   </tr>\n   <tr>\n      <td>Permutations </td>\n      <td>Sorting</td>\n   </tr>\n   <tr>\n      <td>BLAS Support </td>\n      <td>Linear Algebra</td>\n   </tr>\n   <tr>\n      <td>Eigensystems </td>\n      <td>Fast Fourier Transforms</td>\n   </tr>\n   <tr>\n      <td>Quadrature </td>\n      <td>Random Numbers</td>\n   </tr>\n   <tr>\n      <td>Quasi-Random Sequences </td>\n      <td>Random Distributions</td>\n   </tr>\n   <tr>\n      <td>Statistics </td>\n      <td>Histograms</td>\n   </tr>\n   <tr>\n      <td>N-Tuples </td>\n      <td>Monte Carlo Integration</td>\n   </tr>\n   <tr>\n      <td>Simulated Annealing </td>\n      <td>Differential Equations</td>\n   </tr>\n   <tr>\n      <td>Interpolation </td>\n      <td>Numerical Differentiation</td>\n   </tr>\n   <tr>\n      <td>Chebyshev Approximation </td>\n      <td>Series Acceleration</td>\n   </tr>\n   <tr>\n      <td>Discrete Hankel Transforms </td>\n      <td>Root-Finding</td>\n   </tr>\n   <tr>\n      <td>Minimization </td>\n      <td>Least-Squares Fitting</td>\n   </tr>\n   <tr>\n      <td>Physical Constants </td>\n      <td>IEEE Floating-Point</td>\n   </tr>\n   <tr>\n      <td>Discrete Wavelet Transforms </td>\n      <td>Basis splines</td>\n   </tr>\n</table>\n{% endraw %}\n\nGSL的Linux下的配置很简单，照着它的INSTALL文件一步一步来就可以了。CMAKE大法HAO!\n\n``` bash\n./configure\nmake\nmake install\nmake clean\n```\n\n同样的，GSL也可以在Windows环境下配置，下面记录了如何在Windows环境下使用 Visual Studio 和 CMakeGUI 编译测试GSL。\n\n## 使用CMAKE编译成.SLN文件\n\n打开CMAKEGUI，将输入代码路径选为GSL源代码地址，输出路径设为自己想要的输出路径。点击 “Configure“，选择Visual Studio2013为编译器，点击Finish后会进行必要的配置。然后将表格里面的选项都打上勾，再次点击”Configure“，等待完成之后点击”Generate“。完成之后，就可以在输出路径下看到GSL.sln文件了。\n\n## 使用Visual Studio生成解决方案\n\n使用 Visual Studio 打开刚才生成的.SLN文件，分别在Debug和Release模式下生成解决方案，等待完成即可。\n\n当完成后，你应该可以在路径下看到这样一张图，我们主要关注的文件夹是\\bin，\\gsl，\\Debug和\\Release。\n\n\n## 加入环境变量\n\n修改环境变量的Path，将\\GSL_Build_Path\\bin\\Debug加入，这主要是为了\\Debug文件夹下面的gsl.dll文件。如果不进行这一步的话，一会虽然可以编译，但是却不能运行。\n\n这里顺便注释一句，当使用第三方库的时候，如果需要动态链接库的支持，其中一种方法就是将DLL文件的路径加入到Path中去。\n\n## 建立Visual Studio属性表\n\nVisual Studio可以通过建立工程属性表的方法来配置工程选项，一个OpenCV的例子可以参见Yuanbo She的这篇博文 [Opencv 完美配置攻略 2014 (Win8.1 + Opencv 2.4.8 + VS 2013)](http://my.phirobot.com/blog/2014-02-opencv_configuration_in_vs.html)。\n\n配置文件中主要是包含文件和静态链接库LIB的路径设置。下面把我的贴出来，只需要根据GSL的生成路径做相应修改即可。注意我的属性表中保留了OpenCV的内容，如果不需要的话，尽可以删掉。上面的博文对这张属性表如何配置讲得很清楚，有问题可以去参考。\n\n``` html\n<?xml version=\"1.0\" encoding=\"utf-8\"?>\n<Project ToolsVersion=\"4.0\" xmlns=\"http://schemas.microsoft.com/developer/msbuild/2003\">\n  <ImportGroup Label=\"PropertySheets\" />\n  <PropertyGroup Label=\"UserMacros\" />\n  <PropertyGroup>\n        <IncludePath>$(OPENCV249)\\include;E:\\GSLCode\\gsl-build\\;$(IncludePath)</IncludePath>\n        <LibraryPath Condition=\"'$(Platform)'=='Win32'\">$(OPENCV249)\\x86\\vc12\\lib;E:\\GSLCode\\gsl-build\\Debug;$(LibraryPath)</LibraryPath>\n        <LibraryPath Condition=\"'$(Platform)'=='X64'\">$(OPENCV249)\\x64\\vc12\\lib;E:\\GSLCode\\gsl-build\\Debug;$(LibraryPath)</LibraryPath>\n  </PropertyGroup>\n  <ItemDefinitionGroup>\n        <Link Condition=\"'$(Configuration)'=='Debug'\">\n          <AdditionalDependencies>opencv_calib3d249d.lib;opencv_contrib249d.lib;opencv_core249d.lib;opencv_features2d249d.lib;opencv_flann249d.lib;opencv_gpu249d.lib;opencv_highgui249d.lib;opencv_imgproc249d.lib;opencv_legacy249d.lib;opencv_ml249d.lib;opencv_nonfree249d.lib;opencv_objdetect249d.lib;opencv_ocl249d.lib;opencv_photo249d.lib;opencv_stitching249d.lib;opencv_superres249d.lib;opencv_ts249d.lib;opencv_video249d.lib;opencv_videostab249d.lib;gsl.lib;gslcblas.lib;%(AdditionalDependencies)</AdditionalDependencies>\n        </Link>\n        <Link Condition=\"'$(Configuration)'=='Release'\">\n          <AdditionalDependencies>opencv_calib3d249.lib;opencv_contrib249.lib;opencv_core249.lib;opencv_features2d249.lib;opencv_flann249.lib;opencv_gpu249.lib;opencv_highgui249.lib;opencv_imgproc249.lib;opencv_legacy249.lib;opencv_ml249.lib;opencv_nonfree249.lib;opencv_objdetect249.lib;opencv_ocl249.lib;opencv_photo249.lib;opencv_stitching249.lib;opencv_superres249.lib;opencv_ts249.lib;opencv_video249.lib;opencv_videostab249.lib;gsl.lib;gslcblas.lib;%(AdditionalDependencies)</AdditionalDependencies>\n        </Link>\n  </ItemDefinitionGroup>\n  <ItemGroup />\n</Project>\n```\n\n在以后建立Visual Studio工程的时候，在属性窗口直接添加现有属性表就可以了！\n\n## 测试\n\n在项目网站的教程上直接找到一段代码，进行测试，输出贝塞尔函数的值。\n\n\n``` cpp\n#include <stdio.h>\n#include <gsl/gsl_sf_bessel.h>\nint main(void)\n{\n\tdouble x = 5.0;\n\tdouble y = gsl_sf_bessel_J0(x);\n\tprintf(\"J0(%g) = %.18e\\n\", x, y);\n\treturn 0;\n}\n```\n\n控制台输出正确：\n{% raw %}\n<p><img src=\"http://i.imgur.com/uXhVvwS.jpg\" width=\"600\" height=\"200\"></p>\n{% endraw %}\n","slug":"gsl-with-vs","published":1,"updated":"2017-02-26T12:09:28.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cizve07ar000nl61h7ciz58no","content":"<p>GSL是一个GNU支持的科学计算库，提供了很丰富的数值计算方法。本文介绍了GSL库在Windows环境下使用VisualStudio进行编译构建的过程。</p>\n<p><img src=\"/img/gsl_picture.jpg\" alt=\"GSL is GNU Sentific Library\"><br><a id=\"more\"></a><br><a href=\"http://www.gnu.org/software/gsl/\" target=\"_blank\" rel=\"external\">GSL 的项目主页</a>提供的说明来看，GSL支持如下的科学计算：</p>\n<p>（下面的这张表格的HTML使用的是<a href=\"http://pressbin.com/tools/excel_to_html_table/index.html\" target=\"_blank\" rel=\"external\">No-Cruft Excel to HTML Table Converter</a>生成的）<br>\n<table>\n   <tr>\n      <td>Complex Numbers </td>\n      <td>Roots of Polynomials</td>\n   </tr>\n   <tr>\n      <td>Special Functions </td>\n      <td>Vectors and Matrices</td>\n   </tr>\n   <tr>\n      <td>Permutations </td>\n      <td>Sorting</td>\n   </tr>\n   <tr>\n      <td>BLAS Support </td>\n      <td>Linear Algebra</td>\n   </tr>\n   <tr>\n      <td>Eigensystems </td>\n      <td>Fast Fourier Transforms</td>\n   </tr>\n   <tr>\n      <td>Quadrature </td>\n      <td>Random Numbers</td>\n   </tr>\n   <tr>\n      <td>Quasi-Random Sequences </td>\n      <td>Random Distributions</td>\n   </tr>\n   <tr>\n      <td>Statistics </td>\n      <td>Histograms</td>\n   </tr>\n   <tr>\n      <td>N-Tuples </td>\n      <td>Monte Carlo Integration</td>\n   </tr>\n   <tr>\n      <td>Simulated Annealing </td>\n      <td>Differential Equations</td>\n   </tr>\n   <tr>\n      <td>Interpolation </td>\n      <td>Numerical Differentiation</td>\n   </tr>\n   <tr>\n      <td>Chebyshev Approximation </td>\n      <td>Series Acceleration</td>\n   </tr>\n   <tr>\n      <td>Discrete Hankel Transforms </td>\n      <td>Root-Finding</td>\n   </tr>\n   <tr>\n      <td>Minimization </td>\n      <td>Least-Squares Fitting</td>\n   </tr>\n   <tr>\n      <td>Physical Constants </td>\n      <td>IEEE Floating-Point</td>\n   </tr>\n   <tr>\n      <td>Discrete Wavelet Transforms </td>\n      <td>Basis splines</td>\n   </tr>\n</table>\n</p>\n<p>GSL的Linux下的配置很简单，照着它的INSTALL文件一步一步来就可以了。CMAKE大法HAO!</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\">./configure</div><div class=\"line\">make</div><div class=\"line\">make install</div><div class=\"line\">make clean</div></pre></td></tr></table></figure>\n<p>同样的，GSL也可以在Windows环境下配置，下面记录了如何在Windows环境下使用 Visual Studio 和 CMakeGUI 编译测试GSL。</p>\n<h2 id=\"使用CMAKE编译成-SLN文件\"><a href=\"#使用CMAKE编译成-SLN文件\" class=\"headerlink\" title=\"使用CMAKE编译成.SLN文件\"></a>使用CMAKE编译成.SLN文件</h2><p>打开CMAKEGUI，将输入代码路径选为GSL源代码地址，输出路径设为自己想要的输出路径。点击 “Configure“，选择Visual Studio2013为编译器，点击Finish后会进行必要的配置。然后将表格里面的选项都打上勾，再次点击”Configure“，等待完成之后点击”Generate“。完成之后，就可以在输出路径下看到GSL.sln文件了。</p>\n<h2 id=\"使用Visual-Studio生成解决方案\"><a href=\"#使用Visual-Studio生成解决方案\" class=\"headerlink\" title=\"使用Visual Studio生成解决方案\"></a>使用Visual Studio生成解决方案</h2><p>使用 Visual Studio 打开刚才生成的.SLN文件，分别在Debug和Release模式下生成解决方案，等待完成即可。</p>\n<p>当完成后，你应该可以在路径下看到这样一张图，我们主要关注的文件夹是\\bin，\\gsl，\\Debug和\\Release。</p>\n<h2 id=\"加入环境变量\"><a href=\"#加入环境变量\" class=\"headerlink\" title=\"加入环境变量\"></a>加入环境变量</h2><p>修改环境变量的Path，将\\GSL_Build_Path\\bin\\Debug加入，这主要是为了\\Debug文件夹下面的gsl.dll文件。如果不进行这一步的话，一会虽然可以编译，但是却不能运行。</p>\n<p>这里顺便注释一句，当使用第三方库的时候，如果需要动态链接库的支持，其中一种方法就是将DLL文件的路径加入到Path中去。</p>\n<h2 id=\"建立Visual-Studio属性表\"><a href=\"#建立Visual-Studio属性表\" class=\"headerlink\" title=\"建立Visual Studio属性表\"></a>建立Visual Studio属性表</h2><p>Visual Studio可以通过建立工程属性表的方法来配置工程选项，一个OpenCV的例子可以参见Yuanbo She的这篇博文 <a href=\"http://my.phirobot.com/blog/2014-02-opencv_configuration_in_vs.html\" target=\"_blank\" rel=\"external\">Opencv 完美配置攻略 2014 (Win8.1 + Opencv 2.4.8 + VS 2013)</a>。</p>\n<p>配置文件中主要是包含文件和静态链接库LIB的路径设置。下面把我的贴出来，只需要根据GSL的生成路径做相应修改即可。注意我的属性表中保留了OpenCV的内容，如果不需要的话，尽可以删掉。上面的博文对这张属性表如何配置讲得很清楚，有问题可以去参考。</p>\n<figure class=\"highlight html\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div></pre></td><td class=\"code\"><pre><div class=\"line\">&lt;?xml version=\"1.0\" encoding=\"utf-8\"?&gt;</div><div class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">Project</span> <span class=\"attr\">ToolsVersion</span>=<span class=\"string\">\"4.0\"</span> <span class=\"attr\">xmlns</span>=<span class=\"string\">\"http://schemas.microsoft.com/developer/msbuild/2003\"</span>&gt;</span></div><div class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">ImportGroup</span> <span class=\"attr\">Label</span>=<span class=\"string\">\"PropertySheets\"</span> /&gt;</span></div><div class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">PropertyGroup</span> <span class=\"attr\">Label</span>=<span class=\"string\">\"UserMacros\"</span> /&gt;</span></div><div class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">PropertyGroup</span>&gt;</span></div><div class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">IncludePath</span>&gt;</span>$(OPENCV249)\\include;E:\\GSLCode\\gsl-build\\;$(IncludePath)<span class=\"tag\">&lt;/<span class=\"name\">IncludePath</span>&gt;</span></div><div class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">LibraryPath</span> <span class=\"attr\">Condition</span>=<span class=\"string\">\"'$(Platform)'=='Win32'\"</span>&gt;</span>$(OPENCV249)\\x86\\vc12\\lib;E:\\GSLCode\\gsl-build\\Debug;$(LibraryPath)<span class=\"tag\">&lt;/<span class=\"name\">LibraryPath</span>&gt;</span></div><div class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">LibraryPath</span> <span class=\"attr\">Condition</span>=<span class=\"string\">\"'$(Platform)'=='X64'\"</span>&gt;</span>$(OPENCV249)\\x64\\vc12\\lib;E:\\GSLCode\\gsl-build\\Debug;$(LibraryPath)<span class=\"tag\">&lt;/<span class=\"name\">LibraryPath</span>&gt;</span></div><div class=\"line\">  <span class=\"tag\">&lt;/<span class=\"name\">PropertyGroup</span>&gt;</span></div><div class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">ItemDefinitionGroup</span>&gt;</span></div><div class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">Link</span> <span class=\"attr\">Condition</span>=<span class=\"string\">\"'$(Configuration)'=='Debug'\"</span>&gt;</span></div><div class=\"line\">          <span class=\"tag\">&lt;<span class=\"name\">AdditionalDependencies</span>&gt;</span>opencv_calib3d249d.lib;opencv_contrib249d.lib;opencv_core249d.lib;opencv_features2d249d.lib;opencv_flann249d.lib;opencv_gpu249d.lib;opencv_highgui249d.lib;opencv_imgproc249d.lib;opencv_legacy249d.lib;opencv_ml249d.lib;opencv_nonfree249d.lib;opencv_objdetect249d.lib;opencv_ocl249d.lib;opencv_photo249d.lib;opencv_stitching249d.lib;opencv_superres249d.lib;opencv_ts249d.lib;opencv_video249d.lib;opencv_videostab249d.lib;gsl.lib;gslcblas.lib;%(AdditionalDependencies)<span class=\"tag\">&lt;/<span class=\"name\">AdditionalDependencies</span>&gt;</span></div><div class=\"line\">        <span class=\"tag\">&lt;/<span class=\"name\">Link</span>&gt;</span></div><div class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">Link</span> <span class=\"attr\">Condition</span>=<span class=\"string\">\"'$(Configuration)'=='Release'\"</span>&gt;</span></div><div class=\"line\">          <span class=\"tag\">&lt;<span class=\"name\">AdditionalDependencies</span>&gt;</span>opencv_calib3d249.lib;opencv_contrib249.lib;opencv_core249.lib;opencv_features2d249.lib;opencv_flann249.lib;opencv_gpu249.lib;opencv_highgui249.lib;opencv_imgproc249.lib;opencv_legacy249.lib;opencv_ml249.lib;opencv_nonfree249.lib;opencv_objdetect249.lib;opencv_ocl249.lib;opencv_photo249.lib;opencv_stitching249.lib;opencv_superres249.lib;opencv_ts249.lib;opencv_video249.lib;opencv_videostab249.lib;gsl.lib;gslcblas.lib;%(AdditionalDependencies)<span class=\"tag\">&lt;/<span class=\"name\">AdditionalDependencies</span>&gt;</span></div><div class=\"line\">        <span class=\"tag\">&lt;/<span class=\"name\">Link</span>&gt;</span></div><div class=\"line\">  <span class=\"tag\">&lt;/<span class=\"name\">ItemDefinitionGroup</span>&gt;</span></div><div class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">ItemGroup</span> /&gt;</span></div><div class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">Project</span>&gt;</span></div></pre></td></tr></table></figure>\n<p>在以后建立Visual Studio工程的时候，在属性窗口直接添加现有属性表就可以了！</p>\n<h2 id=\"测试\"><a href=\"#测试\" class=\"headerlink\" title=\"测试\"></a>测试</h2><p>在项目网站的教程上直接找到一段代码，进行测试，输出贝塞尔函数的值。</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">include</span> <span class=\"meta-string\">&lt;stdio.h&gt;</span></span></div><div class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">include</span> <span class=\"meta-string\">&lt;gsl/gsl_sf_bessel.h&gt;</span></span></div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">int</span> <span class=\"title\">main</span><span class=\"params\">(<span class=\"keyword\">void</span>)</span></span></div><div class=\"line\">&#123;</div><div class=\"line\">\t<span class=\"keyword\">double</span> x = <span class=\"number\">5.0</span>;</div><div class=\"line\">\t<span class=\"keyword\">double</span> y = gsl_sf_bessel_J0(x);</div><div class=\"line\">\t<span class=\"built_in\">printf</span>(<span class=\"string\">\"J0(%g) = %.18e\\n\"</span>, x, y);</div><div class=\"line\">\t<span class=\"keyword\">return</span> <span class=\"number\">0</span>;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<p>控制台输出正确：<br>\n</p><p><img src=\"http://i.imgur.com/uXhVvwS.jpg\" width=\"600\" height=\"200\"></p>\n<p></p>\n","excerpt":"<p>GSL是一个GNU支持的科学计算库，提供了很丰富的数值计算方法。本文介绍了GSL库在Windows环境下使用VisualStudio进行编译构建的过程。</p>\n<p><img src=\"/img/gsl_picture.jpg\" alt=\"GSL is GNU Sentific Library\"><br>","more":"<br><a href=\"http://www.gnu.org/software/gsl/\">GSL 的项目主页</a>提供的说明来看，GSL支持如下的科学计算：</p>\n<p>（下面的这张表格的HTML使用的是<a href=\"http://pressbin.com/tools/excel_to_html_table/index.html\">No-Cruft Excel to HTML Table Converter</a>生成的）<br>\n<table>\n   <tr>\n      <td>Complex Numbers </td>\n      <td>Roots of Polynomials</td>\n   </tr>\n   <tr>\n      <td>Special Functions </td>\n      <td>Vectors and Matrices</td>\n   </tr>\n   <tr>\n      <td>Permutations </td>\n      <td>Sorting</td>\n   </tr>\n   <tr>\n      <td>BLAS Support </td>\n      <td>Linear Algebra</td>\n   </tr>\n   <tr>\n      <td>Eigensystems </td>\n      <td>Fast Fourier Transforms</td>\n   </tr>\n   <tr>\n      <td>Quadrature </td>\n      <td>Random Numbers</td>\n   </tr>\n   <tr>\n      <td>Quasi-Random Sequences </td>\n      <td>Random Distributions</td>\n   </tr>\n   <tr>\n      <td>Statistics </td>\n      <td>Histograms</td>\n   </tr>\n   <tr>\n      <td>N-Tuples </td>\n      <td>Monte Carlo Integration</td>\n   </tr>\n   <tr>\n      <td>Simulated Annealing </td>\n      <td>Differential Equations</td>\n   </tr>\n   <tr>\n      <td>Interpolation </td>\n      <td>Numerical Differentiation</td>\n   </tr>\n   <tr>\n      <td>Chebyshev Approximation </td>\n      <td>Series Acceleration</td>\n   </tr>\n   <tr>\n      <td>Discrete Hankel Transforms </td>\n      <td>Root-Finding</td>\n   </tr>\n   <tr>\n      <td>Minimization </td>\n      <td>Least-Squares Fitting</td>\n   </tr>\n   <tr>\n      <td>Physical Constants </td>\n      <td>IEEE Floating-Point</td>\n   </tr>\n   <tr>\n      <td>Discrete Wavelet Transforms </td>\n      <td>Basis splines</td>\n   </tr>\n</table>\n</p>\n<p>GSL的Linux下的配置很简单，照着它的INSTALL文件一步一步来就可以了。CMAKE大法HAO!</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\">./configure</div><div class=\"line\">make</div><div class=\"line\">make install</div><div class=\"line\">make clean</div></pre></td></tr></table></figure>\n<p>同样的，GSL也可以在Windows环境下配置，下面记录了如何在Windows环境下使用 Visual Studio 和 CMakeGUI 编译测试GSL。</p>\n<h2 id=\"使用CMAKE编译成-SLN文件\"><a href=\"#使用CMAKE编译成-SLN文件\" class=\"headerlink\" title=\"使用CMAKE编译成.SLN文件\"></a>使用CMAKE编译成.SLN文件</h2><p>打开CMAKEGUI，将输入代码路径选为GSL源代码地址，输出路径设为自己想要的输出路径。点击 “Configure“，选择Visual Studio2013为编译器，点击Finish后会进行必要的配置。然后将表格里面的选项都打上勾，再次点击”Configure“，等待完成之后点击”Generate“。完成之后，就可以在输出路径下看到GSL.sln文件了。</p>\n<h2 id=\"使用Visual-Studio生成解决方案\"><a href=\"#使用Visual-Studio生成解决方案\" class=\"headerlink\" title=\"使用Visual Studio生成解决方案\"></a>使用Visual Studio生成解决方案</h2><p>使用 Visual Studio 打开刚才生成的.SLN文件，分别在Debug和Release模式下生成解决方案，等待完成即可。</p>\n<p>当完成后，你应该可以在路径下看到这样一张图，我们主要关注的文件夹是\\bin，\\gsl，\\Debug和\\Release。</p>\n<h2 id=\"加入环境变量\"><a href=\"#加入环境变量\" class=\"headerlink\" title=\"加入环境变量\"></a>加入环境变量</h2><p>修改环境变量的Path，将\\GSL_Build_Path\\bin\\Debug加入，这主要是为了\\Debug文件夹下面的gsl.dll文件。如果不进行这一步的话，一会虽然可以编译，但是却不能运行。</p>\n<p>这里顺便注释一句，当使用第三方库的时候，如果需要动态链接库的支持，其中一种方法就是将DLL文件的路径加入到Path中去。</p>\n<h2 id=\"建立Visual-Studio属性表\"><a href=\"#建立Visual-Studio属性表\" class=\"headerlink\" title=\"建立Visual Studio属性表\"></a>建立Visual Studio属性表</h2><p>Visual Studio可以通过建立工程属性表的方法来配置工程选项，一个OpenCV的例子可以参见Yuanbo She的这篇博文 <a href=\"http://my.phirobot.com/blog/2014-02-opencv_configuration_in_vs.html\">Opencv 完美配置攻略 2014 (Win8.1 + Opencv 2.4.8 + VS 2013)</a>。</p>\n<p>配置文件中主要是包含文件和静态链接库LIB的路径设置。下面把我的贴出来，只需要根据GSL的生成路径做相应修改即可。注意我的属性表中保留了OpenCV的内容，如果不需要的话，尽可以删掉。上面的博文对这张属性表如何配置讲得很清楚，有问题可以去参考。</p>\n<figure class=\"highlight html\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div></pre></td><td class=\"code\"><pre><div class=\"line\">&lt;?xml version=\"1.0\" encoding=\"utf-8\"?&gt;</div><div class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">Project</span> <span class=\"attr\">ToolsVersion</span>=<span class=\"string\">\"4.0\"</span> <span class=\"attr\">xmlns</span>=<span class=\"string\">\"http://schemas.microsoft.com/developer/msbuild/2003\"</span>&gt;</span></div><div class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">ImportGroup</span> <span class=\"attr\">Label</span>=<span class=\"string\">\"PropertySheets\"</span> /&gt;</span></div><div class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">PropertyGroup</span> <span class=\"attr\">Label</span>=<span class=\"string\">\"UserMacros\"</span> /&gt;</span></div><div class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">PropertyGroup</span>&gt;</span></div><div class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">IncludePath</span>&gt;</span>$(OPENCV249)\\include;E:\\GSLCode\\gsl-build\\;$(IncludePath)<span class=\"tag\">&lt;/<span class=\"name\">IncludePath</span>&gt;</span></div><div class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">LibraryPath</span> <span class=\"attr\">Condition</span>=<span class=\"string\">\"'$(Platform)'=='Win32'\"</span>&gt;</span>$(OPENCV249)\\x86\\vc12\\lib;E:\\GSLCode\\gsl-build\\Debug;$(LibraryPath)<span class=\"tag\">&lt;/<span class=\"name\">LibraryPath</span>&gt;</span></div><div class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">LibraryPath</span> <span class=\"attr\">Condition</span>=<span class=\"string\">\"'$(Platform)'=='X64'\"</span>&gt;</span>$(OPENCV249)\\x64\\vc12\\lib;E:\\GSLCode\\gsl-build\\Debug;$(LibraryPath)<span class=\"tag\">&lt;/<span class=\"name\">LibraryPath</span>&gt;</span></div><div class=\"line\">  <span class=\"tag\">&lt;/<span class=\"name\">PropertyGroup</span>&gt;</span></div><div class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">ItemDefinitionGroup</span>&gt;</span></div><div class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">Link</span> <span class=\"attr\">Condition</span>=<span class=\"string\">\"'$(Configuration)'=='Debug'\"</span>&gt;</span></div><div class=\"line\">          <span class=\"tag\">&lt;<span class=\"name\">AdditionalDependencies</span>&gt;</span>opencv_calib3d249d.lib;opencv_contrib249d.lib;opencv_core249d.lib;opencv_features2d249d.lib;opencv_flann249d.lib;opencv_gpu249d.lib;opencv_highgui249d.lib;opencv_imgproc249d.lib;opencv_legacy249d.lib;opencv_ml249d.lib;opencv_nonfree249d.lib;opencv_objdetect249d.lib;opencv_ocl249d.lib;opencv_photo249d.lib;opencv_stitching249d.lib;opencv_superres249d.lib;opencv_ts249d.lib;opencv_video249d.lib;opencv_videostab249d.lib;gsl.lib;gslcblas.lib;%(AdditionalDependencies)<span class=\"tag\">&lt;/<span class=\"name\">AdditionalDependencies</span>&gt;</span></div><div class=\"line\">        <span class=\"tag\">&lt;/<span class=\"name\">Link</span>&gt;</span></div><div class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">Link</span> <span class=\"attr\">Condition</span>=<span class=\"string\">\"'$(Configuration)'=='Release'\"</span>&gt;</span></div><div class=\"line\">          <span class=\"tag\">&lt;<span class=\"name\">AdditionalDependencies</span>&gt;</span>opencv_calib3d249.lib;opencv_contrib249.lib;opencv_core249.lib;opencv_features2d249.lib;opencv_flann249.lib;opencv_gpu249.lib;opencv_highgui249.lib;opencv_imgproc249.lib;opencv_legacy249.lib;opencv_ml249.lib;opencv_nonfree249.lib;opencv_objdetect249.lib;opencv_ocl249.lib;opencv_photo249.lib;opencv_stitching249.lib;opencv_superres249.lib;opencv_ts249.lib;opencv_video249.lib;opencv_videostab249.lib;gsl.lib;gslcblas.lib;%(AdditionalDependencies)<span class=\"tag\">&lt;/<span class=\"name\">AdditionalDependencies</span>&gt;</span></div><div class=\"line\">        <span class=\"tag\">&lt;/<span class=\"name\">Link</span>&gt;</span></div><div class=\"line\">  <span class=\"tag\">&lt;/<span class=\"name\">ItemDefinitionGroup</span>&gt;</span></div><div class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">ItemGroup</span> /&gt;</span></div><div class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">Project</span>&gt;</span></div></pre></td></tr></table></figure>\n<p>在以后建立Visual Studio工程的时候，在属性窗口直接添加现有属性表就可以了！</p>\n<h2 id=\"测试\"><a href=\"#测试\" class=\"headerlink\" title=\"测试\"></a>测试</h2><p>在项目网站的教程上直接找到一段代码，进行测试，输出贝塞尔函数的值。</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">include</span> <span class=\"meta-string\">&lt;stdio.h&gt;</span></span></div><div class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">include</span> <span class=\"meta-string\">&lt;gsl/gsl_sf_bessel.h&gt;</span></span></div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">int</span> <span class=\"title\">main</span><span class=\"params\">(<span class=\"keyword\">void</span>)</span></div><div class=\"line\"></span>&#123;</div><div class=\"line\">\t<span class=\"keyword\">double</span> x = <span class=\"number\">5.0</span>;</div><div class=\"line\">\t<span class=\"keyword\">double</span> y = gsl_sf_bessel_J0(x);</div><div class=\"line\">\t<span class=\"built_in\">printf</span>(<span class=\"string\">\"J0(%g) = %.18e\\n\"</span>, x, y);</div><div class=\"line\">\t<span class=\"keyword\">return</span> <span class=\"number\">0</span>;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<p>控制台输出正确：<br>\n<p><img src=\"http://i.imgur.com/uXhVvwS.jpg\" width=\"600\" height=\"200\"></p>\n</p>"},{"title":"toy demo - PyTorch + MNIST","date":"2017-03-04T14:37:44.000Z","_content":"本篇文章介绍了使用PyTorch在MNIST数据集上训练MLP和CNN，并记录自己实现过程中的若干问题。\n![MNIST](/img/mnist_example.png)\n<!-- more -->\n\n## 加载MNIST数据集\nPyTorch中提供了MNIST，CIFAR，COCO等常用数据集的加载方法。`MNIST`是`torchvision.datasets`包中的一个类，负责根据传入的参数加载数据集。如果自己之前没有下载过该数据集，可以将`download`参数设置为`True`，会自动下载数据集并解包。如果之前已经下载好了，只需将其路径通过`root`传入即可。\n\n在加载图像后，我们常常需要对图像进行若干预处理。比如减去RGB通道的均值，或者裁剪或翻转图像实现augmentation等，这些操作可以在`torchvision.transforms`包中找到对应的操作。在下面的代码中，通过使用`transforms.Compose()`，我们构造了对数据进行预处理的复合操作序列，`ToTensor`负责将PIL图像转换为Tensor数据（RGB通道从`[0, 255]`范围变为`[0, 1]`）， `Normalize`负责对图像进行规范化。这里需要注意，虽然MNIST中图像都是灰度图像，通道数均为1，但是仍要传入`tuple`。\n\n之后，我们通过`DataLoader`返回一个数据集上的可迭代对象。一会我们通过`for`循环，就可以遍历数据集了。\n\n``` py\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nimport torchvision.datasets as dset\nimport torchvision.transforms as transforms\nimport torch.nn.functional as F\nimport torch.optim as optim\n\ntrans = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (1.0,))])\n\ntrain_set = dset.MNIST(root=root, train=True, transform=trans, download=download)\ntest_set = dset.MNIST(root=root, train=False, transform=trans)\n\nbatch_size = 128\nkwargs = {'num_workers': 1, 'pin_memory': True}\ntrain_loader = torch.utils.data.DataLoader(\n                 dataset=train_set,\n                 batch_size=batch_size,\n                 shuffle=True, **kwargs)\ntest_loader = torch.utils.data.DataLoader(\n                dataset=test_set,\n                batch_size=batch_size,\n                shuffle=False, **kwargs)\n\n```\n\n## 网络构建\n在进行网络构建时，主要通过`torch.nn`包中的已经实现好的卷积层、池化层等进行搭建。例如下面的代码展示了一个具有一个隐含层的MLP网络。`nn.Linear`负责构建全连接层，需要提供输入和输出的通道数，也就是`y = wx+b`中`x`和`y`的维度。\n\n``` py\nclass MLPNet(nn.Module):\n    def __init__(self):\n        super(MLPNet, self).__init__()\n        self.fc1 = nn.Linear(28*28, 500)\n        self.fc2 = nn.Linear(500, 256)\n        self.fc3 = nn.Linear(256, 10)\n        self.ceriation = nn.CrossEntropyLoss()\n    def forward(self, x, target):\n        x = x.view(-1, 28*28)\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = F.relu(self.fc3(x))\n        loss = self.ceriation(x, target)\n        return x, loss\n```\n由于PyTorch可以实现自动求导，所以我们只需实现`forward`过程即可。这里由于池化层和非线性变换都没有参数，所以使用了`nn.functionals`中的对应操作实现。通过看文档，可以发现，一般`nn`里面的各种层，都会在`nn.functionals`里面有其对应。例如卷积层的对应实现，如下所示，需要传入卷积核的权重。\n\n``` py\n# With square kernels and equal stride\nfilters = autograd.Variable(torch.randn(8,4,3,3))\ninputs = autograd.Variable(torch.randn(1,4,5,5))\nF.conv2d(inputs, filters, padding=1)\n```\n\n同样地，我们可以实现LeNet的结构如下。\n\n``` py\nclass MLPNet(nn.Module):\n    def __init__(self):\n        super(MLPNet, self).__init__()\n        self.fc1 = nn.Linear(28*28, 500)\n        self.fc2 = nn.Linear(500, 256)\n        self.fc3 = nn.Linear(256, 10)\n        self.ceriation = nn.CrossEntropyLoss()\n    def forward(self, x, target):\n        x = x.view(-1, 28*28)\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = F.relu(self.fc3(x))\n        loss = self.ceriation(x, target)\n        return x, loss\n    def name(self):\n        return 'mlpnet'\n\nclass LeNet(nn.Module):\n    def __init__(self):\n        super(LeNet, self).__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5, 1)\n        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n        self.fc1 = nn.Linear(4*4*50, 500)\n        self.fc2 = nn.Linear(500, 10)\n        self.ceriation = nn.CrossEntropyLoss()\n    def forward(self, x, target):\n        x = self.conv1(x)\n        x = F.max_pool2d(x, 2, 2)\n        x = F.relu(x)\n        x = self.conv2(x)\n        x = F.max_pool2d(x, 2, 2)\n        x = F.relu(x)\n        x = x.view(-1, 4*4*50)\n        x = self.fc1(x)\n        x = self.fc2(x)\n        loss = self.ceriation(x, target)\n        return x, loss\n```\n\n## 训练与测试\n\n在训练时，我们首先应确定优化方法。这里我们使用带动量的`SGD`方法。下面代码中的`optim.SGD`初始化需要接受网络中待优化的`Parameter`列表（或是迭代器），以及学习率`lr`，动量`momentum`。\n\n``` py\noptimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n```\n\n接下来，我们只需要遍历数据集，同时在每次迭代中清空待优化参数的梯度，前向计算，反向传播以及优化器的迭代求解即可。\n\n``` py\nmodel = MLPNet().cuda()   # 以MLP为例\nfor epoch in xrange(10):\n    # trainning\n    for batch_idx, (x, target) in enumerate(train_loader):\n        optimizer.zero_grad()     #每次都要清空上一步中参数的grad，否则会出错的~\n        x, target = Variable(x.cuda()), Variable(target.cuda())\n        _, loss = model(x, target)   #得到loss\n        loss.backward()              #bp\n        optimizer.step()             #优化器迭代\n        if batch_idx % 100 == 0:\n            print '==>>> epoch: {}, batch index: {}, train loss: {:.6f}'.format(epoch, batch_idx, loss.data[0])\n    # testing\n    correct_cnt, ave_loss = 0, 0\n    for batch_idx, (x, target) in enumerate(test_loader):\n        x, target = Variable(x.cuda(), volatile=True), Variable(target.cuda(), volatile=True)\n        score, loss = model(x, target)\n        _, pred_label = torch.max(score.data, 1)\n        correct_cnt += (pred_label == target.data).sum()\n        ave_loss += loss.data[0]\n    accuracy = correct_cnt*1.0/len(test_loader)/batch_size\n    ave_loss /= len(test_loader)\n\n```\n\n当优化完毕后，需要保存模型。这里[官方文档](http://pytorch.org/docs/notes/serialization.html#recommend-saving-models)给出了推荐的方法，如下所示：\n``` py\ntorch.save(model.state_dict(), PATH)   #保存网络参数\nthe_model = TheModelClass(*args, **kwargs)\nthe_model.load_state_dict(torch.load(PATH))  #读取网络参数\n```\n\n该博客的完整代码可以见：[PyTorch MNIST demo](https://gist.github.com/xmfbit/b27cdbff68870418bdb8cefa86a2d558)。\n","source":"_posts/pytorch-mnist-example.md","raw":"---\ntitle: toy demo - PyTorch + MNIST\ndate: 2017-03-04 22:37:44\ntags:\n     - pytorch\n---\n本篇文章介绍了使用PyTorch在MNIST数据集上训练MLP和CNN，并记录自己实现过程中的若干问题。\n![MNIST](/img/mnist_example.png)\n<!-- more -->\n\n## 加载MNIST数据集\nPyTorch中提供了MNIST，CIFAR，COCO等常用数据集的加载方法。`MNIST`是`torchvision.datasets`包中的一个类，负责根据传入的参数加载数据集。如果自己之前没有下载过该数据集，可以将`download`参数设置为`True`，会自动下载数据集并解包。如果之前已经下载好了，只需将其路径通过`root`传入即可。\n\n在加载图像后，我们常常需要对图像进行若干预处理。比如减去RGB通道的均值，或者裁剪或翻转图像实现augmentation等，这些操作可以在`torchvision.transforms`包中找到对应的操作。在下面的代码中，通过使用`transforms.Compose()`，我们构造了对数据进行预处理的复合操作序列，`ToTensor`负责将PIL图像转换为Tensor数据（RGB通道从`[0, 255]`范围变为`[0, 1]`）， `Normalize`负责对图像进行规范化。这里需要注意，虽然MNIST中图像都是灰度图像，通道数均为1，但是仍要传入`tuple`。\n\n之后，我们通过`DataLoader`返回一个数据集上的可迭代对象。一会我们通过`for`循环，就可以遍历数据集了。\n\n``` py\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nimport torchvision.datasets as dset\nimport torchvision.transforms as transforms\nimport torch.nn.functional as F\nimport torch.optim as optim\n\ntrans = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (1.0,))])\n\ntrain_set = dset.MNIST(root=root, train=True, transform=trans, download=download)\ntest_set = dset.MNIST(root=root, train=False, transform=trans)\n\nbatch_size = 128\nkwargs = {'num_workers': 1, 'pin_memory': True}\ntrain_loader = torch.utils.data.DataLoader(\n                 dataset=train_set,\n                 batch_size=batch_size,\n                 shuffle=True, **kwargs)\ntest_loader = torch.utils.data.DataLoader(\n                dataset=test_set,\n                batch_size=batch_size,\n                shuffle=False, **kwargs)\n\n```\n\n## 网络构建\n在进行网络构建时，主要通过`torch.nn`包中的已经实现好的卷积层、池化层等进行搭建。例如下面的代码展示了一个具有一个隐含层的MLP网络。`nn.Linear`负责构建全连接层，需要提供输入和输出的通道数，也就是`y = wx+b`中`x`和`y`的维度。\n\n``` py\nclass MLPNet(nn.Module):\n    def __init__(self):\n        super(MLPNet, self).__init__()\n        self.fc1 = nn.Linear(28*28, 500)\n        self.fc2 = nn.Linear(500, 256)\n        self.fc3 = nn.Linear(256, 10)\n        self.ceriation = nn.CrossEntropyLoss()\n    def forward(self, x, target):\n        x = x.view(-1, 28*28)\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = F.relu(self.fc3(x))\n        loss = self.ceriation(x, target)\n        return x, loss\n```\n由于PyTorch可以实现自动求导，所以我们只需实现`forward`过程即可。这里由于池化层和非线性变换都没有参数，所以使用了`nn.functionals`中的对应操作实现。通过看文档，可以发现，一般`nn`里面的各种层，都会在`nn.functionals`里面有其对应。例如卷积层的对应实现，如下所示，需要传入卷积核的权重。\n\n``` py\n# With square kernels and equal stride\nfilters = autograd.Variable(torch.randn(8,4,3,3))\ninputs = autograd.Variable(torch.randn(1,4,5,5))\nF.conv2d(inputs, filters, padding=1)\n```\n\n同样地，我们可以实现LeNet的结构如下。\n\n``` py\nclass MLPNet(nn.Module):\n    def __init__(self):\n        super(MLPNet, self).__init__()\n        self.fc1 = nn.Linear(28*28, 500)\n        self.fc2 = nn.Linear(500, 256)\n        self.fc3 = nn.Linear(256, 10)\n        self.ceriation = nn.CrossEntropyLoss()\n    def forward(self, x, target):\n        x = x.view(-1, 28*28)\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = F.relu(self.fc3(x))\n        loss = self.ceriation(x, target)\n        return x, loss\n    def name(self):\n        return 'mlpnet'\n\nclass LeNet(nn.Module):\n    def __init__(self):\n        super(LeNet, self).__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5, 1)\n        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n        self.fc1 = nn.Linear(4*4*50, 500)\n        self.fc2 = nn.Linear(500, 10)\n        self.ceriation = nn.CrossEntropyLoss()\n    def forward(self, x, target):\n        x = self.conv1(x)\n        x = F.max_pool2d(x, 2, 2)\n        x = F.relu(x)\n        x = self.conv2(x)\n        x = F.max_pool2d(x, 2, 2)\n        x = F.relu(x)\n        x = x.view(-1, 4*4*50)\n        x = self.fc1(x)\n        x = self.fc2(x)\n        loss = self.ceriation(x, target)\n        return x, loss\n```\n\n## 训练与测试\n\n在训练时，我们首先应确定优化方法。这里我们使用带动量的`SGD`方法。下面代码中的`optim.SGD`初始化需要接受网络中待优化的`Parameter`列表（或是迭代器），以及学习率`lr`，动量`momentum`。\n\n``` py\noptimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n```\n\n接下来，我们只需要遍历数据集，同时在每次迭代中清空待优化参数的梯度，前向计算，反向传播以及优化器的迭代求解即可。\n\n``` py\nmodel = MLPNet().cuda()   # 以MLP为例\nfor epoch in xrange(10):\n    # trainning\n    for batch_idx, (x, target) in enumerate(train_loader):\n        optimizer.zero_grad()     #每次都要清空上一步中参数的grad，否则会出错的~\n        x, target = Variable(x.cuda()), Variable(target.cuda())\n        _, loss = model(x, target)   #得到loss\n        loss.backward()              #bp\n        optimizer.step()             #优化器迭代\n        if batch_idx % 100 == 0:\n            print '==>>> epoch: {}, batch index: {}, train loss: {:.6f}'.format(epoch, batch_idx, loss.data[0])\n    # testing\n    correct_cnt, ave_loss = 0, 0\n    for batch_idx, (x, target) in enumerate(test_loader):\n        x, target = Variable(x.cuda(), volatile=True), Variable(target.cuda(), volatile=True)\n        score, loss = model(x, target)\n        _, pred_label = torch.max(score.data, 1)\n        correct_cnt += (pred_label == target.data).sum()\n        ave_loss += loss.data[0]\n    accuracy = correct_cnt*1.0/len(test_loader)/batch_size\n    ave_loss /= len(test_loader)\n\n```\n\n当优化完毕后，需要保存模型。这里[官方文档](http://pytorch.org/docs/notes/serialization.html#recommend-saving-models)给出了推荐的方法，如下所示：\n``` py\ntorch.save(model.state_dict(), PATH)   #保存网络参数\nthe_model = TheModelClass(*args, **kwargs)\nthe_model.load_state_dict(torch.load(PATH))  #读取网络参数\n```\n\n该博客的完整代码可以见：[PyTorch MNIST demo](https://gist.github.com/xmfbit/b27cdbff68870418bdb8cefa86a2d558)。\n","slug":"pytorch-mnist-example","published":1,"updated":"2017-03-04T15:14:50.000Z","_id":"cizve07ax000ql61ht5fv39rj","comments":1,"layout":"post","photos":[],"link":"","content":"<p>本篇文章介绍了使用PyTorch在MNIST数据集上训练MLP和CNN，并记录自己实现过程中的若干问题。<br><img src=\"/img/mnist_example.png\" alt=\"MNIST\"><br><a id=\"more\"></a></p>\n<h2 id=\"加载MNIST数据集\"><a href=\"#加载MNIST数据集\" class=\"headerlink\" title=\"加载MNIST数据集\"></a>加载MNIST数据集</h2><p>PyTorch中提供了MNIST，CIFAR，COCO等常用数据集的加载方法。<code>MNIST</code>是<code>torchvision.datasets</code>包中的一个类，负责根据传入的参数加载数据集。如果自己之前没有下载过该数据集，可以将<code>download</code>参数设置为<code>True</code>，会自动下载数据集并解包。如果之前已经下载好了，只需将其路径通过<code>root</code>传入即可。</p>\n<p>在加载图像后，我们常常需要对图像进行若干预处理。比如减去RGB通道的均值，或者裁剪或翻转图像实现augmentation等，这些操作可以在<code>torchvision.transforms</code>包中找到对应的操作。在下面的代码中，通过使用<code>transforms.Compose()</code>，我们构造了对数据进行预处理的复合操作序列，<code>ToTensor</code>负责将PIL图像转换为Tensor数据（RGB通道从<code>[0, 255]</code>范围变为<code>[0, 1]</code>）， <code>Normalize</code>负责对图像进行规范化。这里需要注意，虽然MNIST中图像都是灰度图像，通道数均为1，但是仍要传入<code>tuple</code>。</p>\n<p>之后，我们通过<code>DataLoader</code>返回一个数据集上的可迭代对象。一会我们通过<code>for</code>循环，就可以遍历数据集了。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">import</span> torch</div><div class=\"line\"><span class=\"keyword\">import</span> torch.nn <span class=\"keyword\">as</span> nn</div><div class=\"line\"><span class=\"keyword\">from</span> torch.autograd <span class=\"keyword\">import</span> Variable</div><div class=\"line\"><span class=\"keyword\">import</span> torchvision.datasets <span class=\"keyword\">as</span> dset</div><div class=\"line\"><span class=\"keyword\">import</span> torchvision.transforms <span class=\"keyword\">as</span> transforms</div><div class=\"line\"><span class=\"keyword\">import</span> torch.nn.functional <span class=\"keyword\">as</span> F</div><div class=\"line\"><span class=\"keyword\">import</span> torch.optim <span class=\"keyword\">as</span> optim</div><div class=\"line\"></div><div class=\"line\">trans = transforms.Compose([transforms.ToTensor(), transforms.Normalize((<span class=\"number\">0.5</span>,), (<span class=\"number\">1.0</span>,))])</div><div class=\"line\"></div><div class=\"line\">train_set = dset.MNIST(root=root, train=<span class=\"keyword\">True</span>, transform=trans, download=download)</div><div class=\"line\">test_set = dset.MNIST(root=root, train=<span class=\"keyword\">False</span>, transform=trans)</div><div class=\"line\"></div><div class=\"line\">batch_size = <span class=\"number\">128</span></div><div class=\"line\">kwargs = &#123;<span class=\"string\">'num_workers'</span>: <span class=\"number\">1</span>, <span class=\"string\">'pin_memory'</span>: <span class=\"keyword\">True</span>&#125;</div><div class=\"line\">train_loader = torch.utils.data.DataLoader(</div><div class=\"line\">                 dataset=train_set,</div><div class=\"line\">                 batch_size=batch_size,</div><div class=\"line\">                 shuffle=<span class=\"keyword\">True</span>, **kwargs)</div><div class=\"line\">test_loader = torch.utils.data.DataLoader(</div><div class=\"line\">                dataset=test_set,</div><div class=\"line\">                batch_size=batch_size,</div><div class=\"line\">                shuffle=<span class=\"keyword\">False</span>, **kwargs)</div></pre></td></tr></table></figure>\n<h2 id=\"网络构建\"><a href=\"#网络构建\" class=\"headerlink\" title=\"网络构建\"></a>网络构建</h2><p>在进行网络构建时，主要通过<code>torch.nn</code>包中的已经实现好的卷积层、池化层等进行搭建。例如下面的代码展示了一个具有一个隐含层的MLP网络。<code>nn.Linear</code>负责构建全连接层，需要提供输入和输出的通道数，也就是<code>y = wx+b</code>中<code>x</code>和<code>y</code>的维度。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">MLPNet</span><span class=\"params\">(nn.Module)</span>:</span></div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self)</span>:</span></div><div class=\"line\">        super(MLPNet, self).__init__()</div><div class=\"line\">        self.fc1 = nn.Linear(<span class=\"number\">28</span>*<span class=\"number\">28</span>, <span class=\"number\">500</span>)</div><div class=\"line\">        self.fc2 = nn.Linear(<span class=\"number\">500</span>, <span class=\"number\">256</span>)</div><div class=\"line\">        self.fc3 = nn.Linear(<span class=\"number\">256</span>, <span class=\"number\">10</span>)</div><div class=\"line\">        self.ceriation = nn.CrossEntropyLoss()</div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">forward</span><span class=\"params\">(self, x, target)</span>:</span></div><div class=\"line\">        x = x.view(<span class=\"number\">-1</span>, <span class=\"number\">28</span>*<span class=\"number\">28</span>)</div><div class=\"line\">        x = F.relu(self.fc1(x))</div><div class=\"line\">        x = F.relu(self.fc2(x))</div><div class=\"line\">        x = F.relu(self.fc3(x))</div><div class=\"line\">        loss = self.ceriation(x, target)</div><div class=\"line\">        <span class=\"keyword\">return</span> x, loss</div></pre></td></tr></table></figure>\n<p>由于PyTorch可以实现自动求导，所以我们只需实现<code>forward</code>过程即可。这里由于池化层和非线性变换都没有参数，所以使用了<code>nn.functionals</code>中的对应操作实现。通过看文档，可以发现，一般<code>nn</code>里面的各种层，都会在<code>nn.functionals</code>里面有其对应。例如卷积层的对应实现，如下所示，需要传入卷积核的权重。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\"># With square kernels and equal stride</span></div><div class=\"line\">filters = autograd.Variable(torch.randn(<span class=\"number\">8</span>,<span class=\"number\">4</span>,<span class=\"number\">3</span>,<span class=\"number\">3</span>))</div><div class=\"line\">inputs = autograd.Variable(torch.randn(<span class=\"number\">1</span>,<span class=\"number\">4</span>,<span class=\"number\">5</span>,<span class=\"number\">5</span>))</div><div class=\"line\">F.conv2d(inputs, filters, padding=<span class=\"number\">1</span>)</div></pre></td></tr></table></figure>\n<p>同样地，我们可以实现LeNet的结构如下。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">MLPNet</span><span class=\"params\">(nn.Module)</span>:</span></div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self)</span>:</span></div><div class=\"line\">        super(MLPNet, self).__init__()</div><div class=\"line\">        self.fc1 = nn.Linear(<span class=\"number\">28</span>*<span class=\"number\">28</span>, <span class=\"number\">500</span>)</div><div class=\"line\">        self.fc2 = nn.Linear(<span class=\"number\">500</span>, <span class=\"number\">256</span>)</div><div class=\"line\">        self.fc3 = nn.Linear(<span class=\"number\">256</span>, <span class=\"number\">10</span>)</div><div class=\"line\">        self.ceriation = nn.CrossEntropyLoss()</div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">forward</span><span class=\"params\">(self, x, target)</span>:</span></div><div class=\"line\">        x = x.view(<span class=\"number\">-1</span>, <span class=\"number\">28</span>*<span class=\"number\">28</span>)</div><div class=\"line\">        x = F.relu(self.fc1(x))</div><div class=\"line\">        x = F.relu(self.fc2(x))</div><div class=\"line\">        x = F.relu(self.fc3(x))</div><div class=\"line\">        loss = self.ceriation(x, target)</div><div class=\"line\">        <span class=\"keyword\">return</span> x, loss</div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">name</span><span class=\"params\">(self)</span>:</span></div><div class=\"line\">        <span class=\"keyword\">return</span> <span class=\"string\">'mlpnet'</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">LeNet</span><span class=\"params\">(nn.Module)</span>:</span></div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self)</span>:</span></div><div class=\"line\">        super(LeNet, self).__init__()</div><div class=\"line\">        self.conv1 = nn.Conv2d(<span class=\"number\">1</span>, <span class=\"number\">20</span>, <span class=\"number\">5</span>, <span class=\"number\">1</span>)</div><div class=\"line\">        self.conv2 = nn.Conv2d(<span class=\"number\">20</span>, <span class=\"number\">50</span>, <span class=\"number\">5</span>, <span class=\"number\">1</span>)</div><div class=\"line\">        self.fc1 = nn.Linear(<span class=\"number\">4</span>*<span class=\"number\">4</span>*<span class=\"number\">50</span>, <span class=\"number\">500</span>)</div><div class=\"line\">        self.fc2 = nn.Linear(<span class=\"number\">500</span>, <span class=\"number\">10</span>)</div><div class=\"line\">        self.ceriation = nn.CrossEntropyLoss()</div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">forward</span><span class=\"params\">(self, x, target)</span>:</span></div><div class=\"line\">        x = self.conv1(x)</div><div class=\"line\">        x = F.max_pool2d(x, <span class=\"number\">2</span>, <span class=\"number\">2</span>)</div><div class=\"line\">        x = F.relu(x)</div><div class=\"line\">        x = self.conv2(x)</div><div class=\"line\">        x = F.max_pool2d(x, <span class=\"number\">2</span>, <span class=\"number\">2</span>)</div><div class=\"line\">        x = F.relu(x)</div><div class=\"line\">        x = x.view(<span class=\"number\">-1</span>, <span class=\"number\">4</span>*<span class=\"number\">4</span>*<span class=\"number\">50</span>)</div><div class=\"line\">        x = self.fc1(x)</div><div class=\"line\">        x = self.fc2(x)</div><div class=\"line\">        loss = self.ceriation(x, target)</div><div class=\"line\">        <span class=\"keyword\">return</span> x, loss</div></pre></td></tr></table></figure>\n<h2 id=\"训练与测试\"><a href=\"#训练与测试\" class=\"headerlink\" title=\"训练与测试\"></a>训练与测试</h2><p>在训练时，我们首先应确定优化方法。这里我们使用带动量的<code>SGD</code>方法。下面代码中的<code>optim.SGD</code>初始化需要接受网络中待优化的<code>Parameter</code>列表（或是迭代器），以及学习率<code>lr</code>，动量<code>momentum</code>。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">optimizer = optim.SGD(model.parameters(), lr=<span class=\"number\">0.01</span>, momentum=<span class=\"number\">0.9</span>)</div></pre></td></tr></table></figure>\n<p>接下来，我们只需要遍历数据集，同时在每次迭代中清空待优化参数的梯度，前向计算，反向传播以及优化器的迭代求解即可。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div></pre></td><td class=\"code\"><pre><div class=\"line\">model = MLPNet().cuda()   <span class=\"comment\"># 以MLP为例</span></div><div class=\"line\"><span class=\"keyword\">for</span> epoch <span class=\"keyword\">in</span> xrange(<span class=\"number\">10</span>):</div><div class=\"line\">    <span class=\"comment\"># trainning</span></div><div class=\"line\">    <span class=\"keyword\">for</span> batch_idx, (x, target) <span class=\"keyword\">in</span> enumerate(train_loader):</div><div class=\"line\">        optimizer.zero_grad()     <span class=\"comment\">#每次都要清空上一步中参数的grad，否则会出错的~</span></div><div class=\"line\">        x, target = Variable(x.cuda()), Variable(target.cuda())</div><div class=\"line\">        _, loss = model(x, target)   <span class=\"comment\">#得到loss</span></div><div class=\"line\">        loss.backward()              <span class=\"comment\">#bp</span></div><div class=\"line\">        optimizer.step()             <span class=\"comment\">#优化器迭代</span></div><div class=\"line\">        <span class=\"keyword\">if</span> batch_idx % <span class=\"number\">100</span> == <span class=\"number\">0</span>:</div><div class=\"line\">            <span class=\"keyword\">print</span> <span class=\"string\">'==&gt;&gt;&gt; epoch: &#123;&#125;, batch index: &#123;&#125;, train loss: &#123;:.6f&#125;'</span>.format(epoch, batch_idx, loss.data[<span class=\"number\">0</span>])</div><div class=\"line\">    <span class=\"comment\"># testing</span></div><div class=\"line\">    correct_cnt, ave_loss = <span class=\"number\">0</span>, <span class=\"number\">0</span></div><div class=\"line\">    <span class=\"keyword\">for</span> batch_idx, (x, target) <span class=\"keyword\">in</span> enumerate(test_loader):</div><div class=\"line\">        x, target = Variable(x.cuda(), volatile=<span class=\"keyword\">True</span>), Variable(target.cuda(), volatile=<span class=\"keyword\">True</span>)</div><div class=\"line\">        score, loss = model(x, target)</div><div class=\"line\">        _, pred_label = torch.max(score.data, <span class=\"number\">1</span>)</div><div class=\"line\">        correct_cnt += (pred_label == target.data).sum()</div><div class=\"line\">        ave_loss += loss.data[<span class=\"number\">0</span>]</div><div class=\"line\">    accuracy = correct_cnt*<span class=\"number\">1.0</span>/len(test_loader)/batch_size</div><div class=\"line\">    ave_loss /= len(test_loader)</div></pre></td></tr></table></figure>\n<p>当优化完毕后，需要保存模型。这里<a href=\"http://pytorch.org/docs/notes/serialization.html#recommend-saving-models\" target=\"_blank\" rel=\"external\">官方文档</a>给出了推荐的方法，如下所示：<br><figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\">torch.save(model.state_dict(), PATH)   <span class=\"comment\">#保存网络参数</span></div><div class=\"line\">the_model = TheModelClass(*args, **kwargs)</div><div class=\"line\">the_model.load_state_dict(torch.load(PATH))  <span class=\"comment\">#读取网络参数</span></div></pre></td></tr></table></figure></p>\n<p>该博客的完整代码可以见：<a href=\"https://gist.github.com/xmfbit/b27cdbff68870418bdb8cefa86a2d558\" target=\"_blank\" rel=\"external\">PyTorch MNIST demo</a>。</p>\n","excerpt":"<p>本篇文章介绍了使用PyTorch在MNIST数据集上训练MLP和CNN，并记录自己实现过程中的若干问题。<br><img src=\"/img/mnist_example.png\" alt=\"MNIST\"><br>","more":"</p>\n<h2 id=\"加载MNIST数据集\"><a href=\"#加载MNIST数据集\" class=\"headerlink\" title=\"加载MNIST数据集\"></a>加载MNIST数据集</h2><p>PyTorch中提供了MNIST，CIFAR，COCO等常用数据集的加载方法。<code>MNIST</code>是<code>torchvision.datasets</code>包中的一个类，负责根据传入的参数加载数据集。如果自己之前没有下载过该数据集，可以将<code>download</code>参数设置为<code>True</code>，会自动下载数据集并解包。如果之前已经下载好了，只需将其路径通过<code>root</code>传入即可。</p>\n<p>在加载图像后，我们常常需要对图像进行若干预处理。比如减去RGB通道的均值，或者裁剪或翻转图像实现augmentation等，这些操作可以在<code>torchvision.transforms</code>包中找到对应的操作。在下面的代码中，通过使用<code>transforms.Compose()</code>，我们构造了对数据进行预处理的复合操作序列，<code>ToTensor</code>负责将PIL图像转换为Tensor数据（RGB通道从<code>[0, 255]</code>范围变为<code>[0, 1]</code>）， <code>Normalize</code>负责对图像进行规范化。这里需要注意，虽然MNIST中图像都是灰度图像，通道数均为1，但是仍要传入<code>tuple</code>。</p>\n<p>之后，我们通过<code>DataLoader</code>返回一个数据集上的可迭代对象。一会我们通过<code>for</code>循环，就可以遍历数据集了。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">import</span> torch</div><div class=\"line\"><span class=\"keyword\">import</span> torch.nn <span class=\"keyword\">as</span> nn</div><div class=\"line\"><span class=\"keyword\">from</span> torch.autograd <span class=\"keyword\">import</span> Variable</div><div class=\"line\"><span class=\"keyword\">import</span> torchvision.datasets <span class=\"keyword\">as</span> dset</div><div class=\"line\"><span class=\"keyword\">import</span> torchvision.transforms <span class=\"keyword\">as</span> transforms</div><div class=\"line\"><span class=\"keyword\">import</span> torch.nn.functional <span class=\"keyword\">as</span> F</div><div class=\"line\"><span class=\"keyword\">import</span> torch.optim <span class=\"keyword\">as</span> optim</div><div class=\"line\"></div><div class=\"line\">trans = transforms.Compose([transforms.ToTensor(), transforms.Normalize((<span class=\"number\">0.5</span>,), (<span class=\"number\">1.0</span>,))])</div><div class=\"line\"></div><div class=\"line\">train_set = dset.MNIST(root=root, train=<span class=\"keyword\">True</span>, transform=trans, download=download)</div><div class=\"line\">test_set = dset.MNIST(root=root, train=<span class=\"keyword\">False</span>, transform=trans)</div><div class=\"line\"></div><div class=\"line\">batch_size = <span class=\"number\">128</span></div><div class=\"line\">kwargs = &#123;<span class=\"string\">'num_workers'</span>: <span class=\"number\">1</span>, <span class=\"string\">'pin_memory'</span>: <span class=\"keyword\">True</span>&#125;</div><div class=\"line\">train_loader = torch.utils.data.DataLoader(</div><div class=\"line\">                 dataset=train_set,</div><div class=\"line\">                 batch_size=batch_size,</div><div class=\"line\">                 shuffle=<span class=\"keyword\">True</span>, **kwargs)</div><div class=\"line\">test_loader = torch.utils.data.DataLoader(</div><div class=\"line\">                dataset=test_set,</div><div class=\"line\">                batch_size=batch_size,</div><div class=\"line\">                shuffle=<span class=\"keyword\">False</span>, **kwargs)</div></pre></td></tr></table></figure>\n<h2 id=\"网络构建\"><a href=\"#网络构建\" class=\"headerlink\" title=\"网络构建\"></a>网络构建</h2><p>在进行网络构建时，主要通过<code>torch.nn</code>包中的已经实现好的卷积层、池化层等进行搭建。例如下面的代码展示了一个具有一个隐含层的MLP网络。<code>nn.Linear</code>负责构建全连接层，需要提供输入和输出的通道数，也就是<code>y = wx+b</code>中<code>x</code>和<code>y</code>的维度。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">MLPNet</span><span class=\"params\">(nn.Module)</span>:</span></div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self)</span>:</span></div><div class=\"line\">        super(MLPNet, self).__init__()</div><div class=\"line\">        self.fc1 = nn.Linear(<span class=\"number\">28</span>*<span class=\"number\">28</span>, <span class=\"number\">500</span>)</div><div class=\"line\">        self.fc2 = nn.Linear(<span class=\"number\">500</span>, <span class=\"number\">256</span>)</div><div class=\"line\">        self.fc3 = nn.Linear(<span class=\"number\">256</span>, <span class=\"number\">10</span>)</div><div class=\"line\">        self.ceriation = nn.CrossEntropyLoss()</div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">forward</span><span class=\"params\">(self, x, target)</span>:</span></div><div class=\"line\">        x = x.view(<span class=\"number\">-1</span>, <span class=\"number\">28</span>*<span class=\"number\">28</span>)</div><div class=\"line\">        x = F.relu(self.fc1(x))</div><div class=\"line\">        x = F.relu(self.fc2(x))</div><div class=\"line\">        x = F.relu(self.fc3(x))</div><div class=\"line\">        loss = self.ceriation(x, target)</div><div class=\"line\">        <span class=\"keyword\">return</span> x, loss</div></pre></td></tr></table></figure>\n<p>由于PyTorch可以实现自动求导，所以我们只需实现<code>forward</code>过程即可。这里由于池化层和非线性变换都没有参数，所以使用了<code>nn.functionals</code>中的对应操作实现。通过看文档，可以发现，一般<code>nn</code>里面的各种层，都会在<code>nn.functionals</code>里面有其对应。例如卷积层的对应实现，如下所示，需要传入卷积核的权重。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\"># With square kernels and equal stride</span></div><div class=\"line\">filters = autograd.Variable(torch.randn(<span class=\"number\">8</span>,<span class=\"number\">4</span>,<span class=\"number\">3</span>,<span class=\"number\">3</span>))</div><div class=\"line\">inputs = autograd.Variable(torch.randn(<span class=\"number\">1</span>,<span class=\"number\">4</span>,<span class=\"number\">5</span>,<span class=\"number\">5</span>))</div><div class=\"line\">F.conv2d(inputs, filters, padding=<span class=\"number\">1</span>)</div></pre></td></tr></table></figure>\n<p>同样地，我们可以实现LeNet的结构如下。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">MLPNet</span><span class=\"params\">(nn.Module)</span>:</span></div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self)</span>:</span></div><div class=\"line\">        super(MLPNet, self).__init__()</div><div class=\"line\">        self.fc1 = nn.Linear(<span class=\"number\">28</span>*<span class=\"number\">28</span>, <span class=\"number\">500</span>)</div><div class=\"line\">        self.fc2 = nn.Linear(<span class=\"number\">500</span>, <span class=\"number\">256</span>)</div><div class=\"line\">        self.fc3 = nn.Linear(<span class=\"number\">256</span>, <span class=\"number\">10</span>)</div><div class=\"line\">        self.ceriation = nn.CrossEntropyLoss()</div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">forward</span><span class=\"params\">(self, x, target)</span>:</span></div><div class=\"line\">        x = x.view(<span class=\"number\">-1</span>, <span class=\"number\">28</span>*<span class=\"number\">28</span>)</div><div class=\"line\">        x = F.relu(self.fc1(x))</div><div class=\"line\">        x = F.relu(self.fc2(x))</div><div class=\"line\">        x = F.relu(self.fc3(x))</div><div class=\"line\">        loss = self.ceriation(x, target)</div><div class=\"line\">        <span class=\"keyword\">return</span> x, loss</div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">name</span><span class=\"params\">(self)</span>:</span></div><div class=\"line\">        <span class=\"keyword\">return</span> <span class=\"string\">'mlpnet'</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">LeNet</span><span class=\"params\">(nn.Module)</span>:</span></div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self)</span>:</span></div><div class=\"line\">        super(LeNet, self).__init__()</div><div class=\"line\">        self.conv1 = nn.Conv2d(<span class=\"number\">1</span>, <span class=\"number\">20</span>, <span class=\"number\">5</span>, <span class=\"number\">1</span>)</div><div class=\"line\">        self.conv2 = nn.Conv2d(<span class=\"number\">20</span>, <span class=\"number\">50</span>, <span class=\"number\">5</span>, <span class=\"number\">1</span>)</div><div class=\"line\">        self.fc1 = nn.Linear(<span class=\"number\">4</span>*<span class=\"number\">4</span>*<span class=\"number\">50</span>, <span class=\"number\">500</span>)</div><div class=\"line\">        self.fc2 = nn.Linear(<span class=\"number\">500</span>, <span class=\"number\">10</span>)</div><div class=\"line\">        self.ceriation = nn.CrossEntropyLoss()</div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">forward</span><span class=\"params\">(self, x, target)</span>:</span></div><div class=\"line\">        x = self.conv1(x)</div><div class=\"line\">        x = F.max_pool2d(x, <span class=\"number\">2</span>, <span class=\"number\">2</span>)</div><div class=\"line\">        x = F.relu(x)</div><div class=\"line\">        x = self.conv2(x)</div><div class=\"line\">        x = F.max_pool2d(x, <span class=\"number\">2</span>, <span class=\"number\">2</span>)</div><div class=\"line\">        x = F.relu(x)</div><div class=\"line\">        x = x.view(<span class=\"number\">-1</span>, <span class=\"number\">4</span>*<span class=\"number\">4</span>*<span class=\"number\">50</span>)</div><div class=\"line\">        x = self.fc1(x)</div><div class=\"line\">        x = self.fc2(x)</div><div class=\"line\">        loss = self.ceriation(x, target)</div><div class=\"line\">        <span class=\"keyword\">return</span> x, loss</div></pre></td></tr></table></figure>\n<h2 id=\"训练与测试\"><a href=\"#训练与测试\" class=\"headerlink\" title=\"训练与测试\"></a>训练与测试</h2><p>在训练时，我们首先应确定优化方法。这里我们使用带动量的<code>SGD</code>方法。下面代码中的<code>optim.SGD</code>初始化需要接受网络中待优化的<code>Parameter</code>列表（或是迭代器），以及学习率<code>lr</code>，动量<code>momentum</code>。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">optimizer = optim.SGD(model.parameters(), lr=<span class=\"number\">0.01</span>, momentum=<span class=\"number\">0.9</span>)</div></pre></td></tr></table></figure>\n<p>接下来，我们只需要遍历数据集，同时在每次迭代中清空待优化参数的梯度，前向计算，反向传播以及优化器的迭代求解即可。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div></pre></td><td class=\"code\"><pre><div class=\"line\">model = MLPNet().cuda()   <span class=\"comment\"># 以MLP为例</span></div><div class=\"line\"><span class=\"keyword\">for</span> epoch <span class=\"keyword\">in</span> xrange(<span class=\"number\">10</span>):</div><div class=\"line\">    <span class=\"comment\"># trainning</span></div><div class=\"line\">    <span class=\"keyword\">for</span> batch_idx, (x, target) <span class=\"keyword\">in</span> enumerate(train_loader):</div><div class=\"line\">        optimizer.zero_grad()     <span class=\"comment\">#每次都要清空上一步中参数的grad，否则会出错的~</span></div><div class=\"line\">        x, target = Variable(x.cuda()), Variable(target.cuda())</div><div class=\"line\">        _, loss = model(x, target)   <span class=\"comment\">#得到loss</span></div><div class=\"line\">        loss.backward()              <span class=\"comment\">#bp</span></div><div class=\"line\">        optimizer.step()             <span class=\"comment\">#优化器迭代</span></div><div class=\"line\">        <span class=\"keyword\">if</span> batch_idx % <span class=\"number\">100</span> == <span class=\"number\">0</span>:</div><div class=\"line\">            <span class=\"keyword\">print</span> <span class=\"string\">'==&gt;&gt;&gt; epoch: &#123;&#125;, batch index: &#123;&#125;, train loss: &#123;:.6f&#125;'</span>.format(epoch, batch_idx, loss.data[<span class=\"number\">0</span>])</div><div class=\"line\">    <span class=\"comment\"># testing</span></div><div class=\"line\">    correct_cnt, ave_loss = <span class=\"number\">0</span>, <span class=\"number\">0</span></div><div class=\"line\">    <span class=\"keyword\">for</span> batch_idx, (x, target) <span class=\"keyword\">in</span> enumerate(test_loader):</div><div class=\"line\">        x, target = Variable(x.cuda(), volatile=<span class=\"keyword\">True</span>), Variable(target.cuda(), volatile=<span class=\"keyword\">True</span>)</div><div class=\"line\">        score, loss = model(x, target)</div><div class=\"line\">        _, pred_label = torch.max(score.data, <span class=\"number\">1</span>)</div><div class=\"line\">        correct_cnt += (pred_label == target.data).sum()</div><div class=\"line\">        ave_loss += loss.data[<span class=\"number\">0</span>]</div><div class=\"line\">    accuracy = correct_cnt*<span class=\"number\">1.0</span>/len(test_loader)/batch_size</div><div class=\"line\">    ave_loss /= len(test_loader)</div></pre></td></tr></table></figure>\n<p>当优化完毕后，需要保存模型。这里<a href=\"http://pytorch.org/docs/notes/serialization.html#recommend-saving-models\">官方文档</a>给出了推荐的方法，如下所示：<br><figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\">torch.save(model.state_dict(), PATH)   <span class=\"comment\">#保存网络参数</span></div><div class=\"line\">the_model = TheModelClass(*args, **kwargs)</div><div class=\"line\">the_model.load_state_dict(torch.load(PATH))  <span class=\"comment\">#读取网络参数</span></div></pre></td></tr></table></figure></p>\n<p>该博客的完整代码可以见：<a href=\"https://gist.github.com/xmfbit/b27cdbff68870418bdb8cefa86a2d558\">PyTorch MNIST demo</a>。</p>"},{"title":"Python Regular Expressions （Python 正则表达式)","date":"2014-07-17T11:00:00.000Z","_content":"\n本文来自于Google Developers中对于Python的介绍。[https://developers.google.com/edu/python/regular-expressions](https://developers.google.com/edu/python/regular-expressions \"Google Python Class, Regular Expression\")。\n\n![regex](/img/regex_picture.jpg)\n<!-- more -->\n\n## 认识正则表达式 ##\n\nPython的正则表达式是使用 **re 模块**的。\n\n\n``` py    \n    match = re.search(pattern,str)\n    if match:\n    \tprint 'found',match.group()\n    else:\n        print 'NOT Found!'\n\n```\n\n## 正则表达式的规则 ##\n\n### 基本规则 ###\n- a, x, 9 都是普通字符 (ordinary characters)\n- . (一个点)可以匹配任何单个字符（除了'\\n'）\n- \\w（小写的w）可以匹配一个单词里面的字母，数字或下划线 [a-zA-Z0-9_];\\W （大写的W）可以匹配非单词里的这些元素\n- \\b 匹配单词与非单词的分界\n- \\s（小写的s）匹配一个 whitespace character，包括 space，newline，return，tab，form(\\n\\r\\t\\f)；\\S（大写的S）匹配一个非 whitespace character\n- \\d 匹配十进制数字 [0-9]\n- ^=start，$=end 用来匹配字符串的开始和结束\n- \\ 是转义字符，用 \\. 来匹配串里的'.'，等\n### 一些基本的例子 ###\n\n``` py\n    ## 在字符串'piiig'中查找'iii'\n    match = re.search(r'iii', 'piiig')  # found, match.group() == \"iii\"\n    match = re.search(r'igs', 'piiig')  #  not found, match == None\n\n    ## . 匹配除了\\n的任意字符\n    match = re.search(r'..g', 'piiig')  #  found, match.group() == \"iig\"\n\n    ## \\d 匹配0-9的数字字符, \\w 匹配单词里的字符\n    match = re.search(r'\\d\\d\\d', 'p123g') #  found, match.group() == \"123\"\n    match = re.search(r'\\w\\w\\w', '@@abcd!!') #  found, match.group() == \"abc\"   \n```\n\n### 重复 ###\n可以用'+' '*' '?'来匹配0个，1个或多个重复字符。\n\n- '+' 用来匹配1个或者多个字符\n- '*' 用来匹配0个或者多个字符\n- '?' 用来匹配0个或1个字符\n\n注意，'+'和'*'会匹配尽可能多的字符。\n\n### 一些重复字符的例子 ###\n\n``` py\n    ## i+  匹配1个或者多个'i'\n    match = re.search(r'pi+', 'piiig') #  found, match.group() == \"piii\"\n\n    ## 找到字符串中最左边尽可能长的模式。\n    ## 注意，并没有匹配到第二个 'i+'\n    match = re.search(r'i+', 'piigiiii')  #  found, match.group() == \"ii\"\n\n    ## \\s*  匹配0个或1个空白字符 whitespace\n    match = re.search(r'\\d\\s*\\d\\s*\\d', 'xx1 2   3xx')  #  found, match.group() == \"1 2   3\"\n    match = re.search(r'\\d\\s*\\d\\s*\\d', 'xx12  3xx')    #  found, match.group() == \"12  3\"\n    match = re.search(r'\\d\\s*\\d\\s*\\d', 'xx123xx')      # found, match.group() == \"123\"\n\n    ## ^ 匹配字符串的第一个字符\n    match = re.search(r'^b\\w+', 'foobar')  # not found, match == None\n    ## 与上例对比\n    match = re.search(r'b\\w+', 'foobar')   # found, match.group() == \"bar\"\n```\n\n### Email ###\n考虑一个典型的Email地址：someone@host.com，可以用如下的方式匹配：\n\n``` py\n    match = re.search(r'\\w+@\\w+',str)\n```    \n\n但是，对于这种Email地址 'xyz alice-b@google.com purple monkey' 则不能奏效。\n\n### 使用方括号 ###\n方括号里面的字符表示一个字符集合。[abc]可以被用来匹配'a'或者'b'或者'c'。\\w \\s等都可以用在方括号里，除了'.'以外，它只能用来表示字面意义上的‘点’。所以上面的Email规则可以扩充如下：\n\n``` py\n    match = re.search('r[\\w.-]+@[\\w.-]+',str)\n```\n\n你还可以使用'-'来指定范围，如[a-z]指示的是所有小写字母的集合。所以如果你想构造的字符集合中有'-'，请把它放到末尾[ab-]。另外，前方加上'^'，用来表示取集合的补集，例如[^ab]表示除了'a'和'b'之外的其他字符。\n\n## 操作 ##\n以Email地址为例，如果我们想要分别提取该地址的用户名'someone'和主机名'host.com'该怎么办呢？\n可以在模式中用圆括号指定。\n\n``` py\n    str = 'purple alice-b@google.com monkey dishwasher'\n    match = re.search('([\\w.-]+)@([\\w.-]+)', str)   #用圆括号指定分割\n    if match:\n        print match.group()   ## 'alice-b@google.com' (the whole match)\n        print match.group(1)  ## 'alice-b' (the username, group 1)\n      \tprint match.group(2)  ## 'google.com' (the host, group 2)\n```\n\n### findall 函数\n与group函数只找到最左端的一个匹配不同，findall函数找到字符串中所有与模式匹配的串。\n\n``` py\n    str = 'purple alice@google.com, blah monkey bob@abc.com blah dishwasher'\n    ## findall返回一个包含所有匹配结果的 list\n    emails = re.findall(r'[\\w\\.-]+@[\\w\\.-]+', str) ## ['alice@google.com', 'bob@abc.com']\n    for email in emails:\n        print email\n```\n\n### 在文件中使用findall\n当然可以读入文件的每一行，然后对每一行的内容调用findall，但是为什么不让这一切自动发生呢？\n\n``` py\n\tf = open(filename.txt,'r')\n\tmatches = re.findall(pattern,f.read())\n```\n\n### findall 和分组\n和group的用法相似，也可以指定分组。\n\n``` py\n    str = 'purple alice@google.com, blah monkey bob@abc.com blah dishwasher'\n    ##　返回了一个list\n    tuples = re.findall(r'([\\w\\.-]+)@([\\w\\.-]+)', str)\n    print tuples  ## [('alice', 'google.com'), ('bob', 'abc.com')]\n    ##　list中的元素是tuple\n    for tuple in tuples:\n      print tuple[0]  ## username\n      print tuple[1]  ## host\n```\n\n## 调试 ##\n\n正则表达式异常强大，使用简单的几条规则就可以演变出很多的模式组合。在确定你的模式之前，可能需要很多的调试工作。在一个小的测试集合上测试正则表达式。\n\n## 其他选项\n\n正则表达式还可以设置“选项”。\n\n``` py\n    match = re.search(pat,str,opt)\n```\n\n这些可选项如下：\n\n- IGNORECASE  忽视大小写\n- DOTALL  允许'.'匹配'\\n'\n- MULTILINE  在一个由许多行组成的字符串中，允许'^'和'$'匹配每一行的开始和结束\n","source":"_posts/python-reg-exp.md","raw":"---\ntitle: Python Regular Expressions （Python 正则表达式)\ndate: 2014-07-17 19:00:00\ntags:\n    - python\n---\n\n本文来自于Google Developers中对于Python的介绍。[https://developers.google.com/edu/python/regular-expressions](https://developers.google.com/edu/python/regular-expressions \"Google Python Class, Regular Expression\")。\n\n![regex](/img/regex_picture.jpg)\n<!-- more -->\n\n## 认识正则表达式 ##\n\nPython的正则表达式是使用 **re 模块**的。\n\n\n``` py    \n    match = re.search(pattern,str)\n    if match:\n    \tprint 'found',match.group()\n    else:\n        print 'NOT Found!'\n\n```\n\n## 正则表达式的规则 ##\n\n### 基本规则 ###\n- a, x, 9 都是普通字符 (ordinary characters)\n- . (一个点)可以匹配任何单个字符（除了'\\n'）\n- \\w（小写的w）可以匹配一个单词里面的字母，数字或下划线 [a-zA-Z0-9_];\\W （大写的W）可以匹配非单词里的这些元素\n- \\b 匹配单词与非单词的分界\n- \\s（小写的s）匹配一个 whitespace character，包括 space，newline，return，tab，form(\\n\\r\\t\\f)；\\S（大写的S）匹配一个非 whitespace character\n- \\d 匹配十进制数字 [0-9]\n- ^=start，$=end 用来匹配字符串的开始和结束\n- \\ 是转义字符，用 \\. 来匹配串里的'.'，等\n### 一些基本的例子 ###\n\n``` py\n    ## 在字符串'piiig'中查找'iii'\n    match = re.search(r'iii', 'piiig')  # found, match.group() == \"iii\"\n    match = re.search(r'igs', 'piiig')  #  not found, match == None\n\n    ## . 匹配除了\\n的任意字符\n    match = re.search(r'..g', 'piiig')  #  found, match.group() == \"iig\"\n\n    ## \\d 匹配0-9的数字字符, \\w 匹配单词里的字符\n    match = re.search(r'\\d\\d\\d', 'p123g') #  found, match.group() == \"123\"\n    match = re.search(r'\\w\\w\\w', '@@abcd!!') #  found, match.group() == \"abc\"   \n```\n\n### 重复 ###\n可以用'+' '*' '?'来匹配0个，1个或多个重复字符。\n\n- '+' 用来匹配1个或者多个字符\n- '*' 用来匹配0个或者多个字符\n- '?' 用来匹配0个或1个字符\n\n注意，'+'和'*'会匹配尽可能多的字符。\n\n### 一些重复字符的例子 ###\n\n``` py\n    ## i+  匹配1个或者多个'i'\n    match = re.search(r'pi+', 'piiig') #  found, match.group() == \"piii\"\n\n    ## 找到字符串中最左边尽可能长的模式。\n    ## 注意，并没有匹配到第二个 'i+'\n    match = re.search(r'i+', 'piigiiii')  #  found, match.group() == \"ii\"\n\n    ## \\s*  匹配0个或1个空白字符 whitespace\n    match = re.search(r'\\d\\s*\\d\\s*\\d', 'xx1 2   3xx')  #  found, match.group() == \"1 2   3\"\n    match = re.search(r'\\d\\s*\\d\\s*\\d', 'xx12  3xx')    #  found, match.group() == \"12  3\"\n    match = re.search(r'\\d\\s*\\d\\s*\\d', 'xx123xx')      # found, match.group() == \"123\"\n\n    ## ^ 匹配字符串的第一个字符\n    match = re.search(r'^b\\w+', 'foobar')  # not found, match == None\n    ## 与上例对比\n    match = re.search(r'b\\w+', 'foobar')   # found, match.group() == \"bar\"\n```\n\n### Email ###\n考虑一个典型的Email地址：someone@host.com，可以用如下的方式匹配：\n\n``` py\n    match = re.search(r'\\w+@\\w+',str)\n```    \n\n但是，对于这种Email地址 'xyz alice-b@google.com purple monkey' 则不能奏效。\n\n### 使用方括号 ###\n方括号里面的字符表示一个字符集合。[abc]可以被用来匹配'a'或者'b'或者'c'。\\w \\s等都可以用在方括号里，除了'.'以外，它只能用来表示字面意义上的‘点’。所以上面的Email规则可以扩充如下：\n\n``` py\n    match = re.search('r[\\w.-]+@[\\w.-]+',str)\n```\n\n你还可以使用'-'来指定范围，如[a-z]指示的是所有小写字母的集合。所以如果你想构造的字符集合中有'-'，请把它放到末尾[ab-]。另外，前方加上'^'，用来表示取集合的补集，例如[^ab]表示除了'a'和'b'之外的其他字符。\n\n## 操作 ##\n以Email地址为例，如果我们想要分别提取该地址的用户名'someone'和主机名'host.com'该怎么办呢？\n可以在模式中用圆括号指定。\n\n``` py\n    str = 'purple alice-b@google.com monkey dishwasher'\n    match = re.search('([\\w.-]+)@([\\w.-]+)', str)   #用圆括号指定分割\n    if match:\n        print match.group()   ## 'alice-b@google.com' (the whole match)\n        print match.group(1)  ## 'alice-b' (the username, group 1)\n      \tprint match.group(2)  ## 'google.com' (the host, group 2)\n```\n\n### findall 函数\n与group函数只找到最左端的一个匹配不同，findall函数找到字符串中所有与模式匹配的串。\n\n``` py\n    str = 'purple alice@google.com, blah monkey bob@abc.com blah dishwasher'\n    ## findall返回一个包含所有匹配结果的 list\n    emails = re.findall(r'[\\w\\.-]+@[\\w\\.-]+', str) ## ['alice@google.com', 'bob@abc.com']\n    for email in emails:\n        print email\n```\n\n### 在文件中使用findall\n当然可以读入文件的每一行，然后对每一行的内容调用findall，但是为什么不让这一切自动发生呢？\n\n``` py\n\tf = open(filename.txt,'r')\n\tmatches = re.findall(pattern,f.read())\n```\n\n### findall 和分组\n和group的用法相似，也可以指定分组。\n\n``` py\n    str = 'purple alice@google.com, blah monkey bob@abc.com blah dishwasher'\n    ##　返回了一个list\n    tuples = re.findall(r'([\\w\\.-]+)@([\\w\\.-]+)', str)\n    print tuples  ## [('alice', 'google.com'), ('bob', 'abc.com')]\n    ##　list中的元素是tuple\n    for tuple in tuples:\n      print tuple[0]  ## username\n      print tuple[1]  ## host\n```\n\n## 调试 ##\n\n正则表达式异常强大，使用简单的几条规则就可以演变出很多的模式组合。在确定你的模式之前，可能需要很多的调试工作。在一个小的测试集合上测试正则表达式。\n\n## 其他选项\n\n正则表达式还可以设置“选项”。\n\n``` py\n    match = re.search(pat,str,opt)\n```\n\n这些可选项如下：\n\n- IGNORECASE  忽视大小写\n- DOTALL  允许'.'匹配'\\n'\n- MULTILINE  在一个由许多行组成的字符串中，允许'^'和'$'匹配每一行的开始和结束\n","slug":"python-reg-exp","published":1,"updated":"2017-02-26T12:09:28.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cizve07b6000sl61hd8o0r5lf","content":"<p>本文来自于Google Developers中对于Python的介绍。<a href=\"https://developers.google.com/edu/python/regular-expressions\" title=\"Google Python Class, Regular Expression\" target=\"_blank\" rel=\"external\">https://developers.google.com/edu/python/regular-expressions</a>。</p>\n<p><img src=\"/img/regex_picture.jpg\" alt=\"regex\"><br><a id=\"more\"></a></p>\n<h2 id=\"认识正则表达式\"><a href=\"#认识正则表达式\" class=\"headerlink\" title=\"认识正则表达式\"></a>认识正则表达式</h2><p>Python的正则表达式是使用 <strong>re 模块</strong>的。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\">match = re.search(pattern,str)</div><div class=\"line\"><span class=\"keyword\">if</span> match:</div><div class=\"line\">\t<span class=\"keyword\">print</span> <span class=\"string\">'found'</span>,match.group()</div><div class=\"line\"><span class=\"keyword\">else</span>:</div><div class=\"line\">    <span class=\"keyword\">print</span> <span class=\"string\">'NOT Found!'</span></div></pre></td></tr></table></figure>\n<h2 id=\"正则表达式的规则\"><a href=\"#正则表达式的规则\" class=\"headerlink\" title=\"正则表达式的规则\"></a>正则表达式的规则</h2><h3 id=\"基本规则\"><a href=\"#基本规则\" class=\"headerlink\" title=\"基本规则\"></a>基本规则</h3><ul>\n<li>a, x, 9 都是普通字符 (ordinary characters)</li>\n<li>. (一个点)可以匹配任何单个字符（除了’\\n’）</li>\n<li>\\w（小写的w）可以匹配一个单词里面的字母，数字或下划线 [a-zA-Z0-9_];\\W （大写的W）可以匹配非单词里的这些元素</li>\n<li>\\b 匹配单词与非单词的分界</li>\n<li>\\s（小写的s）匹配一个 whitespace character，包括 space，newline，return，tab，form(\\n\\r\\t\\f)；\\S（大写的S）匹配一个非 whitespace character</li>\n<li>\\d 匹配十进制数字 [0-9]</li>\n<li>^=start，$=end 用来匹配字符串的开始和结束</li>\n<li>\\ 是转义字符，用 . 来匹配串里的’.’，等<h3 id=\"一些基本的例子\"><a href=\"#一些基本的例子\" class=\"headerlink\" title=\"一些基本的例子\"></a>一些基本的例子</h3></li>\n</ul>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\">## 在字符串'piiig'中查找'iii'</span></div><div class=\"line\">match = re.search(<span class=\"string\">r'iii'</span>, <span class=\"string\">'piiig'</span>)  <span class=\"comment\"># found, match.group() == \"iii\"</span></div><div class=\"line\">match = re.search(<span class=\"string\">r'igs'</span>, <span class=\"string\">'piiig'</span>)  <span class=\"comment\">#  not found, match == None</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">## . 匹配除了\\n的任意字符</span></div><div class=\"line\">match = re.search(<span class=\"string\">r'..g'</span>, <span class=\"string\">'piiig'</span>)  <span class=\"comment\">#  found, match.group() == \"iig\"</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">## \\d 匹配0-9的数字字符, \\w 匹配单词里的字符</span></div><div class=\"line\">match = re.search(<span class=\"string\">r'\\d\\d\\d'</span>, <span class=\"string\">'p123g'</span>) <span class=\"comment\">#  found, match.group() == \"123\"</span></div><div class=\"line\">match = re.search(<span class=\"string\">r'\\w\\w\\w'</span>, <span class=\"string\">'@@abcd!!'</span>) <span class=\"comment\">#  found, match.group() == \"abc\"</span></div></pre></td></tr></table></figure>\n<h3 id=\"重复\"><a href=\"#重复\" class=\"headerlink\" title=\"重复\"></a>重复</h3><p>可以用’+’ ‘*’ ‘?’来匹配0个，1个或多个重复字符。</p>\n<ul>\n<li>‘+’ 用来匹配1个或者多个字符</li>\n<li>‘*’ 用来匹配0个或者多个字符</li>\n<li>‘?’ 用来匹配0个或1个字符</li>\n</ul>\n<p>注意，’+’和’*’会匹配尽可能多的字符。</p>\n<h3 id=\"一些重复字符的例子\"><a href=\"#一些重复字符的例子\" class=\"headerlink\" title=\"一些重复字符的例子\"></a>一些重复字符的例子</h3><figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\">## i+  匹配1个或者多个'i'</span></div><div class=\"line\">match = re.search(<span class=\"string\">r'pi+'</span>, <span class=\"string\">'piiig'</span>) <span class=\"comment\">#  found, match.group() == \"piii\"</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">## 找到字符串中最左边尽可能长的模式。</span></div><div class=\"line\"><span class=\"comment\">## 注意，并没有匹配到第二个 'i+'</span></div><div class=\"line\">match = re.search(<span class=\"string\">r'i+'</span>, <span class=\"string\">'piigiiii'</span>)  <span class=\"comment\">#  found, match.group() == \"ii\"</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">## \\s*  匹配0个或1个空白字符 whitespace</span></div><div class=\"line\">match = re.search(<span class=\"string\">r'\\d\\s*\\d\\s*\\d'</span>, <span class=\"string\">'xx1 2   3xx'</span>)  <span class=\"comment\">#  found, match.group() == \"1 2   3\"</span></div><div class=\"line\">match = re.search(<span class=\"string\">r'\\d\\s*\\d\\s*\\d'</span>, <span class=\"string\">'xx12  3xx'</span>)    <span class=\"comment\">#  found, match.group() == \"12  3\"</span></div><div class=\"line\">match = re.search(<span class=\"string\">r'\\d\\s*\\d\\s*\\d'</span>, <span class=\"string\">'xx123xx'</span>)      <span class=\"comment\"># found, match.group() == \"123\"</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">## ^ 匹配字符串的第一个字符</span></div><div class=\"line\">match = re.search(<span class=\"string\">r'^b\\w+'</span>, <span class=\"string\">'foobar'</span>)  <span class=\"comment\"># not found, match == None</span></div><div class=\"line\"><span class=\"comment\">## 与上例对比</span></div><div class=\"line\">match = re.search(<span class=\"string\">r'b\\w+'</span>, <span class=\"string\">'foobar'</span>)   <span class=\"comment\"># found, match.group() == \"bar\"</span></div></pre></td></tr></table></figure>\n<h3 id=\"Email\"><a href=\"#Email\" class=\"headerlink\" title=\"Email\"></a>Email</h3><p>考虑一个典型的Email地址：someone@host.com，可以用如下的方式匹配：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div></pre></td><td class=\"code\"><pre><div class=\"line\">    match = re.search(<span class=\"string\">r'\\w+@\\w+'</span>,str)</div><div class=\"line\">```    </div><div class=\"line\"></div><div class=\"line\">但是，对于这种Email地址 <span class=\"string\">'xyz alice-b@google.com purple monkey'</span> 则不能奏效。</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">### 使用方括号 ###</span></div><div class=\"line\">方括号里面的字符表示一个字符集合。[abc]可以被用来匹配<span class=\"string\">'a'</span>或者<span class=\"string\">'b'</span>或者<span class=\"string\">'c'</span>。\\w \\s等都可以用在方括号里，除了<span class=\"string\">'.'</span>以外，它只能用来表示字面意义上的‘点’。所以上面的Email规则可以扩充如下：</div><div class=\"line\"></div><div class=\"line\">``` py</div><div class=\"line\">    match = re.search(<span class=\"string\">'r[\\w.-]+@[\\w.-]+'</span>,str)</div></pre></td></tr></table></figure>\n<p>你还可以使用’-‘来指定范围，如[a-z]指示的是所有小写字母的集合。所以如果你想构造的字符集合中有’-‘，请把它放到末尾[ab-]。另外，前方加上’^’，用来表示取集合的补集，例如<sup><a href=\"#fn_ab\" id=\"reffn_ab\">ab</a></sup>表示除了’a’和’b’之外的其他字符。</p>\n<h2 id=\"操作\"><a href=\"#操作\" class=\"headerlink\" title=\"操作\"></a>操作</h2><p>以Email地址为例，如果我们想要分别提取该地址的用户名’someone’和主机名’host.com’该怎么办呢？<br>可以在模式中用圆括号指定。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div></pre></td><td class=\"code\"><pre><div class=\"line\">str = <span class=\"string\">'purple alice-b@google.com monkey dishwasher'</span></div><div class=\"line\">match = re.search(<span class=\"string\">'([\\w.-]+)@([\\w.-]+)'</span>, str)   <span class=\"comment\">#用圆括号指定分割</span></div><div class=\"line\"><span class=\"keyword\">if</span> match:</div><div class=\"line\">    <span class=\"keyword\">print</span> match.group()   <span class=\"comment\">## 'alice-b@google.com' (the whole match)</span></div><div class=\"line\">    <span class=\"keyword\">print</span> match.group(<span class=\"number\">1</span>)  <span class=\"comment\">## 'alice-b' (the username, group 1)</span></div><div class=\"line\">  \t<span class=\"keyword\">print</span> match.group(<span class=\"number\">2</span>)  <span class=\"comment\">## 'google.com' (the host, group 2)</span></div></pre></td></tr></table></figure>\n<h3 id=\"findall-函数\"><a href=\"#findall-函数\" class=\"headerlink\" title=\"findall 函数\"></a>findall 函数</h3><p>与group函数只找到最左端的一个匹配不同，findall函数找到字符串中所有与模式匹配的串。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\">str = <span class=\"string\">'purple alice@google.com, blah monkey bob@abc.com blah dishwasher'</span></div><div class=\"line\"><span class=\"comment\">## findall返回一个包含所有匹配结果的 list</span></div><div class=\"line\">emails = re.findall(<span class=\"string\">r'[\\w\\.-]+@[\\w\\.-]+'</span>, str) <span class=\"comment\">## ['alice@google.com', 'bob@abc.com']</span></div><div class=\"line\"><span class=\"keyword\">for</span> email <span class=\"keyword\">in</span> emails:</div><div class=\"line\">    <span class=\"keyword\">print</span> email</div></pre></td></tr></table></figure>\n<h3 id=\"在文件中使用findall\"><a href=\"#在文件中使用findall\" class=\"headerlink\" title=\"在文件中使用findall\"></a>在文件中使用findall</h3><p>当然可以读入文件的每一行，然后对每一行的内容调用findall，但是为什么不让这一切自动发生呢？</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">f = open(filename.txt,<span class=\"string\">'r'</span>)</div><div class=\"line\">matches = re.findall(pattern,f.read())</div></pre></td></tr></table></figure>\n<h3 id=\"findall-和分组\"><a href=\"#findall-和分组\" class=\"headerlink\" title=\"findall 和分组\"></a>findall 和分组</h3><p>和group的用法相似，也可以指定分组。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div></pre></td><td class=\"code\"><pre><div class=\"line\">str = <span class=\"string\">'purple alice@google.com, blah monkey bob@abc.com blah dishwasher'</span></div><div class=\"line\"><span class=\"comment\">##　返回了一个list</span></div><div class=\"line\">tuples = re.findall(<span class=\"string\">r'([\\w\\.-]+)@([\\w\\.-]+)'</span>, str)</div><div class=\"line\"><span class=\"keyword\">print</span> tuples  <span class=\"comment\">## [('alice', 'google.com'), ('bob', 'abc.com')]</span></div><div class=\"line\"><span class=\"comment\">##　list中的元素是tuple</span></div><div class=\"line\"><span class=\"keyword\">for</span> tuple <span class=\"keyword\">in</span> tuples:</div><div class=\"line\">  <span class=\"keyword\">print</span> tuple[<span class=\"number\">0</span>]  <span class=\"comment\">## username</span></div><div class=\"line\">  <span class=\"keyword\">print</span> tuple[<span class=\"number\">1</span>]  <span class=\"comment\">## host</span></div></pre></td></tr></table></figure>\n<h2 id=\"调试\"><a href=\"#调试\" class=\"headerlink\" title=\"调试\"></a>调试</h2><p>正则表达式异常强大，使用简单的几条规则就可以演变出很多的模式组合。在确定你的模式之前，可能需要很多的调试工作。在一个小的测试集合上测试正则表达式。</p>\n<h2 id=\"其他选项\"><a href=\"#其他选项\" class=\"headerlink\" title=\"其他选项\"></a>其他选项</h2><p>正则表达式还可以设置“选项”。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">match = re.search(pat,str,opt)</div></pre></td></tr></table></figure>\n<p>这些可选项如下：</p>\n<ul>\n<li>IGNORECASE  忽视大小写</li>\n<li>DOTALL  允许’.’匹配’\\n’</li>\n<li>MULTILINE  在一个由许多行组成的字符串中，允许’^’和’$’匹配每一行的开始和结束</li>\n</ul>\n","excerpt":"<p>本文来自于Google Developers中对于Python的介绍。<a href=\"https://developers.google.com/edu/python/regular-expressions\" title=\"Google Python Class, Regular Expression\">https://developers.google.com/edu/python/regular-expressions</a>。</p>\n<p><img src=\"/img/regex_picture.jpg\" alt=\"regex\"><br>","more":"</p>\n<h2 id=\"认识正则表达式\"><a href=\"#认识正则表达式\" class=\"headerlink\" title=\"认识正则表达式\"></a>认识正则表达式</h2><p>Python的正则表达式是使用 <strong>re 模块</strong>的。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\">match = re.search(pattern,str)</div><div class=\"line\"><span class=\"keyword\">if</span> match:</div><div class=\"line\">\t<span class=\"keyword\">print</span> <span class=\"string\">'found'</span>,match.group()</div><div class=\"line\"><span class=\"keyword\">else</span>:</div><div class=\"line\">    <span class=\"keyword\">print</span> <span class=\"string\">'NOT Found!'</span></div></pre></td></tr></table></figure>\n<h2 id=\"正则表达式的规则\"><a href=\"#正则表达式的规则\" class=\"headerlink\" title=\"正则表达式的规则\"></a>正则表达式的规则</h2><h3 id=\"基本规则\"><a href=\"#基本规则\" class=\"headerlink\" title=\"基本规则\"></a>基本规则</h3><ul>\n<li>a, x, 9 都是普通字符 (ordinary characters)</li>\n<li>. (一个点)可以匹配任何单个字符（除了’\\n’）</li>\n<li>\\w（小写的w）可以匹配一个单词里面的字母，数字或下划线 [a-zA-Z0-9_];\\W （大写的W）可以匹配非单词里的这些元素</li>\n<li>\\b 匹配单词与非单词的分界</li>\n<li>\\s（小写的s）匹配一个 whitespace character，包括 space，newline，return，tab，form(\\n\\r\\t\\f)；\\S（大写的S）匹配一个非 whitespace character</li>\n<li>\\d 匹配十进制数字 [0-9]</li>\n<li>^=start，$=end 用来匹配字符串的开始和结束</li>\n<li>\\ 是转义字符，用 . 来匹配串里的’.’，等<h3 id=\"一些基本的例子\"><a href=\"#一些基本的例子\" class=\"headerlink\" title=\"一些基本的例子\"></a>一些基本的例子</h3></li>\n</ul>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\">## 在字符串'piiig'中查找'iii'</span></div><div class=\"line\">match = re.search(<span class=\"string\">r'iii'</span>, <span class=\"string\">'piiig'</span>)  <span class=\"comment\"># found, match.group() == \"iii\"</span></div><div class=\"line\">match = re.search(<span class=\"string\">r'igs'</span>, <span class=\"string\">'piiig'</span>)  <span class=\"comment\">#  not found, match == None</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">## . 匹配除了\\n的任意字符</span></div><div class=\"line\">match = re.search(<span class=\"string\">r'..g'</span>, <span class=\"string\">'piiig'</span>)  <span class=\"comment\">#  found, match.group() == \"iig\"</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">## \\d 匹配0-9的数字字符, \\w 匹配单词里的字符</span></div><div class=\"line\">match = re.search(<span class=\"string\">r'\\d\\d\\d'</span>, <span class=\"string\">'p123g'</span>) <span class=\"comment\">#  found, match.group() == \"123\"</span></div><div class=\"line\">match = re.search(<span class=\"string\">r'\\w\\w\\w'</span>, <span class=\"string\">'@@abcd!!'</span>) <span class=\"comment\">#  found, match.group() == \"abc\"</span></div></pre></td></tr></table></figure>\n<h3 id=\"重复\"><a href=\"#重复\" class=\"headerlink\" title=\"重复\"></a>重复</h3><p>可以用’+’ ‘*’ ‘?’来匹配0个，1个或多个重复字符。</p>\n<ul>\n<li>‘+’ 用来匹配1个或者多个字符</li>\n<li>‘*’ 用来匹配0个或者多个字符</li>\n<li>‘?’ 用来匹配0个或1个字符</li>\n</ul>\n<p>注意，’+’和’*’会匹配尽可能多的字符。</p>\n<h3 id=\"一些重复字符的例子\"><a href=\"#一些重复字符的例子\" class=\"headerlink\" title=\"一些重复字符的例子\"></a>一些重复字符的例子</h3><figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\">## i+  匹配1个或者多个'i'</span></div><div class=\"line\">match = re.search(<span class=\"string\">r'pi+'</span>, <span class=\"string\">'piiig'</span>) <span class=\"comment\">#  found, match.group() == \"piii\"</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">## 找到字符串中最左边尽可能长的模式。</span></div><div class=\"line\"><span class=\"comment\">## 注意，并没有匹配到第二个 'i+'</span></div><div class=\"line\">match = re.search(<span class=\"string\">r'i+'</span>, <span class=\"string\">'piigiiii'</span>)  <span class=\"comment\">#  found, match.group() == \"ii\"</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">## \\s*  匹配0个或1个空白字符 whitespace</span></div><div class=\"line\">match = re.search(<span class=\"string\">r'\\d\\s*\\d\\s*\\d'</span>, <span class=\"string\">'xx1 2   3xx'</span>)  <span class=\"comment\">#  found, match.group() == \"1 2   3\"</span></div><div class=\"line\">match = re.search(<span class=\"string\">r'\\d\\s*\\d\\s*\\d'</span>, <span class=\"string\">'xx12  3xx'</span>)    <span class=\"comment\">#  found, match.group() == \"12  3\"</span></div><div class=\"line\">match = re.search(<span class=\"string\">r'\\d\\s*\\d\\s*\\d'</span>, <span class=\"string\">'xx123xx'</span>)      <span class=\"comment\"># found, match.group() == \"123\"</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">## ^ 匹配字符串的第一个字符</span></div><div class=\"line\">match = re.search(<span class=\"string\">r'^b\\w+'</span>, <span class=\"string\">'foobar'</span>)  <span class=\"comment\"># not found, match == None</span></div><div class=\"line\"><span class=\"comment\">## 与上例对比</span></div><div class=\"line\">match = re.search(<span class=\"string\">r'b\\w+'</span>, <span class=\"string\">'foobar'</span>)   <span class=\"comment\"># found, match.group() == \"bar\"</span></div></pre></td></tr></table></figure>\n<h3 id=\"Email\"><a href=\"#Email\" class=\"headerlink\" title=\"Email\"></a>Email</h3><p>考虑一个典型的Email地址：someone@host.com，可以用如下的方式匹配：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div></pre></td><td class=\"code\"><pre><div class=\"line\">    match = re.search(<span class=\"string\">r'\\w+@\\w+'</span>,str)</div><div class=\"line\">```    </div><div class=\"line\"></div><div class=\"line\">但是，对于这种Email地址 <span class=\"string\">'xyz alice-b@google.com purple monkey'</span> 则不能奏效。</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">### 使用方括号 ###</span></div><div class=\"line\">方括号里面的字符表示一个字符集合。[abc]可以被用来匹配<span class=\"string\">'a'</span>或者<span class=\"string\">'b'</span>或者<span class=\"string\">'c'</span>。\\w \\s等都可以用在方括号里，除了<span class=\"string\">'.'</span>以外，它只能用来表示字面意义上的‘点’。所以上面的Email规则可以扩充如下：</div><div class=\"line\"></div><div class=\"line\">``` py</div><div class=\"line\">    match = re.search(<span class=\"string\">'r[\\w.-]+@[\\w.-]+'</span>,str)</div></pre></td></tr></table></figure>\n<p>你还可以使用’-‘来指定范围，如[a-z]指示的是所有小写字母的集合。所以如果你想构造的字符集合中有’-‘，请把它放到末尾[ab-]。另外，前方加上’^’，用来表示取集合的补集，例如<sup><a href=\"#fn_ab\" id=\"reffn_ab\">ab</a></sup>表示除了’a’和’b’之外的其他字符。</p>\n<h2 id=\"操作\"><a href=\"#操作\" class=\"headerlink\" title=\"操作\"></a>操作</h2><p>以Email地址为例，如果我们想要分别提取该地址的用户名’someone’和主机名’host.com’该怎么办呢？<br>可以在模式中用圆括号指定。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div></pre></td><td class=\"code\"><pre><div class=\"line\">str = <span class=\"string\">'purple alice-b@google.com monkey dishwasher'</span></div><div class=\"line\">match = re.search(<span class=\"string\">'([\\w.-]+)@([\\w.-]+)'</span>, str)   <span class=\"comment\">#用圆括号指定分割</span></div><div class=\"line\"><span class=\"keyword\">if</span> match:</div><div class=\"line\">    <span class=\"keyword\">print</span> match.group()   <span class=\"comment\">## 'alice-b@google.com' (the whole match)</span></div><div class=\"line\">    <span class=\"keyword\">print</span> match.group(<span class=\"number\">1</span>)  <span class=\"comment\">## 'alice-b' (the username, group 1)</span></div><div class=\"line\">  \t<span class=\"keyword\">print</span> match.group(<span class=\"number\">2</span>)  <span class=\"comment\">## 'google.com' (the host, group 2)</span></div></pre></td></tr></table></figure>\n<h3 id=\"findall-函数\"><a href=\"#findall-函数\" class=\"headerlink\" title=\"findall 函数\"></a>findall 函数</h3><p>与group函数只找到最左端的一个匹配不同，findall函数找到字符串中所有与模式匹配的串。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\">str = <span class=\"string\">'purple alice@google.com, blah monkey bob@abc.com blah dishwasher'</span></div><div class=\"line\"><span class=\"comment\">## findall返回一个包含所有匹配结果的 list</span></div><div class=\"line\">emails = re.findall(<span class=\"string\">r'[\\w\\.-]+@[\\w\\.-]+'</span>, str) <span class=\"comment\">## ['alice@google.com', 'bob@abc.com']</span></div><div class=\"line\"><span class=\"keyword\">for</span> email <span class=\"keyword\">in</span> emails:</div><div class=\"line\">    <span class=\"keyword\">print</span> email</div></pre></td></tr></table></figure>\n<h3 id=\"在文件中使用findall\"><a href=\"#在文件中使用findall\" class=\"headerlink\" title=\"在文件中使用findall\"></a>在文件中使用findall</h3><p>当然可以读入文件的每一行，然后对每一行的内容调用findall，但是为什么不让这一切自动发生呢？</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">f = open(filename.txt,<span class=\"string\">'r'</span>)</div><div class=\"line\">matches = re.findall(pattern,f.read())</div></pre></td></tr></table></figure>\n<h3 id=\"findall-和分组\"><a href=\"#findall-和分组\" class=\"headerlink\" title=\"findall 和分组\"></a>findall 和分组</h3><p>和group的用法相似，也可以指定分组。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div></pre></td><td class=\"code\"><pre><div class=\"line\">str = <span class=\"string\">'purple alice@google.com, blah monkey bob@abc.com blah dishwasher'</span></div><div class=\"line\"><span class=\"comment\">##　返回了一个list</span></div><div class=\"line\">tuples = re.findall(<span class=\"string\">r'([\\w\\.-]+)@([\\w\\.-]+)'</span>, str)</div><div class=\"line\"><span class=\"keyword\">print</span> tuples  <span class=\"comment\">## [('alice', 'google.com'), ('bob', 'abc.com')]</span></div><div class=\"line\"><span class=\"comment\">##　list中的元素是tuple</span></div><div class=\"line\"><span class=\"keyword\">for</span> tuple <span class=\"keyword\">in</span> tuples:</div><div class=\"line\">  <span class=\"keyword\">print</span> tuple[<span class=\"number\">0</span>]  <span class=\"comment\">## username</span></div><div class=\"line\">  <span class=\"keyword\">print</span> tuple[<span class=\"number\">1</span>]  <span class=\"comment\">## host</span></div></pre></td></tr></table></figure>\n<h2 id=\"调试\"><a href=\"#调试\" class=\"headerlink\" title=\"调试\"></a>调试</h2><p>正则表达式异常强大，使用简单的几条规则就可以演变出很多的模式组合。在确定你的模式之前，可能需要很多的调试工作。在一个小的测试集合上测试正则表达式。</p>\n<h2 id=\"其他选项\"><a href=\"#其他选项\" class=\"headerlink\" title=\"其他选项\"></a>其他选项</h2><p>正则表达式还可以设置“选项”。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">match = re.search(pat,str,opt)</div></pre></td></tr></table></figure>\n<p>这些可选项如下：</p>\n<ul>\n<li>IGNORECASE  忽视大小写</li>\n<li>DOTALL  允许’.’匹配’\\n’</li>\n<li>MULTILINE  在一个由许多行组成的字符串中，允许’^’和’$’匹配每一行的开始和结束</li>\n</ul>"},{"title":"B站视频“线性代数的本质”观后感","date":"2017-02-05T11:16:32.000Z","_content":"线性代数对于现代科技的重要性不言而喻，在机器学习领域也是重要的工具。最近，在B站上发现了这样的一个视频：[线性代数的本质](http://www.bilibili.com/video/av6731067/index_1.html)。这个视频共有十集左右，每集大约10分钟，从线性空间入手，介绍线性变换，并由此介绍矩阵的相关性质，动画做的很棒，翻译得也很好，用了几天时间零散地把几集视频看完，记录一些心得。\n![](/img/video_linear_alg_essential.png)\n\n<!-- more -->\n## 从线性空间和线性变换讲起\nBIT的教学水平，不同的老师参差不齐。所幸的是，大一教授线性代数一课的吴惠彬老师，是一位既有深厚学问又善于教课的老师。我们所用的教材，正是吴老师主编的。和这个视频不同，教材是以线性方程组入手来介绍矩阵，而后引入到线性空间和线性变换。吴老师还曾经告诫大家，线性空间和线性变换一章较为抽象晦涩，不易理解。当然，很多年过去了，我也在研究生阶段继续修了线性代数的深入课程，如今想来，其实线性变换和矩阵的联系更为紧密一些。但是我们的教材之所以不用线性变换为引子，可能正是由于对于大一新生来说，过去十二年所受的基础教育都是操作一些看得见摸得着的数和几何体，一时难以接受线性空间这种东西吧~而也正是从这门课和高数开始，思考问题的方式和初等数学不一样了。我们更多地考察运动和极限，将具有共性的东西抽象出来，研究它们的等价性质。\n\n而抽象这个问题，在本视频中，通过将问题聚焦在二维（偶有提及三维）平面并辅之以动画的形式避开了（所以这个视频也只能算是有趣的不严肃的科普性质）。视频从线性空间入手，首先介绍了向量和基的概念，而后引入出贯穿视频系列始终的**线性变换**。什么是线性变换呢？从“数”的角度来看，定义在线性空间上，满足叠加性和齐次性就是线性变换。而本视频则从“形”的角度出发（注意，这也是本视频贯穿始终的指导思想：用“形”来思考），给出了下面两个条件：\n- 变换前后原点不动\n- 变换前后平行等距线仍然保持平行等距（但是距离值可能会变）。\n\n## 线性变换与矩阵的关系\n视频在阐述线性变换和矩阵关系的时候一带而过，不是很好。下面是我写的一个补充说明。\n\n在由一组基向量$\\alpha_i, i = 1,2,\\dots,n$张成的线性空间$\\mathcal{V}$上，任何一个向量$v$都可以表示为这组基的线性组合，也就是\n$$v = \\sum_{i=1}^{n}k_i\\alpha_i$$\n\n则线性变换$\\mathcal{T}$对$v$作用之后，有，\n$$u = \\mathcal{T}(v) = \\mathcal{T}(\\sum_{i=1}^{n}k_i\\alpha_i)$$\n\n根据线性变换的叠加性，有，\n$$u = \\sum_{i=1}^{n}k_i\\mathcal{T}(\\alpha_i)$$\n\n设$\\alpha_i$经过线性变换$\\mathcal{T}$作用后，变换为$\\beta_i$，那么，\n$$u = \\sum_{i=1}^{n}k_i\\beta_i$$\n\n也就是说，\n$$u = \\begin{bmatrix}\\mathcal{T}(\\alpha_1), \\mathcal{T}(\\alpha_2), \\cdots, \\mathcal{T}(\\alpha_n)\\end{bmatrix}\n\\begin{bmatrix}k_1\\\\\\\\ k_2\\\\\\\\ \\vdots\\\\\\\\ k_n\\end{bmatrix}$$\n\n上式说明，一个抽象的线性变换可以使用一个具体的矩阵来代表。这个矩阵的列，就是线性变换之后的那组基。\n\n举个例子，旋转变换。如果旋转$\\frac{\\pi}{4}$，那么原来坐标轴方向的单位向量的$i$和$j$分别转到了$(\\frac{\\sqrt{2}}{2},\\frac{\\sqrt{2}}{2})$和$(-\\frac{\\sqrt{2}}{2},\\frac{\\sqrt{2}}{2})$。所以描述这个线性变换的矩阵为\n\n$$A = \\begin{bmatrix}\\frac{\\sqrt{2}}{2} & -\\frac{\\sqrt{2}}{2}\\\\\\\\ \\frac{\\sqrt{2}}{2}& \\frac{\\sqrt{2}}{2}\\end{bmatrix}$$\n\n矩阵$A$的两列分别为变换后的基向量坐标。\n\n## 矩阵乘法\n那么矩阵乘法的意义也就有了，那就是对向量做线性变换。例如上面的矩阵$A$与$x = \\begin{bmatrix}-1\\\\\\\\ 0\\end{bmatrix}$相乘，就是求取向量$-1i+0j$在旋转变换之后的新坐标。这时候，$-1$和$0$就是上面推导过程中的$k_1$和$k_2$，所以，\n\n$$Ax = -1\\begin{bmatrix}\\frac{\\sqrt{2}}{2}\\\\\\\\ \\frac{\\sqrt{2}}{2}\\end{bmatrix} + 0\\begin{bmatrix}-\\frac{\\sqrt{2}}{2}\\\\\\\\\\frac{\\sqrt{2}}{2}\\end{bmatrix} $$\n\n而矩阵与矩阵相乘呢？只要把右边的矩阵按列分块即可。其实这是矩阵与多个向量相乘的简写形式。\n\n所以，对于任意向量$x$，$Ax$的结果就是组成矩阵$A$的各列向量的线性组合。这个由矩阵$A$的各列张成的空间叫做矩阵的列空间。\n\n而那些使得方程$Ax=0$成立的$x$，张成的空间为矩阵的核空间。\n\n矩阵的秩的意义就是矩阵列空间的维数。\n\n## 行列式\n仍然考虑二维矩阵，我们已经知道它表示了二维平面上的线性变换。而矩阵行列式的意义，就表明了变换前后，单位面积的正方形被放缩的比例。如果变换之后，$i$向量跑到了$j$向量的左手边，那么需要加上负号，表明在这个变换过程中，二维平面其实进过了一次翻转。\n\n## 点积叉积和对偶性\n这部分的对偶性这个概念比较玄一些，这里就不讲了。。。对于向量的点积，我们已经知道，\n$$\\langle v, u \\rangle = \\sum_{i=1}^{n}v_iu_i$$\n\n从几何的角度看，向量的点积是将向量$u$向向量$v$的方向投影（有正负），投影长度乘上$v$的长度得到的。\n\n按照上面矩阵乘法的思路，$u$可以看做代表一个从2维到1维的线性变换（也就是从$\\mathbb{R}^2$到$\\mathbb{R}$）。而根据上述矩阵变换和矩阵乘法的对应关系可知，变换后的结果就等于$v$的两个分量分别乘上两个基向量（也就是$u$的两个分量），这样我们就得到了向量点积的数值计算方法。\n","source":"_posts/video-linear-alg-essential-property.md","raw":"---\ntitle: B站视频“线性代数的本质”观后感\ndate: 2017-02-05 19:16:32\ntags:\n    - math\n---\n线性代数对于现代科技的重要性不言而喻，在机器学习领域也是重要的工具。最近，在B站上发现了这样的一个视频：[线性代数的本质](http://www.bilibili.com/video/av6731067/index_1.html)。这个视频共有十集左右，每集大约10分钟，从线性空间入手，介绍线性变换，并由此介绍矩阵的相关性质，动画做的很棒，翻译得也很好，用了几天时间零散地把几集视频看完，记录一些心得。\n![](/img/video_linear_alg_essential.png)\n\n<!-- more -->\n## 从线性空间和线性变换讲起\nBIT的教学水平，不同的老师参差不齐。所幸的是，大一教授线性代数一课的吴惠彬老师，是一位既有深厚学问又善于教课的老师。我们所用的教材，正是吴老师主编的。和这个视频不同，教材是以线性方程组入手来介绍矩阵，而后引入到线性空间和线性变换。吴老师还曾经告诫大家，线性空间和线性变换一章较为抽象晦涩，不易理解。当然，很多年过去了，我也在研究生阶段继续修了线性代数的深入课程，如今想来，其实线性变换和矩阵的联系更为紧密一些。但是我们的教材之所以不用线性变换为引子，可能正是由于对于大一新生来说，过去十二年所受的基础教育都是操作一些看得见摸得着的数和几何体，一时难以接受线性空间这种东西吧~而也正是从这门课和高数开始，思考问题的方式和初等数学不一样了。我们更多地考察运动和极限，将具有共性的东西抽象出来，研究它们的等价性质。\n\n而抽象这个问题，在本视频中，通过将问题聚焦在二维（偶有提及三维）平面并辅之以动画的形式避开了（所以这个视频也只能算是有趣的不严肃的科普性质）。视频从线性空间入手，首先介绍了向量和基的概念，而后引入出贯穿视频系列始终的**线性变换**。什么是线性变换呢？从“数”的角度来看，定义在线性空间上，满足叠加性和齐次性就是线性变换。而本视频则从“形”的角度出发（注意，这也是本视频贯穿始终的指导思想：用“形”来思考），给出了下面两个条件：\n- 变换前后原点不动\n- 变换前后平行等距线仍然保持平行等距（但是距离值可能会变）。\n\n## 线性变换与矩阵的关系\n视频在阐述线性变换和矩阵关系的时候一带而过，不是很好。下面是我写的一个补充说明。\n\n在由一组基向量$\\alpha_i, i = 1,2,\\dots,n$张成的线性空间$\\mathcal{V}$上，任何一个向量$v$都可以表示为这组基的线性组合，也就是\n$$v = \\sum_{i=1}^{n}k_i\\alpha_i$$\n\n则线性变换$\\mathcal{T}$对$v$作用之后，有，\n$$u = \\mathcal{T}(v) = \\mathcal{T}(\\sum_{i=1}^{n}k_i\\alpha_i)$$\n\n根据线性变换的叠加性，有，\n$$u = \\sum_{i=1}^{n}k_i\\mathcal{T}(\\alpha_i)$$\n\n设$\\alpha_i$经过线性变换$\\mathcal{T}$作用后，变换为$\\beta_i$，那么，\n$$u = \\sum_{i=1}^{n}k_i\\beta_i$$\n\n也就是说，\n$$u = \\begin{bmatrix}\\mathcal{T}(\\alpha_1), \\mathcal{T}(\\alpha_2), \\cdots, \\mathcal{T}(\\alpha_n)\\end{bmatrix}\n\\begin{bmatrix}k_1\\\\\\\\ k_2\\\\\\\\ \\vdots\\\\\\\\ k_n\\end{bmatrix}$$\n\n上式说明，一个抽象的线性变换可以使用一个具体的矩阵来代表。这个矩阵的列，就是线性变换之后的那组基。\n\n举个例子，旋转变换。如果旋转$\\frac{\\pi}{4}$，那么原来坐标轴方向的单位向量的$i$和$j$分别转到了$(\\frac{\\sqrt{2}}{2},\\frac{\\sqrt{2}}{2})$和$(-\\frac{\\sqrt{2}}{2},\\frac{\\sqrt{2}}{2})$。所以描述这个线性变换的矩阵为\n\n$$A = \\begin{bmatrix}\\frac{\\sqrt{2}}{2} & -\\frac{\\sqrt{2}}{2}\\\\\\\\ \\frac{\\sqrt{2}}{2}& \\frac{\\sqrt{2}}{2}\\end{bmatrix}$$\n\n矩阵$A$的两列分别为变换后的基向量坐标。\n\n## 矩阵乘法\n那么矩阵乘法的意义也就有了，那就是对向量做线性变换。例如上面的矩阵$A$与$x = \\begin{bmatrix}-1\\\\\\\\ 0\\end{bmatrix}$相乘，就是求取向量$-1i+0j$在旋转变换之后的新坐标。这时候，$-1$和$0$就是上面推导过程中的$k_1$和$k_2$，所以，\n\n$$Ax = -1\\begin{bmatrix}\\frac{\\sqrt{2}}{2}\\\\\\\\ \\frac{\\sqrt{2}}{2}\\end{bmatrix} + 0\\begin{bmatrix}-\\frac{\\sqrt{2}}{2}\\\\\\\\\\frac{\\sqrt{2}}{2}\\end{bmatrix} $$\n\n而矩阵与矩阵相乘呢？只要把右边的矩阵按列分块即可。其实这是矩阵与多个向量相乘的简写形式。\n\n所以，对于任意向量$x$，$Ax$的结果就是组成矩阵$A$的各列向量的线性组合。这个由矩阵$A$的各列张成的空间叫做矩阵的列空间。\n\n而那些使得方程$Ax=0$成立的$x$，张成的空间为矩阵的核空间。\n\n矩阵的秩的意义就是矩阵列空间的维数。\n\n## 行列式\n仍然考虑二维矩阵，我们已经知道它表示了二维平面上的线性变换。而矩阵行列式的意义，就表明了变换前后，单位面积的正方形被放缩的比例。如果变换之后，$i$向量跑到了$j$向量的左手边，那么需要加上负号，表明在这个变换过程中，二维平面其实进过了一次翻转。\n\n## 点积叉积和对偶性\n这部分的对偶性这个概念比较玄一些，这里就不讲了。。。对于向量的点积，我们已经知道，\n$$\\langle v, u \\rangle = \\sum_{i=1}^{n}v_iu_i$$\n\n从几何的角度看，向量的点积是将向量$u$向向量$v$的方向投影（有正负），投影长度乘上$v$的长度得到的。\n\n按照上面矩阵乘法的思路，$u$可以看做代表一个从2维到1维的线性变换（也就是从$\\mathbb{R}^2$到$\\mathbb{R}$）。而根据上述矩阵变换和矩阵乘法的对应关系可知，变换后的结果就等于$v$的两个分量分别乘上两个基向量（也就是$u$的两个分量），这样我们就得到了向量点积的数值计算方法。\n","slug":"video-linear-alg-essential-property","published":1,"updated":"2017-02-26T12:09:28.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cizve07bd000ul61h3e2ekohs","content":"<p>线性代数对于现代科技的重要性不言而喻，在机器学习领域也是重要的工具。最近，在B站上发现了这样的一个视频：<a href=\"http://www.bilibili.com/video/av6731067/index_1.html\" target=\"_blank\" rel=\"external\">线性代数的本质</a>。这个视频共有十集左右，每集大约10分钟，从线性空间入手，介绍线性变换，并由此介绍矩阵的相关性质，动画做的很棒，翻译得也很好，用了几天时间零散地把几集视频看完，记录一些心得。<br><img src=\"/img/video_linear_alg_essential.png\" alt=\"\"></p>\n<a id=\"more\"></a>\n<h2 id=\"从线性空间和线性变换讲起\"><a href=\"#从线性空间和线性变换讲起\" class=\"headerlink\" title=\"从线性空间和线性变换讲起\"></a>从线性空间和线性变换讲起</h2><p>BIT的教学水平，不同的老师参差不齐。所幸的是，大一教授线性代数一课的吴惠彬老师，是一位既有深厚学问又善于教课的老师。我们所用的教材，正是吴老师主编的。和这个视频不同，教材是以线性方程组入手来介绍矩阵，而后引入到线性空间和线性变换。吴老师还曾经告诫大家，线性空间和线性变换一章较为抽象晦涩，不易理解。当然，很多年过去了，我也在研究生阶段继续修了线性代数的深入课程，如今想来，其实线性变换和矩阵的联系更为紧密一些。但是我们的教材之所以不用线性变换为引子，可能正是由于对于大一新生来说，过去十二年所受的基础教育都是操作一些看得见摸得着的数和几何体，一时难以接受线性空间这种东西吧~而也正是从这门课和高数开始，思考问题的方式和初等数学不一样了。我们更多地考察运动和极限，将具有共性的东西抽象出来，研究它们的等价性质。</p>\n<p>而抽象这个问题，在本视频中，通过将问题聚焦在二维（偶有提及三维）平面并辅之以动画的形式避开了（所以这个视频也只能算是有趣的不严肃的科普性质）。视频从线性空间入手，首先介绍了向量和基的概念，而后引入出贯穿视频系列始终的<strong>线性变换</strong>。什么是线性变换呢？从“数”的角度来看，定义在线性空间上，满足叠加性和齐次性就是线性变换。而本视频则从“形”的角度出发（注意，这也是本视频贯穿始终的指导思想：用“形”来思考），给出了下面两个条件：</p>\n<ul>\n<li>变换前后原点不动</li>\n<li>变换前后平行等距线仍然保持平行等距（但是距离值可能会变）。</li>\n</ul>\n<h2 id=\"线性变换与矩阵的关系\"><a href=\"#线性变换与矩阵的关系\" class=\"headerlink\" title=\"线性变换与矩阵的关系\"></a>线性变换与矩阵的关系</h2><p>视频在阐述线性变换和矩阵关系的时候一带而过，不是很好。下面是我写的一个补充说明。</p>\n<p>在由一组基向量$\\alpha_i, i = 1,2,\\dots,n$张成的线性空间$\\mathcal{V}$上，任何一个向量$v$都可以表示为这组基的线性组合，也就是</p>\n<script type=\"math/tex; mode=display\">v = \\sum_{i=1}^{n}k_i\\alpha_i</script><p>则线性变换$\\mathcal{T}$对$v$作用之后，有，</p>\n<script type=\"math/tex; mode=display\">u = \\mathcal{T}(v) = \\mathcal{T}(\\sum_{i=1}^{n}k_i\\alpha_i)</script><p>根据线性变换的叠加性，有，</p>\n<script type=\"math/tex; mode=display\">u = \\sum_{i=1}^{n}k_i\\mathcal{T}(\\alpha_i)</script><p>设$\\alpha_i$经过线性变换$\\mathcal{T}$作用后，变换为$\\beta_i$，那么，</p>\n<script type=\"math/tex; mode=display\">u = \\sum_{i=1}^{n}k_i\\beta_i</script><p>也就是说，</p>\n<script type=\"math/tex; mode=display\">u = \\begin{bmatrix}\\mathcal{T}(\\alpha_1), \\mathcal{T}(\\alpha_2), \\cdots, \\mathcal{T}(\\alpha_n)\\end{bmatrix}\n\\begin{bmatrix}k_1\\\\\\\\ k_2\\\\\\\\ \\vdots\\\\\\\\ k_n\\end{bmatrix}</script><p>上式说明，一个抽象的线性变换可以使用一个具体的矩阵来代表。这个矩阵的列，就是线性变换之后的那组基。</p>\n<p>举个例子，旋转变换。如果旋转$\\frac{\\pi}{4}$，那么原来坐标轴方向的单位向量的$i$和$j$分别转到了$(\\frac{\\sqrt{2}}{2},\\frac{\\sqrt{2}}{2})$和$(-\\frac{\\sqrt{2}}{2},\\frac{\\sqrt{2}}{2})$。所以描述这个线性变换的矩阵为</p>\n<script type=\"math/tex; mode=display\">A = \\begin{bmatrix}\\frac{\\sqrt{2}}{2} & -\\frac{\\sqrt{2}}{2}\\\\\\\\ \\frac{\\sqrt{2}}{2}& \\frac{\\sqrt{2}}{2}\\end{bmatrix}</script><p>矩阵$A$的两列分别为变换后的基向量坐标。</p>\n<h2 id=\"矩阵乘法\"><a href=\"#矩阵乘法\" class=\"headerlink\" title=\"矩阵乘法\"></a>矩阵乘法</h2><p>那么矩阵乘法的意义也就有了，那就是对向量做线性变换。例如上面的矩阵$A$与$x = \\begin{bmatrix}-1\\\\ 0\\end{bmatrix}$相乘，就是求取向量$-1i+0j$在旋转变换之后的新坐标。这时候，$-1$和$0$就是上面推导过程中的$k_1$和$k_2$，所以，</p>\n<script type=\"math/tex; mode=display\">Ax = -1\\begin{bmatrix}\\frac{\\sqrt{2}}{2}\\\\\\\\ \\frac{\\sqrt{2}}{2}\\end{bmatrix} + 0\\begin{bmatrix}-\\frac{\\sqrt{2}}{2}\\\\\\\\\\frac{\\sqrt{2}}{2}\\end{bmatrix}</script><p>而矩阵与矩阵相乘呢？只要把右边的矩阵按列分块即可。其实这是矩阵与多个向量相乘的简写形式。</p>\n<p>所以，对于任意向量$x$，$Ax$的结果就是组成矩阵$A$的各列向量的线性组合。这个由矩阵$A$的各列张成的空间叫做矩阵的列空间。</p>\n<p>而那些使得方程$Ax=0$成立的$x$，张成的空间为矩阵的核空间。</p>\n<p>矩阵的秩的意义就是矩阵列空间的维数。</p>\n<h2 id=\"行列式\"><a href=\"#行列式\" class=\"headerlink\" title=\"行列式\"></a>行列式</h2><p>仍然考虑二维矩阵，我们已经知道它表示了二维平面上的线性变换。而矩阵行列式的意义，就表明了变换前后，单位面积的正方形被放缩的比例。如果变换之后，$i$向量跑到了$j$向量的左手边，那么需要加上负号，表明在这个变换过程中，二维平面其实进过了一次翻转。</p>\n<h2 id=\"点积叉积和对偶性\"><a href=\"#点积叉积和对偶性\" class=\"headerlink\" title=\"点积叉积和对偶性\"></a>点积叉积和对偶性</h2><p>这部分的对偶性这个概念比较玄一些，这里就不讲了。。。对于向量的点积，我们已经知道，</p>\n<script type=\"math/tex; mode=display\">\\langle v, u \\rangle = \\sum_{i=1}^{n}v_iu_i</script><p>从几何的角度看，向量的点积是将向量$u$向向量$v$的方向投影（有正负），投影长度乘上$v$的长度得到的。</p>\n<p>按照上面矩阵乘法的思路，$u$可以看做代表一个从2维到1维的线性变换（也就是从$\\mathbb{R}^2$到$\\mathbb{R}$）。而根据上述矩阵变换和矩阵乘法的对应关系可知，变换后的结果就等于$v$的两个分量分别乘上两个基向量（也就是$u$的两个分量），这样我们就得到了向量点积的数值计算方法。</p>\n","excerpt":"<p>线性代数对于现代科技的重要性不言而喻，在机器学习领域也是重要的工具。最近，在B站上发现了这样的一个视频：<a href=\"http://www.bilibili.com/video/av6731067/index_1.html\">线性代数的本质</a>。这个视频共有十集左右，每集大约10分钟，从线性空间入手，介绍线性变换，并由此介绍矩阵的相关性质，动画做的很棒，翻译得也很好，用了几天时间零散地把几集视频看完，记录一些心得。<br><img src=\"/img/video_linear_alg_essential.png\" alt=\"\"></p>","more":"<h2 id=\"从线性空间和线性变换讲起\"><a href=\"#从线性空间和线性变换讲起\" class=\"headerlink\" title=\"从线性空间和线性变换讲起\"></a>从线性空间和线性变换讲起</h2><p>BIT的教学水平，不同的老师参差不齐。所幸的是，大一教授线性代数一课的吴惠彬老师，是一位既有深厚学问又善于教课的老师。我们所用的教材，正是吴老师主编的。和这个视频不同，教材是以线性方程组入手来介绍矩阵，而后引入到线性空间和线性变换。吴老师还曾经告诫大家，线性空间和线性变换一章较为抽象晦涩，不易理解。当然，很多年过去了，我也在研究生阶段继续修了线性代数的深入课程，如今想来，其实线性变换和矩阵的联系更为紧密一些。但是我们的教材之所以不用线性变换为引子，可能正是由于对于大一新生来说，过去十二年所受的基础教育都是操作一些看得见摸得着的数和几何体，一时难以接受线性空间这种东西吧~而也正是从这门课和高数开始，思考问题的方式和初等数学不一样了。我们更多地考察运动和极限，将具有共性的东西抽象出来，研究它们的等价性质。</p>\n<p>而抽象这个问题，在本视频中，通过将问题聚焦在二维（偶有提及三维）平面并辅之以动画的形式避开了（所以这个视频也只能算是有趣的不严肃的科普性质）。视频从线性空间入手，首先介绍了向量和基的概念，而后引入出贯穿视频系列始终的<strong>线性变换</strong>。什么是线性变换呢？从“数”的角度来看，定义在线性空间上，满足叠加性和齐次性就是线性变换。而本视频则从“形”的角度出发（注意，这也是本视频贯穿始终的指导思想：用“形”来思考），给出了下面两个条件：</p>\n<ul>\n<li>变换前后原点不动</li>\n<li>变换前后平行等距线仍然保持平行等距（但是距离值可能会变）。</li>\n</ul>\n<h2 id=\"线性变换与矩阵的关系\"><a href=\"#线性变换与矩阵的关系\" class=\"headerlink\" title=\"线性变换与矩阵的关系\"></a>线性变换与矩阵的关系</h2><p>视频在阐述线性变换和矩阵关系的时候一带而过，不是很好。下面是我写的一个补充说明。</p>\n<p>在由一组基向量$\\alpha_i, i = 1,2,\\dots,n$张成的线性空间$\\mathcal{V}$上，任何一个向量$v$都可以表示为这组基的线性组合，也就是</p>\n<script type=\"math/tex; mode=display\">v = \\sum_{i=1}^{n}k_i\\alpha_i</script><p>则线性变换$\\mathcal{T}$对$v$作用之后，有，</p>\n<script type=\"math/tex; mode=display\">u = \\mathcal{T}(v) = \\mathcal{T}(\\sum_{i=1}^{n}k_i\\alpha_i)</script><p>根据线性变换的叠加性，有，</p>\n<script type=\"math/tex; mode=display\">u = \\sum_{i=1}^{n}k_i\\mathcal{T}(\\alpha_i)</script><p>设$\\alpha_i$经过线性变换$\\mathcal{T}$作用后，变换为$\\beta_i$，那么，</p>\n<script type=\"math/tex; mode=display\">u = \\sum_{i=1}^{n}k_i\\beta_i</script><p>也就是说，</p>\n<script type=\"math/tex; mode=display\">u = \\begin{bmatrix}\\mathcal{T}(\\alpha_1), \\mathcal{T}(\\alpha_2), \\cdots, \\mathcal{T}(\\alpha_n)\\end{bmatrix}\n\\begin{bmatrix}k_1\\\\\\\\ k_2\\\\\\\\ \\vdots\\\\\\\\ k_n\\end{bmatrix}</script><p>上式说明，一个抽象的线性变换可以使用一个具体的矩阵来代表。这个矩阵的列，就是线性变换之后的那组基。</p>\n<p>举个例子，旋转变换。如果旋转$\\frac{\\pi}{4}$，那么原来坐标轴方向的单位向量的$i$和$j$分别转到了$(\\frac{\\sqrt{2}}{2},\\frac{\\sqrt{2}}{2})$和$(-\\frac{\\sqrt{2}}{2},\\frac{\\sqrt{2}}{2})$。所以描述这个线性变换的矩阵为</p>\n<script type=\"math/tex; mode=display\">A = \\begin{bmatrix}\\frac{\\sqrt{2}}{2} & -\\frac{\\sqrt{2}}{2}\\\\\\\\ \\frac{\\sqrt{2}}{2}& \\frac{\\sqrt{2}}{2}\\end{bmatrix}</script><p>矩阵$A$的两列分别为变换后的基向量坐标。</p>\n<h2 id=\"矩阵乘法\"><a href=\"#矩阵乘法\" class=\"headerlink\" title=\"矩阵乘法\"></a>矩阵乘法</h2><p>那么矩阵乘法的意义也就有了，那就是对向量做线性变换。例如上面的矩阵$A$与$x = \\begin{bmatrix}-1\\\\ 0\\end{bmatrix}$相乘，就是求取向量$-1i+0j$在旋转变换之后的新坐标。这时候，$-1$和$0$就是上面推导过程中的$k_1$和$k_2$，所以，</p>\n<script type=\"math/tex; mode=display\">Ax = -1\\begin{bmatrix}\\frac{\\sqrt{2}}{2}\\\\\\\\ \\frac{\\sqrt{2}}{2}\\end{bmatrix} + 0\\begin{bmatrix}-\\frac{\\sqrt{2}}{2}\\\\\\\\\\frac{\\sqrt{2}}{2}\\end{bmatrix}</script><p>而矩阵与矩阵相乘呢？只要把右边的矩阵按列分块即可。其实这是矩阵与多个向量相乘的简写形式。</p>\n<p>所以，对于任意向量$x$，$Ax$的结果就是组成矩阵$A$的各列向量的线性组合。这个由矩阵$A$的各列张成的空间叫做矩阵的列空间。</p>\n<p>而那些使得方程$Ax=0$成立的$x$，张成的空间为矩阵的核空间。</p>\n<p>矩阵的秩的意义就是矩阵列空间的维数。</p>\n<h2 id=\"行列式\"><a href=\"#行列式\" class=\"headerlink\" title=\"行列式\"></a>行列式</h2><p>仍然考虑二维矩阵，我们已经知道它表示了二维平面上的线性变换。而矩阵行列式的意义，就表明了变换前后，单位面积的正方形被放缩的比例。如果变换之后，$i$向量跑到了$j$向量的左手边，那么需要加上负号，表明在这个变换过程中，二维平面其实进过了一次翻转。</p>\n<h2 id=\"点积叉积和对偶性\"><a href=\"#点积叉积和对偶性\" class=\"headerlink\" title=\"点积叉积和对偶性\"></a>点积叉积和对偶性</h2><p>这部分的对偶性这个概念比较玄一些，这里就不讲了。。。对于向量的点积，我们已经知道，</p>\n<script type=\"math/tex; mode=display\">\\langle v, u \\rangle = \\sum_{i=1}^{n}v_iu_i</script><p>从几何的角度看，向量的点积是将向量$u$向向量$v$的方向投影（有正负），投影长度乘上$v$的长度得到的。</p>\n<p>按照上面矩阵乘法的思路，$u$可以看做代表一个从2维到1维的线性变换（也就是从$\\mathbb{R}^2$到$\\mathbb{R}$）。而根据上述矩阵变换和矩阵乘法的对应关系可知，变换后的结果就等于$v$的两个分量分别乘上两个基向量（也就是$u$的两个分量），这样我们就得到了向量点积的数值计算方法。</p>"},{"title":"在Caffe中使用Baidu warpctc实现CTC Loss的计算","date":"2017-02-22T07:34:32.000Z","_content":"CTC(Connectionist Temporal Classification) Loss 函数多用于序列有监督学习，优点是不需要对齐输入数据及标签。本文内容并不涉及CTC Loss的原理介绍，而是关于如何在Caffe中移植Baidu美研院实现的[warp-ctc](https://github.com/baidu-research/warp-ctc)，并利用其实现一个LSTM + CTC Loss的验证码识别demo。下面这张图引用自warp-ctc的[项目页面](https://github.com/baidu-research/warp-ctc)。本文介绍内容的相关代码可以参见我的GitHub项目[warpctc-caffe](https://github.com/xmfbit/warpctc-caffe)\n![CTC Loss](/img/warpctc_intro.png)\n<!-- more -->\n\n## 移植warp-ctc\n本节介绍了如何将`warp-ctc`的源码在Caffe中进行编译。\n\n首先，我们将`warp-ctc`的项目代码从GitHub上clone下来。在Caffe的`include/caffe`和`src/caffe`下分别创建名为`3rdparty`的文件夹，将warp-ctc中的头文件和实现文件分别放到对应的文件夹下。之后，我们需要对其代码和配置进行修改，才能在Caffe中顺利编译。\n\n由于`warp-ctc`中使用了`C++11`的相关技术，所以需要修改Caffe的`Makefile`文件，添加`C++11`支持，可以参见[Makefile](https://github.com/xmfbit/warpctc-caffe/blob/master/Makefile)。\n\n对Caffe的修改就是这么简单，之后我们需要修改`warp-ctc`中的代码文件。这里的修改多且乱，边改边试着编译，所以可能其中也有不必要的修改。最后的目的就是能够使用GPU进行CTC Loss的计算。\n\n`warp-ctc`提供了CPU多线程的计算，这里我直接将相应的`openmp`并行化语句删掉了。\n\n另外，需要将warp-ctc中包含CUDA代码的相关头文件后缀名改为`cuh`，这样才能够通过编译。否则编译器会给出找不到`__host__`和`__device__`等等关键字的错误。\n\n对于详细的修改配置，还请参见GitHub相应的[代码文件](https://github.com/xmfbit/warpctc-caffe/blob/master/include/caffe/3rdparty/detail/hostdevice.cuh)。\n\n## 实现CTC Loss计算\n编译没有问题后，我们可以编写`ctc_loss_layer`实现CTC Loss的计算。在实现时，注意参考文件`ctc.h`。这个文件中给出了使用`warp-ctc`进行CTC Loss计算的全部API接口。\n\n`ctc_loss_layer`继承自`loss_layer`，主要是前向和反向计算的实现。由于`warp-ctc`中只对单精度浮点数`float`进行支持，所以，对于双精度网络参数，直接将其设置为`NOT_IMPLEMENTED`，如下所示。\n\n``` cpp\ntemplate <>\nvoid CtcLossLayer<double>::Forward_cpu(\n    const vector<Blob<double>*>& bottom, const vector<Blob<double>*>& top) {\n    NOT_IMPLEMENTED;\n}\n\ntemplate <>\nvoid CtcLossLayer<double>::Backward_cpu(const vector<Blob<double>*>& top,\n    const vector<bool>& propagate_down, const vector<Blob<double>*>& bottom) {\n    NOT_IMPLEMENTED;\n}\n```\n\n使用`warp-ctc`相关接口进行CTC Loss计算的步骤如下：\n\n- 设置`ctcOptions`，指定使用CPU或GPU进行计算，并指定CPU计算的线程数和GPU计算的CUDA stream。\n- 调用`get_workspace_size()`函数，预先为计算分配空间（分配的内存根据计算平台不同位于CPU或GPU）。\n- 调用`compute_ctc_loss()`函数，计算`loss`和`gradient`。\n\n其中，在第三步中计算`gradient`时，可以直接将对应`blob`的`cpu/gpu_diff`指针传入，作为`gradient`。\n\n这部分的实现代码分别位于`include/caffe/layers`和`src/caffe/layers/`下。\n\n## 验证码数字识别\n本部分相关代码位于`examples/warpctc`文件夹下。实验方案如下。\n\n- 使用`Python`中的`capycha`进行包含`0-9`数字的验证码图片的产生，图片中数字个数从`1`到`MAX_LEN`不等。\n- 使用`10`作为`blank_label`，将所有的标签序列在后面补`blank_label`以达到同样的长度`MAX_LEN`。\n- 将图像的每一列看做一个time step，网络模型使用`image data->2LSTM->fc->CTC Loss`，简单粗暴。\n- 模型训练过程中，数据输入使用`HDF5`格式。\n\n### 数据产生\n使用`captcha`生成验证码图片。[这里](https://pypi.python.org/pypi/captcha/0.1.1)是一个简单的API demo。默认生成的图片大小为`160x60`。我们将其长宽缩小一半，使用`80x30`的彩色图片作为输入。\n\n使用`python`中的`h5py`模块生成`HDF5`格式的数据文件。将全部图片分为两部分，80%作为训练集，20%作为验证集。\n\n### LSTM的输入\n在Caffe中已经有了`lstm_layer`的实现。`lstm_layer`要求输入的序列`blob`为`TxNx...`，也就是说我们需要\n### 训练\n\n### 结论\n","source":"_posts/warpctc-caffe.md","raw":"---\ntitle: 在Caffe中使用Baidu warpctc实现CTC Loss的计算\ndate: 2017-02-22 15:34:32\ntags:\n     - caffe\n---\nCTC(Connectionist Temporal Classification) Loss 函数多用于序列有监督学习，优点是不需要对齐输入数据及标签。本文内容并不涉及CTC Loss的原理介绍，而是关于如何在Caffe中移植Baidu美研院实现的[warp-ctc](https://github.com/baidu-research/warp-ctc)，并利用其实现一个LSTM + CTC Loss的验证码识别demo。下面这张图引用自warp-ctc的[项目页面](https://github.com/baidu-research/warp-ctc)。本文介绍内容的相关代码可以参见我的GitHub项目[warpctc-caffe](https://github.com/xmfbit/warpctc-caffe)\n![CTC Loss](/img/warpctc_intro.png)\n<!-- more -->\n\n## 移植warp-ctc\n本节介绍了如何将`warp-ctc`的源码在Caffe中进行编译。\n\n首先，我们将`warp-ctc`的项目代码从GitHub上clone下来。在Caffe的`include/caffe`和`src/caffe`下分别创建名为`3rdparty`的文件夹，将warp-ctc中的头文件和实现文件分别放到对应的文件夹下。之后，我们需要对其代码和配置进行修改，才能在Caffe中顺利编译。\n\n由于`warp-ctc`中使用了`C++11`的相关技术，所以需要修改Caffe的`Makefile`文件，添加`C++11`支持，可以参见[Makefile](https://github.com/xmfbit/warpctc-caffe/blob/master/Makefile)。\n\n对Caffe的修改就是这么简单，之后我们需要修改`warp-ctc`中的代码文件。这里的修改多且乱，边改边试着编译，所以可能其中也有不必要的修改。最后的目的就是能够使用GPU进行CTC Loss的计算。\n\n`warp-ctc`提供了CPU多线程的计算，这里我直接将相应的`openmp`并行化语句删掉了。\n\n另外，需要将warp-ctc中包含CUDA代码的相关头文件后缀名改为`cuh`，这样才能够通过编译。否则编译器会给出找不到`__host__`和`__device__`等等关键字的错误。\n\n对于详细的修改配置，还请参见GitHub相应的[代码文件](https://github.com/xmfbit/warpctc-caffe/blob/master/include/caffe/3rdparty/detail/hostdevice.cuh)。\n\n## 实现CTC Loss计算\n编译没有问题后，我们可以编写`ctc_loss_layer`实现CTC Loss的计算。在实现时，注意参考文件`ctc.h`。这个文件中给出了使用`warp-ctc`进行CTC Loss计算的全部API接口。\n\n`ctc_loss_layer`继承自`loss_layer`，主要是前向和反向计算的实现。由于`warp-ctc`中只对单精度浮点数`float`进行支持，所以，对于双精度网络参数，直接将其设置为`NOT_IMPLEMENTED`，如下所示。\n\n``` cpp\ntemplate <>\nvoid CtcLossLayer<double>::Forward_cpu(\n    const vector<Blob<double>*>& bottom, const vector<Blob<double>*>& top) {\n    NOT_IMPLEMENTED;\n}\n\ntemplate <>\nvoid CtcLossLayer<double>::Backward_cpu(const vector<Blob<double>*>& top,\n    const vector<bool>& propagate_down, const vector<Blob<double>*>& bottom) {\n    NOT_IMPLEMENTED;\n}\n```\n\n使用`warp-ctc`相关接口进行CTC Loss计算的步骤如下：\n\n- 设置`ctcOptions`，指定使用CPU或GPU进行计算，并指定CPU计算的线程数和GPU计算的CUDA stream。\n- 调用`get_workspace_size()`函数，预先为计算分配空间（分配的内存根据计算平台不同位于CPU或GPU）。\n- 调用`compute_ctc_loss()`函数，计算`loss`和`gradient`。\n\n其中，在第三步中计算`gradient`时，可以直接将对应`blob`的`cpu/gpu_diff`指针传入，作为`gradient`。\n\n这部分的实现代码分别位于`include/caffe/layers`和`src/caffe/layers/`下。\n\n## 验证码数字识别\n本部分相关代码位于`examples/warpctc`文件夹下。实验方案如下。\n\n- 使用`Python`中的`capycha`进行包含`0-9`数字的验证码图片的产生，图片中数字个数从`1`到`MAX_LEN`不等。\n- 使用`10`作为`blank_label`，将所有的标签序列在后面补`blank_label`以达到同样的长度`MAX_LEN`。\n- 将图像的每一列看做一个time step，网络模型使用`image data->2LSTM->fc->CTC Loss`，简单粗暴。\n- 模型训练过程中，数据输入使用`HDF5`格式。\n\n### 数据产生\n使用`captcha`生成验证码图片。[这里](https://pypi.python.org/pypi/captcha/0.1.1)是一个简单的API demo。默认生成的图片大小为`160x60`。我们将其长宽缩小一半，使用`80x30`的彩色图片作为输入。\n\n使用`python`中的`h5py`模块生成`HDF5`格式的数据文件。将全部图片分为两部分，80%作为训练集，20%作为验证集。\n\n### LSTM的输入\n在Caffe中已经有了`lstm_layer`的实现。`lstm_layer`要求输入的序列`blob`为`TxNx...`，也就是说我们需要\n### 训练\n\n### 结论\n","slug":"warpctc-caffe","published":1,"updated":"2017-02-26T12:09:28.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cizve07bg000wl61haq90ueiw","content":"<p>CTC(Connectionist Temporal Classification) Loss 函数多用于序列有监督学习，优点是不需要对齐输入数据及标签。本文内容并不涉及CTC Loss的原理介绍，而是关于如何在Caffe中移植Baidu美研院实现的<a href=\"https://github.com/baidu-research/warp-ctc\" target=\"_blank\" rel=\"external\">warp-ctc</a>，并利用其实现一个LSTM + CTC Loss的验证码识别demo。下面这张图引用自warp-ctc的<a href=\"https://github.com/baidu-research/warp-ctc\" target=\"_blank\" rel=\"external\">项目页面</a>。本文介绍内容的相关代码可以参见我的GitHub项目<a href=\"https://github.com/xmfbit/warpctc-caffe\" target=\"_blank\" rel=\"external\">warpctc-caffe</a><br><img src=\"/img/warpctc_intro.png\" alt=\"CTC Loss\"><br><a id=\"more\"></a></p>\n<h2 id=\"移植warp-ctc\"><a href=\"#移植warp-ctc\" class=\"headerlink\" title=\"移植warp-ctc\"></a>移植warp-ctc</h2><p>本节介绍了如何将<code>warp-ctc</code>的源码在Caffe中进行编译。</p>\n<p>首先，我们将<code>warp-ctc</code>的项目代码从GitHub上clone下来。在Caffe的<code>include/caffe</code>和<code>src/caffe</code>下分别创建名为<code>3rdparty</code>的文件夹，将warp-ctc中的头文件和实现文件分别放到对应的文件夹下。之后，我们需要对其代码和配置进行修改，才能在Caffe中顺利编译。</p>\n<p>由于<code>warp-ctc</code>中使用了<code>C++11</code>的相关技术，所以需要修改Caffe的<code>Makefile</code>文件，添加<code>C++11</code>支持，可以参见<a href=\"https://github.com/xmfbit/warpctc-caffe/blob/master/Makefile\" target=\"_blank\" rel=\"external\">Makefile</a>。</p>\n<p>对Caffe的修改就是这么简单，之后我们需要修改<code>warp-ctc</code>中的代码文件。这里的修改多且乱，边改边试着编译，所以可能其中也有不必要的修改。最后的目的就是能够使用GPU进行CTC Loss的计算。</p>\n<p><code>warp-ctc</code>提供了CPU多线程的计算，这里我直接将相应的<code>openmp</code>并行化语句删掉了。</p>\n<p>另外，需要将warp-ctc中包含CUDA代码的相关头文件后缀名改为<code>cuh</code>，这样才能够通过编译。否则编译器会给出找不到<code>__host__</code>和<code>__device__</code>等等关键字的错误。</p>\n<p>对于详细的修改配置，还请参见GitHub相应的<a href=\"https://github.com/xmfbit/warpctc-caffe/blob/master/include/caffe/3rdparty/detail/hostdevice.cuh\" target=\"_blank\" rel=\"external\">代码文件</a>。</p>\n<h2 id=\"实现CTC-Loss计算\"><a href=\"#实现CTC-Loss计算\" class=\"headerlink\" title=\"实现CTC Loss计算\"></a>实现CTC Loss计算</h2><p>编译没有问题后，我们可以编写<code>ctc_loss_layer</code>实现CTC Loss的计算。在实现时，注意参考文件<code>ctc.h</code>。这个文件中给出了使用<code>warp-ctc</code>进行CTC Loss计算的全部API接口。</p>\n<p><code>ctc_loss_layer</code>继承自<code>loss_layer</code>，主要是前向和反向计算的实现。由于<code>warp-ctc</code>中只对单精度浮点数<code>float</code>进行支持，所以，对于双精度网络参数，直接将其设置为<code>NOT_IMPLEMENTED</code>，如下所示。</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">template</span> &lt;&gt;</div><div class=\"line\"><span class=\"keyword\">void</span> CtcLossLayer&lt;<span class=\"keyword\">double</span>&gt;::Forward_cpu(</div><div class=\"line\">    <span class=\"keyword\">const</span> <span class=\"built_in\">vector</span>&lt;Blob&lt;<span class=\"keyword\">double</span>&gt;*&gt;&amp; bottom, <span class=\"keyword\">const</span> <span class=\"built_in\">vector</span>&lt;Blob&lt;<span class=\"keyword\">double</span>&gt;*&gt;&amp; top) &#123;</div><div class=\"line\">    NOT_IMPLEMENTED;</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\"><span class=\"keyword\">template</span> &lt;&gt;</div><div class=\"line\"><span class=\"keyword\">void</span> CtcLossLayer&lt;<span class=\"keyword\">double</span>&gt;::Backward_cpu(<span class=\"keyword\">const</span> <span class=\"built_in\">vector</span>&lt;Blob&lt;<span class=\"keyword\">double</span>&gt;*&gt;&amp; top,</div><div class=\"line\">    <span class=\"keyword\">const</span> <span class=\"built_in\">vector</span>&lt;<span class=\"keyword\">bool</span>&gt;&amp; propagate_down, <span class=\"keyword\">const</span> <span class=\"built_in\">vector</span>&lt;Blob&lt;<span class=\"keyword\">double</span>&gt;*&gt;&amp; bottom) &#123;</div><div class=\"line\">    NOT_IMPLEMENTED;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<p>使用<code>warp-ctc</code>相关接口进行CTC Loss计算的步骤如下：</p>\n<ul>\n<li>设置<code>ctcOptions</code>，指定使用CPU或GPU进行计算，并指定CPU计算的线程数和GPU计算的CUDA stream。</li>\n<li>调用<code>get_workspace_size()</code>函数，预先为计算分配空间（分配的内存根据计算平台不同位于CPU或GPU）。</li>\n<li>调用<code>compute_ctc_loss()</code>函数，计算<code>loss</code>和<code>gradient</code>。</li>\n</ul>\n<p>其中，在第三步中计算<code>gradient</code>时，可以直接将对应<code>blob</code>的<code>cpu/gpu_diff</code>指针传入，作为<code>gradient</code>。</p>\n<p>这部分的实现代码分别位于<code>include/caffe/layers</code>和<code>src/caffe/layers/</code>下。</p>\n<h2 id=\"验证码数字识别\"><a href=\"#验证码数字识别\" class=\"headerlink\" title=\"验证码数字识别\"></a>验证码数字识别</h2><p>本部分相关代码位于<code>examples/warpctc</code>文件夹下。实验方案如下。</p>\n<ul>\n<li>使用<code>Python</code>中的<code>capycha</code>进行包含<code>0-9</code>数字的验证码图片的产生，图片中数字个数从<code>1</code>到<code>MAX_LEN</code>不等。</li>\n<li>使用<code>10</code>作为<code>blank_label</code>，将所有的标签序列在后面补<code>blank_label</code>以达到同样的长度<code>MAX_LEN</code>。</li>\n<li>将图像的每一列看做一个time step，网络模型使用<code>image data-&gt;2LSTM-&gt;fc-&gt;CTC Loss</code>，简单粗暴。</li>\n<li>模型训练过程中，数据输入使用<code>HDF5</code>格式。</li>\n</ul>\n<h3 id=\"数据产生\"><a href=\"#数据产生\" class=\"headerlink\" title=\"数据产生\"></a>数据产生</h3><p>使用<code>captcha</code>生成验证码图片。<a href=\"https://pypi.python.org/pypi/captcha/0.1.1\" target=\"_blank\" rel=\"external\">这里</a>是一个简单的API demo。默认生成的图片大小为<code>160x60</code>。我们将其长宽缩小一半，使用<code>80x30</code>的彩色图片作为输入。</p>\n<p>使用<code>python</code>中的<code>h5py</code>模块生成<code>HDF5</code>格式的数据文件。将全部图片分为两部分，80%作为训练集，20%作为验证集。</p>\n<h3 id=\"LSTM的输入\"><a href=\"#LSTM的输入\" class=\"headerlink\" title=\"LSTM的输入\"></a>LSTM的输入</h3><p>在Caffe中已经有了<code>lstm_layer</code>的实现。<code>lstm_layer</code>要求输入的序列<code>blob</code>为<code>TxNx...</code>，也就是说我们需要</p>\n<h3 id=\"训练\"><a href=\"#训练\" class=\"headerlink\" title=\"训练\"></a>训练</h3><h3 id=\"结论\"><a href=\"#结论\" class=\"headerlink\" title=\"结论\"></a>结论</h3>","excerpt":"<p>CTC(Connectionist Temporal Classification) Loss 函数多用于序列有监督学习，优点是不需要对齐输入数据及标签。本文内容并不涉及CTC Loss的原理介绍，而是关于如何在Caffe中移植Baidu美研院实现的<a href=\"https://github.com/baidu-research/warp-ctc\">warp-ctc</a>，并利用其实现一个LSTM + CTC Loss的验证码识别demo。下面这张图引用自warp-ctc的<a href=\"https://github.com/baidu-research/warp-ctc\">项目页面</a>。本文介绍内容的相关代码可以参见我的GitHub项目<a href=\"https://github.com/xmfbit/warpctc-caffe\">warpctc-caffe</a><br><img src=\"/img/warpctc_intro.png\" alt=\"CTC Loss\"><br>","more":"</p>\n<h2 id=\"移植warp-ctc\"><a href=\"#移植warp-ctc\" class=\"headerlink\" title=\"移植warp-ctc\"></a>移植warp-ctc</h2><p>本节介绍了如何将<code>warp-ctc</code>的源码在Caffe中进行编译。</p>\n<p>首先，我们将<code>warp-ctc</code>的项目代码从GitHub上clone下来。在Caffe的<code>include/caffe</code>和<code>src/caffe</code>下分别创建名为<code>3rdparty</code>的文件夹，将warp-ctc中的头文件和实现文件分别放到对应的文件夹下。之后，我们需要对其代码和配置进行修改，才能在Caffe中顺利编译。</p>\n<p>由于<code>warp-ctc</code>中使用了<code>C++11</code>的相关技术，所以需要修改Caffe的<code>Makefile</code>文件，添加<code>C++11</code>支持，可以参见<a href=\"https://github.com/xmfbit/warpctc-caffe/blob/master/Makefile\">Makefile</a>。</p>\n<p>对Caffe的修改就是这么简单，之后我们需要修改<code>warp-ctc</code>中的代码文件。这里的修改多且乱，边改边试着编译，所以可能其中也有不必要的修改。最后的目的就是能够使用GPU进行CTC Loss的计算。</p>\n<p><code>warp-ctc</code>提供了CPU多线程的计算，这里我直接将相应的<code>openmp</code>并行化语句删掉了。</p>\n<p>另外，需要将warp-ctc中包含CUDA代码的相关头文件后缀名改为<code>cuh</code>，这样才能够通过编译。否则编译器会给出找不到<code>__host__</code>和<code>__device__</code>等等关键字的错误。</p>\n<p>对于详细的修改配置，还请参见GitHub相应的<a href=\"https://github.com/xmfbit/warpctc-caffe/blob/master/include/caffe/3rdparty/detail/hostdevice.cuh\">代码文件</a>。</p>\n<h2 id=\"实现CTC-Loss计算\"><a href=\"#实现CTC-Loss计算\" class=\"headerlink\" title=\"实现CTC Loss计算\"></a>实现CTC Loss计算</h2><p>编译没有问题后，我们可以编写<code>ctc_loss_layer</code>实现CTC Loss的计算。在实现时，注意参考文件<code>ctc.h</code>。这个文件中给出了使用<code>warp-ctc</code>进行CTC Loss计算的全部API接口。</p>\n<p><code>ctc_loss_layer</code>继承自<code>loss_layer</code>，主要是前向和反向计算的实现。由于<code>warp-ctc</code>中只对单精度浮点数<code>float</code>进行支持，所以，对于双精度网络参数，直接将其设置为<code>NOT_IMPLEMENTED</code>，如下所示。</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">template</span> &lt;&gt;</div><div class=\"line\"><span class=\"keyword\">void</span> CtcLossLayer&lt;<span class=\"keyword\">double</span>&gt;::Forward_cpu(</div><div class=\"line\">    <span class=\"keyword\">const</span> <span class=\"built_in\">vector</span>&lt;Blob&lt;<span class=\"keyword\">double</span>&gt;*&gt;&amp; bottom, <span class=\"keyword\">const</span> <span class=\"built_in\">vector</span>&lt;Blob&lt;<span class=\"keyword\">double</span>&gt;*&gt;&amp; top) &#123;</div><div class=\"line\">    NOT_IMPLEMENTED;</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\"><span class=\"keyword\">template</span> &lt;&gt;</div><div class=\"line\"><span class=\"keyword\">void</span> CtcLossLayer&lt;<span class=\"keyword\">double</span>&gt;::Backward_cpu(<span class=\"keyword\">const</span> <span class=\"built_in\">vector</span>&lt;Blob&lt;<span class=\"keyword\">double</span>&gt;*&gt;&amp; top,</div><div class=\"line\">    <span class=\"keyword\">const</span> <span class=\"built_in\">vector</span>&lt;<span class=\"keyword\">bool</span>&gt;&amp; propagate_down, <span class=\"keyword\">const</span> <span class=\"built_in\">vector</span>&lt;Blob&lt;<span class=\"keyword\">double</span>&gt;*&gt;&amp; bottom) &#123;</div><div class=\"line\">    NOT_IMPLEMENTED;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<p>使用<code>warp-ctc</code>相关接口进行CTC Loss计算的步骤如下：</p>\n<ul>\n<li>设置<code>ctcOptions</code>，指定使用CPU或GPU进行计算，并指定CPU计算的线程数和GPU计算的CUDA stream。</li>\n<li>调用<code>get_workspace_size()</code>函数，预先为计算分配空间（分配的内存根据计算平台不同位于CPU或GPU）。</li>\n<li>调用<code>compute_ctc_loss()</code>函数，计算<code>loss</code>和<code>gradient</code>。</li>\n</ul>\n<p>其中，在第三步中计算<code>gradient</code>时，可以直接将对应<code>blob</code>的<code>cpu/gpu_diff</code>指针传入，作为<code>gradient</code>。</p>\n<p>这部分的实现代码分别位于<code>include/caffe/layers</code>和<code>src/caffe/layers/</code>下。</p>\n<h2 id=\"验证码数字识别\"><a href=\"#验证码数字识别\" class=\"headerlink\" title=\"验证码数字识别\"></a>验证码数字识别</h2><p>本部分相关代码位于<code>examples/warpctc</code>文件夹下。实验方案如下。</p>\n<ul>\n<li>使用<code>Python</code>中的<code>capycha</code>进行包含<code>0-9</code>数字的验证码图片的产生，图片中数字个数从<code>1</code>到<code>MAX_LEN</code>不等。</li>\n<li>使用<code>10</code>作为<code>blank_label</code>，将所有的标签序列在后面补<code>blank_label</code>以达到同样的长度<code>MAX_LEN</code>。</li>\n<li>将图像的每一列看做一个time step，网络模型使用<code>image data-&gt;2LSTM-&gt;fc-&gt;CTC Loss</code>，简单粗暴。</li>\n<li>模型训练过程中，数据输入使用<code>HDF5</code>格式。</li>\n</ul>\n<h3 id=\"数据产生\"><a href=\"#数据产生\" class=\"headerlink\" title=\"数据产生\"></a>数据产生</h3><p>使用<code>captcha</code>生成验证码图片。<a href=\"https://pypi.python.org/pypi/captcha/0.1.1\">这里</a>是一个简单的API demo。默认生成的图片大小为<code>160x60</code>。我们将其长宽缩小一半，使用<code>80x30</code>的彩色图片作为输入。</p>\n<p>使用<code>python</code>中的<code>h5py</code>模块生成<code>HDF5</code>格式的数据文件。将全部图片分为两部分，80%作为训练集，20%作为验证集。</p>\n<h3 id=\"LSTM的输入\"><a href=\"#LSTM的输入\" class=\"headerlink\" title=\"LSTM的输入\"></a>LSTM的输入</h3><p>在Caffe中已经有了<code>lstm_layer</code>的实现。<code>lstm_layer</code>要求输入的序列<code>blob</code>为<code>TxNx...</code>，也就是说我们需要</p>\n<h3 id=\"训练\"><a href=\"#训练\" class=\"headerlink\" title=\"训练\"></a>训练</h3><h3 id=\"结论\"><a href=\"#结论\" class=\"headerlink\" title=\"结论\"></a>结论</h3>"},{"title":"PyTorch简介","date":"2017-02-25T11:23:39.000Z","_content":"这是一份阅读PyTorch教程的笔记，记录jupyter notebook的关键点。原地址位于[GitHub repo](https://github.com/pytorch/tutorials/blob/master/Deep%20Learning%20with%20PyTorch.ipynb)。\n![PyTorch Logo](/img/pytorch_logo.png)\n\n<!-- more -->\n## PyTorch简介\n[PyTorch](https://github.com/pytorch/pytorch)是一个较新的深度学习框架。从名字可以看出，其和Torch不同之处在于PyTorch使用了Python作为开发语言，所谓“Python first”。一方面，使用者可以将其作为加入了GPU支持的`numpy`，另一方面，PyTorch也是强大的深度学习框架。\n\n目前有很多深度学习框架，PyTorch主推的功能是动态网络模型。例如在Caffe中，使用者通过编写网络结构的`prototxt`进行定义，网络结构是不能变化的。而PyTorch中的网络结构不再只能是一成不变的。同时PyTorch实现了多达上百种op的自动求导（AutoGrad）。\n\n## Tensors\n`Tensor`，即`numpy`中的多维数组。上面已经提到过，PyTorch对其加入了GPU支持。同时，PyTorch中的`Tensor`可以与`numpy`中的`array`很方便地进行互相转换。\n\n通过`Tensor(shape)`便可以创建所需要大小的`tensor`。如下所示。\n\n``` py\nx = torch.Tensor(5, 3)  # construct a 5x3 matrix, uninitialized\n# 或者随机填充\ny = torch.rand(5, 3)    # construct a randomly initialized matrix\n# 使用size方法可以获得tensor的shape信息，torch.Size 可以看做 tuple\nx.size()                # out: torch.Size([5, 3])\n```\n\nPyTorch中已经实现了很多常用的`op`，如下所示。\n\n``` py\n# addition: syntax 1\nx + y                  # out: [torch.FloatTensor of size 5x3]\n\n# addition: syntax 2\ntorch.add(x, y)        # 或者使用torch包中的显式的op名称\n\n# addition: giving an output tensor\nresult = torch.Tensor(5, 3)  # 预先定义size\ntorch.add(x, y, out=result)  # 结果被填充到变量result\n\n# 对于加法运算，其实没必要这么复杂\nout = x + y                  # 无需预先定义size\n\n# torch包中带有下划线的op说明是就地进行的，如下所示\n# addition: in-place\ny.add_(x)              # 将x加到y上\n# 其他的例子: x.copy_(y), x.t_().\n```\n\nPyTorch中的元素索引方式和`numpy`相同。\n\n``` py\n# standard numpy-like indexing with all bells and whistles\nx[:,1]                 # out: [torch.FloatTensor of size 5]\n```\n\n对于更多的`op`，可以参见PyTorch的[文档页面](http://pytorch.org/docs/torch.html)。\n\n`Tensor`可以和`numpy`中的数组进行很方便地转换。并且转换前后并没有发生内存的复制（这里文档里面没有明说？），所以修改其中某一方的值，也会引起另一方的改变。如下所示。\n\n``` py\n# Tensor 转为 np.array\na = torch.ones(5)    # out: [torch.FloatTensor of size 5]\n# 使用 numpy方法即可实现转换\nb = a.numpy()        # out: array([ 1.,  1.,  1.,  1.,  1.], dtype=float32)\n# 注意！a的值的变化同样引起b的变化\na.add_(1)\nprint(a)\nprint(b)             # a b的值都变成2\n\n# np.array 转为Tensor\nimport numpy as np\na = np.ones(5)\n# 使用torch.from_numpy即可实现转换\nb = torch.from_numpy(a)  # out: [torch.DoubleTensor of size 5]\nnp.add(a, 1, out=a)\nprint(a)\nprint(b)            # a b的值都变为2\n```\n\nPyTorch中使用GPU计算很简单，通过调用`.cuda()`方法，很容易实现GPU支持。\n\n``` py\n# let us run this cell only if CUDA is available\nif torch.cuda.is_available():\n    print('cuda is avaliable')\n    x = x.cuda()\n    y = y.cuda()\n    x + y          # 在GPU上进行计算\n```\n\n## Neural Network\n说完了数据类型`Tensor`，下一步便是如何实现一个神经网络。首先，对[自动求导](http://pytorch.org/docs/autograd.html)做一说明。\n\n我们需要关注的是`autograd.Variable`。这个东西包装了`Tensor`。一旦你完成了计算，就可以使用`.backward()`方法自动得到（以该`Variable`为叶子节点的那个）网络中参数的梯度。`Variable`有一个名叫`data`的字段，可以通过它获得被包装起来的那个原始的`Tensor`数据。同时，使用`grad`字段，可以获取梯度（也是一个`Variable`）。\n\n`Variable`是计算图的节点，同时`Function`实现了变量之间的变换。它们互相联系，构成了用于计算的无环图。每个`Variable`有一个`creator`的字段，表明了它是由哪个`Function`创建的（除了用户自己显式创建的那些，这时候`creator`是`None`）。\n\n当进行反向传播计算梯度时，如果`Variable`是标量（比如最终的`loss`是欧氏距离或者交叉熵），那么`backward()`函数不需要参数。然而如果`Variable`有不止一个元素的时候，需要为其中的每个元素指明其（由上层传导来的）梯度（也就是一个和`Variable`shape匹配的`Tensor`）。看下面的说明代码。\n\n``` py\nfrom torch.autograd import Variable\nx = Variable(torch.ones(2, 2), requires_grad = True)\nx     # x 包装了一个2x2的Tensor\n\"\"\"\nVariable containing:\n 1  1\n 1  1\n[torch.FloatTensor of size 2x2]\n\"\"\"\n# Variable进行计算\n# y was created as a result of an operation,\n# so it has a creator\ny = x + 2\ny.creator    # out: <torch.autograd._functions.basic_ops.AddConstant at 0x7fa1cc158c08>\n\nz = y * y * 3  \nout = z.mean()   # out: Variable containing: 27 [torch.FloatTensor of size 1]\n\n# let's backprop now\nout.backward()  # 其实相当于 out.backward(torch.Tensor([1.0]))\n\n# print gradients d(out)/dx\nx.grad\n\"\"\"\nVariable containing:\n 4.5000  4.5000\n 4.5000  4.5000\n[torch.FloatTensor of size 2x2]\n\"\"\"\n```\n\n下面的代码就是结果不是标量，而是普通的`Tensor`的例子。\n``` py\n# 也可以通过Tensor显式地创建Variable\nx = torch.randn(3)\nx = Variable(x, requires_grad = True)\n# 一个更复杂的 op例子\ny = x * 2\nwhile y.data.norm() < 1000:\n    y = y * 2\n\n# 计算 dy/dx\ngradients = torch.FloatTensor([0.1, 1.0, 0.0001])\ny.backward(gradients)\nx.grad\n\"\"\"\nVariable containing:\n  204.8000\n 2048.0000\n    0.2048\n[torch.FloatTensor of size 3]\n\"\"\"\n```\n\n说完了NN的构成元素`Variable`，下面可以介绍如何使用PyTorch构建网络了。这部分主要使用了`torch.nn`包。我们自定义的网络结构是由若干的`layer`组成的，我们将其设置为 `nn.Module`的子类，只要使用方法`forward(input)`就可以返回网络的`output`。下面的代码展示了如何建立一个包含有`conv`和`max-pooling`和`fc`层的简单CNN网络。\n\n``` py\nimport torch.nn as nn                 # 以我的理解，貌似有参数的都在nn里面\nimport torch.nn.functional as F       # 没有参数的（如pooling和relu）都在functional里面？\n\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(1, 6, 5) # 1 input image channel, 6 output channels, 5x5 square convolution kernel\n        self.conv2 = nn.Conv2d(6, 16, 5)\n        # 一会可以看到，input是1x32x32大小的。经过计算，conv-pooling-conv-pooling后大小为16x5x5。\n        # 所以fc层的第一个参数是 16x5x5\n        self.fc1   = nn.Linear(16*5*5, 120) # an affine operation: y = Wx + b\n        self.fc2   = nn.Linear(120, 84)\n        self.fc3   = nn.Linear(84, 10)\n\n    def forward(self, x):\n        # 你可以发现，构建计算图的过程是在前向计算中完成的，也许这可以让你体会到所谓的动态图结构\n        # 同时，我们无需实现 backward，这是被自动求导实现的\n        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2)) # Max pooling over a (2, 2) window\n        x = F.max_pool2d(F.relu(self.conv2(x)), 2) # If the size is a square you can only specify a single number\n        x = x.view(-1, self.num_flat_features(x))  # 把它拉直\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n\n    def num_flat_features(self, x):\n        size = x.size()[1:] # all dimensions except the batch dimension\n        num_features = 1\n        for s in size:\n            num_features *= s\n        return num_features\n\n# 实例化Net对象\nnet = Net()\nnet     # 给出了网络结构\n\"\"\"\nNet (\n  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n  (fc1): Linear (400 -> 120)\n  (fc2): Linear (120 -> 84)\n  (fc3): Linear (84 -> 10)\n)\n\"\"\"\n```\n\n我们可以列出网络中的所有参数。\n\n``` py\nparams = list(net.parameters())\nprint(len(params))      # out: 10, 5个权重，5个bias\nprint(params[0].size())  # conv1's weight out: torch.Size([6, 1, 5, 5])\nprint(params[1].size())  # conv1's bias, out: torch.Size([6])\n```\n\n给出网络的输入，得到网络的输出。并进行反向传播梯度。\n\n``` py\ninput = Variable(torch.randn(1, 1, 32, 32))\nout = net(input)         # 重载了()运算符？\nnet.zero_grad()          # bp前，把所有参数的grad buffer清零\nout.backward(torch.randn(1, 10))\n```\n\n注意一点，`torch.nn`只支持mini-batch。所以如果你的输入只有一个样例的时候，使用`input.unsqueeze(0)`人为给它加上一个维度，让它变成一个4-D的`Tensor`。\n\n## 网络训练\n给定target和网络的output，就可以计算loss函数了。在`torch.nn`中已经[实现好了一些loss函数](http://pytorch.org/docs/nn.html#loss-functions)。\n\n``` py\noutput = net(input)\ntarget = Variable(torch.range(1, 10))  # a dummy target, for example\n# 使用平均平方误差，即欧几里得距离\ncriterion = nn.MSELoss()\nloss = criterion(output, target)\nloss\n\"\"\"\nVariable containing:\n 38.6049\n[torch.FloatTensor of size 1]\n\"\"\"\n```\n\n网络的整体结构如下所示。\n\n```\ninput -> conv2d -> relu -> maxpool2d -> conv2d -> relu -> maxpool2d  \n      -> view -> linear -> relu -> linear -> relu -> linear\n      -> MSELoss\n      -> loss\n```\n\n我们可以使用`previous_functions`来获得该节点前面`Function`的信息。\n\n```\n# For illustration, let us follow a few steps backward\nprint(loss.creator) # MSELoss\nprint(loss.creator.previous_functions[0][0]) # Linear\nprint(loss.creator.previous_functions[0][0].previous_functions[0][0]) # ReLU\n\"\"\"\n<torch.nn._functions.thnn.auto.MSELoss object at 0x7fa18011db40>\n<torch.nn._functions.linear.Linear object at 0x7fa18011da78>\n<torch.nn._functions.thnn.auto.Threshold object at 0x7fa18011d9b0>\n\"\"\"\n```\n\n进行反向传播后，让我们查看一下参数的变化。\n\n``` py\n# now we shall call loss.backward(), and have a look at conv1's bias gradients before and after the backward.\nnet.zero_grad() # zeroes the gradient buffers of all parameters\nprint('conv1.bias.grad before backward')\nprint(net.conv1.bias.grad)\nloss.backward()\nprint('conv1.bias.grad after backward')\nprint(net.conv1.bias.grad)\n```\n\n计算梯度后，自然需要更新参数了。简单的方法可以自己手写：\n\n``` py\nlearning_rate = 0.01\nfor f in net.parameters():\n    f.data.sub_(f.grad.data * learning_rate)\n```\n\n不过，`torch.optim`中已经提供了若干优化方法（SGD, Nesterov-SGD, Adam, RMSProp, etc）。如下所示。\n\n``` py\nimport torch.optim as optim\n# create your optimizer\noptimizer = optim.SGD(net.parameters(), lr = 0.01)\n# in your training loop:\noptimizer.zero_grad() # zero the gradient buffers\noutput = net(input)\nloss = criterion(output, target)\nloss.backward()\noptimizer.step() # Does the update\n```\n\n## 数据载入\n由于PyTorch的Python接口和`np.array`之间的方便转换，所以可以使用其他任何数据读入的方法（例如OpenCV等）。特别地，对于vision的数据，PyTorch提供了`torchvision`包，可以方便地载入常用的数据集（Imagenet, CIFAR10, MNIST, etc），同时提供了图像的各种变换方法。下面以CIFAR为例子。\n\n``` py\nimport torchvision\nimport torchvision.transforms as transforms\n\n# The output of torchvision datasets are PILImage images of range [0, 1].\n# We transform them to Tensors of normalized range [-1, 1]\n# Compose: Composes several transforms together.\n# see http://pytorch.org/docs/torchvision/transforms.html?highlight=transforms\ntransform=transforms.Compose([transforms.ToTensor(),\n                              transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n                             ])   # torchvision.transforms.Normalize(mean, std)\n# 读取CIFAR10数据集                             \ntrainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n# 使用DataLoader\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n                                          shuffle=True, num_workers=2)\n# Test集，设置train = False\ntestset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\ntestloader = torch.utils.data.DataLoader(testset, batch_size=4,\n                                          shuffle=False, num_workers=2)\nclasses = ('plane', 'car', 'bird', 'cat',\n           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n```\n\n接下来，我们对上面部分的CNN网络进行小修，设置第一个`conv`层接受3通道的输入。并使用交叉熵定义loss。\n\n``` py\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(3, 6, 5)\n        self.pool  = nn.MaxPool2d(2,2)\n        self.conv2 = nn.Conv2d(6, 16, 5)\n        self.fc1   = nn.Linear(16*5*5, 120)\n        self.fc2   = nn.Linear(120, 84)\n        self.fc3   = nn.Linear(84, 10)\n\n    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = x.view(-1, 16*5*5)\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n\nnet = Net()\n# use a Classification Cross-Entropy loss\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n```\n\n接下来，我们进行模型的训练。我们loop over整个dataset两次，对每个mini-batch进行参数的更新。并且设置每隔2000个mini-batch打印一次loss。\n\n``` py\nfor epoch in range(2): # loop over the dataset multiple times\n\n    running_loss = 0.0\n    for i, data in enumerate(trainloader, 0):\n        # get the inputs\n        inputs, labels = data\n\n        # wrap them in Variable\n        inputs, labels = Variable(inputs), Variable(labels)\n\n        # zero the parameter gradients\n        optimizer.zero_grad()\n\n        # forward + backward + optimize\n        outputs = net(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()        \n        optimizer.step()\n\n        # print statistics\n        running_loss += loss.data[0]\n        if i % 2000 == 1999: # print every 2000 mini-batches\n            print('[%d, %5d] loss: %.3f' % (epoch+1, i+1, running_loss / 2000))\n            running_loss = 0.0\nprint('Finished Training')\n```\n\n我们在测试集上选取一个mini-batch（也就是4张，见上面`testloader`的定义），进行测试。\n\n``` py\ndataiter = iter(testloader)\nimages, labels = dataiter.next()   # 得到image和对应的label\noutputs = net(Variable(images))\n\n# the outputs are energies for the 10 classes.\n# Higher the energy for a class, the more the network\n# thinks that the image is of the particular class\n# So, let's get the index of the highest energy\n_, predicted = torch.max(outputs.data, 1)   # 找出分数最高的对应的channel，即为top-1类别\n\nprint('Predicted: ', ' '.join('%5s'% classes[predicted[j][0]] for j in range(4)))\n```\n\n测试一下整个测试集合上的表现。\n\n``` py\ncorrect = 0\ntotal = 0\nfor data in testloader:     # 每一个test mini-batch\n    images, labels = data\n    outputs = net(Variable(images))\n    _, predicted = torch.max(outputs.data, 1)\n    total += labels.size(0)\n    correct += (predicted == labels).sum()\n\nprint('Accuracy of the network on the 10000 test images: %d %%' % (100 * correct / total))\n```\n\n对哪一类的预测精度更高呢？\n\n``` py\nclass_correct = list(0. for i in range(10))\nclass_total = list(0. for i in range(10))\nfor data in testloader:\n    images, labels = data\n    outputs = net(Variable(images))\n    _, predicted = torch.max(outputs.data, 1)\n    c = (predicted == labels).squeeze()\n    for i in range(4):\n        label = labels[i]\n        class_correct[label] += c[i]\n        class_total[label] += 1\n```\n\n上面这些训练和测试都是在CPU上进行的，如何迁移到GPU？很简单，同样用`.cuda()`方法就行了。\n\n``` py\nnet.cuda()\n```\n\n不过记得在每次训练测试的迭代中，`images`和`label`也要传送到GPU上才可以。\n\n``` py\ninputs, labels = Variable(inputs.cuda()), Variable(labels.cuda())\n```\n## 更多的例子和教程\n[更多的例子](https://github.com/pytorch/examples)\n[更多的教程](https://github.com/pytorch/tutorials)\n","source":"_posts/pytorch-tutor-01.md","raw":"---\ntitle: PyTorch简介\ndate: 2017-02-25 19:23:39\ntags:\n    - pytorch\n---\n这是一份阅读PyTorch教程的笔记，记录jupyter notebook的关键点。原地址位于[GitHub repo](https://github.com/pytorch/tutorials/blob/master/Deep%20Learning%20with%20PyTorch.ipynb)。\n![PyTorch Logo](/img/pytorch_logo.png)\n\n<!-- more -->\n## PyTorch简介\n[PyTorch](https://github.com/pytorch/pytorch)是一个较新的深度学习框架。从名字可以看出，其和Torch不同之处在于PyTorch使用了Python作为开发语言，所谓“Python first”。一方面，使用者可以将其作为加入了GPU支持的`numpy`，另一方面，PyTorch也是强大的深度学习框架。\n\n目前有很多深度学习框架，PyTorch主推的功能是动态网络模型。例如在Caffe中，使用者通过编写网络结构的`prototxt`进行定义，网络结构是不能变化的。而PyTorch中的网络结构不再只能是一成不变的。同时PyTorch实现了多达上百种op的自动求导（AutoGrad）。\n\n## Tensors\n`Tensor`，即`numpy`中的多维数组。上面已经提到过，PyTorch对其加入了GPU支持。同时，PyTorch中的`Tensor`可以与`numpy`中的`array`很方便地进行互相转换。\n\n通过`Tensor(shape)`便可以创建所需要大小的`tensor`。如下所示。\n\n``` py\nx = torch.Tensor(5, 3)  # construct a 5x3 matrix, uninitialized\n# 或者随机填充\ny = torch.rand(5, 3)    # construct a randomly initialized matrix\n# 使用size方法可以获得tensor的shape信息，torch.Size 可以看做 tuple\nx.size()                # out: torch.Size([5, 3])\n```\n\nPyTorch中已经实现了很多常用的`op`，如下所示。\n\n``` py\n# addition: syntax 1\nx + y                  # out: [torch.FloatTensor of size 5x3]\n\n# addition: syntax 2\ntorch.add(x, y)        # 或者使用torch包中的显式的op名称\n\n# addition: giving an output tensor\nresult = torch.Tensor(5, 3)  # 预先定义size\ntorch.add(x, y, out=result)  # 结果被填充到变量result\n\n# 对于加法运算，其实没必要这么复杂\nout = x + y                  # 无需预先定义size\n\n# torch包中带有下划线的op说明是就地进行的，如下所示\n# addition: in-place\ny.add_(x)              # 将x加到y上\n# 其他的例子: x.copy_(y), x.t_().\n```\n\nPyTorch中的元素索引方式和`numpy`相同。\n\n``` py\n# standard numpy-like indexing with all bells and whistles\nx[:,1]                 # out: [torch.FloatTensor of size 5]\n```\n\n对于更多的`op`，可以参见PyTorch的[文档页面](http://pytorch.org/docs/torch.html)。\n\n`Tensor`可以和`numpy`中的数组进行很方便地转换。并且转换前后并没有发生内存的复制（这里文档里面没有明说？），所以修改其中某一方的值，也会引起另一方的改变。如下所示。\n\n``` py\n# Tensor 转为 np.array\na = torch.ones(5)    # out: [torch.FloatTensor of size 5]\n# 使用 numpy方法即可实现转换\nb = a.numpy()        # out: array([ 1.,  1.,  1.,  1.,  1.], dtype=float32)\n# 注意！a的值的变化同样引起b的变化\na.add_(1)\nprint(a)\nprint(b)             # a b的值都变成2\n\n# np.array 转为Tensor\nimport numpy as np\na = np.ones(5)\n# 使用torch.from_numpy即可实现转换\nb = torch.from_numpy(a)  # out: [torch.DoubleTensor of size 5]\nnp.add(a, 1, out=a)\nprint(a)\nprint(b)            # a b的值都变为2\n```\n\nPyTorch中使用GPU计算很简单，通过调用`.cuda()`方法，很容易实现GPU支持。\n\n``` py\n# let us run this cell only if CUDA is available\nif torch.cuda.is_available():\n    print('cuda is avaliable')\n    x = x.cuda()\n    y = y.cuda()\n    x + y          # 在GPU上进行计算\n```\n\n## Neural Network\n说完了数据类型`Tensor`，下一步便是如何实现一个神经网络。首先，对[自动求导](http://pytorch.org/docs/autograd.html)做一说明。\n\n我们需要关注的是`autograd.Variable`。这个东西包装了`Tensor`。一旦你完成了计算，就可以使用`.backward()`方法自动得到（以该`Variable`为叶子节点的那个）网络中参数的梯度。`Variable`有一个名叫`data`的字段，可以通过它获得被包装起来的那个原始的`Tensor`数据。同时，使用`grad`字段，可以获取梯度（也是一个`Variable`）。\n\n`Variable`是计算图的节点，同时`Function`实现了变量之间的变换。它们互相联系，构成了用于计算的无环图。每个`Variable`有一个`creator`的字段，表明了它是由哪个`Function`创建的（除了用户自己显式创建的那些，这时候`creator`是`None`）。\n\n当进行反向传播计算梯度时，如果`Variable`是标量（比如最终的`loss`是欧氏距离或者交叉熵），那么`backward()`函数不需要参数。然而如果`Variable`有不止一个元素的时候，需要为其中的每个元素指明其（由上层传导来的）梯度（也就是一个和`Variable`shape匹配的`Tensor`）。看下面的说明代码。\n\n``` py\nfrom torch.autograd import Variable\nx = Variable(torch.ones(2, 2), requires_grad = True)\nx     # x 包装了一个2x2的Tensor\n\"\"\"\nVariable containing:\n 1  1\n 1  1\n[torch.FloatTensor of size 2x2]\n\"\"\"\n# Variable进行计算\n# y was created as a result of an operation,\n# so it has a creator\ny = x + 2\ny.creator    # out: <torch.autograd._functions.basic_ops.AddConstant at 0x7fa1cc158c08>\n\nz = y * y * 3  \nout = z.mean()   # out: Variable containing: 27 [torch.FloatTensor of size 1]\n\n# let's backprop now\nout.backward()  # 其实相当于 out.backward(torch.Tensor([1.0]))\n\n# print gradients d(out)/dx\nx.grad\n\"\"\"\nVariable containing:\n 4.5000  4.5000\n 4.5000  4.5000\n[torch.FloatTensor of size 2x2]\n\"\"\"\n```\n\n下面的代码就是结果不是标量，而是普通的`Tensor`的例子。\n``` py\n# 也可以通过Tensor显式地创建Variable\nx = torch.randn(3)\nx = Variable(x, requires_grad = True)\n# 一个更复杂的 op例子\ny = x * 2\nwhile y.data.norm() < 1000:\n    y = y * 2\n\n# 计算 dy/dx\ngradients = torch.FloatTensor([0.1, 1.0, 0.0001])\ny.backward(gradients)\nx.grad\n\"\"\"\nVariable containing:\n  204.8000\n 2048.0000\n    0.2048\n[torch.FloatTensor of size 3]\n\"\"\"\n```\n\n说完了NN的构成元素`Variable`，下面可以介绍如何使用PyTorch构建网络了。这部分主要使用了`torch.nn`包。我们自定义的网络结构是由若干的`layer`组成的，我们将其设置为 `nn.Module`的子类，只要使用方法`forward(input)`就可以返回网络的`output`。下面的代码展示了如何建立一个包含有`conv`和`max-pooling`和`fc`层的简单CNN网络。\n\n``` py\nimport torch.nn as nn                 # 以我的理解，貌似有参数的都在nn里面\nimport torch.nn.functional as F       # 没有参数的（如pooling和relu）都在functional里面？\n\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(1, 6, 5) # 1 input image channel, 6 output channels, 5x5 square convolution kernel\n        self.conv2 = nn.Conv2d(6, 16, 5)\n        # 一会可以看到，input是1x32x32大小的。经过计算，conv-pooling-conv-pooling后大小为16x5x5。\n        # 所以fc层的第一个参数是 16x5x5\n        self.fc1   = nn.Linear(16*5*5, 120) # an affine operation: y = Wx + b\n        self.fc2   = nn.Linear(120, 84)\n        self.fc3   = nn.Linear(84, 10)\n\n    def forward(self, x):\n        # 你可以发现，构建计算图的过程是在前向计算中完成的，也许这可以让你体会到所谓的动态图结构\n        # 同时，我们无需实现 backward，这是被自动求导实现的\n        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2)) # Max pooling over a (2, 2) window\n        x = F.max_pool2d(F.relu(self.conv2(x)), 2) # If the size is a square you can only specify a single number\n        x = x.view(-1, self.num_flat_features(x))  # 把它拉直\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n\n    def num_flat_features(self, x):\n        size = x.size()[1:] # all dimensions except the batch dimension\n        num_features = 1\n        for s in size:\n            num_features *= s\n        return num_features\n\n# 实例化Net对象\nnet = Net()\nnet     # 给出了网络结构\n\"\"\"\nNet (\n  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n  (fc1): Linear (400 -> 120)\n  (fc2): Linear (120 -> 84)\n  (fc3): Linear (84 -> 10)\n)\n\"\"\"\n```\n\n我们可以列出网络中的所有参数。\n\n``` py\nparams = list(net.parameters())\nprint(len(params))      # out: 10, 5个权重，5个bias\nprint(params[0].size())  # conv1's weight out: torch.Size([6, 1, 5, 5])\nprint(params[1].size())  # conv1's bias, out: torch.Size([6])\n```\n\n给出网络的输入，得到网络的输出。并进行反向传播梯度。\n\n``` py\ninput = Variable(torch.randn(1, 1, 32, 32))\nout = net(input)         # 重载了()运算符？\nnet.zero_grad()          # bp前，把所有参数的grad buffer清零\nout.backward(torch.randn(1, 10))\n```\n\n注意一点，`torch.nn`只支持mini-batch。所以如果你的输入只有一个样例的时候，使用`input.unsqueeze(0)`人为给它加上一个维度，让它变成一个4-D的`Tensor`。\n\n## 网络训练\n给定target和网络的output，就可以计算loss函数了。在`torch.nn`中已经[实现好了一些loss函数](http://pytorch.org/docs/nn.html#loss-functions)。\n\n``` py\noutput = net(input)\ntarget = Variable(torch.range(1, 10))  # a dummy target, for example\n# 使用平均平方误差，即欧几里得距离\ncriterion = nn.MSELoss()\nloss = criterion(output, target)\nloss\n\"\"\"\nVariable containing:\n 38.6049\n[torch.FloatTensor of size 1]\n\"\"\"\n```\n\n网络的整体结构如下所示。\n\n```\ninput -> conv2d -> relu -> maxpool2d -> conv2d -> relu -> maxpool2d  \n      -> view -> linear -> relu -> linear -> relu -> linear\n      -> MSELoss\n      -> loss\n```\n\n我们可以使用`previous_functions`来获得该节点前面`Function`的信息。\n\n```\n# For illustration, let us follow a few steps backward\nprint(loss.creator) # MSELoss\nprint(loss.creator.previous_functions[0][0]) # Linear\nprint(loss.creator.previous_functions[0][0].previous_functions[0][0]) # ReLU\n\"\"\"\n<torch.nn._functions.thnn.auto.MSELoss object at 0x7fa18011db40>\n<torch.nn._functions.linear.Linear object at 0x7fa18011da78>\n<torch.nn._functions.thnn.auto.Threshold object at 0x7fa18011d9b0>\n\"\"\"\n```\n\n进行反向传播后，让我们查看一下参数的变化。\n\n``` py\n# now we shall call loss.backward(), and have a look at conv1's bias gradients before and after the backward.\nnet.zero_grad() # zeroes the gradient buffers of all parameters\nprint('conv1.bias.grad before backward')\nprint(net.conv1.bias.grad)\nloss.backward()\nprint('conv1.bias.grad after backward')\nprint(net.conv1.bias.grad)\n```\n\n计算梯度后，自然需要更新参数了。简单的方法可以自己手写：\n\n``` py\nlearning_rate = 0.01\nfor f in net.parameters():\n    f.data.sub_(f.grad.data * learning_rate)\n```\n\n不过，`torch.optim`中已经提供了若干优化方法（SGD, Nesterov-SGD, Adam, RMSProp, etc）。如下所示。\n\n``` py\nimport torch.optim as optim\n# create your optimizer\noptimizer = optim.SGD(net.parameters(), lr = 0.01)\n# in your training loop:\noptimizer.zero_grad() # zero the gradient buffers\noutput = net(input)\nloss = criterion(output, target)\nloss.backward()\noptimizer.step() # Does the update\n```\n\n## 数据载入\n由于PyTorch的Python接口和`np.array`之间的方便转换，所以可以使用其他任何数据读入的方法（例如OpenCV等）。特别地，对于vision的数据，PyTorch提供了`torchvision`包，可以方便地载入常用的数据集（Imagenet, CIFAR10, MNIST, etc），同时提供了图像的各种变换方法。下面以CIFAR为例子。\n\n``` py\nimport torchvision\nimport torchvision.transforms as transforms\n\n# The output of torchvision datasets are PILImage images of range [0, 1].\n# We transform them to Tensors of normalized range [-1, 1]\n# Compose: Composes several transforms together.\n# see http://pytorch.org/docs/torchvision/transforms.html?highlight=transforms\ntransform=transforms.Compose([transforms.ToTensor(),\n                              transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n                             ])   # torchvision.transforms.Normalize(mean, std)\n# 读取CIFAR10数据集                             \ntrainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n# 使用DataLoader\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n                                          shuffle=True, num_workers=2)\n# Test集，设置train = False\ntestset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\ntestloader = torch.utils.data.DataLoader(testset, batch_size=4,\n                                          shuffle=False, num_workers=2)\nclasses = ('plane', 'car', 'bird', 'cat',\n           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n```\n\n接下来，我们对上面部分的CNN网络进行小修，设置第一个`conv`层接受3通道的输入。并使用交叉熵定义loss。\n\n``` py\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(3, 6, 5)\n        self.pool  = nn.MaxPool2d(2,2)\n        self.conv2 = nn.Conv2d(6, 16, 5)\n        self.fc1   = nn.Linear(16*5*5, 120)\n        self.fc2   = nn.Linear(120, 84)\n        self.fc3   = nn.Linear(84, 10)\n\n    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = x.view(-1, 16*5*5)\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n\nnet = Net()\n# use a Classification Cross-Entropy loss\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n```\n\n接下来，我们进行模型的训练。我们loop over整个dataset两次，对每个mini-batch进行参数的更新。并且设置每隔2000个mini-batch打印一次loss。\n\n``` py\nfor epoch in range(2): # loop over the dataset multiple times\n\n    running_loss = 0.0\n    for i, data in enumerate(trainloader, 0):\n        # get the inputs\n        inputs, labels = data\n\n        # wrap them in Variable\n        inputs, labels = Variable(inputs), Variable(labels)\n\n        # zero the parameter gradients\n        optimizer.zero_grad()\n\n        # forward + backward + optimize\n        outputs = net(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()        \n        optimizer.step()\n\n        # print statistics\n        running_loss += loss.data[0]\n        if i % 2000 == 1999: # print every 2000 mini-batches\n            print('[%d, %5d] loss: %.3f' % (epoch+1, i+1, running_loss / 2000))\n            running_loss = 0.0\nprint('Finished Training')\n```\n\n我们在测试集上选取一个mini-batch（也就是4张，见上面`testloader`的定义），进行测试。\n\n``` py\ndataiter = iter(testloader)\nimages, labels = dataiter.next()   # 得到image和对应的label\noutputs = net(Variable(images))\n\n# the outputs are energies for the 10 classes.\n# Higher the energy for a class, the more the network\n# thinks that the image is of the particular class\n# So, let's get the index of the highest energy\n_, predicted = torch.max(outputs.data, 1)   # 找出分数最高的对应的channel，即为top-1类别\n\nprint('Predicted: ', ' '.join('%5s'% classes[predicted[j][0]] for j in range(4)))\n```\n\n测试一下整个测试集合上的表现。\n\n``` py\ncorrect = 0\ntotal = 0\nfor data in testloader:     # 每一个test mini-batch\n    images, labels = data\n    outputs = net(Variable(images))\n    _, predicted = torch.max(outputs.data, 1)\n    total += labels.size(0)\n    correct += (predicted == labels).sum()\n\nprint('Accuracy of the network on the 10000 test images: %d %%' % (100 * correct / total))\n```\n\n对哪一类的预测精度更高呢？\n\n``` py\nclass_correct = list(0. for i in range(10))\nclass_total = list(0. for i in range(10))\nfor data in testloader:\n    images, labels = data\n    outputs = net(Variable(images))\n    _, predicted = torch.max(outputs.data, 1)\n    c = (predicted == labels).squeeze()\n    for i in range(4):\n        label = labels[i]\n        class_correct[label] += c[i]\n        class_total[label] += 1\n```\n\n上面这些训练和测试都是在CPU上进行的，如何迁移到GPU？很简单，同样用`.cuda()`方法就行了。\n\n``` py\nnet.cuda()\n```\n\n不过记得在每次训练测试的迭代中，`images`和`label`也要传送到GPU上才可以。\n\n``` py\ninputs, labels = Variable(inputs.cuda()), Variable(labels.cuda())\n```\n## 更多的例子和教程\n[更多的例子](https://github.com/pytorch/examples)\n[更多的教程](https://github.com/pytorch/tutorials)\n","slug":"pytorch-tutor-01","published":1,"updated":"2017-02-28T15:30:01.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cizve07bk000zl61hfpyggucp","content":"<p>这是一份阅读PyTorch教程的笔记，记录jupyter notebook的关键点。原地址位于<a href=\"https://github.com/pytorch/tutorials/blob/master/Deep%20Learning%20with%20PyTorch.ipynb\" target=\"_blank\" rel=\"external\">GitHub repo</a>。<br><img src=\"/img/pytorch_logo.png\" alt=\"PyTorch Logo\"></p>\n<a id=\"more\"></a>\n<h2 id=\"PyTorch简介\"><a href=\"#PyTorch简介\" class=\"headerlink\" title=\"PyTorch简介\"></a>PyTorch简介</h2><p><a href=\"https://github.com/pytorch/pytorch\" target=\"_blank\" rel=\"external\">PyTorch</a>是一个较新的深度学习框架。从名字可以看出，其和Torch不同之处在于PyTorch使用了Python作为开发语言，所谓“Python first”。一方面，使用者可以将其作为加入了GPU支持的<code>numpy</code>，另一方面，PyTorch也是强大的深度学习框架。</p>\n<p>目前有很多深度学习框架，PyTorch主推的功能是动态网络模型。例如在Caffe中，使用者通过编写网络结构的<code>prototxt</code>进行定义，网络结构是不能变化的。而PyTorch中的网络结构不再只能是一成不变的。同时PyTorch实现了多达上百种op的自动求导（AutoGrad）。</p>\n<h2 id=\"Tensors\"><a href=\"#Tensors\" class=\"headerlink\" title=\"Tensors\"></a>Tensors</h2><p><code>Tensor</code>，即<code>numpy</code>中的多维数组。上面已经提到过，PyTorch对其加入了GPU支持。同时，PyTorch中的<code>Tensor</code>可以与<code>numpy</code>中的<code>array</code>很方便地进行互相转换。</p>\n<p>通过<code>Tensor(shape)</code>便可以创建所需要大小的<code>tensor</code>。如下所示。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\">x = torch.Tensor(<span class=\"number\">5</span>, <span class=\"number\">3</span>)  <span class=\"comment\"># construct a 5x3 matrix, uninitialized</span></div><div class=\"line\"><span class=\"comment\"># 或者随机填充</span></div><div class=\"line\">y = torch.rand(<span class=\"number\">5</span>, <span class=\"number\">3</span>)    <span class=\"comment\"># construct a randomly initialized matrix</span></div><div class=\"line\"><span class=\"comment\"># 使用size方法可以获得tensor的shape信息，torch.Size 可以看做 tuple</span></div><div class=\"line\">x.size()                <span class=\"comment\"># out: torch.Size([5, 3])</span></div></pre></td></tr></table></figure>\n<p>PyTorch中已经实现了很多常用的<code>op</code>，如下所示。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\"># addition: syntax 1</span></div><div class=\"line\">x + y                  <span class=\"comment\"># out: [torch.FloatTensor of size 5x3]</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># addition: syntax 2</span></div><div class=\"line\">torch.add(x, y)        <span class=\"comment\"># 或者使用torch包中的显式的op名称</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># addition: giving an output tensor</span></div><div class=\"line\">result = torch.Tensor(<span class=\"number\">5</span>, <span class=\"number\">3</span>)  <span class=\"comment\"># 预先定义size</span></div><div class=\"line\">torch.add(x, y, out=result)  <span class=\"comment\"># 结果被填充到变量result</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># 对于加法运算，其实没必要这么复杂</span></div><div class=\"line\">out = x + y                  <span class=\"comment\"># 无需预先定义size</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># torch包中带有下划线的op说明是就地进行的，如下所示</span></div><div class=\"line\"><span class=\"comment\"># addition: in-place</span></div><div class=\"line\">y.add_(x)              <span class=\"comment\"># 将x加到y上</span></div><div class=\"line\"><span class=\"comment\"># 其他的例子: x.copy_(y), x.t_().</span></div></pre></td></tr></table></figure>\n<p>PyTorch中的元素索引方式和<code>numpy</code>相同。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\"># standard numpy-like indexing with all bells and whistles</span></div><div class=\"line\">x[:,<span class=\"number\">1</span>]                 <span class=\"comment\"># out: [torch.FloatTensor of size 5]</span></div></pre></td></tr></table></figure>\n<p>对于更多的<code>op</code>，可以参见PyTorch的<a href=\"http://pytorch.org/docs/torch.html\" target=\"_blank\" rel=\"external\">文档页面</a>。</p>\n<p><code>Tensor</code>可以和<code>numpy</code>中的数组进行很方便地转换。并且转换前后并没有发生内存的复制（这里文档里面没有明说？），所以修改其中某一方的值，也会引起另一方的改变。如下所示。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\"># Tensor 转为 np.array</span></div><div class=\"line\">a = torch.ones(<span class=\"number\">5</span>)    <span class=\"comment\"># out: [torch.FloatTensor of size 5]</span></div><div class=\"line\"><span class=\"comment\"># 使用 numpy方法即可实现转换</span></div><div class=\"line\">b = a.numpy()        <span class=\"comment\"># out: array([ 1.,  1.,  1.,  1.,  1.], dtype=float32)</span></div><div class=\"line\"><span class=\"comment\"># 注意！a的值的变化同样引起b的变化</span></div><div class=\"line\">a.add_(<span class=\"number\">1</span>)</div><div class=\"line\">print(a)</div><div class=\"line\">print(b)             <span class=\"comment\"># a b的值都变成2</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># np.array 转为Tensor</span></div><div class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</div><div class=\"line\">a = np.ones(<span class=\"number\">5</span>)</div><div class=\"line\"><span class=\"comment\"># 使用torch.from_numpy即可实现转换</span></div><div class=\"line\">b = torch.from_numpy(a)  <span class=\"comment\"># out: [torch.DoubleTensor of size 5]</span></div><div class=\"line\">np.add(a, <span class=\"number\">1</span>, out=a)</div><div class=\"line\">print(a)</div><div class=\"line\">print(b)            <span class=\"comment\"># a b的值都变为2</span></div></pre></td></tr></table></figure>\n<p>PyTorch中使用GPU计算很简单，通过调用<code>.cuda()</code>方法，很容易实现GPU支持。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\"># let us run this cell only if CUDA is available</span></div><div class=\"line\"><span class=\"keyword\">if</span> torch.cuda.is_available():</div><div class=\"line\">    print(<span class=\"string\">'cuda is avaliable'</span>)</div><div class=\"line\">    x = x.cuda()</div><div class=\"line\">    y = y.cuda()</div><div class=\"line\">    x + y          <span class=\"comment\"># 在GPU上进行计算</span></div></pre></td></tr></table></figure>\n<h2 id=\"Neural-Network\"><a href=\"#Neural-Network\" class=\"headerlink\" title=\"Neural Network\"></a>Neural Network</h2><p>说完了数据类型<code>Tensor</code>，下一步便是如何实现一个神经网络。首先，对<a href=\"http://pytorch.org/docs/autograd.html\" target=\"_blank\" rel=\"external\">自动求导</a>做一说明。</p>\n<p>我们需要关注的是<code>autograd.Variable</code>。这个东西包装了<code>Tensor</code>。一旦你完成了计算，就可以使用<code>.backward()</code>方法自动得到（以该<code>Variable</code>为叶子节点的那个）网络中参数的梯度。<code>Variable</code>有一个名叫<code>data</code>的字段，可以通过它获得被包装起来的那个原始的<code>Tensor</code>数据。同时，使用<code>grad</code>字段，可以获取梯度（也是一个<code>Variable</code>）。</p>\n<p><code>Variable</code>是计算图的节点，同时<code>Function</code>实现了变量之间的变换。它们互相联系，构成了用于计算的无环图。每个<code>Variable</code>有一个<code>creator</code>的字段，表明了它是由哪个<code>Function</code>创建的（除了用户自己显式创建的那些，这时候<code>creator</code>是<code>None</code>）。</p>\n<p>当进行反向传播计算梯度时，如果<code>Variable</code>是标量（比如最终的<code>loss</code>是欧氏距离或者交叉熵），那么<code>backward()</code>函数不需要参数。然而如果<code>Variable</code>有不止一个元素的时候，需要为其中的每个元素指明其（由上层传导来的）梯度（也就是一个和<code>Variable</code>shape匹配的<code>Tensor</code>）。看下面的说明代码。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">from</span> torch.autograd <span class=\"keyword\">import</span> Variable</div><div class=\"line\">x = Variable(torch.ones(<span class=\"number\">2</span>, <span class=\"number\">2</span>), requires_grad = <span class=\"keyword\">True</span>)</div><div class=\"line\">x     <span class=\"comment\"># x 包装了一个2x2的Tensor</span></div><div class=\"line\"><span class=\"string\">\"\"\"</span></div><div class=\"line\">Variable containing:</div><div class=\"line\"> 1  1</div><div class=\"line\"> 1  1</div><div class=\"line\">[torch.FloatTensor of size 2x2]</div><div class=\"line\">\"\"\"</div><div class=\"line\"><span class=\"comment\"># Variable进行计算</span></div><div class=\"line\"><span class=\"comment\"># y was created as a result of an operation,</span></div><div class=\"line\"><span class=\"comment\"># so it has a creator</span></div><div class=\"line\">y = x + <span class=\"number\">2</span></div><div class=\"line\">y.creator    <span class=\"comment\"># out: &lt;torch.autograd._functions.basic_ops.AddConstant at 0x7fa1cc158c08&gt;</span></div><div class=\"line\"></div><div class=\"line\">z = y * y * <span class=\"number\">3</span>  </div><div class=\"line\">out = z.mean()   <span class=\"comment\"># out: Variable containing: 27 [torch.FloatTensor of size 1]</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># let's backprop now</span></div><div class=\"line\">out.backward()  <span class=\"comment\"># 其实相当于 out.backward(torch.Tensor([1.0]))</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># print gradients d(out)/dx</span></div><div class=\"line\">x.grad</div><div class=\"line\"><span class=\"string\">\"\"\"</span></div><div class=\"line\">Variable containing:</div><div class=\"line\"> 4.5000  4.5000</div><div class=\"line\"> 4.5000  4.5000</div><div class=\"line\">[torch.FloatTensor of size 2x2]</div><div class=\"line\">\"\"\"</div></pre></td></tr></table></figure>\n<p>下面的代码就是结果不是标量，而是普通的<code>Tensor</code>的例子。<br><figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\"># 也可以通过Tensor显式地创建Variable</span></div><div class=\"line\">x = torch.randn(<span class=\"number\">3</span>)</div><div class=\"line\">x = Variable(x, requires_grad = <span class=\"keyword\">True</span>)</div><div class=\"line\"><span class=\"comment\"># 一个更复杂的 op例子</span></div><div class=\"line\">y = x * <span class=\"number\">2</span></div><div class=\"line\"><span class=\"keyword\">while</span> y.data.norm() &lt; <span class=\"number\">1000</span>:</div><div class=\"line\">    y = y * <span class=\"number\">2</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># 计算 dy/dx</span></div><div class=\"line\">gradients = torch.FloatTensor([<span class=\"number\">0.1</span>, <span class=\"number\">1.0</span>, <span class=\"number\">0.0001</span>])</div><div class=\"line\">y.backward(gradients)</div><div class=\"line\">x.grad</div><div class=\"line\"><span class=\"string\">\"\"\"</span></div><div class=\"line\">Variable containing:</div><div class=\"line\">  204.8000</div><div class=\"line\"> 2048.0000</div><div class=\"line\">    0.2048</div><div class=\"line\">[torch.FloatTensor of size 3]</div><div class=\"line\">\"\"\"</div></pre></td></tr></table></figure></p>\n<p>说完了NN的构成元素<code>Variable</code>，下面可以介绍如何使用PyTorch构建网络了。这部分主要使用了<code>torch.nn</code>包。我们自定义的网络结构是由若干的<code>layer</code>组成的，我们将其设置为 <code>nn.Module</code>的子类，只要使用方法<code>forward(input)</code>就可以返回网络的<code>output</code>。下面的代码展示了如何建立一个包含有<code>conv</code>和<code>max-pooling</code>和<code>fc</code>层的简单CNN网络。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">import</span> torch.nn <span class=\"keyword\">as</span> nn                 <span class=\"comment\"># 以我的理解，貌似有参数的都在nn里面</span></div><div class=\"line\"><span class=\"keyword\">import</span> torch.nn.functional <span class=\"keyword\">as</span> F       <span class=\"comment\"># 没有参数的（如pooling和relu）都在functional里面？</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Net</span><span class=\"params\">(nn.Module)</span>:</span></div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self)</span>:</span></div><div class=\"line\">        super(Net, self).__init__()</div><div class=\"line\">        self.conv1 = nn.Conv2d(<span class=\"number\">1</span>, <span class=\"number\">6</span>, <span class=\"number\">5</span>) <span class=\"comment\"># 1 input image channel, 6 output channels, 5x5 square convolution kernel</span></div><div class=\"line\">        self.conv2 = nn.Conv2d(<span class=\"number\">6</span>, <span class=\"number\">16</span>, <span class=\"number\">5</span>)</div><div class=\"line\">        <span class=\"comment\"># 一会可以看到，input是1x32x32大小的。经过计算，conv-pooling-conv-pooling后大小为16x5x5。</span></div><div class=\"line\">        <span class=\"comment\"># 所以fc层的第一个参数是 16x5x5</span></div><div class=\"line\">        self.fc1   = nn.Linear(<span class=\"number\">16</span>*<span class=\"number\">5</span>*<span class=\"number\">5</span>, <span class=\"number\">120</span>) <span class=\"comment\"># an affine operation: y = Wx + b</span></div><div class=\"line\">        self.fc2   = nn.Linear(<span class=\"number\">120</span>, <span class=\"number\">84</span>)</div><div class=\"line\">        self.fc3   = nn.Linear(<span class=\"number\">84</span>, <span class=\"number\">10</span>)</div><div class=\"line\"></div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">forward</span><span class=\"params\">(self, x)</span>:</span></div><div class=\"line\">        <span class=\"comment\"># 你可以发现，构建计算图的过程是在前向计算中完成的，也许这可以让你体会到所谓的动态图结构</span></div><div class=\"line\">        <span class=\"comment\"># 同时，我们无需实现 backward，这是被自动求导实现的</span></div><div class=\"line\">        x = F.max_pool2d(F.relu(self.conv1(x)), (<span class=\"number\">2</span>, <span class=\"number\">2</span>)) <span class=\"comment\"># Max pooling over a (2, 2) window</span></div><div class=\"line\">        x = F.max_pool2d(F.relu(self.conv2(x)), <span class=\"number\">2</span>) <span class=\"comment\"># If the size is a square you can only specify a single number</span></div><div class=\"line\">        x = x.view(<span class=\"number\">-1</span>, self.num_flat_features(x))  <span class=\"comment\"># 把它拉直</span></div><div class=\"line\">        x = F.relu(self.fc1(x))</div><div class=\"line\">        x = F.relu(self.fc2(x))</div><div class=\"line\">        x = self.fc3(x)</div><div class=\"line\">        <span class=\"keyword\">return</span> x</div><div class=\"line\"></div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">num_flat_features</span><span class=\"params\">(self, x)</span>:</span></div><div class=\"line\">        size = x.size()[<span class=\"number\">1</span>:] <span class=\"comment\"># all dimensions except the batch dimension</span></div><div class=\"line\">        num_features = <span class=\"number\">1</span></div><div class=\"line\">        <span class=\"keyword\">for</span> s <span class=\"keyword\">in</span> size:</div><div class=\"line\">            num_features *= s</div><div class=\"line\">        <span class=\"keyword\">return</span> num_features</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># 实例化Net对象</span></div><div class=\"line\">net = Net()</div><div class=\"line\">net     <span class=\"comment\"># 给出了网络结构</span></div><div class=\"line\"><span class=\"string\">\"\"\"</span></div><div class=\"line\">Net (</div><div class=\"line\">  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))</div><div class=\"line\">  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))</div><div class=\"line\">  (fc1): Linear (400 -&gt; 120)</div><div class=\"line\">  (fc2): Linear (120 -&gt; 84)</div><div class=\"line\">  (fc3): Linear (84 -&gt; 10)</div><div class=\"line\">)</div><div class=\"line\">\"\"\"</div></pre></td></tr></table></figure>\n<p>我们可以列出网络中的所有参数。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\">params = list(net.parameters())</div><div class=\"line\">print(len(params))      <span class=\"comment\"># out: 10, 5个权重，5个bias</span></div><div class=\"line\">print(params[<span class=\"number\">0</span>].size())  <span class=\"comment\"># conv1's weight out: torch.Size([6, 1, 5, 5])</span></div><div class=\"line\">print(params[<span class=\"number\">1</span>].size())  <span class=\"comment\"># conv1's bias, out: torch.Size([6])</span></div></pre></td></tr></table></figure>\n<p>给出网络的输入，得到网络的输出。并进行反向传播梯度。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\">input = Variable(torch.randn(<span class=\"number\">1</span>, <span class=\"number\">1</span>, <span class=\"number\">32</span>, <span class=\"number\">32</span>))</div><div class=\"line\">out = net(input)         <span class=\"comment\"># 重载了()运算符？</span></div><div class=\"line\">net.zero_grad()          <span class=\"comment\"># bp前，把所有参数的grad buffer清零</span></div><div class=\"line\">out.backward(torch.randn(<span class=\"number\">1</span>, <span class=\"number\">10</span>))</div></pre></td></tr></table></figure>\n<p>注意一点，<code>torch.nn</code>只支持mini-batch。所以如果你的输入只有一个样例的时候，使用<code>input.unsqueeze(0)</code>人为给它加上一个维度，让它变成一个4-D的<code>Tensor</code>。</p>\n<h2 id=\"网络训练\"><a href=\"#网络训练\" class=\"headerlink\" title=\"网络训练\"></a>网络训练</h2><p>给定target和网络的output，就可以计算loss函数了。在<code>torch.nn</code>中已经<a href=\"http://pytorch.org/docs/nn.html#loss-functions\" target=\"_blank\" rel=\"external\">实现好了一些loss函数</a>。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div></pre></td><td class=\"code\"><pre><div class=\"line\">output = net(input)</div><div class=\"line\">target = Variable(torch.range(<span class=\"number\">1</span>, <span class=\"number\">10</span>))  <span class=\"comment\"># a dummy target, for example</span></div><div class=\"line\"><span class=\"comment\"># 使用平均平方误差，即欧几里得距离</span></div><div class=\"line\">criterion = nn.MSELoss()</div><div class=\"line\">loss = criterion(output, target)</div><div class=\"line\">loss</div><div class=\"line\"><span class=\"string\">\"\"\"</span></div><div class=\"line\">Variable containing:</div><div class=\"line\"> 38.6049</div><div class=\"line\">[torch.FloatTensor of size 1]</div><div class=\"line\">\"\"\"</div></pre></td></tr></table></figure>\n<p>网络的整体结构如下所示。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\">input -&gt; conv2d -&gt; relu -&gt; maxpool2d -&gt; conv2d -&gt; relu -&gt; maxpool2d  </div><div class=\"line\">      -&gt; view -&gt; linear -&gt; relu -&gt; linear -&gt; relu -&gt; linear</div><div class=\"line\">      -&gt; MSELoss</div><div class=\"line\">      -&gt; loss</div></pre></td></tr></table></figure>\n<p>我们可以使用<code>previous_functions</code>来获得该节点前面<code>Function</code>的信息。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div></pre></td><td class=\"code\"><pre><div class=\"line\"># For illustration, let us follow a few steps backward</div><div class=\"line\">print(loss.creator) # MSELoss</div><div class=\"line\">print(loss.creator.previous_functions[0][0]) # Linear</div><div class=\"line\">print(loss.creator.previous_functions[0][0].previous_functions[0][0]) # ReLU</div><div class=\"line\">&quot;&quot;&quot;</div><div class=\"line\">&lt;torch.nn._functions.thnn.auto.MSELoss object at 0x7fa18011db40&gt;</div><div class=\"line\">&lt;torch.nn._functions.linear.Linear object at 0x7fa18011da78&gt;</div><div class=\"line\">&lt;torch.nn._functions.thnn.auto.Threshold object at 0x7fa18011d9b0&gt;</div><div class=\"line\">&quot;&quot;&quot;</div></pre></td></tr></table></figure>\n<p>进行反向传播后，让我们查看一下参数的变化。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\"># now we shall call loss.backward(), and have a look at conv1's bias gradients before and after the backward.</span></div><div class=\"line\">net.zero_grad() <span class=\"comment\"># zeroes the gradient buffers of all parameters</span></div><div class=\"line\">print(<span class=\"string\">'conv1.bias.grad before backward'</span>)</div><div class=\"line\">print(net.conv1.bias.grad)</div><div class=\"line\">loss.backward()</div><div class=\"line\">print(<span class=\"string\">'conv1.bias.grad after backward'</span>)</div><div class=\"line\">print(net.conv1.bias.grad)</div></pre></td></tr></table></figure>\n<p>计算梯度后，自然需要更新参数了。简单的方法可以自己手写：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\">learning_rate = <span class=\"number\">0.01</span></div><div class=\"line\"><span class=\"keyword\">for</span> f <span class=\"keyword\">in</span> net.parameters():</div><div class=\"line\">    f.data.sub_(f.grad.data * learning_rate)</div></pre></td></tr></table></figure>\n<p>不过，<code>torch.optim</code>中已经提供了若干优化方法（SGD, Nesterov-SGD, Adam, RMSProp, etc）。如下所示。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">import</span> torch.optim <span class=\"keyword\">as</span> optim</div><div class=\"line\"><span class=\"comment\"># create your optimizer</span></div><div class=\"line\">optimizer = optim.SGD(net.parameters(), lr = <span class=\"number\">0.01</span>)</div><div class=\"line\"><span class=\"comment\"># in your training loop:</span></div><div class=\"line\">optimizer.zero_grad() <span class=\"comment\"># zero the gradient buffers</span></div><div class=\"line\">output = net(input)</div><div class=\"line\">loss = criterion(output, target)</div><div class=\"line\">loss.backward()</div><div class=\"line\">optimizer.step() <span class=\"comment\"># Does the update</span></div></pre></td></tr></table></figure>\n<h2 id=\"数据载入\"><a href=\"#数据载入\" class=\"headerlink\" title=\"数据载入\"></a>数据载入</h2><p>由于PyTorch的Python接口和<code>np.array</code>之间的方便转换，所以可以使用其他任何数据读入的方法（例如OpenCV等）。特别地，对于vision的数据，PyTorch提供了<code>torchvision</code>包，可以方便地载入常用的数据集（Imagenet, CIFAR10, MNIST, etc），同时提供了图像的各种变换方法。下面以CIFAR为例子。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">import</span> torchvision</div><div class=\"line\"><span class=\"keyword\">import</span> torchvision.transforms <span class=\"keyword\">as</span> transforms</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># The output of torchvision datasets are PILImage images of range [0, 1].</span></div><div class=\"line\"><span class=\"comment\"># We transform them to Tensors of normalized range [-1, 1]</span></div><div class=\"line\"><span class=\"comment\"># Compose: Composes several transforms together.</span></div><div class=\"line\"><span class=\"comment\"># see http://pytorch.org/docs/torchvision/transforms.html?highlight=transforms</span></div><div class=\"line\">transform=transforms.Compose([transforms.ToTensor(),</div><div class=\"line\">                              transforms.Normalize((<span class=\"number\">0.5</span>, <span class=\"number\">0.5</span>, <span class=\"number\">0.5</span>), (<span class=\"number\">0.5</span>, <span class=\"number\">0.5</span>, <span class=\"number\">0.5</span>)),</div><div class=\"line\">                             ])   <span class=\"comment\"># torchvision.transforms.Normalize(mean, std)</span></div><div class=\"line\"><span class=\"comment\"># 读取CIFAR10数据集                             </span></div><div class=\"line\">trainset = torchvision.datasets.CIFAR10(root=<span class=\"string\">'./data'</span>, train=<span class=\"keyword\">True</span>, download=<span class=\"keyword\">True</span>, transform=transform)</div><div class=\"line\"><span class=\"comment\"># 使用DataLoader</span></div><div class=\"line\">trainloader = torch.utils.data.DataLoader(trainset, batch_size=<span class=\"number\">4</span>,</div><div class=\"line\">                                          shuffle=<span class=\"keyword\">True</span>, num_workers=<span class=\"number\">2</span>)</div><div class=\"line\"><span class=\"comment\"># Test集，设置train = False</span></div><div class=\"line\">testset = torchvision.datasets.CIFAR10(root=<span class=\"string\">'./data'</span>, train=<span class=\"keyword\">False</span>, download=<span class=\"keyword\">True</span>, transform=transform)</div><div class=\"line\">testloader = torch.utils.data.DataLoader(testset, batch_size=<span class=\"number\">4</span>,</div><div class=\"line\">                                          shuffle=<span class=\"keyword\">False</span>, num_workers=<span class=\"number\">2</span>)</div><div class=\"line\">classes = (<span class=\"string\">'plane'</span>, <span class=\"string\">'car'</span>, <span class=\"string\">'bird'</span>, <span class=\"string\">'cat'</span>,</div><div class=\"line\">           <span class=\"string\">'deer'</span>, <span class=\"string\">'dog'</span>, <span class=\"string\">'frog'</span>, <span class=\"string\">'horse'</span>, <span class=\"string\">'ship'</span>, <span class=\"string\">'truck'</span>)</div></pre></td></tr></table></figure>\n<p>接下来，我们对上面部分的CNN网络进行小修，设置第一个<code>conv</code>层接受3通道的输入。并使用交叉熵定义loss。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Net</span><span class=\"params\">(nn.Module)</span>:</span></div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self)</span>:</span></div><div class=\"line\">        super(Net, self).__init__()</div><div class=\"line\">        self.conv1 = nn.Conv2d(<span class=\"number\">3</span>, <span class=\"number\">6</span>, <span class=\"number\">5</span>)</div><div class=\"line\">        self.pool  = nn.MaxPool2d(<span class=\"number\">2</span>,<span class=\"number\">2</span>)</div><div class=\"line\">        self.conv2 = nn.Conv2d(<span class=\"number\">6</span>, <span class=\"number\">16</span>, <span class=\"number\">5</span>)</div><div class=\"line\">        self.fc1   = nn.Linear(<span class=\"number\">16</span>*<span class=\"number\">5</span>*<span class=\"number\">5</span>, <span class=\"number\">120</span>)</div><div class=\"line\">        self.fc2   = nn.Linear(<span class=\"number\">120</span>, <span class=\"number\">84</span>)</div><div class=\"line\">        self.fc3   = nn.Linear(<span class=\"number\">84</span>, <span class=\"number\">10</span>)</div><div class=\"line\"></div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">forward</span><span class=\"params\">(self, x)</span>:</span></div><div class=\"line\">        x = self.pool(F.relu(self.conv1(x)))</div><div class=\"line\">        x = self.pool(F.relu(self.conv2(x)))</div><div class=\"line\">        x = x.view(<span class=\"number\">-1</span>, <span class=\"number\">16</span>*<span class=\"number\">5</span>*<span class=\"number\">5</span>)</div><div class=\"line\">        x = F.relu(self.fc1(x))</div><div class=\"line\">        x = F.relu(self.fc2(x))</div><div class=\"line\">        x = self.fc3(x)</div><div class=\"line\">        <span class=\"keyword\">return</span> x</div><div class=\"line\"></div><div class=\"line\">net = Net()</div><div class=\"line\"><span class=\"comment\"># use a Classification Cross-Entropy loss</span></div><div class=\"line\">criterion = nn.CrossEntropyLoss()</div><div class=\"line\">optimizer = optim.SGD(net.parameters(), lr=<span class=\"number\">0.001</span>, momentum=<span class=\"number\">0.9</span>)</div></pre></td></tr></table></figure>\n<p>接下来，我们进行模型的训练。我们loop over整个dataset两次，对每个mini-batch进行参数的更新。并且设置每隔2000个mini-batch打印一次loss。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">for</span> epoch <span class=\"keyword\">in</span> range(<span class=\"number\">2</span>): <span class=\"comment\"># loop over the dataset multiple times</span></div><div class=\"line\"></div><div class=\"line\">    running_loss = <span class=\"number\">0.0</span></div><div class=\"line\">    <span class=\"keyword\">for</span> i, data <span class=\"keyword\">in</span> enumerate(trainloader, <span class=\"number\">0</span>):</div><div class=\"line\">        <span class=\"comment\"># get the inputs</span></div><div class=\"line\">        inputs, labels = data</div><div class=\"line\"></div><div class=\"line\">        <span class=\"comment\"># wrap them in Variable</span></div><div class=\"line\">        inputs, labels = Variable(inputs), Variable(labels)</div><div class=\"line\"></div><div class=\"line\">        <span class=\"comment\"># zero the parameter gradients</span></div><div class=\"line\">        optimizer.zero_grad()</div><div class=\"line\"></div><div class=\"line\">        <span class=\"comment\"># forward + backward + optimize</span></div><div class=\"line\">        outputs = net(inputs)</div><div class=\"line\">        loss = criterion(outputs, labels)</div><div class=\"line\">        loss.backward()        </div><div class=\"line\">        optimizer.step()</div><div class=\"line\"></div><div class=\"line\">        <span class=\"comment\"># print statistics</span></div><div class=\"line\">        running_loss += loss.data[<span class=\"number\">0</span>]</div><div class=\"line\">        <span class=\"keyword\">if</span> i % <span class=\"number\">2000</span> == <span class=\"number\">1999</span>: <span class=\"comment\"># print every 2000 mini-batches</span></div><div class=\"line\">            print(<span class=\"string\">'[%d, %5d] loss: %.3f'</span> % (epoch+<span class=\"number\">1</span>, i+<span class=\"number\">1</span>, running_loss / <span class=\"number\">2000</span>))</div><div class=\"line\">            running_loss = <span class=\"number\">0.0</span></div><div class=\"line\">print(<span class=\"string\">'Finished Training'</span>)</div></pre></td></tr></table></figure>\n<p>我们在测试集上选取一个mini-batch（也就是4张，见上面<code>testloader</code>的定义），进行测试。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div></pre></td><td class=\"code\"><pre><div class=\"line\">dataiter = iter(testloader)</div><div class=\"line\">images, labels = dataiter.next()   <span class=\"comment\"># 得到image和对应的label</span></div><div class=\"line\">outputs = net(Variable(images))</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># the outputs are energies for the 10 classes.</span></div><div class=\"line\"><span class=\"comment\"># Higher the energy for a class, the more the network</span></div><div class=\"line\"><span class=\"comment\"># thinks that the image is of the particular class</span></div><div class=\"line\"><span class=\"comment\"># So, let's get the index of the highest energy</span></div><div class=\"line\">_, predicted = torch.max(outputs.data, <span class=\"number\">1</span>)   <span class=\"comment\"># 找出分数最高的对应的channel，即为top-1类别</span></div><div class=\"line\"></div><div class=\"line\">print(<span class=\"string\">'Predicted: '</span>, <span class=\"string\">' '</span>.join(<span class=\"string\">'%5s'</span>% classes[predicted[j][<span class=\"number\">0</span>]] <span class=\"keyword\">for</span> j <span class=\"keyword\">in</span> range(<span class=\"number\">4</span>)))</div></pre></td></tr></table></figure>\n<p>测试一下整个测试集合上的表现。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div></pre></td><td class=\"code\"><pre><div class=\"line\">correct = <span class=\"number\">0</span></div><div class=\"line\">total = <span class=\"number\">0</span></div><div class=\"line\"><span class=\"keyword\">for</span> data <span class=\"keyword\">in</span> testloader:     <span class=\"comment\"># 每一个test mini-batch</span></div><div class=\"line\">    images, labels = data</div><div class=\"line\">    outputs = net(Variable(images))</div><div class=\"line\">    _, predicted = torch.max(outputs.data, <span class=\"number\">1</span>)</div><div class=\"line\">    total += labels.size(<span class=\"number\">0</span>)</div><div class=\"line\">    correct += (predicted == labels).sum()</div><div class=\"line\"></div><div class=\"line\">print(<span class=\"string\">'Accuracy of the network on the 10000 test images: %d %%'</span> % (<span class=\"number\">100</span> * correct / total))</div></pre></td></tr></table></figure>\n<p>对哪一类的预测精度更高呢？</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div></pre></td><td class=\"code\"><pre><div class=\"line\">class_correct = list(<span class=\"number\">0.</span> <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">10</span>))</div><div class=\"line\">class_total = list(<span class=\"number\">0.</span> <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">10</span>))</div><div class=\"line\"><span class=\"keyword\">for</span> data <span class=\"keyword\">in</span> testloader:</div><div class=\"line\">    images, labels = data</div><div class=\"line\">    outputs = net(Variable(images))</div><div class=\"line\">    _, predicted = torch.max(outputs.data, <span class=\"number\">1</span>)</div><div class=\"line\">    c = (predicted == labels).squeeze()</div><div class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">4</span>):</div><div class=\"line\">        label = labels[i]</div><div class=\"line\">        class_correct[label] += c[i]</div><div class=\"line\">        class_total[label] += <span class=\"number\">1</span></div></pre></td></tr></table></figure>\n<p>上面这些训练和测试都是在CPU上进行的，如何迁移到GPU？很简单，同样用<code>.cuda()</code>方法就行了。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">net.cuda()</div></pre></td></tr></table></figure>\n<p>不过记得在每次训练测试的迭代中，<code>images</code>和<code>label</code>也要传送到GPU上才可以。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">inputs, labels = Variable(inputs.cuda()), Variable(labels.cuda())</div></pre></td></tr></table></figure>\n<h2 id=\"更多的例子和教程\"><a href=\"#更多的例子和教程\" class=\"headerlink\" title=\"更多的例子和教程\"></a>更多的例子和教程</h2><p><a href=\"https://github.com/pytorch/examples\" target=\"_blank\" rel=\"external\">更多的例子</a><br><a href=\"https://github.com/pytorch/tutorials\" target=\"_blank\" rel=\"external\">更多的教程</a></p>\n","excerpt":"<p>这是一份阅读PyTorch教程的笔记，记录jupyter notebook的关键点。原地址位于<a href=\"https://github.com/pytorch/tutorials/blob/master/Deep%20Learning%20with%20PyTorch.ipynb\">GitHub repo</a>。<br><img src=\"/img/pytorch_logo.png\" alt=\"PyTorch Logo\"></p>","more":"<h2 id=\"PyTorch简介\"><a href=\"#PyTorch简介\" class=\"headerlink\" title=\"PyTorch简介\"></a>PyTorch简介</h2><p><a href=\"https://github.com/pytorch/pytorch\">PyTorch</a>是一个较新的深度学习框架。从名字可以看出，其和Torch不同之处在于PyTorch使用了Python作为开发语言，所谓“Python first”。一方面，使用者可以将其作为加入了GPU支持的<code>numpy</code>，另一方面，PyTorch也是强大的深度学习框架。</p>\n<p>目前有很多深度学习框架，PyTorch主推的功能是动态网络模型。例如在Caffe中，使用者通过编写网络结构的<code>prototxt</code>进行定义，网络结构是不能变化的。而PyTorch中的网络结构不再只能是一成不变的。同时PyTorch实现了多达上百种op的自动求导（AutoGrad）。</p>\n<h2 id=\"Tensors\"><a href=\"#Tensors\" class=\"headerlink\" title=\"Tensors\"></a>Tensors</h2><p><code>Tensor</code>，即<code>numpy</code>中的多维数组。上面已经提到过，PyTorch对其加入了GPU支持。同时，PyTorch中的<code>Tensor</code>可以与<code>numpy</code>中的<code>array</code>很方便地进行互相转换。</p>\n<p>通过<code>Tensor(shape)</code>便可以创建所需要大小的<code>tensor</code>。如下所示。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\">x = torch.Tensor(<span class=\"number\">5</span>, <span class=\"number\">3</span>)  <span class=\"comment\"># construct a 5x3 matrix, uninitialized</span></div><div class=\"line\"><span class=\"comment\"># 或者随机填充</span></div><div class=\"line\">y = torch.rand(<span class=\"number\">5</span>, <span class=\"number\">3</span>)    <span class=\"comment\"># construct a randomly initialized matrix</span></div><div class=\"line\"><span class=\"comment\"># 使用size方法可以获得tensor的shape信息，torch.Size 可以看做 tuple</span></div><div class=\"line\">x.size()                <span class=\"comment\"># out: torch.Size([5, 3])</span></div></pre></td></tr></table></figure>\n<p>PyTorch中已经实现了很多常用的<code>op</code>，如下所示。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\"># addition: syntax 1</span></div><div class=\"line\">x + y                  <span class=\"comment\"># out: [torch.FloatTensor of size 5x3]</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># addition: syntax 2</span></div><div class=\"line\">torch.add(x, y)        <span class=\"comment\"># 或者使用torch包中的显式的op名称</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># addition: giving an output tensor</span></div><div class=\"line\">result = torch.Tensor(<span class=\"number\">5</span>, <span class=\"number\">3</span>)  <span class=\"comment\"># 预先定义size</span></div><div class=\"line\">torch.add(x, y, out=result)  <span class=\"comment\"># 结果被填充到变量result</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># 对于加法运算，其实没必要这么复杂</span></div><div class=\"line\">out = x + y                  <span class=\"comment\"># 无需预先定义size</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># torch包中带有下划线的op说明是就地进行的，如下所示</span></div><div class=\"line\"><span class=\"comment\"># addition: in-place</span></div><div class=\"line\">y.add_(x)              <span class=\"comment\"># 将x加到y上</span></div><div class=\"line\"><span class=\"comment\"># 其他的例子: x.copy_(y), x.t_().</span></div></pre></td></tr></table></figure>\n<p>PyTorch中的元素索引方式和<code>numpy</code>相同。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\"># standard numpy-like indexing with all bells and whistles</span></div><div class=\"line\">x[:,<span class=\"number\">1</span>]                 <span class=\"comment\"># out: [torch.FloatTensor of size 5]</span></div></pre></td></tr></table></figure>\n<p>对于更多的<code>op</code>，可以参见PyTorch的<a href=\"http://pytorch.org/docs/torch.html\">文档页面</a>。</p>\n<p><code>Tensor</code>可以和<code>numpy</code>中的数组进行很方便地转换。并且转换前后并没有发生内存的复制（这里文档里面没有明说？），所以修改其中某一方的值，也会引起另一方的改变。如下所示。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\"># Tensor 转为 np.array</span></div><div class=\"line\">a = torch.ones(<span class=\"number\">5</span>)    <span class=\"comment\"># out: [torch.FloatTensor of size 5]</span></div><div class=\"line\"><span class=\"comment\"># 使用 numpy方法即可实现转换</span></div><div class=\"line\">b = a.numpy()        <span class=\"comment\"># out: array([ 1.,  1.,  1.,  1.,  1.], dtype=float32)</span></div><div class=\"line\"><span class=\"comment\"># 注意！a的值的变化同样引起b的变化</span></div><div class=\"line\">a.add_(<span class=\"number\">1</span>)</div><div class=\"line\">print(a)</div><div class=\"line\">print(b)             <span class=\"comment\"># a b的值都变成2</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># np.array 转为Tensor</span></div><div class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</div><div class=\"line\">a = np.ones(<span class=\"number\">5</span>)</div><div class=\"line\"><span class=\"comment\"># 使用torch.from_numpy即可实现转换</span></div><div class=\"line\">b = torch.from_numpy(a)  <span class=\"comment\"># out: [torch.DoubleTensor of size 5]</span></div><div class=\"line\">np.add(a, <span class=\"number\">1</span>, out=a)</div><div class=\"line\">print(a)</div><div class=\"line\">print(b)            <span class=\"comment\"># a b的值都变为2</span></div></pre></td></tr></table></figure>\n<p>PyTorch中使用GPU计算很简单，通过调用<code>.cuda()</code>方法，很容易实现GPU支持。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\"># let us run this cell only if CUDA is available</span></div><div class=\"line\"><span class=\"keyword\">if</span> torch.cuda.is_available():</div><div class=\"line\">    print(<span class=\"string\">'cuda is avaliable'</span>)</div><div class=\"line\">    x = x.cuda()</div><div class=\"line\">    y = y.cuda()</div><div class=\"line\">    x + y          <span class=\"comment\"># 在GPU上进行计算</span></div></pre></td></tr></table></figure>\n<h2 id=\"Neural-Network\"><a href=\"#Neural-Network\" class=\"headerlink\" title=\"Neural Network\"></a>Neural Network</h2><p>说完了数据类型<code>Tensor</code>，下一步便是如何实现一个神经网络。首先，对<a href=\"http://pytorch.org/docs/autograd.html\">自动求导</a>做一说明。</p>\n<p>我们需要关注的是<code>autograd.Variable</code>。这个东西包装了<code>Tensor</code>。一旦你完成了计算，就可以使用<code>.backward()</code>方法自动得到（以该<code>Variable</code>为叶子节点的那个）网络中参数的梯度。<code>Variable</code>有一个名叫<code>data</code>的字段，可以通过它获得被包装起来的那个原始的<code>Tensor</code>数据。同时，使用<code>grad</code>字段，可以获取梯度（也是一个<code>Variable</code>）。</p>\n<p><code>Variable</code>是计算图的节点，同时<code>Function</code>实现了变量之间的变换。它们互相联系，构成了用于计算的无环图。每个<code>Variable</code>有一个<code>creator</code>的字段，表明了它是由哪个<code>Function</code>创建的（除了用户自己显式创建的那些，这时候<code>creator</code>是<code>None</code>）。</p>\n<p>当进行反向传播计算梯度时，如果<code>Variable</code>是标量（比如最终的<code>loss</code>是欧氏距离或者交叉熵），那么<code>backward()</code>函数不需要参数。然而如果<code>Variable</code>有不止一个元素的时候，需要为其中的每个元素指明其（由上层传导来的）梯度（也就是一个和<code>Variable</code>shape匹配的<code>Tensor</code>）。看下面的说明代码。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">from</span> torch.autograd <span class=\"keyword\">import</span> Variable</div><div class=\"line\">x = Variable(torch.ones(<span class=\"number\">2</span>, <span class=\"number\">2</span>), requires_grad = <span class=\"keyword\">True</span>)</div><div class=\"line\">x     <span class=\"comment\"># x 包装了一个2x2的Tensor</span></div><div class=\"line\"><span class=\"string\">\"\"\"</div><div class=\"line\">Variable containing:</div><div class=\"line\"> 1  1</div><div class=\"line\"> 1  1</div><div class=\"line\">[torch.FloatTensor of size 2x2]</div><div class=\"line\">\"\"\"</span></div><div class=\"line\"><span class=\"comment\"># Variable进行计算</span></div><div class=\"line\"><span class=\"comment\"># y was created as a result of an operation,</span></div><div class=\"line\"><span class=\"comment\"># so it has a creator</span></div><div class=\"line\">y = x + <span class=\"number\">2</span></div><div class=\"line\">y.creator    <span class=\"comment\"># out: &lt;torch.autograd._functions.basic_ops.AddConstant at 0x7fa1cc158c08&gt;</span></div><div class=\"line\"></div><div class=\"line\">z = y * y * <span class=\"number\">3</span>  </div><div class=\"line\">out = z.mean()   <span class=\"comment\"># out: Variable containing: 27 [torch.FloatTensor of size 1]</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># let's backprop now</span></div><div class=\"line\">out.backward()  <span class=\"comment\"># 其实相当于 out.backward(torch.Tensor([1.0]))</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># print gradients d(out)/dx</span></div><div class=\"line\">x.grad</div><div class=\"line\"><span class=\"string\">\"\"\"</div><div class=\"line\">Variable containing:</div><div class=\"line\"> 4.5000  4.5000</div><div class=\"line\"> 4.5000  4.5000</div><div class=\"line\">[torch.FloatTensor of size 2x2]</div><div class=\"line\">\"\"\"</span></div></pre></td></tr></table></figure>\n<p>下面的代码就是结果不是标量，而是普通的<code>Tensor</code>的例子。<br><figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\"># 也可以通过Tensor显式地创建Variable</span></div><div class=\"line\">x = torch.randn(<span class=\"number\">3</span>)</div><div class=\"line\">x = Variable(x, requires_grad = <span class=\"keyword\">True</span>)</div><div class=\"line\"><span class=\"comment\"># 一个更复杂的 op例子</span></div><div class=\"line\">y = x * <span class=\"number\">2</span></div><div class=\"line\"><span class=\"keyword\">while</span> y.data.norm() &lt; <span class=\"number\">1000</span>:</div><div class=\"line\">    y = y * <span class=\"number\">2</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># 计算 dy/dx</span></div><div class=\"line\">gradients = torch.FloatTensor([<span class=\"number\">0.1</span>, <span class=\"number\">1.0</span>, <span class=\"number\">0.0001</span>])</div><div class=\"line\">y.backward(gradients)</div><div class=\"line\">x.grad</div><div class=\"line\"><span class=\"string\">\"\"\"</div><div class=\"line\">Variable containing:</div><div class=\"line\">  204.8000</div><div class=\"line\"> 2048.0000</div><div class=\"line\">    0.2048</div><div class=\"line\">[torch.FloatTensor of size 3]</div><div class=\"line\">\"\"\"</span></div></pre></td></tr></table></figure></p>\n<p>说完了NN的构成元素<code>Variable</code>，下面可以介绍如何使用PyTorch构建网络了。这部分主要使用了<code>torch.nn</code>包。我们自定义的网络结构是由若干的<code>layer</code>组成的，我们将其设置为 <code>nn.Module</code>的子类，只要使用方法<code>forward(input)</code>就可以返回网络的<code>output</code>。下面的代码展示了如何建立一个包含有<code>conv</code>和<code>max-pooling</code>和<code>fc</code>层的简单CNN网络。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">import</span> torch.nn <span class=\"keyword\">as</span> nn                 <span class=\"comment\"># 以我的理解，貌似有参数的都在nn里面</span></div><div class=\"line\"><span class=\"keyword\">import</span> torch.nn.functional <span class=\"keyword\">as</span> F       <span class=\"comment\"># 没有参数的（如pooling和relu）都在functional里面？</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Net</span><span class=\"params\">(nn.Module)</span>:</span></div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self)</span>:</span></div><div class=\"line\">        super(Net, self).__init__()</div><div class=\"line\">        self.conv1 = nn.Conv2d(<span class=\"number\">1</span>, <span class=\"number\">6</span>, <span class=\"number\">5</span>) <span class=\"comment\"># 1 input image channel, 6 output channels, 5x5 square convolution kernel</span></div><div class=\"line\">        self.conv2 = nn.Conv2d(<span class=\"number\">6</span>, <span class=\"number\">16</span>, <span class=\"number\">5</span>)</div><div class=\"line\">        <span class=\"comment\"># 一会可以看到，input是1x32x32大小的。经过计算，conv-pooling-conv-pooling后大小为16x5x5。</span></div><div class=\"line\">        <span class=\"comment\"># 所以fc层的第一个参数是 16x5x5</span></div><div class=\"line\">        self.fc1   = nn.Linear(<span class=\"number\">16</span>*<span class=\"number\">5</span>*<span class=\"number\">5</span>, <span class=\"number\">120</span>) <span class=\"comment\"># an affine operation: y = Wx + b</span></div><div class=\"line\">        self.fc2   = nn.Linear(<span class=\"number\">120</span>, <span class=\"number\">84</span>)</div><div class=\"line\">        self.fc3   = nn.Linear(<span class=\"number\">84</span>, <span class=\"number\">10</span>)</div><div class=\"line\"></div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">forward</span><span class=\"params\">(self, x)</span>:</span></div><div class=\"line\">        <span class=\"comment\"># 你可以发现，构建计算图的过程是在前向计算中完成的，也许这可以让你体会到所谓的动态图结构</span></div><div class=\"line\">        <span class=\"comment\"># 同时，我们无需实现 backward，这是被自动求导实现的</span></div><div class=\"line\">        x = F.max_pool2d(F.relu(self.conv1(x)), (<span class=\"number\">2</span>, <span class=\"number\">2</span>)) <span class=\"comment\"># Max pooling over a (2, 2) window</span></div><div class=\"line\">        x = F.max_pool2d(F.relu(self.conv2(x)), <span class=\"number\">2</span>) <span class=\"comment\"># If the size is a square you can only specify a single number</span></div><div class=\"line\">        x = x.view(<span class=\"number\">-1</span>, self.num_flat_features(x))  <span class=\"comment\"># 把它拉直</span></div><div class=\"line\">        x = F.relu(self.fc1(x))</div><div class=\"line\">        x = F.relu(self.fc2(x))</div><div class=\"line\">        x = self.fc3(x)</div><div class=\"line\">        <span class=\"keyword\">return</span> x</div><div class=\"line\"></div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">num_flat_features</span><span class=\"params\">(self, x)</span>:</span></div><div class=\"line\">        size = x.size()[<span class=\"number\">1</span>:] <span class=\"comment\"># all dimensions except the batch dimension</span></div><div class=\"line\">        num_features = <span class=\"number\">1</span></div><div class=\"line\">        <span class=\"keyword\">for</span> s <span class=\"keyword\">in</span> size:</div><div class=\"line\">            num_features *= s</div><div class=\"line\">        <span class=\"keyword\">return</span> num_features</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># 实例化Net对象</span></div><div class=\"line\">net = Net()</div><div class=\"line\">net     <span class=\"comment\"># 给出了网络结构</span></div><div class=\"line\"><span class=\"string\">\"\"\"</div><div class=\"line\">Net (</div><div class=\"line\">  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))</div><div class=\"line\">  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))</div><div class=\"line\">  (fc1): Linear (400 -&gt; 120)</div><div class=\"line\">  (fc2): Linear (120 -&gt; 84)</div><div class=\"line\">  (fc3): Linear (84 -&gt; 10)</div><div class=\"line\">)</div><div class=\"line\">\"\"\"</span></div></pre></td></tr></table></figure>\n<p>我们可以列出网络中的所有参数。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\">params = list(net.parameters())</div><div class=\"line\">print(len(params))      <span class=\"comment\"># out: 10, 5个权重，5个bias</span></div><div class=\"line\">print(params[<span class=\"number\">0</span>].size())  <span class=\"comment\"># conv1's weight out: torch.Size([6, 1, 5, 5])</span></div><div class=\"line\">print(params[<span class=\"number\">1</span>].size())  <span class=\"comment\"># conv1's bias, out: torch.Size([6])</span></div></pre></td></tr></table></figure>\n<p>给出网络的输入，得到网络的输出。并进行反向传播梯度。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\">input = Variable(torch.randn(<span class=\"number\">1</span>, <span class=\"number\">1</span>, <span class=\"number\">32</span>, <span class=\"number\">32</span>))</div><div class=\"line\">out = net(input)         <span class=\"comment\"># 重载了()运算符？</span></div><div class=\"line\">net.zero_grad()          <span class=\"comment\"># bp前，把所有参数的grad buffer清零</span></div><div class=\"line\">out.backward(torch.randn(<span class=\"number\">1</span>, <span class=\"number\">10</span>))</div></pre></td></tr></table></figure>\n<p>注意一点，<code>torch.nn</code>只支持mini-batch。所以如果你的输入只有一个样例的时候，使用<code>input.unsqueeze(0)</code>人为给它加上一个维度，让它变成一个4-D的<code>Tensor</code>。</p>\n<h2 id=\"网络训练\"><a href=\"#网络训练\" class=\"headerlink\" title=\"网络训练\"></a>网络训练</h2><p>给定target和网络的output，就可以计算loss函数了。在<code>torch.nn</code>中已经<a href=\"http://pytorch.org/docs/nn.html#loss-functions\">实现好了一些loss函数</a>。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div></pre></td><td class=\"code\"><pre><div class=\"line\">output = net(input)</div><div class=\"line\">target = Variable(torch.range(<span class=\"number\">1</span>, <span class=\"number\">10</span>))  <span class=\"comment\"># a dummy target, for example</span></div><div class=\"line\"><span class=\"comment\"># 使用平均平方误差，即欧几里得距离</span></div><div class=\"line\">criterion = nn.MSELoss()</div><div class=\"line\">loss = criterion(output, target)</div><div class=\"line\">loss</div><div class=\"line\"><span class=\"string\">\"\"\"</div><div class=\"line\">Variable containing:</div><div class=\"line\"> 38.6049</div><div class=\"line\">[torch.FloatTensor of size 1]</div><div class=\"line\">\"\"\"</span></div></pre></td></tr></table></figure>\n<p>网络的整体结构如下所示。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\">input -&gt; conv2d -&gt; relu -&gt; maxpool2d -&gt; conv2d -&gt; relu -&gt; maxpool2d  </div><div class=\"line\">      -&gt; view -&gt; linear -&gt; relu -&gt; linear -&gt; relu -&gt; linear</div><div class=\"line\">      -&gt; MSELoss</div><div class=\"line\">      -&gt; loss</div></pre></td></tr></table></figure>\n<p>我们可以使用<code>previous_functions</code>来获得该节点前面<code>Function</code>的信息。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div></pre></td><td class=\"code\"><pre><div class=\"line\"># For illustration, let us follow a few steps backward</div><div class=\"line\">print(loss.creator) # MSELoss</div><div class=\"line\">print(loss.creator.previous_functions[0][0]) # Linear</div><div class=\"line\">print(loss.creator.previous_functions[0][0].previous_functions[0][0]) # ReLU</div><div class=\"line\">&quot;&quot;&quot;</div><div class=\"line\">&lt;torch.nn._functions.thnn.auto.MSELoss object at 0x7fa18011db40&gt;</div><div class=\"line\">&lt;torch.nn._functions.linear.Linear object at 0x7fa18011da78&gt;</div><div class=\"line\">&lt;torch.nn._functions.thnn.auto.Threshold object at 0x7fa18011d9b0&gt;</div><div class=\"line\">&quot;&quot;&quot;</div></pre></td></tr></table></figure>\n<p>进行反向传播后，让我们查看一下参数的变化。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\"># now we shall call loss.backward(), and have a look at conv1's bias gradients before and after the backward.</span></div><div class=\"line\">net.zero_grad() <span class=\"comment\"># zeroes the gradient buffers of all parameters</span></div><div class=\"line\">print(<span class=\"string\">'conv1.bias.grad before backward'</span>)</div><div class=\"line\">print(net.conv1.bias.grad)</div><div class=\"line\">loss.backward()</div><div class=\"line\">print(<span class=\"string\">'conv1.bias.grad after backward'</span>)</div><div class=\"line\">print(net.conv1.bias.grad)</div></pre></td></tr></table></figure>\n<p>计算梯度后，自然需要更新参数了。简单的方法可以自己手写：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\">learning_rate = <span class=\"number\">0.01</span></div><div class=\"line\"><span class=\"keyword\">for</span> f <span class=\"keyword\">in</span> net.parameters():</div><div class=\"line\">    f.data.sub_(f.grad.data * learning_rate)</div></pre></td></tr></table></figure>\n<p>不过，<code>torch.optim</code>中已经提供了若干优化方法（SGD, Nesterov-SGD, Adam, RMSProp, etc）。如下所示。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">import</span> torch.optim <span class=\"keyword\">as</span> optim</div><div class=\"line\"><span class=\"comment\"># create your optimizer</span></div><div class=\"line\">optimizer = optim.SGD(net.parameters(), lr = <span class=\"number\">0.01</span>)</div><div class=\"line\"><span class=\"comment\"># in your training loop:</span></div><div class=\"line\">optimizer.zero_grad() <span class=\"comment\"># zero the gradient buffers</span></div><div class=\"line\">output = net(input)</div><div class=\"line\">loss = criterion(output, target)</div><div class=\"line\">loss.backward()</div><div class=\"line\">optimizer.step() <span class=\"comment\"># Does the update</span></div></pre></td></tr></table></figure>\n<h2 id=\"数据载入\"><a href=\"#数据载入\" class=\"headerlink\" title=\"数据载入\"></a>数据载入</h2><p>由于PyTorch的Python接口和<code>np.array</code>之间的方便转换，所以可以使用其他任何数据读入的方法（例如OpenCV等）。特别地，对于vision的数据，PyTorch提供了<code>torchvision</code>包，可以方便地载入常用的数据集（Imagenet, CIFAR10, MNIST, etc），同时提供了图像的各种变换方法。下面以CIFAR为例子。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">import</span> torchvision</div><div class=\"line\"><span class=\"keyword\">import</span> torchvision.transforms <span class=\"keyword\">as</span> transforms</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># The output of torchvision datasets are PILImage images of range [0, 1].</span></div><div class=\"line\"><span class=\"comment\"># We transform them to Tensors of normalized range [-1, 1]</span></div><div class=\"line\"><span class=\"comment\"># Compose: Composes several transforms together.</span></div><div class=\"line\"><span class=\"comment\"># see http://pytorch.org/docs/torchvision/transforms.html?highlight=transforms</span></div><div class=\"line\">transform=transforms.Compose([transforms.ToTensor(),</div><div class=\"line\">                              transforms.Normalize((<span class=\"number\">0.5</span>, <span class=\"number\">0.5</span>, <span class=\"number\">0.5</span>), (<span class=\"number\">0.5</span>, <span class=\"number\">0.5</span>, <span class=\"number\">0.5</span>)),</div><div class=\"line\">                             ])   <span class=\"comment\"># torchvision.transforms.Normalize(mean, std)</span></div><div class=\"line\"><span class=\"comment\"># 读取CIFAR10数据集                             </span></div><div class=\"line\">trainset = torchvision.datasets.CIFAR10(root=<span class=\"string\">'./data'</span>, train=<span class=\"keyword\">True</span>, download=<span class=\"keyword\">True</span>, transform=transform)</div><div class=\"line\"><span class=\"comment\"># 使用DataLoader</span></div><div class=\"line\">trainloader = torch.utils.data.DataLoader(trainset, batch_size=<span class=\"number\">4</span>,</div><div class=\"line\">                                          shuffle=<span class=\"keyword\">True</span>, num_workers=<span class=\"number\">2</span>)</div><div class=\"line\"><span class=\"comment\"># Test集，设置train = False</span></div><div class=\"line\">testset = torchvision.datasets.CIFAR10(root=<span class=\"string\">'./data'</span>, train=<span class=\"keyword\">False</span>, download=<span class=\"keyword\">True</span>, transform=transform)</div><div class=\"line\">testloader = torch.utils.data.DataLoader(testset, batch_size=<span class=\"number\">4</span>,</div><div class=\"line\">                                          shuffle=<span class=\"keyword\">False</span>, num_workers=<span class=\"number\">2</span>)</div><div class=\"line\">classes = (<span class=\"string\">'plane'</span>, <span class=\"string\">'car'</span>, <span class=\"string\">'bird'</span>, <span class=\"string\">'cat'</span>,</div><div class=\"line\">           <span class=\"string\">'deer'</span>, <span class=\"string\">'dog'</span>, <span class=\"string\">'frog'</span>, <span class=\"string\">'horse'</span>, <span class=\"string\">'ship'</span>, <span class=\"string\">'truck'</span>)</div></pre></td></tr></table></figure>\n<p>接下来，我们对上面部分的CNN网络进行小修，设置第一个<code>conv</code>层接受3通道的输入。并使用交叉熵定义loss。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Net</span><span class=\"params\">(nn.Module)</span>:</span></div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self)</span>:</span></div><div class=\"line\">        super(Net, self).__init__()</div><div class=\"line\">        self.conv1 = nn.Conv2d(<span class=\"number\">3</span>, <span class=\"number\">6</span>, <span class=\"number\">5</span>)</div><div class=\"line\">        self.pool  = nn.MaxPool2d(<span class=\"number\">2</span>,<span class=\"number\">2</span>)</div><div class=\"line\">        self.conv2 = nn.Conv2d(<span class=\"number\">6</span>, <span class=\"number\">16</span>, <span class=\"number\">5</span>)</div><div class=\"line\">        self.fc1   = nn.Linear(<span class=\"number\">16</span>*<span class=\"number\">5</span>*<span class=\"number\">5</span>, <span class=\"number\">120</span>)</div><div class=\"line\">        self.fc2   = nn.Linear(<span class=\"number\">120</span>, <span class=\"number\">84</span>)</div><div class=\"line\">        self.fc3   = nn.Linear(<span class=\"number\">84</span>, <span class=\"number\">10</span>)</div><div class=\"line\"></div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">forward</span><span class=\"params\">(self, x)</span>:</span></div><div class=\"line\">        x = self.pool(F.relu(self.conv1(x)))</div><div class=\"line\">        x = self.pool(F.relu(self.conv2(x)))</div><div class=\"line\">        x = x.view(<span class=\"number\">-1</span>, <span class=\"number\">16</span>*<span class=\"number\">5</span>*<span class=\"number\">5</span>)</div><div class=\"line\">        x = F.relu(self.fc1(x))</div><div class=\"line\">        x = F.relu(self.fc2(x))</div><div class=\"line\">        x = self.fc3(x)</div><div class=\"line\">        <span class=\"keyword\">return</span> x</div><div class=\"line\"></div><div class=\"line\">net = Net()</div><div class=\"line\"><span class=\"comment\"># use a Classification Cross-Entropy loss</span></div><div class=\"line\">criterion = nn.CrossEntropyLoss()</div><div class=\"line\">optimizer = optim.SGD(net.parameters(), lr=<span class=\"number\">0.001</span>, momentum=<span class=\"number\">0.9</span>)</div></pre></td></tr></table></figure>\n<p>接下来，我们进行模型的训练。我们loop over整个dataset两次，对每个mini-batch进行参数的更新。并且设置每隔2000个mini-batch打印一次loss。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">for</span> epoch <span class=\"keyword\">in</span> range(<span class=\"number\">2</span>): <span class=\"comment\"># loop over the dataset multiple times</span></div><div class=\"line\"></div><div class=\"line\">    running_loss = <span class=\"number\">0.0</span></div><div class=\"line\">    <span class=\"keyword\">for</span> i, data <span class=\"keyword\">in</span> enumerate(trainloader, <span class=\"number\">0</span>):</div><div class=\"line\">        <span class=\"comment\"># get the inputs</span></div><div class=\"line\">        inputs, labels = data</div><div class=\"line\"></div><div class=\"line\">        <span class=\"comment\"># wrap them in Variable</span></div><div class=\"line\">        inputs, labels = Variable(inputs), Variable(labels)</div><div class=\"line\"></div><div class=\"line\">        <span class=\"comment\"># zero the parameter gradients</span></div><div class=\"line\">        optimizer.zero_grad()</div><div class=\"line\"></div><div class=\"line\">        <span class=\"comment\"># forward + backward + optimize</span></div><div class=\"line\">        outputs = net(inputs)</div><div class=\"line\">        loss = criterion(outputs, labels)</div><div class=\"line\">        loss.backward()        </div><div class=\"line\">        optimizer.step()</div><div class=\"line\"></div><div class=\"line\">        <span class=\"comment\"># print statistics</span></div><div class=\"line\">        running_loss += loss.data[<span class=\"number\">0</span>]</div><div class=\"line\">        <span class=\"keyword\">if</span> i % <span class=\"number\">2000</span> == <span class=\"number\">1999</span>: <span class=\"comment\"># print every 2000 mini-batches</span></div><div class=\"line\">            print(<span class=\"string\">'[%d, %5d] loss: %.3f'</span> % (epoch+<span class=\"number\">1</span>, i+<span class=\"number\">1</span>, running_loss / <span class=\"number\">2000</span>))</div><div class=\"line\">            running_loss = <span class=\"number\">0.0</span></div><div class=\"line\">print(<span class=\"string\">'Finished Training'</span>)</div></pre></td></tr></table></figure>\n<p>我们在测试集上选取一个mini-batch（也就是4张，见上面<code>testloader</code>的定义），进行测试。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div></pre></td><td class=\"code\"><pre><div class=\"line\">dataiter = iter(testloader)</div><div class=\"line\">images, labels = dataiter.next()   <span class=\"comment\"># 得到image和对应的label</span></div><div class=\"line\">outputs = net(Variable(images))</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># the outputs are energies for the 10 classes.</span></div><div class=\"line\"><span class=\"comment\"># Higher the energy for a class, the more the network</span></div><div class=\"line\"><span class=\"comment\"># thinks that the image is of the particular class</span></div><div class=\"line\"><span class=\"comment\"># So, let's get the index of the highest energy</span></div><div class=\"line\">_, predicted = torch.max(outputs.data, <span class=\"number\">1</span>)   <span class=\"comment\"># 找出分数最高的对应的channel，即为top-1类别</span></div><div class=\"line\"></div><div class=\"line\">print(<span class=\"string\">'Predicted: '</span>, <span class=\"string\">' '</span>.join(<span class=\"string\">'%5s'</span>% classes[predicted[j][<span class=\"number\">0</span>]] <span class=\"keyword\">for</span> j <span class=\"keyword\">in</span> range(<span class=\"number\">4</span>)))</div></pre></td></tr></table></figure>\n<p>测试一下整个测试集合上的表现。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div></pre></td><td class=\"code\"><pre><div class=\"line\">correct = <span class=\"number\">0</span></div><div class=\"line\">total = <span class=\"number\">0</span></div><div class=\"line\"><span class=\"keyword\">for</span> data <span class=\"keyword\">in</span> testloader:     <span class=\"comment\"># 每一个test mini-batch</span></div><div class=\"line\">    images, labels = data</div><div class=\"line\">    outputs = net(Variable(images))</div><div class=\"line\">    _, predicted = torch.max(outputs.data, <span class=\"number\">1</span>)</div><div class=\"line\">    total += labels.size(<span class=\"number\">0</span>)</div><div class=\"line\">    correct += (predicted == labels).sum()</div><div class=\"line\"></div><div class=\"line\">print(<span class=\"string\">'Accuracy of the network on the 10000 test images: %d %%'</span> % (<span class=\"number\">100</span> * correct / total))</div></pre></td></tr></table></figure>\n<p>对哪一类的预测精度更高呢？</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div></pre></td><td class=\"code\"><pre><div class=\"line\">class_correct = list(<span class=\"number\">0.</span> <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">10</span>))</div><div class=\"line\">class_total = list(<span class=\"number\">0.</span> <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">10</span>))</div><div class=\"line\"><span class=\"keyword\">for</span> data <span class=\"keyword\">in</span> testloader:</div><div class=\"line\">    images, labels = data</div><div class=\"line\">    outputs = net(Variable(images))</div><div class=\"line\">    _, predicted = torch.max(outputs.data, <span class=\"number\">1</span>)</div><div class=\"line\">    c = (predicted == labels).squeeze()</div><div class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">4</span>):</div><div class=\"line\">        label = labels[i]</div><div class=\"line\">        class_correct[label] += c[i]</div><div class=\"line\">        class_total[label] += <span class=\"number\">1</span></div></pre></td></tr></table></figure>\n<p>上面这些训练和测试都是在CPU上进行的，如何迁移到GPU？很简单，同样用<code>.cuda()</code>方法就行了。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">net.cuda()</div></pre></td></tr></table></figure>\n<p>不过记得在每次训练测试的迭代中，<code>images</code>和<code>label</code>也要传送到GPU上才可以。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">inputs, labels = Variable(inputs.cuda()), Variable(labels.cuda())</div></pre></td></tr></table></figure>\n<h2 id=\"更多的例子和教程\"><a href=\"#更多的例子和教程\" class=\"headerlink\" title=\"更多的例子和教程\"></a>更多的例子和教程</h2><p><a href=\"https://github.com/pytorch/examples\">更多的例子</a><br><a href=\"https://github.com/pytorch/tutorials\">更多的教程</a></p>"},{"title":"Windows环境下使用Doxygen生成注释文档","date":"2016-12-16T11:00:00.000Z","_content":"\nDoxygen 是一种很好用的代码注释生成工具，然而和很多国外的工具软件一样，在中文环境下，它的使用总是会出现一些问题，也就是中文注释文档出现乱码。经过调试，终于是解决了这个问题。\n\n![Doxygen](/img/doxygen_picture.png)\n<!-- more -->\n## 安装 Doxygen\n\nDoxygen 在Windows平台下的安装比较简单，[Doxygen的项目主页](http://www.doxygen.nl/)提供了下载和安装的使用说明，可以下载它们的官方使用手册进行阅读。对于Windows，提供了源代码编译安装和直接安装程序安装两种方式，可以自行选择。\n\n安装成功后，使用命令行命令\n\n``` bash\ndoxygen --help\n```\n\n就可以查看帮助文档，对应参数含义一目了然，降低了入手难度。\n\n使用命令，\n\n\n``` bash\ndoxygen -g doxygen_filename\n```\n\n就可以在当前目录下建立一个doxygen配置文件，用文本编辑器打开就可以编辑里面的配置选项。\n\n使用命令，\n\n``` bash\ndoxygen doxygen_filename\n```\n\n就可以生成注释文档了。\n\n下面就来说一说对中文的支持。\n\n## 生成 HTML 格式文档\n\n中文之所以乱码，很多时候是由于编码和译码格式不同，所以我们需要先知道自己代码文件的编码方式。我的代码都是建立在Visual Studio上的，可以通过VS的高级保存选项查看自己代码文件的存储编码格式。对于中文版的VS，一般应该是GB2312。\n\n我们打开 Doxygen 的配置文件，将里面的 INPUT_ENCODING 改为我们代码文件的编码格式，这里就改成 GB2312。\n\n这样一来，编译出来的 HTML 页面就不会有中文乱码了。\n\n## 生成Latex 格式文档\n\n生成 Latex 需要本机上安装有 Latex 的编译环境。如果是中文用户，推荐的是CTEX套件，可以到他们的网站上去下载。\n\n可以看到，Doxygen为Latex文件的编译生成了make文件，我们在命令行窗口中执行make命令就可以完成编译，然而这时候会发现编译出错，pdf文档无法生成。\n\n打开生成的refman.latex文档，添加宏包 CJKutf8。然后找到 `\\begin{document}`一行，将其改为\n\n``` latex\n\\begin{document}\n\\begin{CJK}{UTF8}{gbsn}\n```\n\n也就是说为正文提供了CJK环境，这样中文文本就可以正常编译了。\n\n相应的，我们要将结尾的 `\\end{document)`改为：\n``` latex\n\\end{CJK}\n\\end{document}\n```\n\n这样，运行make命令之后，就可以看到中文的注释文档了。\n","source":"_posts/use-doxygen.md","raw":"---\ntitle: Windows环境下使用Doxygen生成注释文档\ndate: 2016-12-16 19:00:00\ntags:\n    - tool\n    - doxygen\n---\n\nDoxygen 是一种很好用的代码注释生成工具，然而和很多国外的工具软件一样，在中文环境下，它的使用总是会出现一些问题，也就是中文注释文档出现乱码。经过调试，终于是解决了这个问题。\n\n![Doxygen](/img/doxygen_picture.png)\n<!-- more -->\n## 安装 Doxygen\n\nDoxygen 在Windows平台下的安装比较简单，[Doxygen的项目主页](http://www.doxygen.nl/)提供了下载和安装的使用说明，可以下载它们的官方使用手册进行阅读。对于Windows，提供了源代码编译安装和直接安装程序安装两种方式，可以自行选择。\n\n安装成功后，使用命令行命令\n\n``` bash\ndoxygen --help\n```\n\n就可以查看帮助文档，对应参数含义一目了然，降低了入手难度。\n\n使用命令，\n\n\n``` bash\ndoxygen -g doxygen_filename\n```\n\n就可以在当前目录下建立一个doxygen配置文件，用文本编辑器打开就可以编辑里面的配置选项。\n\n使用命令，\n\n``` bash\ndoxygen doxygen_filename\n```\n\n就可以生成注释文档了。\n\n下面就来说一说对中文的支持。\n\n## 生成 HTML 格式文档\n\n中文之所以乱码，很多时候是由于编码和译码格式不同，所以我们需要先知道自己代码文件的编码方式。我的代码都是建立在Visual Studio上的，可以通过VS的高级保存选项查看自己代码文件的存储编码格式。对于中文版的VS，一般应该是GB2312。\n\n我们打开 Doxygen 的配置文件，将里面的 INPUT_ENCODING 改为我们代码文件的编码格式，这里就改成 GB2312。\n\n这样一来，编译出来的 HTML 页面就不会有中文乱码了。\n\n## 生成Latex 格式文档\n\n生成 Latex 需要本机上安装有 Latex 的编译环境。如果是中文用户，推荐的是CTEX套件，可以到他们的网站上去下载。\n\n可以看到，Doxygen为Latex文件的编译生成了make文件，我们在命令行窗口中执行make命令就可以完成编译，然而这时候会发现编译出错，pdf文档无法生成。\n\n打开生成的refman.latex文档，添加宏包 CJKutf8。然后找到 `\\begin{document}`一行，将其改为\n\n``` latex\n\\begin{document}\n\\begin{CJK}{UTF8}{gbsn}\n```\n\n也就是说为正文提供了CJK环境，这样中文文本就可以正常编译了。\n\n相应的，我们要将结尾的 `\\end{document)`改为：\n``` latex\n\\end{CJK}\n\\end{document}\n```\n\n这样，运行make命令之后，就可以看到中文的注释文档了。\n","slug":"use-doxygen","published":1,"updated":"2017-02-26T12:09:28.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cizve07bo0011l61h81lhzd47","content":"<p>Doxygen 是一种很好用的代码注释生成工具，然而和很多国外的工具软件一样，在中文环境下，它的使用总是会出现一些问题，也就是中文注释文档出现乱码。经过调试，终于是解决了这个问题。</p>\n<p><img src=\"/img/doxygen_picture.png\" alt=\"Doxygen\"><br><a id=\"more\"></a></p>\n<h2 id=\"安装-Doxygen\"><a href=\"#安装-Doxygen\" class=\"headerlink\" title=\"安装 Doxygen\"></a>安装 Doxygen</h2><p>Doxygen 在Windows平台下的安装比较简单，<a href=\"http://www.doxygen.nl/\" target=\"_blank\" rel=\"external\">Doxygen的项目主页</a>提供了下载和安装的使用说明，可以下载它们的官方使用手册进行阅读。对于Windows，提供了源代码编译安装和直接安装程序安装两种方式，可以自行选择。</p>\n<p>安装成功后，使用命令行命令</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">doxygen --help</div></pre></td></tr></table></figure>\n<p>就可以查看帮助文档，对应参数含义一目了然，降低了入手难度。</p>\n<p>使用命令，</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">doxygen -g doxygen_filename</div></pre></td></tr></table></figure>\n<p>就可以在当前目录下建立一个doxygen配置文件，用文本编辑器打开就可以编辑里面的配置选项。</p>\n<p>使用命令，</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">doxygen doxygen_filename</div></pre></td></tr></table></figure>\n<p>就可以生成注释文档了。</p>\n<p>下面就来说一说对中文的支持。</p>\n<h2 id=\"生成-HTML-格式文档\"><a href=\"#生成-HTML-格式文档\" class=\"headerlink\" title=\"生成 HTML 格式文档\"></a>生成 HTML 格式文档</h2><p>中文之所以乱码，很多时候是由于编码和译码格式不同，所以我们需要先知道自己代码文件的编码方式。我的代码都是建立在Visual Studio上的，可以通过VS的高级保存选项查看自己代码文件的存储编码格式。对于中文版的VS，一般应该是GB2312。</p>\n<p>我们打开 Doxygen 的配置文件，将里面的 INPUT_ENCODING 改为我们代码文件的编码格式，这里就改成 GB2312。</p>\n<p>这样一来，编译出来的 HTML 页面就不会有中文乱码了。</p>\n<h2 id=\"生成Latex-格式文档\"><a href=\"#生成Latex-格式文档\" class=\"headerlink\" title=\"生成Latex 格式文档\"></a>生成Latex 格式文档</h2><p>生成 Latex 需要本机上安装有 Latex 的编译环境。如果是中文用户，推荐的是CTEX套件，可以到他们的网站上去下载。</p>\n<p>可以看到，Doxygen为Latex文件的编译生成了make文件，我们在命令行窗口中执行make命令就可以完成编译，然而这时候会发现编译出错，pdf文档无法生成。</p>\n<p>打开生成的refman.latex文档，添加宏包 CJKutf8。然后找到 <code>\\begin{document}</code>一行，将其改为</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">\\begin&#123;document&#125;</div><div class=\"line\">\\begin&#123;CJK&#125;&#123;UTF8&#125;&#123;gbsn&#125;</div></pre></td></tr></table></figure>\n<p>也就是说为正文提供了CJK环境，这样中文文本就可以正常编译了。</p>\n<p>相应的，我们要将结尾的 <code>\\end{document)</code>改为：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">\\end&#123;CJK&#125;</div><div class=\"line\">\\end&#123;document&#125;</div></pre></td></tr></table></figure></p>\n<p>这样，运行make命令之后，就可以看到中文的注释文档了。</p>\n","excerpt":"<p>Doxygen 是一种很好用的代码注释生成工具，然而和很多国外的工具软件一样，在中文环境下，它的使用总是会出现一些问题，也就是中文注释文档出现乱码。经过调试，终于是解决了这个问题。</p>\n<p><img src=\"/img/doxygen_picture.png\" alt=\"Doxygen\"><br>","more":"</p>\n<h2 id=\"安装-Doxygen\"><a href=\"#安装-Doxygen\" class=\"headerlink\" title=\"安装 Doxygen\"></a>安装 Doxygen</h2><p>Doxygen 在Windows平台下的安装比较简单，<a href=\"http://www.doxygen.nl/\">Doxygen的项目主页</a>提供了下载和安装的使用说明，可以下载它们的官方使用手册进行阅读。对于Windows，提供了源代码编译安装和直接安装程序安装两种方式，可以自行选择。</p>\n<p>安装成功后，使用命令行命令</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">doxygen --help</div></pre></td></tr></table></figure>\n<p>就可以查看帮助文档，对应参数含义一目了然，降低了入手难度。</p>\n<p>使用命令，</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">doxygen -g doxygen_filename</div></pre></td></tr></table></figure>\n<p>就可以在当前目录下建立一个doxygen配置文件，用文本编辑器打开就可以编辑里面的配置选项。</p>\n<p>使用命令，</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">doxygen doxygen_filename</div></pre></td></tr></table></figure>\n<p>就可以生成注释文档了。</p>\n<p>下面就来说一说对中文的支持。</p>\n<h2 id=\"生成-HTML-格式文档\"><a href=\"#生成-HTML-格式文档\" class=\"headerlink\" title=\"生成 HTML 格式文档\"></a>生成 HTML 格式文档</h2><p>中文之所以乱码，很多时候是由于编码和译码格式不同，所以我们需要先知道自己代码文件的编码方式。我的代码都是建立在Visual Studio上的，可以通过VS的高级保存选项查看自己代码文件的存储编码格式。对于中文版的VS，一般应该是GB2312。</p>\n<p>我们打开 Doxygen 的配置文件，将里面的 INPUT_ENCODING 改为我们代码文件的编码格式，这里就改成 GB2312。</p>\n<p>这样一来，编译出来的 HTML 页面就不会有中文乱码了。</p>\n<h2 id=\"生成Latex-格式文档\"><a href=\"#生成Latex-格式文档\" class=\"headerlink\" title=\"生成Latex 格式文档\"></a>生成Latex 格式文档</h2><p>生成 Latex 需要本机上安装有 Latex 的编译环境。如果是中文用户，推荐的是CTEX套件，可以到他们的网站上去下载。</p>\n<p>可以看到，Doxygen为Latex文件的编译生成了make文件，我们在命令行窗口中执行make命令就可以完成编译，然而这时候会发现编译出错，pdf文档无法生成。</p>\n<p>打开生成的refman.latex文档，添加宏包 CJKutf8。然后找到 <code>\\begin{document}</code>一行，将其改为</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">\\begin&#123;document&#125;</div><div class=\"line\">\\begin&#123;CJK&#125;&#123;UTF8&#125;&#123;gbsn&#125;</div></pre></td></tr></table></figure>\n<p>也就是说为正文提供了CJK环境，这样中文文本就可以正常编译了。</p>\n<p>相应的，我们要将结尾的 <code>\\end{document)</code>改为：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">\\end&#123;CJK&#125;</div><div class=\"line\">\\end&#123;document&#125;</div></pre></td></tr></table></figure></p>\n<p>这样，运行make命令之后，就可以看到中文的注释文档了。</p>"},{"title":"YOLO 论文阅读","date":"2017-02-04T10:49:22.000Z","_content":"YOLO(**Y**ou **O**nly **L**ook **O**nce)是一个流行的目标检测方法，和Faster RCNN等state of the art方法比起来，主打检测速度快。截止到目前为止（2017年2月初），YOLO已经发布了两个版本，在下文中分别称为[YOLO V1](https://arxiv.org/abs/1506.02640)和[YOLO V2](https://arxiv.org/abs/1612.08242)。YOLO V2的代码目前作为[Darknet](http://pjreddie.com/darknet/yolo/)的一部分开源在[GitHub]()。在这篇博客中，记录了阅读YOLO两个版本论文中的重点内容，并着重总结V2版本的改进。\n\n![YOLO V2的检测效果示意](/img/yolo2_result.png)\n<!-- more -->\n\n## YOLO V1\n这里不妨把YOLO V1论文[\"You Only Look Once: Unitied, Real-Time Object Detection\"](https://arxiv.org/abs/1506.02640)的摘要部分意译如下：\n\n> 我们提出了一种新的物体检测方法YOLO。之前的目标检测方法大多把分类器重新调整用于检测（这里应该是在说以前的方法大多将检测问题看做分类问题，使用滑动窗提取特征并使用分类器进行分类）。我们将检测问题看做回归，分别给出bounding box的位置和对应的类别概率。对于给定输入图像，只需使用CNN网络计算一次，就同时给出bounding box位置和类别概率。由于整个pipeline都是同一个网络，所以很容易进行端到端的训练。\n> YOLO相当快。base model可以达到45fps，一个更小的model（Fast YOLO），可以达到155fps，同时mAP是其他可以实现实时检测方法的两倍。和目前的State of the art方法相比，YOLO的localization误差较大，但是对于背景有更低的误检（False Positives）。同时，YOLO能够学习到更加泛化的特征，例如艺术品中物体的检测等任务，表现很好。\n\n和以往较为常见的方法，如HoG+SVM的行人检测，DPM，RCNN系方法等不同，YOLO接受image作为输入，直接输出bounding box的位置和对应位置为某一类的概率。我们可以把它和目前的State of the art的Faster RCNN方法对比。Faster RCNN方法需要两个网络RPN和Fast RCNN，其中前者负责接受图像输入，并提出proposal。后续Fast RCNN再给出这些proposal是某一类的概率。也正是因为这种“直来直去”的方法，YOLO才能达到这么快的速度。也真是令人感叹，网络参数够多，数据够多，什么映射关系都能学出来。。。下图就是YOLO的检测系统示意图。在test阶段，经过单个CNN网络前向计算后，再经过非极大值抑制，就可以给出检测结果。\n![YOLO V1检测系统示意图](/img/yolo1_detection_system.png)\n\n### 基本思路\n![基础思路示意图](/img/yolo1_basic_idea.png)\n\n- 网格划分：将输入image划分为$S \\times S$个grid cell，如果image中某个object box的中心落在某个grid cell内部，那么这个cell就对检测该object负责（responsible for detection that object）。同时，每个grid cell同时预测$B$个bounding box的位置和一个置信度。这个置信度并不只是该bounding box是待检测目标的概率，而是该bounding box是待检测目标的概率乘上该bounding box和真实位置的IoU的积。通过乘上这个交并比，反映出该bounding box预测位置的精度。如下式所示：\n\n$$\\text{confidence} = P(\\text{Object})\\times \\text{IoU}_{\\text{pred}}^{\\text{truth}}$$\n\n- 网络输出：每个bounding box对应于5个输出，分别是$x,y,w,h$和上述提到的置信度。其中，$x,y$代表bounding box的中心离开其所在grid cell边界的偏移。$w,h$代表bounding box真实宽高相对于整幅图像的比例。$x,y,w,h$这几个参数都已经被bounded到了区间$[0,1]$上。除此以外，每个grid cell还产生$C$个条件概率，$P(\\text{Class}_i|\\text{Object})$。注意，我们不管$B$的大小，每个grid cell只产生一组这样的概率。在test的非极大值抑制阶段，对于每个bounding box，我们应该按照下式衡量该框是否应该予以保留。\n$$\\text{confidence}\\times P(\\text{Class}_i|\\text{Object}) = P(\\text{Class}_i)\\times \\text{IoU}_{\\text{pred}}^{\\text{truth}}$$\n\n- 实际参数：在PASCAL VOC进行测试时，使用$S = 7$, $B=2$。由于共有20类，故$C=20$。所以，我们的网络输出大小为$7\\times 7 \\times 30$\n\n### 网络模型结构\nInspired by GoogLeNet，但是没有采取inception的结构，simply使用了$1\\times 1$的卷积核。base model共有24个卷积层，后面接2个全连接层，如下图所示。\n![YOLO的网络结构示意图](/img/yolo1_network_arch.png)\n\n另外，Fast YOLO使用了更小的网络结构（9个卷积层，且filter的数目也少了），其他部分完全一样。\n\n### 训练\n同很多人一样，这里作者也是先在ImageNet上做了预训练。使用上图网络结构中的前20个卷积层，后面接一个average-pooling层和全连接层，在ImageNet 1000类数据集上训练了一周，达到了88%的top-5准确率。\n\n由于[Ren的论文](https://arxiv.org/abs/1504.06066)提到给预训练的模型加上额外的卷积层和全连接层能够提升性能，所以我们加上了剩下的4个卷积层和2个全连接层，权值为随机初始化。同时，我们把网络输入的分辨率从$224\\times 224$提升到了$448 \\times 448$。\n\n在最后一层，我们使用了线性激活函数；其它层使用了leaky ReLU激活函数。如下所示：\n$$\nf(x)=\n\\begin{cases}\nx, &\\text{if}\\ x > 0 \\\\\\\\\n0.1x, &\\text{otherwise}\n\\end{cases}\n$$\n\n很重要的问题就是定义很好的loss function，为此，作者提出了以下几点说明：\n\n- loss的形式采用误差平方和的形式（真是把回归进行到底了。。。）\n- 由于很多的grid cell没有目标物体存在，所以给有目标存在的bounding box和没有目标存在的bounding box设置了不同的比例因子进行平衡。具体来说，\n$$\\lambda_{\\text{coord}} = 5，\\lambda_{\\text{noobj}} = 0.5$$\n- 直接使用$w$和$h$，这样大的box的差异在loss中占得比重会和小的box不平衡，所以这里使用$\\sqrt{w}$和$\\sqrt{h}$。\n- 上文中已经提到，同一个grid cell会提出多个bounding box。在training阶段，我们只想让一个bounding box对应object。所以，我们计算每个bounding box和ground truth的IoU，以此为标准得到最好的那个bounding box，其他的认为no obj。\n\nloss函数的具体形式见下图（实在是不想打这一大串公式。。）。其中，$\\mathbb{1}_i$表示是否有目标出现在第$i$个grid cell。\n\n$\\mathbb{1}_{i,j}$表示第$i$个grid cell的第$j$个bounding box是否对某个目标负责。\n![YOLO的损失函数定义](/img/yolo1_loss_fun.png)\n\n在进行反向传播时，由于loss都是二次形式，所以导数形式都是统一的。下面是Darknet中[detection_layer.c](https://github.com/pjreddie/darknet/blob/master/src/detection_layer.c)中训练部分的代码。在代码中，计算了cost（也就是loss）和delta（也就是反向的导数），\n\n``` c\nif(state.train){\n    float avg_iou = 0;\n    float avg_cat = 0;\n    float avg_allcat = 0;\n    float avg_obj = 0;\n    float avg_anyobj = 0;\n    int count = 0;\n    *(l.cost) = 0;\n    int size = l.inputs * l.batch;\n    memset(l.delta, 0, size * sizeof(float));\n    for (b = 0; b < l.batch; ++b){\n        int index = b*l.inputs;\n        // for each grid cell\n        for (i = 0; i < locations; ++i) {   // locations = S * S = 49\n            int truth_index = (b*locations + i)*(1+l.coords+l.classes);\n            int is_obj = state.truth[truth_index];\n            // for each bbox\n            for (j = 0; j < l.n; ++j) {     // l.n = B = 2\n                int p_index = index + locations*l.classes + i*l.n + j;\n                l.delta[p_index] = l.noobject_scale*(0 - l.output[p_index]);\n                // 因为no obj对应的bbox很多，而responsible的只有一个\n                // 这里统一加上，如果一会判断该bbox responsible for object，再把它减去\n                *(l.cost) += l.noobject_scale*pow(l.output[p_index], 2);  \n                avg_anyobj += l.output[p_index];\n            }\n\n            int best_index = -1;\n            float best_iou = 0;\n            float best_rmse = 20;\n            // 该grid cell没有目标，直接返回\n            if (!is_obj){\n                continue;\n            }\n            // 否则，找出responsible的bounding box，计算其他几项的loss\n            int class_index = index + i*l.classes;\n            for(j = 0; j < l.classes; ++j) {\n                l.delta[class_index+j] = l.class_scale * (state.truth[truth_index+1+j] - l.output[class_index+j]);\n                *(l.cost) += l.class_scale * pow(state.truth[truth_index+1+j] - l.output[class_index+j], 2);\n                if(state.truth[truth_index + 1 + j]) avg_cat += l.output[class_index+j];\n                avg_allcat += l.output[class_index+j];\n            }\n\n            box truth = float_to_box(state.truth + truth_index + 1 + l.classes);\n            truth.x /= l.side;\n            truth.y /= l.side;\n            // 找到最好的IoU，对应的bbox是responsible的，记录其index\n            for(j = 0; j < l.n; ++j){\n                int box_index = index + locations*(l.classes + l.n) + (i*l.n + j) * l.coords;\n                box out = float_to_box(l.output + box_index);\n                out.x /= l.side;\n                out.y /= l.side;\n\n                if (l.sqrt){\n                    out.w = out.w*out.w;\n                    out.h = out.h*out.h;\n                }\n\n                float iou  = box_iou(out, truth);\n                //iou = 0;\n                float rmse = box_rmse(out, truth);\n                if(best_iou > 0 || iou > 0){\n                    if(iou > best_iou){\n                        best_iou = iou;\n                        best_index = j;\n                    }\n                }else{\n                    if(rmse < best_rmse){\n                        best_rmse = rmse;\n                        best_index = j;\n                    }\n                }\n            }\n\n            if(l.forced){\n                if(truth.w*truth.h < .1){\n                    best_index = 1;\n                }else{\n                    best_index = 0;\n                }\n            }\n            if(l.random && *(state.net.seen) < 64000){\n                best_index = rand()%l.n;\n            }\n\n            int box_index = index + locations*(l.classes + l.n) + (i*l.n + best_index) * l.coords;\n            int tbox_index = truth_index + 1 + l.classes;\n\n            box out = float_to_box(l.output + box_index);\n            out.x /= l.side;\n            out.y /= l.side;\n            if (l.sqrt) {\n                out.w = out.w*out.w;\n                out.h = out.h*out.h;\n            }\n            float iou  = box_iou(out, truth);\n\n            //printf(\"%d,\", best_index);\n            int p_index = index + locations*l.classes + i*l.n + best_index;\n            *(l.cost) -= l.noobject_scale * pow(l.output[p_index], 2);  // 还记得我们曾经统一加过吗？这里需要减去了\n            *(l.cost) += l.object_scale * pow(1-l.output[p_index], 2);\n            avg_obj += l.output[p_index];\n            l.delta[p_index] = l.object_scale * (1.-l.output[p_index]);\n\n            if(l.rescore){\n                l.delta[p_index] = l.object_scale * (iou - l.output[p_index]);\n            }\n\n            l.delta[box_index+0] = l.coord_scale*(state.truth[tbox_index + 0] - l.output[box_index + 0]);\n            l.delta[box_index+1] = l.coord_scale*(state.truth[tbox_index + 1] - l.output[box_index + 1]);\n            l.delta[box_index+2] = l.coord_scale*(state.truth[tbox_index + 2] - l.output[box_index + 2]);\n            l.delta[box_index+3] = l.coord_scale*(state.truth[tbox_index + 3] - l.output[box_index + 3]);\n            if(l.sqrt){\n                l.delta[box_index+2] = l.coord_scale*(sqrt(state.truth[tbox_index + 2]) - l.output[box_index + 2]);\n                l.delta[box_index+3] = l.coord_scale*(sqrt(state.truth[tbox_index + 3]) - l.output[box_index + 3]);\n            }\n\n            *(l.cost) += pow(1-iou, 2);\n            avg_iou += iou;\n            ++count;\n        }\n    }\n\n```\n\n## YOLO V2\nYOLO V2是原作者在V1基础上做出改进后提出的。为了达到题目中所称的Better，Faster，Stronger的目标，主要改进点如下。当然，具体内容还是要深入论文。\n- 受到Faster RCNN方法的启发，引入了anchor。同时使用了K-Means方法，对anchor数量进行了讨论，在精度和速度之间做出折中；\n- 修改了网络结构，去掉了全连接层，改成了全卷积结构；\n- 引入了WordTree结构，将检测和分类问题做成了一个统一的框架，并充分利用ImageNet和COCO数据集的数据。\n\n下面，还是先把论文的摘要意译如下：\n>我们引入了YOLO 9000模型，它是实时物体检测的State of the art的工作，能够检测超过9K类目标。首先，我们对以前的工作（YOLO V1）做出了若干改进，使其成为了一种在实时检测方法内在PASCAL VOC和COCO上State of the art的效果。通过一种新颖的多尺度训练犯法（multi-scale training method）， YOLO V2适用于不同大小尺寸输入的image，在精度和效率上达到了很好地trade-off。在67fps的速度下，VOC2007上达到了76.8mAP的精度。40fps时，达到了78.6mAP，已经超过了Faster RCNN with ResNet和SSD的精度，同时比它们更快！最后，我们提出了一种能够同时进行检测任务和分类任务的联合训练方法。使用这种方法，我们在COCO detection dataset和ImageNet classification dataset上同时训练模型。这种方法使得我们能够对那些没有label上detection data的数据做出detection的预测。我们使用ImageNet的detection任务做了验证。YOLO 9000在仅有200类中44类的detection data的情况下，仍然在ImageNet datection任务中取得了19.7mAP的成绩。对于不在COCO中的156类，YOLO9000成绩为16.0mAP。我们使得YOLO9000能够在保证实时的前提下对9K类目标进行检测。\n\n根据论文结构的安排，将从Better，Faster和Stronger三个方面对论文提出的各条改进措施进行介绍。\n\n## Better\n在YOLO V1的基础上，作者提出了不少的改进来进一步提升算法的性能（mAP），主要改进措施包括网络结构的改进（第1，3，5，6条）和Anchor Box的引进（第3，4，5条）以及训练方法（第2，7条）。\n\n### 改进1：引入BN层（Batch Normalization）\nBatch Normalization能够加快模型收敛，并提供一定的正则化。作者在每个conv层都加上了了BN层，同时去掉了原来模型中的drop out部分，这带来了2%的性能提升。\n\n### 改进2：高分辨率分类器（High Resolution Classifier）\nYOLO V1首先在ImageNet上以$224\\times 224$大小图像作为输入进行训练，之后在检测任务中提升到$448\\times 448$。这里，作者在训练完224大小的分类网络后，首先调整网络大小为$448\\times 448$，然后在ImageNet上进行fine tuning（10个epoch）。也就是得到了一个高分辨率的cls。再把它用detection上训练。这样，能够提升4%。\n\n### 改进3：引入Anchor Box\nYOLO V1中直接在CNN后面街上全连接层，直接回归bounding box的参数。这里引入了Faster RCNN中的anchor box概念，不再直接回归bounding box的参数，而是相对于anchor box的参数。\n\n作者去掉后面的fc层和最后一个max pooling层，以期得到更高分辨率的feature map。同时，shrink网络接受$416\\times 416$（而不是448）大小的输入image。这是因为我们想要最后的feature map大小是奇数，这样就能够得到一个center cell（比较大的目标，更有可能占据中间的位置）。由于YOLO conv-pooling的效应是将image downsamplig 32倍，所以最后feature map大小为$416/32 = 13$。\n\n与YOLO V1不同的是，我们不再对同一个grid cell下的bounding box统一产生一个数量为$C$的类别概率，而是对于每一个bounding box都产生对应的$C$类概率。和YOLO V1一样的是，我们仍然产生confidence，意义也完全一样。\n\n使用anchor后，我们的精度accuracy降低了，不过recall上来了。（这也较好理解。原来每个grid cell内部只有2个bounding box，造成recall不高。现在recall高上来了，accuracy会下降一些）。\n\n### 改进4：Dimension Cluster\n在引入anchor box后，一个问题就是如何确定anchor的位置和大小？Faster RCNN中是手工选定的，每隔stride设定一个anchor，并根据不同的面积比例和长宽比例产生9个anchor box。在本文中，作者使用了聚类方法对如何选取anchor box做了探究。这点应该是论文中很有新意的地方。\n\n这里对作者使用的方法不再过多赘述，强调以下两点：\n- 作者使用的聚类方法是K-Means；\n- 相似性度量不用欧氏距离，而是用IoU，定义如下：\n$$d(\\text{box}, \\text{centroid}) = 1-\\text{IoU}(\\text{box}, \\text{centroid})$$\n\n使用不同的$k$，聚类实验结果如下，作者折中采用了$k = 5$。而且经过实验，发现当取$k=9$时候，已经能够超过Faster RCNN采用的手工固定anchor box的方法。下图右侧图是在COCO和VOC数据集上$k=5$的聚类后结果。这些box可以作为anchor box使用。\n![](/img/yolo2_cluster_result.png)\n\n### 改进5：直接位置预测（Direct Location Prediction）\n我们仍然延续了YOLO V1中的思路，预测box相对于grid cell的位置。使用sigmoid函数作为激活函数，使得最终输出值落在$[0, 1]$这个区间上。\n\n在output的feature map上，对于每个cell（共计$13\\times 13$个），给出对应每个bounding box的输出$t_x$, $t_y$, $t_w$, $t_h$。每个cell共计$k=5$个bounding box。如何由这几个参数确定bounding box的真实位置呢？见下图。\n\n![确定bbox的位置](/img/yolo2_bbox_location.png)\n\n设该grid cell距离图像左上角的offset是$(c_x, c_y)$，那么bounding box的位置和宽高计算如下。注意，box的位置是相对于grid cell的，而宽高是相对于anchor box的。\n![bounding box参数的计算方法](/img/yolo2_bbox_param.png)\n\n### 改进6：Fine-Gained Features\n这个trick是受Faster RCNN和SSD方法中使用多个不同feature map提高算法对不同分辨率目标物体的检测能力的启发，加入了一个pass-through层，直接将倒数第二层的$26\\times 26$大小的feature map加进来。\n\n在具体实现时，是将higher resolution（也就是$26\\times 26$）的feature map stacking在一起。比如，原大小为$26\\times 26 \\times 512$的feature map，因为我们要将其变为$13\\times 13$大小，所以，将在空间上相近的点移到后面的channel上去，这部分可以参考Darknet中`reorg_layer`的实现。\n\n使用这一扩展之后的feature map，提高了1%的性能提升。\n\n### 改进7：多尺度训练（Multi-Scale Training）\n在实际应用时，输入的图像大小有可能是变化的。我们也将这一点考虑进来。因为我们的网络是全卷积神经网络，只有conv和pooling层，没有全连接层，所以可以适应不同大小的图像输入。所以网络结构上是没问题的。\n\n具体来说，在训练的时候，我们每隔一定的epoch（例如10）就随机改变网络的输入图像大小。由于我们的网络最终降采样的比例是$32$，所以随机生成的图像大小为$32$的倍数，即$\\lbrace 320, 352, \\dots, 608\\rbrace$。\n\n在实际使用中，如果输入图像的分辨率较低，YOLO V2可以在不错的精度下达到很快的检测速度。这可以被用应在计算能力有限的场合（无GPU或者GPU很弱）或多路视频信号的实时处理。如果输入图像的分辨率较高，YOLO V2可以作为state of the art的检测器，并仍能获得不错的检测速度。对于目前流行的检测方法（Faster RCNN，SSD，YOLO）的精度和帧率之间的关系，见下图。可以看到，作者在30fps处画了一条竖线，这是算法能否达到实时处理的分水岭。Faster RCNN败下阵来，而YOLO V2的不同点代表了不同输入图像分辨率下算法的表现。对于详细数据，见图下的表格对比（VOC 2007上进行测试）。\n\n![不同检测方法的对比](/img/yolo2_different_methods_comparation.png)\n![不同检测方法的对比](/img/yolo2_different_methods_comparation_in_table.png)\n\n### 总结\n在Better这部分的末尾，作者给出了一个表格，指出了主要提升性能的措施。例外是网络结构上改为带Anchor box的全卷积网络结构（提升了recall，但对mAP基本无影响）和使用新的网络（计算量少了~33%）。\n![不同改进措施的影响](/img/yolo2_different_methods_improvement.png)\n\n## Faster\n这部分的改进为网络结构的变化。包括Faster RCNN在内的很多检测方法都使用VGG-16作为base network。VGG-16精度够高但是计算量较大（对于大小为$224\\times 224$的单幅输入图像，卷积层就需要30.69B次浮点运算）。在YOLO V1中，我们的网络结构较为简单，在精度上不如VGG-16（ImageNet测试，88.0% vs 90.0%）。\n\n在YOLO V2中，我们使用了一种新的网络结构Darknet-19（因为base network有19个卷积层）。和VGG相类似，我们的卷积核也都是$3\\times 3$大小，同时每次pooling操作后channel数double。另外，在NIN工作基础上，我们在网络最后使用了global average pooling层，同时使用$1\\times 1$大小的卷积核来做feature map的压缩（分辨率不变，channel减小，新的元素是原来相应位置不同channel的线性组合），同时使用了Batch Normalization技术。具体的网络结构见下表。Darknet-19计算量大大减小，同时精度超过了VGG-16。\n![Darknet-19的网络结构](/img/yolo2_dartnet_19_structure.png)\n\n在训练过程中，首先在ImageNet 1K上进行分类器的训练。使用数据增强技术（如随机裁剪、旋转、饱和度变化等）。和上面的讨论相对应，首先使用$224\\times 224$大小的图像进行训练，再使用$448\\times 448$的图像进行fine tuning，具体训练参数设置可以参见论文和对应的代码。这里不再多说。\n\n然后，我们对检测任务进行训练。对网络结构进行微调，去掉最后的卷积层（因为它是我们当初为了得到1K类的分类置信度加上去的），增加$3$个$3\\times 3$大小，channel数为$1024$的卷积层，并每个都跟着一个$1\\times 1$大小的卷积层，channel数由我们最终的检测任务需要的输出决定。对于VOC的检测任务来说，我们需要预测$5$个box，每个需要$5$个参数（相对位置，相对大小和置信度），同时$20$个类别的置信度，所以输出共$5\\times(5+20)=125$。从YOLO V2的`yolo_voc.cfg`[文件](https://github.com/pjreddie/darknet/blob/master/cfg/yolo.cfg)中，我们也可以看到如下的对应结构：\n\n```\n[convolutional]\nbatch_normalize=1\nsize=3\nstride=1\npad=1\nfilters=1024\nactivation=leaky\n\n[convolutional]\nsize=1\nstride=1\npad=1\nfilters=125\nactivation=linear\n```\n\n同时，加上上文提到的pass-through结构。\n\n## Stronger\n未完待续\n","source":"_posts/yolo-paper.md","raw":"---\ntitle: YOLO 论文阅读\ndate: 2017-02-04 18:49:22\ntags:\n    - paper\n    - yolo\n---\nYOLO(**Y**ou **O**nly **L**ook **O**nce)是一个流行的目标检测方法，和Faster RCNN等state of the art方法比起来，主打检测速度快。截止到目前为止（2017年2月初），YOLO已经发布了两个版本，在下文中分别称为[YOLO V1](https://arxiv.org/abs/1506.02640)和[YOLO V2](https://arxiv.org/abs/1612.08242)。YOLO V2的代码目前作为[Darknet](http://pjreddie.com/darknet/yolo/)的一部分开源在[GitHub]()。在这篇博客中，记录了阅读YOLO两个版本论文中的重点内容，并着重总结V2版本的改进。\n\n![YOLO V2的检测效果示意](/img/yolo2_result.png)\n<!-- more -->\n\n## YOLO V1\n这里不妨把YOLO V1论文[\"You Only Look Once: Unitied, Real-Time Object Detection\"](https://arxiv.org/abs/1506.02640)的摘要部分意译如下：\n\n> 我们提出了一种新的物体检测方法YOLO。之前的目标检测方法大多把分类器重新调整用于检测（这里应该是在说以前的方法大多将检测问题看做分类问题，使用滑动窗提取特征并使用分类器进行分类）。我们将检测问题看做回归，分别给出bounding box的位置和对应的类别概率。对于给定输入图像，只需使用CNN网络计算一次，就同时给出bounding box位置和类别概率。由于整个pipeline都是同一个网络，所以很容易进行端到端的训练。\n> YOLO相当快。base model可以达到45fps，一个更小的model（Fast YOLO），可以达到155fps，同时mAP是其他可以实现实时检测方法的两倍。和目前的State of the art方法相比，YOLO的localization误差较大，但是对于背景有更低的误检（False Positives）。同时，YOLO能够学习到更加泛化的特征，例如艺术品中物体的检测等任务，表现很好。\n\n和以往较为常见的方法，如HoG+SVM的行人检测，DPM，RCNN系方法等不同，YOLO接受image作为输入，直接输出bounding box的位置和对应位置为某一类的概率。我们可以把它和目前的State of the art的Faster RCNN方法对比。Faster RCNN方法需要两个网络RPN和Fast RCNN，其中前者负责接受图像输入，并提出proposal。后续Fast RCNN再给出这些proposal是某一类的概率。也正是因为这种“直来直去”的方法，YOLO才能达到这么快的速度。也真是令人感叹，网络参数够多，数据够多，什么映射关系都能学出来。。。下图就是YOLO的检测系统示意图。在test阶段，经过单个CNN网络前向计算后，再经过非极大值抑制，就可以给出检测结果。\n![YOLO V1检测系统示意图](/img/yolo1_detection_system.png)\n\n### 基本思路\n![基础思路示意图](/img/yolo1_basic_idea.png)\n\n- 网格划分：将输入image划分为$S \\times S$个grid cell，如果image中某个object box的中心落在某个grid cell内部，那么这个cell就对检测该object负责（responsible for detection that object）。同时，每个grid cell同时预测$B$个bounding box的位置和一个置信度。这个置信度并不只是该bounding box是待检测目标的概率，而是该bounding box是待检测目标的概率乘上该bounding box和真实位置的IoU的积。通过乘上这个交并比，反映出该bounding box预测位置的精度。如下式所示：\n\n$$\\text{confidence} = P(\\text{Object})\\times \\text{IoU}_{\\text{pred}}^{\\text{truth}}$$\n\n- 网络输出：每个bounding box对应于5个输出，分别是$x,y,w,h$和上述提到的置信度。其中，$x,y$代表bounding box的中心离开其所在grid cell边界的偏移。$w,h$代表bounding box真实宽高相对于整幅图像的比例。$x,y,w,h$这几个参数都已经被bounded到了区间$[0,1]$上。除此以外，每个grid cell还产生$C$个条件概率，$P(\\text{Class}_i|\\text{Object})$。注意，我们不管$B$的大小，每个grid cell只产生一组这样的概率。在test的非极大值抑制阶段，对于每个bounding box，我们应该按照下式衡量该框是否应该予以保留。\n$$\\text{confidence}\\times P(\\text{Class}_i|\\text{Object}) = P(\\text{Class}_i)\\times \\text{IoU}_{\\text{pred}}^{\\text{truth}}$$\n\n- 实际参数：在PASCAL VOC进行测试时，使用$S = 7$, $B=2$。由于共有20类，故$C=20$。所以，我们的网络输出大小为$7\\times 7 \\times 30$\n\n### 网络模型结构\nInspired by GoogLeNet，但是没有采取inception的结构，simply使用了$1\\times 1$的卷积核。base model共有24个卷积层，后面接2个全连接层，如下图所示。\n![YOLO的网络结构示意图](/img/yolo1_network_arch.png)\n\n另外，Fast YOLO使用了更小的网络结构（9个卷积层，且filter的数目也少了），其他部分完全一样。\n\n### 训练\n同很多人一样，这里作者也是先在ImageNet上做了预训练。使用上图网络结构中的前20个卷积层，后面接一个average-pooling层和全连接层，在ImageNet 1000类数据集上训练了一周，达到了88%的top-5准确率。\n\n由于[Ren的论文](https://arxiv.org/abs/1504.06066)提到给预训练的模型加上额外的卷积层和全连接层能够提升性能，所以我们加上了剩下的4个卷积层和2个全连接层，权值为随机初始化。同时，我们把网络输入的分辨率从$224\\times 224$提升到了$448 \\times 448$。\n\n在最后一层，我们使用了线性激活函数；其它层使用了leaky ReLU激活函数。如下所示：\n$$\nf(x)=\n\\begin{cases}\nx, &\\text{if}\\ x > 0 \\\\\\\\\n0.1x, &\\text{otherwise}\n\\end{cases}\n$$\n\n很重要的问题就是定义很好的loss function，为此，作者提出了以下几点说明：\n\n- loss的形式采用误差平方和的形式（真是把回归进行到底了。。。）\n- 由于很多的grid cell没有目标物体存在，所以给有目标存在的bounding box和没有目标存在的bounding box设置了不同的比例因子进行平衡。具体来说，\n$$\\lambda_{\\text{coord}} = 5，\\lambda_{\\text{noobj}} = 0.5$$\n- 直接使用$w$和$h$，这样大的box的差异在loss中占得比重会和小的box不平衡，所以这里使用$\\sqrt{w}$和$\\sqrt{h}$。\n- 上文中已经提到，同一个grid cell会提出多个bounding box。在training阶段，我们只想让一个bounding box对应object。所以，我们计算每个bounding box和ground truth的IoU，以此为标准得到最好的那个bounding box，其他的认为no obj。\n\nloss函数的具体形式见下图（实在是不想打这一大串公式。。）。其中，$\\mathbb{1}_i$表示是否有目标出现在第$i$个grid cell。\n\n$\\mathbb{1}_{i,j}$表示第$i$个grid cell的第$j$个bounding box是否对某个目标负责。\n![YOLO的损失函数定义](/img/yolo1_loss_fun.png)\n\n在进行反向传播时，由于loss都是二次形式，所以导数形式都是统一的。下面是Darknet中[detection_layer.c](https://github.com/pjreddie/darknet/blob/master/src/detection_layer.c)中训练部分的代码。在代码中，计算了cost（也就是loss）和delta（也就是反向的导数），\n\n``` c\nif(state.train){\n    float avg_iou = 0;\n    float avg_cat = 0;\n    float avg_allcat = 0;\n    float avg_obj = 0;\n    float avg_anyobj = 0;\n    int count = 0;\n    *(l.cost) = 0;\n    int size = l.inputs * l.batch;\n    memset(l.delta, 0, size * sizeof(float));\n    for (b = 0; b < l.batch; ++b){\n        int index = b*l.inputs;\n        // for each grid cell\n        for (i = 0; i < locations; ++i) {   // locations = S * S = 49\n            int truth_index = (b*locations + i)*(1+l.coords+l.classes);\n            int is_obj = state.truth[truth_index];\n            // for each bbox\n            for (j = 0; j < l.n; ++j) {     // l.n = B = 2\n                int p_index = index + locations*l.classes + i*l.n + j;\n                l.delta[p_index] = l.noobject_scale*(0 - l.output[p_index]);\n                // 因为no obj对应的bbox很多，而responsible的只有一个\n                // 这里统一加上，如果一会判断该bbox responsible for object，再把它减去\n                *(l.cost) += l.noobject_scale*pow(l.output[p_index], 2);  \n                avg_anyobj += l.output[p_index];\n            }\n\n            int best_index = -1;\n            float best_iou = 0;\n            float best_rmse = 20;\n            // 该grid cell没有目标，直接返回\n            if (!is_obj){\n                continue;\n            }\n            // 否则，找出responsible的bounding box，计算其他几项的loss\n            int class_index = index + i*l.classes;\n            for(j = 0; j < l.classes; ++j) {\n                l.delta[class_index+j] = l.class_scale * (state.truth[truth_index+1+j] - l.output[class_index+j]);\n                *(l.cost) += l.class_scale * pow(state.truth[truth_index+1+j] - l.output[class_index+j], 2);\n                if(state.truth[truth_index + 1 + j]) avg_cat += l.output[class_index+j];\n                avg_allcat += l.output[class_index+j];\n            }\n\n            box truth = float_to_box(state.truth + truth_index + 1 + l.classes);\n            truth.x /= l.side;\n            truth.y /= l.side;\n            // 找到最好的IoU，对应的bbox是responsible的，记录其index\n            for(j = 0; j < l.n; ++j){\n                int box_index = index + locations*(l.classes + l.n) + (i*l.n + j) * l.coords;\n                box out = float_to_box(l.output + box_index);\n                out.x /= l.side;\n                out.y /= l.side;\n\n                if (l.sqrt){\n                    out.w = out.w*out.w;\n                    out.h = out.h*out.h;\n                }\n\n                float iou  = box_iou(out, truth);\n                //iou = 0;\n                float rmse = box_rmse(out, truth);\n                if(best_iou > 0 || iou > 0){\n                    if(iou > best_iou){\n                        best_iou = iou;\n                        best_index = j;\n                    }\n                }else{\n                    if(rmse < best_rmse){\n                        best_rmse = rmse;\n                        best_index = j;\n                    }\n                }\n            }\n\n            if(l.forced){\n                if(truth.w*truth.h < .1){\n                    best_index = 1;\n                }else{\n                    best_index = 0;\n                }\n            }\n            if(l.random && *(state.net.seen) < 64000){\n                best_index = rand()%l.n;\n            }\n\n            int box_index = index + locations*(l.classes + l.n) + (i*l.n + best_index) * l.coords;\n            int tbox_index = truth_index + 1 + l.classes;\n\n            box out = float_to_box(l.output + box_index);\n            out.x /= l.side;\n            out.y /= l.side;\n            if (l.sqrt) {\n                out.w = out.w*out.w;\n                out.h = out.h*out.h;\n            }\n            float iou  = box_iou(out, truth);\n\n            //printf(\"%d,\", best_index);\n            int p_index = index + locations*l.classes + i*l.n + best_index;\n            *(l.cost) -= l.noobject_scale * pow(l.output[p_index], 2);  // 还记得我们曾经统一加过吗？这里需要减去了\n            *(l.cost) += l.object_scale * pow(1-l.output[p_index], 2);\n            avg_obj += l.output[p_index];\n            l.delta[p_index] = l.object_scale * (1.-l.output[p_index]);\n\n            if(l.rescore){\n                l.delta[p_index] = l.object_scale * (iou - l.output[p_index]);\n            }\n\n            l.delta[box_index+0] = l.coord_scale*(state.truth[tbox_index + 0] - l.output[box_index + 0]);\n            l.delta[box_index+1] = l.coord_scale*(state.truth[tbox_index + 1] - l.output[box_index + 1]);\n            l.delta[box_index+2] = l.coord_scale*(state.truth[tbox_index + 2] - l.output[box_index + 2]);\n            l.delta[box_index+3] = l.coord_scale*(state.truth[tbox_index + 3] - l.output[box_index + 3]);\n            if(l.sqrt){\n                l.delta[box_index+2] = l.coord_scale*(sqrt(state.truth[tbox_index + 2]) - l.output[box_index + 2]);\n                l.delta[box_index+3] = l.coord_scale*(sqrt(state.truth[tbox_index + 3]) - l.output[box_index + 3]);\n            }\n\n            *(l.cost) += pow(1-iou, 2);\n            avg_iou += iou;\n            ++count;\n        }\n    }\n\n```\n\n## YOLO V2\nYOLO V2是原作者在V1基础上做出改进后提出的。为了达到题目中所称的Better，Faster，Stronger的目标，主要改进点如下。当然，具体内容还是要深入论文。\n- 受到Faster RCNN方法的启发，引入了anchor。同时使用了K-Means方法，对anchor数量进行了讨论，在精度和速度之间做出折中；\n- 修改了网络结构，去掉了全连接层，改成了全卷积结构；\n- 引入了WordTree结构，将检测和分类问题做成了一个统一的框架，并充分利用ImageNet和COCO数据集的数据。\n\n下面，还是先把论文的摘要意译如下：\n>我们引入了YOLO 9000模型，它是实时物体检测的State of the art的工作，能够检测超过9K类目标。首先，我们对以前的工作（YOLO V1）做出了若干改进，使其成为了一种在实时检测方法内在PASCAL VOC和COCO上State of the art的效果。通过一种新颖的多尺度训练犯法（multi-scale training method）， YOLO V2适用于不同大小尺寸输入的image，在精度和效率上达到了很好地trade-off。在67fps的速度下，VOC2007上达到了76.8mAP的精度。40fps时，达到了78.6mAP，已经超过了Faster RCNN with ResNet和SSD的精度，同时比它们更快！最后，我们提出了一种能够同时进行检测任务和分类任务的联合训练方法。使用这种方法，我们在COCO detection dataset和ImageNet classification dataset上同时训练模型。这种方法使得我们能够对那些没有label上detection data的数据做出detection的预测。我们使用ImageNet的detection任务做了验证。YOLO 9000在仅有200类中44类的detection data的情况下，仍然在ImageNet datection任务中取得了19.7mAP的成绩。对于不在COCO中的156类，YOLO9000成绩为16.0mAP。我们使得YOLO9000能够在保证实时的前提下对9K类目标进行检测。\n\n根据论文结构的安排，将从Better，Faster和Stronger三个方面对论文提出的各条改进措施进行介绍。\n\n## Better\n在YOLO V1的基础上，作者提出了不少的改进来进一步提升算法的性能（mAP），主要改进措施包括网络结构的改进（第1，3，5，6条）和Anchor Box的引进（第3，4，5条）以及训练方法（第2，7条）。\n\n### 改进1：引入BN层（Batch Normalization）\nBatch Normalization能够加快模型收敛，并提供一定的正则化。作者在每个conv层都加上了了BN层，同时去掉了原来模型中的drop out部分，这带来了2%的性能提升。\n\n### 改进2：高分辨率分类器（High Resolution Classifier）\nYOLO V1首先在ImageNet上以$224\\times 224$大小图像作为输入进行训练，之后在检测任务中提升到$448\\times 448$。这里，作者在训练完224大小的分类网络后，首先调整网络大小为$448\\times 448$，然后在ImageNet上进行fine tuning（10个epoch）。也就是得到了一个高分辨率的cls。再把它用detection上训练。这样，能够提升4%。\n\n### 改进3：引入Anchor Box\nYOLO V1中直接在CNN后面街上全连接层，直接回归bounding box的参数。这里引入了Faster RCNN中的anchor box概念，不再直接回归bounding box的参数，而是相对于anchor box的参数。\n\n作者去掉后面的fc层和最后一个max pooling层，以期得到更高分辨率的feature map。同时，shrink网络接受$416\\times 416$（而不是448）大小的输入image。这是因为我们想要最后的feature map大小是奇数，这样就能够得到一个center cell（比较大的目标，更有可能占据中间的位置）。由于YOLO conv-pooling的效应是将image downsamplig 32倍，所以最后feature map大小为$416/32 = 13$。\n\n与YOLO V1不同的是，我们不再对同一个grid cell下的bounding box统一产生一个数量为$C$的类别概率，而是对于每一个bounding box都产生对应的$C$类概率。和YOLO V1一样的是，我们仍然产生confidence，意义也完全一样。\n\n使用anchor后，我们的精度accuracy降低了，不过recall上来了。（这也较好理解。原来每个grid cell内部只有2个bounding box，造成recall不高。现在recall高上来了，accuracy会下降一些）。\n\n### 改进4：Dimension Cluster\n在引入anchor box后，一个问题就是如何确定anchor的位置和大小？Faster RCNN中是手工选定的，每隔stride设定一个anchor，并根据不同的面积比例和长宽比例产生9个anchor box。在本文中，作者使用了聚类方法对如何选取anchor box做了探究。这点应该是论文中很有新意的地方。\n\n这里对作者使用的方法不再过多赘述，强调以下两点：\n- 作者使用的聚类方法是K-Means；\n- 相似性度量不用欧氏距离，而是用IoU，定义如下：\n$$d(\\text{box}, \\text{centroid}) = 1-\\text{IoU}(\\text{box}, \\text{centroid})$$\n\n使用不同的$k$，聚类实验结果如下，作者折中采用了$k = 5$。而且经过实验，发现当取$k=9$时候，已经能够超过Faster RCNN采用的手工固定anchor box的方法。下图右侧图是在COCO和VOC数据集上$k=5$的聚类后结果。这些box可以作为anchor box使用。\n![](/img/yolo2_cluster_result.png)\n\n### 改进5：直接位置预测（Direct Location Prediction）\n我们仍然延续了YOLO V1中的思路，预测box相对于grid cell的位置。使用sigmoid函数作为激活函数，使得最终输出值落在$[0, 1]$这个区间上。\n\n在output的feature map上，对于每个cell（共计$13\\times 13$个），给出对应每个bounding box的输出$t_x$, $t_y$, $t_w$, $t_h$。每个cell共计$k=5$个bounding box。如何由这几个参数确定bounding box的真实位置呢？见下图。\n\n![确定bbox的位置](/img/yolo2_bbox_location.png)\n\n设该grid cell距离图像左上角的offset是$(c_x, c_y)$，那么bounding box的位置和宽高计算如下。注意，box的位置是相对于grid cell的，而宽高是相对于anchor box的。\n![bounding box参数的计算方法](/img/yolo2_bbox_param.png)\n\n### 改进6：Fine-Gained Features\n这个trick是受Faster RCNN和SSD方法中使用多个不同feature map提高算法对不同分辨率目标物体的检测能力的启发，加入了一个pass-through层，直接将倒数第二层的$26\\times 26$大小的feature map加进来。\n\n在具体实现时，是将higher resolution（也就是$26\\times 26$）的feature map stacking在一起。比如，原大小为$26\\times 26 \\times 512$的feature map，因为我们要将其变为$13\\times 13$大小，所以，将在空间上相近的点移到后面的channel上去，这部分可以参考Darknet中`reorg_layer`的实现。\n\n使用这一扩展之后的feature map，提高了1%的性能提升。\n\n### 改进7：多尺度训练（Multi-Scale Training）\n在实际应用时，输入的图像大小有可能是变化的。我们也将这一点考虑进来。因为我们的网络是全卷积神经网络，只有conv和pooling层，没有全连接层，所以可以适应不同大小的图像输入。所以网络结构上是没问题的。\n\n具体来说，在训练的时候，我们每隔一定的epoch（例如10）就随机改变网络的输入图像大小。由于我们的网络最终降采样的比例是$32$，所以随机生成的图像大小为$32$的倍数，即$\\lbrace 320, 352, \\dots, 608\\rbrace$。\n\n在实际使用中，如果输入图像的分辨率较低，YOLO V2可以在不错的精度下达到很快的检测速度。这可以被用应在计算能力有限的场合（无GPU或者GPU很弱）或多路视频信号的实时处理。如果输入图像的分辨率较高，YOLO V2可以作为state of the art的检测器，并仍能获得不错的检测速度。对于目前流行的检测方法（Faster RCNN，SSD，YOLO）的精度和帧率之间的关系，见下图。可以看到，作者在30fps处画了一条竖线，这是算法能否达到实时处理的分水岭。Faster RCNN败下阵来，而YOLO V2的不同点代表了不同输入图像分辨率下算法的表现。对于详细数据，见图下的表格对比（VOC 2007上进行测试）。\n\n![不同检测方法的对比](/img/yolo2_different_methods_comparation.png)\n![不同检测方法的对比](/img/yolo2_different_methods_comparation_in_table.png)\n\n### 总结\n在Better这部分的末尾，作者给出了一个表格，指出了主要提升性能的措施。例外是网络结构上改为带Anchor box的全卷积网络结构（提升了recall，但对mAP基本无影响）和使用新的网络（计算量少了~33%）。\n![不同改进措施的影响](/img/yolo2_different_methods_improvement.png)\n\n## Faster\n这部分的改进为网络结构的变化。包括Faster RCNN在内的很多检测方法都使用VGG-16作为base network。VGG-16精度够高但是计算量较大（对于大小为$224\\times 224$的单幅输入图像，卷积层就需要30.69B次浮点运算）。在YOLO V1中，我们的网络结构较为简单，在精度上不如VGG-16（ImageNet测试，88.0% vs 90.0%）。\n\n在YOLO V2中，我们使用了一种新的网络结构Darknet-19（因为base network有19个卷积层）。和VGG相类似，我们的卷积核也都是$3\\times 3$大小，同时每次pooling操作后channel数double。另外，在NIN工作基础上，我们在网络最后使用了global average pooling层，同时使用$1\\times 1$大小的卷积核来做feature map的压缩（分辨率不变，channel减小，新的元素是原来相应位置不同channel的线性组合），同时使用了Batch Normalization技术。具体的网络结构见下表。Darknet-19计算量大大减小，同时精度超过了VGG-16。\n![Darknet-19的网络结构](/img/yolo2_dartnet_19_structure.png)\n\n在训练过程中，首先在ImageNet 1K上进行分类器的训练。使用数据增强技术（如随机裁剪、旋转、饱和度变化等）。和上面的讨论相对应，首先使用$224\\times 224$大小的图像进行训练，再使用$448\\times 448$的图像进行fine tuning，具体训练参数设置可以参见论文和对应的代码。这里不再多说。\n\n然后，我们对检测任务进行训练。对网络结构进行微调，去掉最后的卷积层（因为它是我们当初为了得到1K类的分类置信度加上去的），增加$3$个$3\\times 3$大小，channel数为$1024$的卷积层，并每个都跟着一个$1\\times 1$大小的卷积层，channel数由我们最终的检测任务需要的输出决定。对于VOC的检测任务来说，我们需要预测$5$个box，每个需要$5$个参数（相对位置，相对大小和置信度），同时$20$个类别的置信度，所以输出共$5\\times(5+20)=125$。从YOLO V2的`yolo_voc.cfg`[文件](https://github.com/pjreddie/darknet/blob/master/cfg/yolo.cfg)中，我们也可以看到如下的对应结构：\n\n```\n[convolutional]\nbatch_normalize=1\nsize=3\nstride=1\npad=1\nfilters=1024\nactivation=leaky\n\n[convolutional]\nsize=1\nstride=1\npad=1\nfilters=125\nactivation=linear\n```\n\n同时，加上上文提到的pass-through结构。\n\n## Stronger\n未完待续\n","slug":"yolo-paper","published":1,"updated":"2017-02-26T12:09:28.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cizve07bp0013l61h3wp08044","content":"<p>YOLO(<strong>Y</strong>ou <strong>O</strong>nly <strong>L</strong>ook <strong>O</strong>nce)是一个流行的目标检测方法，和Faster RCNN等state of the art方法比起来，主打检测速度快。截止到目前为止（2017年2月初），YOLO已经发布了两个版本，在下文中分别称为<a href=\"https://arxiv.org/abs/1506.02640\" target=\"_blank\" rel=\"external\">YOLO V1</a>和<a href=\"https://arxiv.org/abs/1612.08242\" target=\"_blank\" rel=\"external\">YOLO V2</a>。YOLO V2的代码目前作为<a href=\"http://pjreddie.com/darknet/yolo/\" target=\"_blank\" rel=\"external\">Darknet</a>的一部分开源在<a href=\"\">GitHub</a>。在这篇博客中，记录了阅读YOLO两个版本论文中的重点内容，并着重总结V2版本的改进。</p>\n<p><img src=\"/img/yolo2_result.png\" alt=\"YOLO V2的检测效果示意\"><br><a id=\"more\"></a></p>\n<h2 id=\"YOLO-V1\"><a href=\"#YOLO-V1\" class=\"headerlink\" title=\"YOLO V1\"></a>YOLO V1</h2><p>这里不妨把YOLO V1论文<a href=\"https://arxiv.org/abs/1506.02640\" target=\"_blank\" rel=\"external\">“You Only Look Once: Unitied, Real-Time Object Detection”</a>的摘要部分意译如下：</p>\n<blockquote>\n<p>我们提出了一种新的物体检测方法YOLO。之前的目标检测方法大多把分类器重新调整用于检测（这里应该是在说以前的方法大多将检测问题看做分类问题，使用滑动窗提取特征并使用分类器进行分类）。我们将检测问题看做回归，分别给出bounding box的位置和对应的类别概率。对于给定输入图像，只需使用CNN网络计算一次，就同时给出bounding box位置和类别概率。由于整个pipeline都是同一个网络，所以很容易进行端到端的训练。<br>YOLO相当快。base model可以达到45fps，一个更小的model（Fast YOLO），可以达到155fps，同时mAP是其他可以实现实时检测方法的两倍。和目前的State of the art方法相比，YOLO的localization误差较大，但是对于背景有更低的误检（False Positives）。同时，YOLO能够学习到更加泛化的特征，例如艺术品中物体的检测等任务，表现很好。</p>\n</blockquote>\n<p>和以往较为常见的方法，如HoG+SVM的行人检测，DPM，RCNN系方法等不同，YOLO接受image作为输入，直接输出bounding box的位置和对应位置为某一类的概率。我们可以把它和目前的State of the art的Faster RCNN方法对比。Faster RCNN方法需要两个网络RPN和Fast RCNN，其中前者负责接受图像输入，并提出proposal。后续Fast RCNN再给出这些proposal是某一类的概率。也正是因为这种“直来直去”的方法，YOLO才能达到这么快的速度。也真是令人感叹，网络参数够多，数据够多，什么映射关系都能学出来。。。下图就是YOLO的检测系统示意图。在test阶段，经过单个CNN网络前向计算后，再经过非极大值抑制，就可以给出检测结果。<br><img src=\"/img/yolo1_detection_system.png\" alt=\"YOLO V1检测系统示意图\"></p>\n<h3 id=\"基本思路\"><a href=\"#基本思路\" class=\"headerlink\" title=\"基本思路\"></a>基本思路</h3><p><img src=\"/img/yolo1_basic_idea.png\" alt=\"基础思路示意图\"></p>\n<ul>\n<li>网格划分：将输入image划分为$S \\times S$个grid cell，如果image中某个object box的中心落在某个grid cell内部，那么这个cell就对检测该object负责（responsible for detection that object）。同时，每个grid cell同时预测$B$个bounding box的位置和一个置信度。这个置信度并不只是该bounding box是待检测目标的概率，而是该bounding box是待检测目标的概率乘上该bounding box和真实位置的IoU的积。通过乘上这个交并比，反映出该bounding box预测位置的精度。如下式所示：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\\text{confidence} = P(\\text{Object})\\times \\text{IoU}_{\\text{pred}}^{\\text{truth}}</script><ul>\n<li><p>网络输出：每个bounding box对应于5个输出，分别是$x,y,w,h$和上述提到的置信度。其中，$x,y$代表bounding box的中心离开其所在grid cell边界的偏移。$w,h$代表bounding box真实宽高相对于整幅图像的比例。$x,y,w,h$这几个参数都已经被bounded到了区间$[0,1]$上。除此以外，每个grid cell还产生$C$个条件概率，$P(\\text{Class}_i|\\text{Object})$。注意，我们不管$B$的大小，每个grid cell只产生一组这样的概率。在test的非极大值抑制阶段，对于每个bounding box，我们应该按照下式衡量该框是否应该予以保留。</p>\n<script type=\"math/tex; mode=display\">\\text{confidence}\\times P(\\text{Class}_i|\\text{Object}) = P(\\text{Class}_i)\\times \\text{IoU}_{\\text{pred}}^{\\text{truth}}</script></li>\n<li><p>实际参数：在PASCAL VOC进行测试时，使用$S = 7$, $B=2$。由于共有20类，故$C=20$。所以，我们的网络输出大小为$7\\times 7 \\times 30$</p>\n</li>\n</ul>\n<h3 id=\"网络模型结构\"><a href=\"#网络模型结构\" class=\"headerlink\" title=\"网络模型结构\"></a>网络模型结构</h3><p>Inspired by GoogLeNet，但是没有采取inception的结构，simply使用了$1\\times 1$的卷积核。base model共有24个卷积层，后面接2个全连接层，如下图所示。<br><img src=\"/img/yolo1_network_arch.png\" alt=\"YOLO的网络结构示意图\"></p>\n<p>另外，Fast YOLO使用了更小的网络结构（9个卷积层，且filter的数目也少了），其他部分完全一样。</p>\n<h3 id=\"训练\"><a href=\"#训练\" class=\"headerlink\" title=\"训练\"></a>训练</h3><p>同很多人一样，这里作者也是先在ImageNet上做了预训练。使用上图网络结构中的前20个卷积层，后面接一个average-pooling层和全连接层，在ImageNet 1000类数据集上训练了一周，达到了88%的top-5准确率。</p>\n<p>由于<a href=\"https://arxiv.org/abs/1504.06066\" target=\"_blank\" rel=\"external\">Ren的论文</a>提到给预训练的模型加上额外的卷积层和全连接层能够提升性能，所以我们加上了剩下的4个卷积层和2个全连接层，权值为随机初始化。同时，我们把网络输入的分辨率从$224\\times 224$提升到了$448 \\times 448$。</p>\n<p>在最后一层，我们使用了线性激活函数；其它层使用了leaky ReLU激活函数。如下所示：</p>\n<script type=\"math/tex; mode=display\">\nf(x)=\n\\begin{cases}\nx, &\\text{if}\\ x > 0 \\\\\\\\\n0.1x, &\\text{otherwise}\n\\end{cases}</script><p>很重要的问题就是定义很好的loss function，为此，作者提出了以下几点说明：</p>\n<ul>\n<li>loss的形式采用误差平方和的形式（真是把回归进行到底了。。。）</li>\n<li>由于很多的grid cell没有目标物体存在，所以给有目标存在的bounding box和没有目标存在的bounding box设置了不同的比例因子进行平衡。具体来说，<script type=\"math/tex; mode=display\">\\lambda_{\\text{coord}} = 5，\\lambda_{\\text{noobj}} = 0.5</script></li>\n<li>直接使用$w$和$h$，这样大的box的差异在loss中占得比重会和小的box不平衡，所以这里使用$\\sqrt{w}$和$\\sqrt{h}$。</li>\n<li>上文中已经提到，同一个grid cell会提出多个bounding box。在training阶段，我们只想让一个bounding box对应object。所以，我们计算每个bounding box和ground truth的IoU，以此为标准得到最好的那个bounding box，其他的认为no obj。</li>\n</ul>\n<p>loss函数的具体形式见下图（实在是不想打这一大串公式。。）。其中，$\\mathbb{1}_i$表示是否有目标出现在第$i$个grid cell。</p>\n<p>$\\mathbb{1}_{i,j}$表示第$i$个grid cell的第$j$个bounding box是否对某个目标负责。<br><img src=\"/img/yolo1_loss_fun.png\" alt=\"YOLO的损失函数定义\"></p>\n<p>在进行反向传播时，由于loss都是二次形式，所以导数形式都是统一的。下面是Darknet中<a href=\"https://github.com/pjreddie/darknet/blob/master/src/detection_layer.c\" target=\"_blank\" rel=\"external\">detection_layer.c</a>中训练部分的代码。在代码中，计算了cost（也就是loss）和delta（也就是反向的导数），</p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div><div class=\"line\">47</div><div class=\"line\">48</div><div class=\"line\">49</div><div class=\"line\">50</div><div class=\"line\">51</div><div class=\"line\">52</div><div class=\"line\">53</div><div class=\"line\">54</div><div class=\"line\">55</div><div class=\"line\">56</div><div class=\"line\">57</div><div class=\"line\">58</div><div class=\"line\">59</div><div class=\"line\">60</div><div class=\"line\">61</div><div class=\"line\">62</div><div class=\"line\">63</div><div class=\"line\">64</div><div class=\"line\">65</div><div class=\"line\">66</div><div class=\"line\">67</div><div class=\"line\">68</div><div class=\"line\">69</div><div class=\"line\">70</div><div class=\"line\">71</div><div class=\"line\">72</div><div class=\"line\">73</div><div class=\"line\">74</div><div class=\"line\">75</div><div class=\"line\">76</div><div class=\"line\">77</div><div class=\"line\">78</div><div class=\"line\">79</div><div class=\"line\">80</div><div class=\"line\">81</div><div class=\"line\">82</div><div class=\"line\">83</div><div class=\"line\">84</div><div class=\"line\">85</div><div class=\"line\">86</div><div class=\"line\">87</div><div class=\"line\">88</div><div class=\"line\">89</div><div class=\"line\">90</div><div class=\"line\">91</div><div class=\"line\">92</div><div class=\"line\">93</div><div class=\"line\">94</div><div class=\"line\">95</div><div class=\"line\">96</div><div class=\"line\">97</div><div class=\"line\">98</div><div class=\"line\">99</div><div class=\"line\">100</div><div class=\"line\">101</div><div class=\"line\">102</div><div class=\"line\">103</div><div class=\"line\">104</div><div class=\"line\">105</div><div class=\"line\">106</div><div class=\"line\">107</div><div class=\"line\">108</div><div class=\"line\">109</div><div class=\"line\">110</div><div class=\"line\">111</div><div class=\"line\">112</div><div class=\"line\">113</div><div class=\"line\">114</div><div class=\"line\">115</div><div class=\"line\">116</div><div class=\"line\">117</div><div class=\"line\">118</div><div class=\"line\">119</div><div class=\"line\">120</div><div class=\"line\">121</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">if</span>(state.train)&#123;</div><div class=\"line\">    <span class=\"keyword\">float</span> avg_iou = <span class=\"number\">0</span>;</div><div class=\"line\">    <span class=\"keyword\">float</span> avg_cat = <span class=\"number\">0</span>;</div><div class=\"line\">    <span class=\"keyword\">float</span> avg_allcat = <span class=\"number\">0</span>;</div><div class=\"line\">    <span class=\"keyword\">float</span> avg_obj = <span class=\"number\">0</span>;</div><div class=\"line\">    <span class=\"keyword\">float</span> avg_anyobj = <span class=\"number\">0</span>;</div><div class=\"line\">    <span class=\"keyword\">int</span> count = <span class=\"number\">0</span>;</div><div class=\"line\">    *(l.cost) = <span class=\"number\">0</span>;</div><div class=\"line\">    <span class=\"keyword\">int</span> size = l.inputs * l.batch;</div><div class=\"line\">    <span class=\"built_in\">memset</span>(l.delta, <span class=\"number\">0</span>, size * <span class=\"keyword\">sizeof</span>(<span class=\"keyword\">float</span>));</div><div class=\"line\">    <span class=\"keyword\">for</span> (b = <span class=\"number\">0</span>; b &lt; l.batch; ++b)&#123;</div><div class=\"line\">        <span class=\"keyword\">int</span> index = b*l.inputs;</div><div class=\"line\">        <span class=\"comment\">// for each grid cell</span></div><div class=\"line\">        <span class=\"keyword\">for</span> (i = <span class=\"number\">0</span>; i &lt; locations; ++i) &#123;   <span class=\"comment\">// locations = S * S = 49</span></div><div class=\"line\">            <span class=\"keyword\">int</span> truth_index = (b*locations + i)*(<span class=\"number\">1</span>+l.coords+l.classes);</div><div class=\"line\">            <span class=\"keyword\">int</span> is_obj = state.truth[truth_index];</div><div class=\"line\">            <span class=\"comment\">// for each bbox</span></div><div class=\"line\">            <span class=\"keyword\">for</span> (j = <span class=\"number\">0</span>; j &lt; l.n; ++j) &#123;     <span class=\"comment\">// l.n = B = 2</span></div><div class=\"line\">                <span class=\"keyword\">int</span> p_index = index + locations*l.classes + i*l.n + j;</div><div class=\"line\">                l.delta[p_index] = l.noobject_scale*(<span class=\"number\">0</span> - l.output[p_index]);</div><div class=\"line\">                <span class=\"comment\">// 因为no obj对应的bbox很多，而responsible的只有一个</span></div><div class=\"line\">                <span class=\"comment\">// 这里统一加上，如果一会判断该bbox responsible for object，再把它减去</span></div><div class=\"line\">                *(l.cost) += l.noobject_scale*<span class=\"built_in\">pow</span>(l.output[p_index], <span class=\"number\">2</span>);  </div><div class=\"line\">                avg_anyobj += l.output[p_index];</div><div class=\"line\">            &#125;</div><div class=\"line\"></div><div class=\"line\">            <span class=\"keyword\">int</span> best_index = <span class=\"number\">-1</span>;</div><div class=\"line\">            <span class=\"keyword\">float</span> best_iou = <span class=\"number\">0</span>;</div><div class=\"line\">            <span class=\"keyword\">float</span> best_rmse = <span class=\"number\">20</span>;</div><div class=\"line\">            <span class=\"comment\">// 该grid cell没有目标，直接返回</span></div><div class=\"line\">            <span class=\"keyword\">if</span> (!is_obj)&#123;</div><div class=\"line\">                <span class=\"keyword\">continue</span>;</div><div class=\"line\">            &#125;</div><div class=\"line\">            <span class=\"comment\">// 否则，找出responsible的bounding box，计算其他几项的loss</span></div><div class=\"line\">            <span class=\"keyword\">int</span> class_index = index + i*l.classes;</div><div class=\"line\">            <span class=\"keyword\">for</span>(j = <span class=\"number\">0</span>; j &lt; l.classes; ++j) &#123;</div><div class=\"line\">                l.delta[class_index+j] = l.class_scale * (state.truth[truth_index+<span class=\"number\">1</span>+j] - l.output[class_index+j]);</div><div class=\"line\">                *(l.cost) += l.class_scale * <span class=\"built_in\">pow</span>(state.truth[truth_index+<span class=\"number\">1</span>+j] - l.output[class_index+j], <span class=\"number\">2</span>);</div><div class=\"line\">                <span class=\"keyword\">if</span>(state.truth[truth_index + <span class=\"number\">1</span> + j]) avg_cat += l.output[class_index+j];</div><div class=\"line\">                avg_allcat += l.output[class_index+j];</div><div class=\"line\">            &#125;</div><div class=\"line\"></div><div class=\"line\">            box truth = float_to_box(state.truth + truth_index + <span class=\"number\">1</span> + l.classes);</div><div class=\"line\">            truth.x /= l.side;</div><div class=\"line\">            truth.y /= l.side;</div><div class=\"line\">            <span class=\"comment\">// 找到最好的IoU，对应的bbox是responsible的，记录其index</span></div><div class=\"line\">            <span class=\"keyword\">for</span>(j = <span class=\"number\">0</span>; j &lt; l.n; ++j)&#123;</div><div class=\"line\">                <span class=\"keyword\">int</span> box_index = index + locations*(l.classes + l.n) + (i*l.n + j) * l.coords;</div><div class=\"line\">                box out = float_to_box(l.output + box_index);</div><div class=\"line\">                out.x /= l.side;</div><div class=\"line\">                out.y /= l.side;</div><div class=\"line\"></div><div class=\"line\">                <span class=\"keyword\">if</span> (l.<span class=\"built_in\">sqrt</span>)&#123;</div><div class=\"line\">                    out.w = out.w*out.w;</div><div class=\"line\">                    out.h = out.h*out.h;</div><div class=\"line\">                &#125;</div><div class=\"line\"></div><div class=\"line\">                <span class=\"keyword\">float</span> iou  = box_iou(out, truth);</div><div class=\"line\">                <span class=\"comment\">//iou = 0;</span></div><div class=\"line\">                <span class=\"keyword\">float</span> rmse = box_rmse(out, truth);</div><div class=\"line\">                <span class=\"keyword\">if</span>(best_iou &gt; <span class=\"number\">0</span> || iou &gt; <span class=\"number\">0</span>)&#123;</div><div class=\"line\">                    <span class=\"keyword\">if</span>(iou &gt; best_iou)&#123;</div><div class=\"line\">                        best_iou = iou;</div><div class=\"line\">                        best_index = j;</div><div class=\"line\">                    &#125;</div><div class=\"line\">                &#125;<span class=\"keyword\">else</span>&#123;</div><div class=\"line\">                    <span class=\"keyword\">if</span>(rmse &lt; best_rmse)&#123;</div><div class=\"line\">                        best_rmse = rmse;</div><div class=\"line\">                        best_index = j;</div><div class=\"line\">                    &#125;</div><div class=\"line\">                &#125;</div><div class=\"line\">            &#125;</div><div class=\"line\"></div><div class=\"line\">            <span class=\"keyword\">if</span>(l.forced)&#123;</div><div class=\"line\">                <span class=\"keyword\">if</span>(truth.w*truth.h &lt; <span class=\"number\">.1</span>)&#123;</div><div class=\"line\">                    best_index = <span class=\"number\">1</span>;</div><div class=\"line\">                &#125;<span class=\"keyword\">else</span>&#123;</div><div class=\"line\">                    best_index = <span class=\"number\">0</span>;</div><div class=\"line\">                &#125;</div><div class=\"line\">            &#125;</div><div class=\"line\">            <span class=\"keyword\">if</span>(l.random &amp;&amp; *(state.net.seen) &lt; <span class=\"number\">64000</span>)&#123;</div><div class=\"line\">                best_index = rand()%l.n;</div><div class=\"line\">            &#125;</div><div class=\"line\"></div><div class=\"line\">            <span class=\"keyword\">int</span> box_index = index + locations*(l.classes + l.n) + (i*l.n + best_index) * l.coords;</div><div class=\"line\">            <span class=\"keyword\">int</span> tbox_index = truth_index + <span class=\"number\">1</span> + l.classes;</div><div class=\"line\"></div><div class=\"line\">            box out = float_to_box(l.output + box_index);</div><div class=\"line\">            out.x /= l.side;</div><div class=\"line\">            out.y /= l.side;</div><div class=\"line\">            <span class=\"keyword\">if</span> (l.<span class=\"built_in\">sqrt</span>) &#123;</div><div class=\"line\">                out.w = out.w*out.w;</div><div class=\"line\">                out.h = out.h*out.h;</div><div class=\"line\">            &#125;</div><div class=\"line\">            <span class=\"keyword\">float</span> iou  = box_iou(out, truth);</div><div class=\"line\"></div><div class=\"line\">            <span class=\"comment\">//printf(\"%d,\", best_index);</span></div><div class=\"line\">            <span class=\"keyword\">int</span> p_index = index + locations*l.classes + i*l.n + best_index;</div><div class=\"line\">            *(l.cost) -= l.noobject_scale * <span class=\"built_in\">pow</span>(l.output[p_index], <span class=\"number\">2</span>);  <span class=\"comment\">// 还记得我们曾经统一加过吗？这里需要减去了</span></div><div class=\"line\">            *(l.cost) += l.object_scale * <span class=\"built_in\">pow</span>(<span class=\"number\">1</span>-l.output[p_index], <span class=\"number\">2</span>);</div><div class=\"line\">            avg_obj += l.output[p_index];</div><div class=\"line\">            l.delta[p_index] = l.object_scale * (<span class=\"number\">1.</span>-l.output[p_index]);</div><div class=\"line\"></div><div class=\"line\">            <span class=\"keyword\">if</span>(l.rescore)&#123;</div><div class=\"line\">                l.delta[p_index] = l.object_scale * (iou - l.output[p_index]);</div><div class=\"line\">            &#125;</div><div class=\"line\"></div><div class=\"line\">            l.delta[box_index+<span class=\"number\">0</span>] = l.coord_scale*(state.truth[tbox_index + <span class=\"number\">0</span>] - l.output[box_index + <span class=\"number\">0</span>]);</div><div class=\"line\">            l.delta[box_index+<span class=\"number\">1</span>] = l.coord_scale*(state.truth[tbox_index + <span class=\"number\">1</span>] - l.output[box_index + <span class=\"number\">1</span>]);</div><div class=\"line\">            l.delta[box_index+<span class=\"number\">2</span>] = l.coord_scale*(state.truth[tbox_index + <span class=\"number\">2</span>] - l.output[box_index + <span class=\"number\">2</span>]);</div><div class=\"line\">            l.delta[box_index+<span class=\"number\">3</span>] = l.coord_scale*(state.truth[tbox_index + <span class=\"number\">3</span>] - l.output[box_index + <span class=\"number\">3</span>]);</div><div class=\"line\">            <span class=\"keyword\">if</span>(l.<span class=\"built_in\">sqrt</span>)&#123;</div><div class=\"line\">                l.delta[box_index+<span class=\"number\">2</span>] = l.coord_scale*(<span class=\"built_in\">sqrt</span>(state.truth[tbox_index + <span class=\"number\">2</span>]) - l.output[box_index + <span class=\"number\">2</span>]);</div><div class=\"line\">                l.delta[box_index+<span class=\"number\">3</span>] = l.coord_scale*(<span class=\"built_in\">sqrt</span>(state.truth[tbox_index + <span class=\"number\">3</span>]) - l.output[box_index + <span class=\"number\">3</span>]);</div><div class=\"line\">            &#125;</div><div class=\"line\"></div><div class=\"line\">            *(l.cost) += <span class=\"built_in\">pow</span>(<span class=\"number\">1</span>-iou, <span class=\"number\">2</span>);</div><div class=\"line\">            avg_iou += iou;</div><div class=\"line\">            ++count;</div><div class=\"line\">        &#125;</div><div class=\"line\">    &#125;</div></pre></td></tr></table></figure>\n<h2 id=\"YOLO-V2\"><a href=\"#YOLO-V2\" class=\"headerlink\" title=\"YOLO V2\"></a>YOLO V2</h2><p>YOLO V2是原作者在V1基础上做出改进后提出的。为了达到题目中所称的Better，Faster，Stronger的目标，主要改进点如下。当然，具体内容还是要深入论文。</p>\n<ul>\n<li>受到Faster RCNN方法的启发，引入了anchor。同时使用了K-Means方法，对anchor数量进行了讨论，在精度和速度之间做出折中；</li>\n<li>修改了网络结构，去掉了全连接层，改成了全卷积结构；</li>\n<li>引入了WordTree结构，将检测和分类问题做成了一个统一的框架，并充分利用ImageNet和COCO数据集的数据。</li>\n</ul>\n<p>下面，还是先把论文的摘要意译如下：</p>\n<blockquote>\n<p>我们引入了YOLO 9000模型，它是实时物体检测的State of the art的工作，能够检测超过9K类目标。首先，我们对以前的工作（YOLO V1）做出了若干改进，使其成为了一种在实时检测方法内在PASCAL VOC和COCO上State of the art的效果。通过一种新颖的多尺度训练犯法（multi-scale training method）， YOLO V2适用于不同大小尺寸输入的image，在精度和效率上达到了很好地trade-off。在67fps的速度下，VOC2007上达到了76.8mAP的精度。40fps时，达到了78.6mAP，已经超过了Faster RCNN with ResNet和SSD的精度，同时比它们更快！最后，我们提出了一种能够同时进行检测任务和分类任务的联合训练方法。使用这种方法，我们在COCO detection dataset和ImageNet classification dataset上同时训练模型。这种方法使得我们能够对那些没有label上detection data的数据做出detection的预测。我们使用ImageNet的detection任务做了验证。YOLO 9000在仅有200类中44类的detection data的情况下，仍然在ImageNet datection任务中取得了19.7mAP的成绩。对于不在COCO中的156类，YOLO9000成绩为16.0mAP。我们使得YOLO9000能够在保证实时的前提下对9K类目标进行检测。</p>\n</blockquote>\n<p>根据论文结构的安排，将从Better，Faster和Stronger三个方面对论文提出的各条改进措施进行介绍。</p>\n<h2 id=\"Better\"><a href=\"#Better\" class=\"headerlink\" title=\"Better\"></a>Better</h2><p>在YOLO V1的基础上，作者提出了不少的改进来进一步提升算法的性能（mAP），主要改进措施包括网络结构的改进（第1，3，5，6条）和Anchor Box的引进（第3，4，5条）以及训练方法（第2，7条）。</p>\n<h3 id=\"改进1：引入BN层（Batch-Normalization）\"><a href=\"#改进1：引入BN层（Batch-Normalization）\" class=\"headerlink\" title=\"改进1：引入BN层（Batch Normalization）\"></a>改进1：引入BN层（Batch Normalization）</h3><p>Batch Normalization能够加快模型收敛，并提供一定的正则化。作者在每个conv层都加上了了BN层，同时去掉了原来模型中的drop out部分，这带来了2%的性能提升。</p>\n<h3 id=\"改进2：高分辨率分类器（High-Resolution-Classifier）\"><a href=\"#改进2：高分辨率分类器（High-Resolution-Classifier）\" class=\"headerlink\" title=\"改进2：高分辨率分类器（High Resolution Classifier）\"></a>改进2：高分辨率分类器（High Resolution Classifier）</h3><p>YOLO V1首先在ImageNet上以$224\\times 224$大小图像作为输入进行训练，之后在检测任务中提升到$448\\times 448$。这里，作者在训练完224大小的分类网络后，首先调整网络大小为$448\\times 448$，然后在ImageNet上进行fine tuning（10个epoch）。也就是得到了一个高分辨率的cls。再把它用detection上训练。这样，能够提升4%。</p>\n<h3 id=\"改进3：引入Anchor-Box\"><a href=\"#改进3：引入Anchor-Box\" class=\"headerlink\" title=\"改进3：引入Anchor Box\"></a>改进3：引入Anchor Box</h3><p>YOLO V1中直接在CNN后面街上全连接层，直接回归bounding box的参数。这里引入了Faster RCNN中的anchor box概念，不再直接回归bounding box的参数，而是相对于anchor box的参数。</p>\n<p>作者去掉后面的fc层和最后一个max pooling层，以期得到更高分辨率的feature map。同时，shrink网络接受$416\\times 416$（而不是448）大小的输入image。这是因为我们想要最后的feature map大小是奇数，这样就能够得到一个center cell（比较大的目标，更有可能占据中间的位置）。由于YOLO conv-pooling的效应是将image downsamplig 32倍，所以最后feature map大小为$416/32 = 13$。</p>\n<p>与YOLO V1不同的是，我们不再对同一个grid cell下的bounding box统一产生一个数量为$C$的类别概率，而是对于每一个bounding box都产生对应的$C$类概率。和YOLO V1一样的是，我们仍然产生confidence，意义也完全一样。</p>\n<p>使用anchor后，我们的精度accuracy降低了，不过recall上来了。（这也较好理解。原来每个grid cell内部只有2个bounding box，造成recall不高。现在recall高上来了，accuracy会下降一些）。</p>\n<h3 id=\"改进4：Dimension-Cluster\"><a href=\"#改进4：Dimension-Cluster\" class=\"headerlink\" title=\"改进4：Dimension Cluster\"></a>改进4：Dimension Cluster</h3><p>在引入anchor box后，一个问题就是如何确定anchor的位置和大小？Faster RCNN中是手工选定的，每隔stride设定一个anchor，并根据不同的面积比例和长宽比例产生9个anchor box。在本文中，作者使用了聚类方法对如何选取anchor box做了探究。这点应该是论文中很有新意的地方。</p>\n<p>这里对作者使用的方法不再过多赘述，强调以下两点：</p>\n<ul>\n<li>作者使用的聚类方法是K-Means；</li>\n<li>相似性度量不用欧氏距离，而是用IoU，定义如下：<script type=\"math/tex; mode=display\">d(\\text{box}, \\text{centroid}) = 1-\\text{IoU}(\\text{box}, \\text{centroid})</script></li>\n</ul>\n<p>使用不同的$k$，聚类实验结果如下，作者折中采用了$k = 5$。而且经过实验，发现当取$k=9$时候，已经能够超过Faster RCNN采用的手工固定anchor box的方法。下图右侧图是在COCO和VOC数据集上$k=5$的聚类后结果。这些box可以作为anchor box使用。<br><img src=\"/img/yolo2_cluster_result.png\" alt=\"\"></p>\n<h3 id=\"改进5：直接位置预测（Direct-Location-Prediction）\"><a href=\"#改进5：直接位置预测（Direct-Location-Prediction）\" class=\"headerlink\" title=\"改进5：直接位置预测（Direct Location Prediction）\"></a>改进5：直接位置预测（Direct Location Prediction）</h3><p>我们仍然延续了YOLO V1中的思路，预测box相对于grid cell的位置。使用sigmoid函数作为激活函数，使得最终输出值落在$[0, 1]$这个区间上。</p>\n<p>在output的feature map上，对于每个cell（共计$13\\times 13$个），给出对应每个bounding box的输出$t_x$, $t_y$, $t_w$, $t_h$。每个cell共计$k=5$个bounding box。如何由这几个参数确定bounding box的真实位置呢？见下图。</p>\n<p><img src=\"/img/yolo2_bbox_location.png\" alt=\"确定bbox的位置\"></p>\n<p>设该grid cell距离图像左上角的offset是$(c_x, c_y)$，那么bounding box的位置和宽高计算如下。注意，box的位置是相对于grid cell的，而宽高是相对于anchor box的。<br><img src=\"/img/yolo2_bbox_param.png\" alt=\"bounding box参数的计算方法\"></p>\n<h3 id=\"改进6：Fine-Gained-Features\"><a href=\"#改进6：Fine-Gained-Features\" class=\"headerlink\" title=\"改进6：Fine-Gained Features\"></a>改进6：Fine-Gained Features</h3><p>这个trick是受Faster RCNN和SSD方法中使用多个不同feature map提高算法对不同分辨率目标物体的检测能力的启发，加入了一个pass-through层，直接将倒数第二层的$26\\times 26$大小的feature map加进来。</p>\n<p>在具体实现时，是将higher resolution（也就是$26\\times 26$）的feature map stacking在一起。比如，原大小为$26\\times 26 \\times 512$的feature map，因为我们要将其变为$13\\times 13$大小，所以，将在空间上相近的点移到后面的channel上去，这部分可以参考Darknet中<code>reorg_layer</code>的实现。</p>\n<p>使用这一扩展之后的feature map，提高了1%的性能提升。</p>\n<h3 id=\"改进7：多尺度训练（Multi-Scale-Training）\"><a href=\"#改进7：多尺度训练（Multi-Scale-Training）\" class=\"headerlink\" title=\"改进7：多尺度训练（Multi-Scale Training）\"></a>改进7：多尺度训练（Multi-Scale Training）</h3><p>在实际应用时，输入的图像大小有可能是变化的。我们也将这一点考虑进来。因为我们的网络是全卷积神经网络，只有conv和pooling层，没有全连接层，所以可以适应不同大小的图像输入。所以网络结构上是没问题的。</p>\n<p>具体来说，在训练的时候，我们每隔一定的epoch（例如10）就随机改变网络的输入图像大小。由于我们的网络最终降采样的比例是$32$，所以随机生成的图像大小为$32$的倍数，即$\\lbrace 320, 352, \\dots, 608\\rbrace$。</p>\n<p>在实际使用中，如果输入图像的分辨率较低，YOLO V2可以在不错的精度下达到很快的检测速度。这可以被用应在计算能力有限的场合（无GPU或者GPU很弱）或多路视频信号的实时处理。如果输入图像的分辨率较高，YOLO V2可以作为state of the art的检测器，并仍能获得不错的检测速度。对于目前流行的检测方法（Faster RCNN，SSD，YOLO）的精度和帧率之间的关系，见下图。可以看到，作者在30fps处画了一条竖线，这是算法能否达到实时处理的分水岭。Faster RCNN败下阵来，而YOLO V2的不同点代表了不同输入图像分辨率下算法的表现。对于详细数据，见图下的表格对比（VOC 2007上进行测试）。</p>\n<p><img src=\"/img/yolo2_different_methods_comparation.png\" alt=\"不同检测方法的对比\"><br><img src=\"/img/yolo2_different_methods_comparation_in_table.png\" alt=\"不同检测方法的对比\"></p>\n<h3 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h3><p>在Better这部分的末尾，作者给出了一个表格，指出了主要提升性能的措施。例外是网络结构上改为带Anchor box的全卷积网络结构（提升了recall，但对mAP基本无影响）和使用新的网络（计算量少了~33%）。<br><img src=\"/img/yolo2_different_methods_improvement.png\" alt=\"不同改进措施的影响\"></p>\n<h2 id=\"Faster\"><a href=\"#Faster\" class=\"headerlink\" title=\"Faster\"></a>Faster</h2><p>这部分的改进为网络结构的变化。包括Faster RCNN在内的很多检测方法都使用VGG-16作为base network。VGG-16精度够高但是计算量较大（对于大小为$224\\times 224$的单幅输入图像，卷积层就需要30.69B次浮点运算）。在YOLO V1中，我们的网络结构较为简单，在精度上不如VGG-16（ImageNet测试，88.0% vs 90.0%）。</p>\n<p>在YOLO V2中，我们使用了一种新的网络结构Darknet-19（因为base network有19个卷积层）。和VGG相类似，我们的卷积核也都是$3\\times 3$大小，同时每次pooling操作后channel数double。另外，在NIN工作基础上，我们在网络最后使用了global average pooling层，同时使用$1\\times 1$大小的卷积核来做feature map的压缩（分辨率不变，channel减小，新的元素是原来相应位置不同channel的线性组合），同时使用了Batch Normalization技术。具体的网络结构见下表。Darknet-19计算量大大减小，同时精度超过了VGG-16。<br><img src=\"/img/yolo2_dartnet_19_structure.png\" alt=\"Darknet-19的网络结构\"></p>\n<p>在训练过程中，首先在ImageNet 1K上进行分类器的训练。使用数据增强技术（如随机裁剪、旋转、饱和度变化等）。和上面的讨论相对应，首先使用$224\\times 224$大小的图像进行训练，再使用$448\\times 448$的图像进行fine tuning，具体训练参数设置可以参见论文和对应的代码。这里不再多说。</p>\n<p>然后，我们对检测任务进行训练。对网络结构进行微调，去掉最后的卷积层（因为它是我们当初为了得到1K类的分类置信度加上去的），增加$3$个$3\\times 3$大小，channel数为$1024$的卷积层，并每个都跟着一个$1\\times 1$大小的卷积层，channel数由我们最终的检测任务需要的输出决定。对于VOC的检测任务来说，我们需要预测$5$个box，每个需要$5$个参数（相对位置，相对大小和置信度），同时$20$个类别的置信度，所以输出共$5\\times(5+20)=125$。从YOLO V2的<code>yolo_voc.cfg</code><a href=\"https://github.com/pjreddie/darknet/blob/master/cfg/yolo.cfg\" target=\"_blank\" rel=\"external\">文件</a>中，我们也可以看到如下的对应结构：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div></pre></td><td class=\"code\"><pre><div class=\"line\">[convolutional]</div><div class=\"line\">batch_normalize=1</div><div class=\"line\">size=3</div><div class=\"line\">stride=1</div><div class=\"line\">pad=1</div><div class=\"line\">filters=1024</div><div class=\"line\">activation=leaky</div><div class=\"line\"></div><div class=\"line\">[convolutional]</div><div class=\"line\">size=1</div><div class=\"line\">stride=1</div><div class=\"line\">pad=1</div><div class=\"line\">filters=125</div><div class=\"line\">activation=linear</div></pre></td></tr></table></figure>\n<p>同时，加上上文提到的pass-through结构。</p>\n<h2 id=\"Stronger\"><a href=\"#Stronger\" class=\"headerlink\" title=\"Stronger\"></a>Stronger</h2><p>未完待续</p>\n","excerpt":"<p>YOLO(<strong>Y</strong>ou <strong>O</strong>nly <strong>L</strong>ook <strong>O</strong>nce)是一个流行的目标检测方法，和Faster RCNN等state of the art方法比起来，主打检测速度快。截止到目前为止（2017年2月初），YOLO已经发布了两个版本，在下文中分别称为<a href=\"https://arxiv.org/abs/1506.02640\">YOLO V1</a>和<a href=\"https://arxiv.org/abs/1612.08242\">YOLO V2</a>。YOLO V2的代码目前作为<a href=\"http://pjreddie.com/darknet/yolo/\">Darknet</a>的一部分开源在<a href=\"\">GitHub</a>。在这篇博客中，记录了阅读YOLO两个版本论文中的重点内容，并着重总结V2版本的改进。</p>\n<p><img src=\"/img/yolo2_result.png\" alt=\"YOLO V2的检测效果示意\"><br>","more":"</p>\n<h2 id=\"YOLO-V1\"><a href=\"#YOLO-V1\" class=\"headerlink\" title=\"YOLO V1\"></a>YOLO V1</h2><p>这里不妨把YOLO V1论文<a href=\"https://arxiv.org/abs/1506.02640\">“You Only Look Once: Unitied, Real-Time Object Detection”</a>的摘要部分意译如下：</p>\n<blockquote>\n<p>我们提出了一种新的物体检测方法YOLO。之前的目标检测方法大多把分类器重新调整用于检测（这里应该是在说以前的方法大多将检测问题看做分类问题，使用滑动窗提取特征并使用分类器进行分类）。我们将检测问题看做回归，分别给出bounding box的位置和对应的类别概率。对于给定输入图像，只需使用CNN网络计算一次，就同时给出bounding box位置和类别概率。由于整个pipeline都是同一个网络，所以很容易进行端到端的训练。<br>YOLO相当快。base model可以达到45fps，一个更小的model（Fast YOLO），可以达到155fps，同时mAP是其他可以实现实时检测方法的两倍。和目前的State of the art方法相比，YOLO的localization误差较大，但是对于背景有更低的误检（False Positives）。同时，YOLO能够学习到更加泛化的特征，例如艺术品中物体的检测等任务，表现很好。</p>\n</blockquote>\n<p>和以往较为常见的方法，如HoG+SVM的行人检测，DPM，RCNN系方法等不同，YOLO接受image作为输入，直接输出bounding box的位置和对应位置为某一类的概率。我们可以把它和目前的State of the art的Faster RCNN方法对比。Faster RCNN方法需要两个网络RPN和Fast RCNN，其中前者负责接受图像输入，并提出proposal。后续Fast RCNN再给出这些proposal是某一类的概率。也正是因为这种“直来直去”的方法，YOLO才能达到这么快的速度。也真是令人感叹，网络参数够多，数据够多，什么映射关系都能学出来。。。下图就是YOLO的检测系统示意图。在test阶段，经过单个CNN网络前向计算后，再经过非极大值抑制，就可以给出检测结果。<br><img src=\"/img/yolo1_detection_system.png\" alt=\"YOLO V1检测系统示意图\"></p>\n<h3 id=\"基本思路\"><a href=\"#基本思路\" class=\"headerlink\" title=\"基本思路\"></a>基本思路</h3><p><img src=\"/img/yolo1_basic_idea.png\" alt=\"基础思路示意图\"></p>\n<ul>\n<li>网格划分：将输入image划分为$S \\times S$个grid cell，如果image中某个object box的中心落在某个grid cell内部，那么这个cell就对检测该object负责（responsible for detection that object）。同时，每个grid cell同时预测$B$个bounding box的位置和一个置信度。这个置信度并不只是该bounding box是待检测目标的概率，而是该bounding box是待检测目标的概率乘上该bounding box和真实位置的IoU的积。通过乘上这个交并比，反映出该bounding box预测位置的精度。如下式所示：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\\text{confidence} = P(\\text{Object})\\times \\text{IoU}_{\\text{pred}}^{\\text{truth}}</script><ul>\n<li><p>网络输出：每个bounding box对应于5个输出，分别是$x,y,w,h$和上述提到的置信度。其中，$x,y$代表bounding box的中心离开其所在grid cell边界的偏移。$w,h$代表bounding box真实宽高相对于整幅图像的比例。$x,y,w,h$这几个参数都已经被bounded到了区间$[0,1]$上。除此以外，每个grid cell还产生$C$个条件概率，$P(\\text{Class}_i|\\text{Object})$。注意，我们不管$B$的大小，每个grid cell只产生一组这样的概率。在test的非极大值抑制阶段，对于每个bounding box，我们应该按照下式衡量该框是否应该予以保留。</p>\n<script type=\"math/tex; mode=display\">\\text{confidence}\\times P(\\text{Class}_i|\\text{Object}) = P(\\text{Class}_i)\\times \\text{IoU}_{\\text{pred}}^{\\text{truth}}</script></li>\n<li><p>实际参数：在PASCAL VOC进行测试时，使用$S = 7$, $B=2$。由于共有20类，故$C=20$。所以，我们的网络输出大小为$7\\times 7 \\times 30$</p>\n</li>\n</ul>\n<h3 id=\"网络模型结构\"><a href=\"#网络模型结构\" class=\"headerlink\" title=\"网络模型结构\"></a>网络模型结构</h3><p>Inspired by GoogLeNet，但是没有采取inception的结构，simply使用了$1\\times 1$的卷积核。base model共有24个卷积层，后面接2个全连接层，如下图所示。<br><img src=\"/img/yolo1_network_arch.png\" alt=\"YOLO的网络结构示意图\"></p>\n<p>另外，Fast YOLO使用了更小的网络结构（9个卷积层，且filter的数目也少了），其他部分完全一样。</p>\n<h3 id=\"训练\"><a href=\"#训练\" class=\"headerlink\" title=\"训练\"></a>训练</h3><p>同很多人一样，这里作者也是先在ImageNet上做了预训练。使用上图网络结构中的前20个卷积层，后面接一个average-pooling层和全连接层，在ImageNet 1000类数据集上训练了一周，达到了88%的top-5准确率。</p>\n<p>由于<a href=\"https://arxiv.org/abs/1504.06066\">Ren的论文</a>提到给预训练的模型加上额外的卷积层和全连接层能够提升性能，所以我们加上了剩下的4个卷积层和2个全连接层，权值为随机初始化。同时，我们把网络输入的分辨率从$224\\times 224$提升到了$448 \\times 448$。</p>\n<p>在最后一层，我们使用了线性激活函数；其它层使用了leaky ReLU激活函数。如下所示：</p>\n<script type=\"math/tex; mode=display\">\nf(x)=\n\\begin{cases}\nx, &\\text{if}\\ x > 0 \\\\\\\\\n0.1x, &\\text{otherwise}\n\\end{cases}</script><p>很重要的问题就是定义很好的loss function，为此，作者提出了以下几点说明：</p>\n<ul>\n<li>loss的形式采用误差平方和的形式（真是把回归进行到底了。。。）</li>\n<li>由于很多的grid cell没有目标物体存在，所以给有目标存在的bounding box和没有目标存在的bounding box设置了不同的比例因子进行平衡。具体来说，<script type=\"math/tex; mode=display\">\\lambda_{\\text{coord}} = 5，\\lambda_{\\text{noobj}} = 0.5</script></li>\n<li>直接使用$w$和$h$，这样大的box的差异在loss中占得比重会和小的box不平衡，所以这里使用$\\sqrt{w}$和$\\sqrt{h}$。</li>\n<li>上文中已经提到，同一个grid cell会提出多个bounding box。在training阶段，我们只想让一个bounding box对应object。所以，我们计算每个bounding box和ground truth的IoU，以此为标准得到最好的那个bounding box，其他的认为no obj。</li>\n</ul>\n<p>loss函数的具体形式见下图（实在是不想打这一大串公式。。）。其中，$\\mathbb{1}_i$表示是否有目标出现在第$i$个grid cell。</p>\n<p>$\\mathbb{1}_{i,j}$表示第$i$个grid cell的第$j$个bounding box是否对某个目标负责。<br><img src=\"/img/yolo1_loss_fun.png\" alt=\"YOLO的损失函数定义\"></p>\n<p>在进行反向传播时，由于loss都是二次形式，所以导数形式都是统一的。下面是Darknet中<a href=\"https://github.com/pjreddie/darknet/blob/master/src/detection_layer.c\">detection_layer.c</a>中训练部分的代码。在代码中，计算了cost（也就是loss）和delta（也就是反向的导数），</p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div><div class=\"line\">47</div><div class=\"line\">48</div><div class=\"line\">49</div><div class=\"line\">50</div><div class=\"line\">51</div><div class=\"line\">52</div><div class=\"line\">53</div><div class=\"line\">54</div><div class=\"line\">55</div><div class=\"line\">56</div><div class=\"line\">57</div><div class=\"line\">58</div><div class=\"line\">59</div><div class=\"line\">60</div><div class=\"line\">61</div><div class=\"line\">62</div><div class=\"line\">63</div><div class=\"line\">64</div><div class=\"line\">65</div><div class=\"line\">66</div><div class=\"line\">67</div><div class=\"line\">68</div><div class=\"line\">69</div><div class=\"line\">70</div><div class=\"line\">71</div><div class=\"line\">72</div><div class=\"line\">73</div><div class=\"line\">74</div><div class=\"line\">75</div><div class=\"line\">76</div><div class=\"line\">77</div><div class=\"line\">78</div><div class=\"line\">79</div><div class=\"line\">80</div><div class=\"line\">81</div><div class=\"line\">82</div><div class=\"line\">83</div><div class=\"line\">84</div><div class=\"line\">85</div><div class=\"line\">86</div><div class=\"line\">87</div><div class=\"line\">88</div><div class=\"line\">89</div><div class=\"line\">90</div><div class=\"line\">91</div><div class=\"line\">92</div><div class=\"line\">93</div><div class=\"line\">94</div><div class=\"line\">95</div><div class=\"line\">96</div><div class=\"line\">97</div><div class=\"line\">98</div><div class=\"line\">99</div><div class=\"line\">100</div><div class=\"line\">101</div><div class=\"line\">102</div><div class=\"line\">103</div><div class=\"line\">104</div><div class=\"line\">105</div><div class=\"line\">106</div><div class=\"line\">107</div><div class=\"line\">108</div><div class=\"line\">109</div><div class=\"line\">110</div><div class=\"line\">111</div><div class=\"line\">112</div><div class=\"line\">113</div><div class=\"line\">114</div><div class=\"line\">115</div><div class=\"line\">116</div><div class=\"line\">117</div><div class=\"line\">118</div><div class=\"line\">119</div><div class=\"line\">120</div><div class=\"line\">121</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">if</span>(state.train)&#123;</div><div class=\"line\">    <span class=\"keyword\">float</span> avg_iou = <span class=\"number\">0</span>;</div><div class=\"line\">    <span class=\"keyword\">float</span> avg_cat = <span class=\"number\">0</span>;</div><div class=\"line\">    <span class=\"keyword\">float</span> avg_allcat = <span class=\"number\">0</span>;</div><div class=\"line\">    <span class=\"keyword\">float</span> avg_obj = <span class=\"number\">0</span>;</div><div class=\"line\">    <span class=\"keyword\">float</span> avg_anyobj = <span class=\"number\">0</span>;</div><div class=\"line\">    <span class=\"keyword\">int</span> count = <span class=\"number\">0</span>;</div><div class=\"line\">    *(l.cost) = <span class=\"number\">0</span>;</div><div class=\"line\">    <span class=\"keyword\">int</span> size = l.inputs * l.batch;</div><div class=\"line\">    <span class=\"built_in\">memset</span>(l.delta, <span class=\"number\">0</span>, size * <span class=\"keyword\">sizeof</span>(<span class=\"keyword\">float</span>));</div><div class=\"line\">    <span class=\"keyword\">for</span> (b = <span class=\"number\">0</span>; b &lt; l.batch; ++b)&#123;</div><div class=\"line\">        <span class=\"keyword\">int</span> index = b*l.inputs;</div><div class=\"line\">        <span class=\"comment\">// for each grid cell</span></div><div class=\"line\">        <span class=\"keyword\">for</span> (i = <span class=\"number\">0</span>; i &lt; locations; ++i) &#123;   <span class=\"comment\">// locations = S * S = 49</span></div><div class=\"line\">            <span class=\"keyword\">int</span> truth_index = (b*locations + i)*(<span class=\"number\">1</span>+l.coords+l.classes);</div><div class=\"line\">            <span class=\"keyword\">int</span> is_obj = state.truth[truth_index];</div><div class=\"line\">            <span class=\"comment\">// for each bbox</span></div><div class=\"line\">            <span class=\"keyword\">for</span> (j = <span class=\"number\">0</span>; j &lt; l.n; ++j) &#123;     <span class=\"comment\">// l.n = B = 2</span></div><div class=\"line\">                <span class=\"keyword\">int</span> p_index = index + locations*l.classes + i*l.n + j;</div><div class=\"line\">                l.delta[p_index] = l.noobject_scale*(<span class=\"number\">0</span> - l.output[p_index]);</div><div class=\"line\">                <span class=\"comment\">// 因为no obj对应的bbox很多，而responsible的只有一个</span></div><div class=\"line\">                <span class=\"comment\">// 这里统一加上，如果一会判断该bbox responsible for object，再把它减去</span></div><div class=\"line\">                *(l.cost) += l.noobject_scale*<span class=\"built_in\">pow</span>(l.output[p_index], <span class=\"number\">2</span>);  </div><div class=\"line\">                avg_anyobj += l.output[p_index];</div><div class=\"line\">            &#125;</div><div class=\"line\"></div><div class=\"line\">            <span class=\"keyword\">int</span> best_index = <span class=\"number\">-1</span>;</div><div class=\"line\">            <span class=\"keyword\">float</span> best_iou = <span class=\"number\">0</span>;</div><div class=\"line\">            <span class=\"keyword\">float</span> best_rmse = <span class=\"number\">20</span>;</div><div class=\"line\">            <span class=\"comment\">// 该grid cell没有目标，直接返回</span></div><div class=\"line\">            <span class=\"keyword\">if</span> (!is_obj)&#123;</div><div class=\"line\">                <span class=\"keyword\">continue</span>;</div><div class=\"line\">            &#125;</div><div class=\"line\">            <span class=\"comment\">// 否则，找出responsible的bounding box，计算其他几项的loss</span></div><div class=\"line\">            <span class=\"keyword\">int</span> class_index = index + i*l.classes;</div><div class=\"line\">            <span class=\"keyword\">for</span>(j = <span class=\"number\">0</span>; j &lt; l.classes; ++j) &#123;</div><div class=\"line\">                l.delta[class_index+j] = l.class_scale * (state.truth[truth_index+<span class=\"number\">1</span>+j] - l.output[class_index+j]);</div><div class=\"line\">                *(l.cost) += l.class_scale * <span class=\"built_in\">pow</span>(state.truth[truth_index+<span class=\"number\">1</span>+j] - l.output[class_index+j], <span class=\"number\">2</span>);</div><div class=\"line\">                <span class=\"keyword\">if</span>(state.truth[truth_index + <span class=\"number\">1</span> + j]) avg_cat += l.output[class_index+j];</div><div class=\"line\">                avg_allcat += l.output[class_index+j];</div><div class=\"line\">            &#125;</div><div class=\"line\"></div><div class=\"line\">            box truth = float_to_box(state.truth + truth_index + <span class=\"number\">1</span> + l.classes);</div><div class=\"line\">            truth.x /= l.side;</div><div class=\"line\">            truth.y /= l.side;</div><div class=\"line\">            <span class=\"comment\">// 找到最好的IoU，对应的bbox是responsible的，记录其index</span></div><div class=\"line\">            <span class=\"keyword\">for</span>(j = <span class=\"number\">0</span>; j &lt; l.n; ++j)&#123;</div><div class=\"line\">                <span class=\"keyword\">int</span> box_index = index + locations*(l.classes + l.n) + (i*l.n + j) * l.coords;</div><div class=\"line\">                box out = float_to_box(l.output + box_index);</div><div class=\"line\">                out.x /= l.side;</div><div class=\"line\">                out.y /= l.side;</div><div class=\"line\"></div><div class=\"line\">                <span class=\"keyword\">if</span> (l.<span class=\"built_in\">sqrt</span>)&#123;</div><div class=\"line\">                    out.w = out.w*out.w;</div><div class=\"line\">                    out.h = out.h*out.h;</div><div class=\"line\">                &#125;</div><div class=\"line\"></div><div class=\"line\">                <span class=\"keyword\">float</span> iou  = box_iou(out, truth);</div><div class=\"line\">                <span class=\"comment\">//iou = 0;</span></div><div class=\"line\">                <span class=\"keyword\">float</span> rmse = box_rmse(out, truth);</div><div class=\"line\">                <span class=\"keyword\">if</span>(best_iou &gt; <span class=\"number\">0</span> || iou &gt; <span class=\"number\">0</span>)&#123;</div><div class=\"line\">                    <span class=\"keyword\">if</span>(iou &gt; best_iou)&#123;</div><div class=\"line\">                        best_iou = iou;</div><div class=\"line\">                        best_index = j;</div><div class=\"line\">                    &#125;</div><div class=\"line\">                &#125;<span class=\"keyword\">else</span>&#123;</div><div class=\"line\">                    <span class=\"keyword\">if</span>(rmse &lt; best_rmse)&#123;</div><div class=\"line\">                        best_rmse = rmse;</div><div class=\"line\">                        best_index = j;</div><div class=\"line\">                    &#125;</div><div class=\"line\">                &#125;</div><div class=\"line\">            &#125;</div><div class=\"line\"></div><div class=\"line\">            <span class=\"keyword\">if</span>(l.forced)&#123;</div><div class=\"line\">                <span class=\"keyword\">if</span>(truth.w*truth.h &lt; <span class=\"number\">.1</span>)&#123;</div><div class=\"line\">                    best_index = <span class=\"number\">1</span>;</div><div class=\"line\">                &#125;<span class=\"keyword\">else</span>&#123;</div><div class=\"line\">                    best_index = <span class=\"number\">0</span>;</div><div class=\"line\">                &#125;</div><div class=\"line\">            &#125;</div><div class=\"line\">            <span class=\"keyword\">if</span>(l.random &amp;&amp; *(state.net.seen) &lt; <span class=\"number\">64000</span>)&#123;</div><div class=\"line\">                best_index = rand()%l.n;</div><div class=\"line\">            &#125;</div><div class=\"line\"></div><div class=\"line\">            <span class=\"keyword\">int</span> box_index = index + locations*(l.classes + l.n) + (i*l.n + best_index) * l.coords;</div><div class=\"line\">            <span class=\"keyword\">int</span> tbox_index = truth_index + <span class=\"number\">1</span> + l.classes;</div><div class=\"line\"></div><div class=\"line\">            box out = float_to_box(l.output + box_index);</div><div class=\"line\">            out.x /= l.side;</div><div class=\"line\">            out.y /= l.side;</div><div class=\"line\">            <span class=\"keyword\">if</span> (l.<span class=\"built_in\">sqrt</span>) &#123;</div><div class=\"line\">                out.w = out.w*out.w;</div><div class=\"line\">                out.h = out.h*out.h;</div><div class=\"line\">            &#125;</div><div class=\"line\">            <span class=\"keyword\">float</span> iou  = box_iou(out, truth);</div><div class=\"line\"></div><div class=\"line\">            <span class=\"comment\">//printf(\"%d,\", best_index);</span></div><div class=\"line\">            <span class=\"keyword\">int</span> p_index = index + locations*l.classes + i*l.n + best_index;</div><div class=\"line\">            *(l.cost) -= l.noobject_scale * <span class=\"built_in\">pow</span>(l.output[p_index], <span class=\"number\">2</span>);  <span class=\"comment\">// 还记得我们曾经统一加过吗？这里需要减去了</span></div><div class=\"line\">            *(l.cost) += l.object_scale * <span class=\"built_in\">pow</span>(<span class=\"number\">1</span>-l.output[p_index], <span class=\"number\">2</span>);</div><div class=\"line\">            avg_obj += l.output[p_index];</div><div class=\"line\">            l.delta[p_index] = l.object_scale * (<span class=\"number\">1.</span>-l.output[p_index]);</div><div class=\"line\"></div><div class=\"line\">            <span class=\"keyword\">if</span>(l.rescore)&#123;</div><div class=\"line\">                l.delta[p_index] = l.object_scale * (iou - l.output[p_index]);</div><div class=\"line\">            &#125;</div><div class=\"line\"></div><div class=\"line\">            l.delta[box_index+<span class=\"number\">0</span>] = l.coord_scale*(state.truth[tbox_index + <span class=\"number\">0</span>] - l.output[box_index + <span class=\"number\">0</span>]);</div><div class=\"line\">            l.delta[box_index+<span class=\"number\">1</span>] = l.coord_scale*(state.truth[tbox_index + <span class=\"number\">1</span>] - l.output[box_index + <span class=\"number\">1</span>]);</div><div class=\"line\">            l.delta[box_index+<span class=\"number\">2</span>] = l.coord_scale*(state.truth[tbox_index + <span class=\"number\">2</span>] - l.output[box_index + <span class=\"number\">2</span>]);</div><div class=\"line\">            l.delta[box_index+<span class=\"number\">3</span>] = l.coord_scale*(state.truth[tbox_index + <span class=\"number\">3</span>] - l.output[box_index + <span class=\"number\">3</span>]);</div><div class=\"line\">            <span class=\"keyword\">if</span>(l.<span class=\"built_in\">sqrt</span>)&#123;</div><div class=\"line\">                l.delta[box_index+<span class=\"number\">2</span>] = l.coord_scale*(<span class=\"built_in\">sqrt</span>(state.truth[tbox_index + <span class=\"number\">2</span>]) - l.output[box_index + <span class=\"number\">2</span>]);</div><div class=\"line\">                l.delta[box_index+<span class=\"number\">3</span>] = l.coord_scale*(<span class=\"built_in\">sqrt</span>(state.truth[tbox_index + <span class=\"number\">3</span>]) - l.output[box_index + <span class=\"number\">3</span>]);</div><div class=\"line\">            &#125;</div><div class=\"line\"></div><div class=\"line\">            *(l.cost) += <span class=\"built_in\">pow</span>(<span class=\"number\">1</span>-iou, <span class=\"number\">2</span>);</div><div class=\"line\">            avg_iou += iou;</div><div class=\"line\">            ++count;</div><div class=\"line\">        &#125;</div><div class=\"line\">    &#125;</div></pre></td></tr></table></figure>\n<h2 id=\"YOLO-V2\"><a href=\"#YOLO-V2\" class=\"headerlink\" title=\"YOLO V2\"></a>YOLO V2</h2><p>YOLO V2是原作者在V1基础上做出改进后提出的。为了达到题目中所称的Better，Faster，Stronger的目标，主要改进点如下。当然，具体内容还是要深入论文。</p>\n<ul>\n<li>受到Faster RCNN方法的启发，引入了anchor。同时使用了K-Means方法，对anchor数量进行了讨论，在精度和速度之间做出折中；</li>\n<li>修改了网络结构，去掉了全连接层，改成了全卷积结构；</li>\n<li>引入了WordTree结构，将检测和分类问题做成了一个统一的框架，并充分利用ImageNet和COCO数据集的数据。</li>\n</ul>\n<p>下面，还是先把论文的摘要意译如下：</p>\n<blockquote>\n<p>我们引入了YOLO 9000模型，它是实时物体检测的State of the art的工作，能够检测超过9K类目标。首先，我们对以前的工作（YOLO V1）做出了若干改进，使其成为了一种在实时检测方法内在PASCAL VOC和COCO上State of the art的效果。通过一种新颖的多尺度训练犯法（multi-scale training method）， YOLO V2适用于不同大小尺寸输入的image，在精度和效率上达到了很好地trade-off。在67fps的速度下，VOC2007上达到了76.8mAP的精度。40fps时，达到了78.6mAP，已经超过了Faster RCNN with ResNet和SSD的精度，同时比它们更快！最后，我们提出了一种能够同时进行检测任务和分类任务的联合训练方法。使用这种方法，我们在COCO detection dataset和ImageNet classification dataset上同时训练模型。这种方法使得我们能够对那些没有label上detection data的数据做出detection的预测。我们使用ImageNet的detection任务做了验证。YOLO 9000在仅有200类中44类的detection data的情况下，仍然在ImageNet datection任务中取得了19.7mAP的成绩。对于不在COCO中的156类，YOLO9000成绩为16.0mAP。我们使得YOLO9000能够在保证实时的前提下对9K类目标进行检测。</p>\n</blockquote>\n<p>根据论文结构的安排，将从Better，Faster和Stronger三个方面对论文提出的各条改进措施进行介绍。</p>\n<h2 id=\"Better\"><a href=\"#Better\" class=\"headerlink\" title=\"Better\"></a>Better</h2><p>在YOLO V1的基础上，作者提出了不少的改进来进一步提升算法的性能（mAP），主要改进措施包括网络结构的改进（第1，3，5，6条）和Anchor Box的引进（第3，4，5条）以及训练方法（第2，7条）。</p>\n<h3 id=\"改进1：引入BN层（Batch-Normalization）\"><a href=\"#改进1：引入BN层（Batch-Normalization）\" class=\"headerlink\" title=\"改进1：引入BN层（Batch Normalization）\"></a>改进1：引入BN层（Batch Normalization）</h3><p>Batch Normalization能够加快模型收敛，并提供一定的正则化。作者在每个conv层都加上了了BN层，同时去掉了原来模型中的drop out部分，这带来了2%的性能提升。</p>\n<h3 id=\"改进2：高分辨率分类器（High-Resolution-Classifier）\"><a href=\"#改进2：高分辨率分类器（High-Resolution-Classifier）\" class=\"headerlink\" title=\"改进2：高分辨率分类器（High Resolution Classifier）\"></a>改进2：高分辨率分类器（High Resolution Classifier）</h3><p>YOLO V1首先在ImageNet上以$224\\times 224$大小图像作为输入进行训练，之后在检测任务中提升到$448\\times 448$。这里，作者在训练完224大小的分类网络后，首先调整网络大小为$448\\times 448$，然后在ImageNet上进行fine tuning（10个epoch）。也就是得到了一个高分辨率的cls。再把它用detection上训练。这样，能够提升4%。</p>\n<h3 id=\"改进3：引入Anchor-Box\"><a href=\"#改进3：引入Anchor-Box\" class=\"headerlink\" title=\"改进3：引入Anchor Box\"></a>改进3：引入Anchor Box</h3><p>YOLO V1中直接在CNN后面街上全连接层，直接回归bounding box的参数。这里引入了Faster RCNN中的anchor box概念，不再直接回归bounding box的参数，而是相对于anchor box的参数。</p>\n<p>作者去掉后面的fc层和最后一个max pooling层，以期得到更高分辨率的feature map。同时，shrink网络接受$416\\times 416$（而不是448）大小的输入image。这是因为我们想要最后的feature map大小是奇数，这样就能够得到一个center cell（比较大的目标，更有可能占据中间的位置）。由于YOLO conv-pooling的效应是将image downsamplig 32倍，所以最后feature map大小为$416/32 = 13$。</p>\n<p>与YOLO V1不同的是，我们不再对同一个grid cell下的bounding box统一产生一个数量为$C$的类别概率，而是对于每一个bounding box都产生对应的$C$类概率。和YOLO V1一样的是，我们仍然产生confidence，意义也完全一样。</p>\n<p>使用anchor后，我们的精度accuracy降低了，不过recall上来了。（这也较好理解。原来每个grid cell内部只有2个bounding box，造成recall不高。现在recall高上来了，accuracy会下降一些）。</p>\n<h3 id=\"改进4：Dimension-Cluster\"><a href=\"#改进4：Dimension-Cluster\" class=\"headerlink\" title=\"改进4：Dimension Cluster\"></a>改进4：Dimension Cluster</h3><p>在引入anchor box后，一个问题就是如何确定anchor的位置和大小？Faster RCNN中是手工选定的，每隔stride设定一个anchor，并根据不同的面积比例和长宽比例产生9个anchor box。在本文中，作者使用了聚类方法对如何选取anchor box做了探究。这点应该是论文中很有新意的地方。</p>\n<p>这里对作者使用的方法不再过多赘述，强调以下两点：</p>\n<ul>\n<li>作者使用的聚类方法是K-Means；</li>\n<li>相似性度量不用欧氏距离，而是用IoU，定义如下：<script type=\"math/tex; mode=display\">d(\\text{box}, \\text{centroid}) = 1-\\text{IoU}(\\text{box}, \\text{centroid})</script></li>\n</ul>\n<p>使用不同的$k$，聚类实验结果如下，作者折中采用了$k = 5$。而且经过实验，发现当取$k=9$时候，已经能够超过Faster RCNN采用的手工固定anchor box的方法。下图右侧图是在COCO和VOC数据集上$k=5$的聚类后结果。这些box可以作为anchor box使用。<br><img src=\"/img/yolo2_cluster_result.png\" alt=\"\"></p>\n<h3 id=\"改进5：直接位置预测（Direct-Location-Prediction）\"><a href=\"#改进5：直接位置预测（Direct-Location-Prediction）\" class=\"headerlink\" title=\"改进5：直接位置预测（Direct Location Prediction）\"></a>改进5：直接位置预测（Direct Location Prediction）</h3><p>我们仍然延续了YOLO V1中的思路，预测box相对于grid cell的位置。使用sigmoid函数作为激活函数，使得最终输出值落在$[0, 1]$这个区间上。</p>\n<p>在output的feature map上，对于每个cell（共计$13\\times 13$个），给出对应每个bounding box的输出$t_x$, $t_y$, $t_w$, $t_h$。每个cell共计$k=5$个bounding box。如何由这几个参数确定bounding box的真实位置呢？见下图。</p>\n<p><img src=\"/img/yolo2_bbox_location.png\" alt=\"确定bbox的位置\"></p>\n<p>设该grid cell距离图像左上角的offset是$(c_x, c_y)$，那么bounding box的位置和宽高计算如下。注意，box的位置是相对于grid cell的，而宽高是相对于anchor box的。<br><img src=\"/img/yolo2_bbox_param.png\" alt=\"bounding box参数的计算方法\"></p>\n<h3 id=\"改进6：Fine-Gained-Features\"><a href=\"#改进6：Fine-Gained-Features\" class=\"headerlink\" title=\"改进6：Fine-Gained Features\"></a>改进6：Fine-Gained Features</h3><p>这个trick是受Faster RCNN和SSD方法中使用多个不同feature map提高算法对不同分辨率目标物体的检测能力的启发，加入了一个pass-through层，直接将倒数第二层的$26\\times 26$大小的feature map加进来。</p>\n<p>在具体实现时，是将higher resolution（也就是$26\\times 26$）的feature map stacking在一起。比如，原大小为$26\\times 26 \\times 512$的feature map，因为我们要将其变为$13\\times 13$大小，所以，将在空间上相近的点移到后面的channel上去，这部分可以参考Darknet中<code>reorg_layer</code>的实现。</p>\n<p>使用这一扩展之后的feature map，提高了1%的性能提升。</p>\n<h3 id=\"改进7：多尺度训练（Multi-Scale-Training）\"><a href=\"#改进7：多尺度训练（Multi-Scale-Training）\" class=\"headerlink\" title=\"改进7：多尺度训练（Multi-Scale Training）\"></a>改进7：多尺度训练（Multi-Scale Training）</h3><p>在实际应用时，输入的图像大小有可能是变化的。我们也将这一点考虑进来。因为我们的网络是全卷积神经网络，只有conv和pooling层，没有全连接层，所以可以适应不同大小的图像输入。所以网络结构上是没问题的。</p>\n<p>具体来说，在训练的时候，我们每隔一定的epoch（例如10）就随机改变网络的输入图像大小。由于我们的网络最终降采样的比例是$32$，所以随机生成的图像大小为$32$的倍数，即$\\lbrace 320, 352, \\dots, 608\\rbrace$。</p>\n<p>在实际使用中，如果输入图像的分辨率较低，YOLO V2可以在不错的精度下达到很快的检测速度。这可以被用应在计算能力有限的场合（无GPU或者GPU很弱）或多路视频信号的实时处理。如果输入图像的分辨率较高，YOLO V2可以作为state of the art的检测器，并仍能获得不错的检测速度。对于目前流行的检测方法（Faster RCNN，SSD，YOLO）的精度和帧率之间的关系，见下图。可以看到，作者在30fps处画了一条竖线，这是算法能否达到实时处理的分水岭。Faster RCNN败下阵来，而YOLO V2的不同点代表了不同输入图像分辨率下算法的表现。对于详细数据，见图下的表格对比（VOC 2007上进行测试）。</p>\n<p><img src=\"/img/yolo2_different_methods_comparation.png\" alt=\"不同检测方法的对比\"><br><img src=\"/img/yolo2_different_methods_comparation_in_table.png\" alt=\"不同检测方法的对比\"></p>\n<h3 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h3><p>在Better这部分的末尾，作者给出了一个表格，指出了主要提升性能的措施。例外是网络结构上改为带Anchor box的全卷积网络结构（提升了recall，但对mAP基本无影响）和使用新的网络（计算量少了~33%）。<br><img src=\"/img/yolo2_different_methods_improvement.png\" alt=\"不同改进措施的影响\"></p>\n<h2 id=\"Faster\"><a href=\"#Faster\" class=\"headerlink\" title=\"Faster\"></a>Faster</h2><p>这部分的改进为网络结构的变化。包括Faster RCNN在内的很多检测方法都使用VGG-16作为base network。VGG-16精度够高但是计算量较大（对于大小为$224\\times 224$的单幅输入图像，卷积层就需要30.69B次浮点运算）。在YOLO V1中，我们的网络结构较为简单，在精度上不如VGG-16（ImageNet测试，88.0% vs 90.0%）。</p>\n<p>在YOLO V2中，我们使用了一种新的网络结构Darknet-19（因为base network有19个卷积层）。和VGG相类似，我们的卷积核也都是$3\\times 3$大小，同时每次pooling操作后channel数double。另外，在NIN工作基础上，我们在网络最后使用了global average pooling层，同时使用$1\\times 1$大小的卷积核来做feature map的压缩（分辨率不变，channel减小，新的元素是原来相应位置不同channel的线性组合），同时使用了Batch Normalization技术。具体的网络结构见下表。Darknet-19计算量大大减小，同时精度超过了VGG-16。<br><img src=\"/img/yolo2_dartnet_19_structure.png\" alt=\"Darknet-19的网络结构\"></p>\n<p>在训练过程中，首先在ImageNet 1K上进行分类器的训练。使用数据增强技术（如随机裁剪、旋转、饱和度变化等）。和上面的讨论相对应，首先使用$224\\times 224$大小的图像进行训练，再使用$448\\times 448$的图像进行fine tuning，具体训练参数设置可以参见论文和对应的代码。这里不再多说。</p>\n<p>然后，我们对检测任务进行训练。对网络结构进行微调，去掉最后的卷积层（因为它是我们当初为了得到1K类的分类置信度加上去的），增加$3$个$3\\times 3$大小，channel数为$1024$的卷积层，并每个都跟着一个$1\\times 1$大小的卷积层，channel数由我们最终的检测任务需要的输出决定。对于VOC的检测任务来说，我们需要预测$5$个box，每个需要$5$个参数（相对位置，相对大小和置信度），同时$20$个类别的置信度，所以输出共$5\\times(5+20)=125$。从YOLO V2的<code>yolo_voc.cfg</code><a href=\"https://github.com/pjreddie/darknet/blob/master/cfg/yolo.cfg\">文件</a>中，我们也可以看到如下的对应结构：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div></pre></td><td class=\"code\"><pre><div class=\"line\">[convolutional]</div><div class=\"line\">batch_normalize=1</div><div class=\"line\">size=3</div><div class=\"line\">stride=1</div><div class=\"line\">pad=1</div><div class=\"line\">filters=1024</div><div class=\"line\">activation=leaky</div><div class=\"line\"></div><div class=\"line\">[convolutional]</div><div class=\"line\">size=1</div><div class=\"line\">stride=1</div><div class=\"line\">pad=1</div><div class=\"line\">filters=125</div><div class=\"line\">activation=linear</div></pre></td></tr></table></figure>\n<p>同时，加上上文提到的pass-through结构。</p>\n<h2 id=\"Stronger\"><a href=\"#Stronger\" class=\"headerlink\" title=\"Stronger\"></a>Stronger</h2><p>未完待续</p>"}],"PostAsset":[],"PostCategory":[],"PostTag":[{"post_id":"cizve079a0000l61h6214fsk5","tag_id":"cizve079m0004l61h012gy084","_id":"cizve07ad000dl61hixhesra2"},{"post_id":"cizve079a0000l61h6214fsk5","tag_id":"cizve07a40008l61h7gbhnin5","_id":"cizve07ag000fl61h8tsw2w6b"},{"post_id":"cizve07ah000hl61hcbllf1kr","tag_id":"cizve079m0004l61h012gy084","_id":"cizve07am000jl61hrpx3kl8z"},{"post_id":"cizve079j0002l61h8ymnztvv","tag_id":"cizve07aa000bl61h76qcef34","_id":"cizve07ar000ml61heys7ny1f"},{"post_id":"cizve079j0002l61h8ymnztvv","tag_id":"cizve07ah000gl61h91govlh2","_id":"cizve07ax000ol61h6ep3g3mf"},{"post_id":"cizve07al000il61h965n49u4","tag_id":"cizve079m0004l61h012gy084","_id":"cizve07b5000rl61h6o4m5dhx"},{"post_id":"cizve079r0005l61hzvv9pn1w","tag_id":"cizve07aa000bl61h76qcef34","_id":"cizve07bg000vl61hbujik6yx"},{"post_id":"cizve079r0005l61hzvv9pn1w","tag_id":"cizve07ah000gl61h91govlh2","_id":"cizve07bi000xl61hdntdu4xv"},{"post_id":"cizve07bg000wl61haq90ueiw","tag_id":"cizve07a40008l61h7gbhnin5","_id":"cizve07bn0010l61he8n5gfsv"},{"post_id":"cizve079v0006l61hbf0puned","tag_id":"cizve07aa000bl61h76qcef34","_id":"cizve07br0014l61h6a7yzd2p"},{"post_id":"cizve079v0006l61hbf0puned","tag_id":"cizve07ah000gl61h91govlh2","_id":"cizve07bs0015l61habfsojrh"},{"post_id":"cizve079y0007l61h77v2wxan","tag_id":"cizve07aa000bl61h76qcef34","_id":"cizve07bt0018l61htfafnc29"},{"post_id":"cizve079y0007l61h77v2wxan","tag_id":"cizve07ah000gl61h91govlh2","_id":"cizve07bt0019l61hmbrwvfsm"},{"post_id":"cizve07a50009l61h7eg1xkfr","tag_id":"cizve07aa000bl61h76qcef34","_id":"cizve07bx001cl61hj8xlpmtm"},{"post_id":"cizve07a50009l61h7eg1xkfr","tag_id":"cizve07ah000gl61h91govlh2","_id":"cizve07bx001dl61h9hkaf105"},{"post_id":"cizve07a8000al61hz0ftb8g2","tag_id":"cizve07aa000bl61h76qcef34","_id":"cizve07bz001gl61hk87tvb95"},{"post_id":"cizve07a8000al61hz0ftb8g2","tag_id":"cizve07ah000gl61h91govlh2","_id":"cizve07bz001hl61hwfwa8s5n"},{"post_id":"cizve07ab000cl61hbw5419kb","tag_id":"cizve07aa000bl61h76qcef34","_id":"cizve07c0001jl61hdqgfsh8r"},{"post_id":"cizve07ab000cl61hbw5419kb","tag_id":"cizve07ah000gl61h91govlh2","_id":"cizve07c0001kl61h26y3fqaf"},{"post_id":"cizve07af000el61h25osqi6t","tag_id":"cizve07aa000bl61h76qcef34","_id":"cizve07c1001ml61hvalqifr9"},{"post_id":"cizve07af000el61h25osqi6t","tag_id":"cizve07ah000gl61h91govlh2","_id":"cizve07c2001nl61hjf3jarft"},{"post_id":"cizve07ar000nl61h7ciz58no","tag_id":"cizve079m0004l61h012gy084","_id":"cizve07c2001pl61h3avjn0bx"},{"post_id":"cizve07ar000nl61h7ciz58no","tag_id":"cizve07c1001ll61hpsxv4a2y","_id":"cizve07c3001ql61hmamyec49"},{"post_id":"cizve07ax000ql61ht5fv39rj","tag_id":"cizve07c2001ol61h9tclgi3x","_id":"cizve07c4001sl61h5hl7e1qk"},{"post_id":"cizve07b6000sl61hd8o0r5lf","tag_id":"cizve07c3001rl61hjnp5sp4x","_id":"cizve07c6001ul61h4khuw24o"},{"post_id":"cizve07bd000ul61h3e2ekohs","tag_id":"cizve07c4001tl61h9qapt9rj","_id":"cizve07c8001wl61hwsf93x0x"},{"post_id":"cizve07bk000zl61hfpyggucp","tag_id":"cizve07c2001ol61h9tclgi3x","_id":"cizve07c9001yl61h0uighuyc"},{"post_id":"cizve07bo0011l61h81lhzd47","tag_id":"cizve079m0004l61h012gy084","_id":"cizve07cd0020l61hhyiql7da"},{"post_id":"cizve07bo0011l61h81lhzd47","tag_id":"cizve07c8001xl61h22it87f2","_id":"cizve07cd0021l61hnarqi6qi"},{"post_id":"cizve07bp0013l61h3wp08044","tag_id":"cizve07ca001zl61h8bg6gqnm","_id":"cizve07ce0023l61h92h9x0vd"},{"post_id":"cizve07bp0013l61h3wp08044","tag_id":"cizve07cd0022l61hrdp7q7kt","_id":"cizve07ce0024l61hkinqkhbe"}],"Tag":[{"name":"tool","_id":"cizve079m0004l61h012gy084"},{"name":"caffe","_id":"cizve07a40008l61h7gbhnin5"},{"name":"cs131","_id":"cizve07aa000bl61h76qcef34"},{"name":"公开课","_id":"cizve07ah000gl61h91govlh2"},{"name":"gsl","_id":"cizve07c1001ll61hpsxv4a2y"},{"name":"pytorch","_id":"cizve07c2001ol61h9tclgi3x"},{"name":"python","_id":"cizve07c3001rl61hjnp5sp4x"},{"name":"math","_id":"cizve07c4001tl61h9qapt9rj"},{"name":"doxygen","_id":"cizve07c8001xl61h22it87f2"},{"name":"paper","_id":"cizve07ca001zl61h8bg6gqnm"},{"name":"yolo","_id":"cizve07cd0022l61hrdp7q7kt"}]}}