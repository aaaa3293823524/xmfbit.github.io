{"meta":{"version":1,"warehouse":"2.2.0"},"models":{"Asset":[{"_id":"source/img/caffe_mathfunctions_gpuisnuclearweapon.jpg","path":"img/caffe_mathfunctions_gpuisnuclearweapon.jpg","modified":0,"renderable":0},{"_id":"source/img/camera_skew.png","path":"img/camera_skew.png","modified":0,"renderable":0},{"_id":"source/img/captcha_test_accuracy.png","path":"img/captcha_test_accuracy.png","modified":0,"renderable":0},{"_id":"source/img/captcha_train_loss.png","path":"img/captcha_train_loss.png","modified":0,"renderable":0},{"_id":"source/img/case_1_m.png","path":"img/case_1_m.png","modified":0,"renderable":0},{"_id":"source/img/case_2_m.png","path":"img/case_2_m.png","modified":0,"renderable":0},{"_id":"source/img/case_3_m.png","path":"img/case_3_m.png","modified":0,"renderable":0},{"_id":"source/img/case_4_m.png","path":"img/case_4_m.png","modified":0,"renderable":0},{"_id":"source/img/case_m_5.png","path":"img/case_m_5.png","modified":0,"renderable":0},{"_id":"source/img/corner_type_1.png","path":"img/corner_type_1.png","modified":0,"renderable":0},{"_id":"source/img/cs131_opticalflow_brightnessconstancy_assumption.png","path":"img/cs131_opticalflow_brightnessconstancy_assumption.png","modified":0,"renderable":0},{"_id":"source/img/effective_cpp_07_joke.jpg","path":"img/effective_cpp_07_joke.jpg","modified":0,"renderable":0},{"_id":"source/img/effectivecpp_04_cwithclass.jpg","path":"img/effectivecpp_04_cwithclass.jpg","modified":0,"renderable":0},{"_id":"source/img/effectivecpp_02_pointers.png","path":"img/effectivecpp_02_pointers.png","modified":0,"renderable":0},{"_id":"source/img/effectivecpp_01_cpp_rely_on_renpin.jpg","path":"img/effectivecpp_01_cpp_rely_on_renpin.jpg","modified":0,"renderable":0},{"_id":"source/img/effectivecpp_diamond.png","path":"img/effectivecpp_diamond.png","modified":0,"renderable":0},{"_id":"source/img/effectivecpp_08_memory_leak_everywherre.jpg","path":"img/effectivecpp_08_memory_leak_everywherre.jpg","modified":0,"renderable":0},{"_id":"source/img/effectivecpp_strategy_pattern.png","path":"img/effectivecpp_strategy_pattern.png","modified":0,"renderable":0},{"_id":"source/img/gsl_picture.jpg","path":"img/gsl_picture.jpg","modified":0,"renderable":0},{"_id":"source/img/helloworld_hexo.png","path":"img/helloworld_hexo.png","modified":0,"renderable":0},{"_id":"source/img/hinton_01_neuron_commucation.png","path":"img/hinton_01_neuron_commucation.png","modified":0,"renderable":0},{"_id":"source/img/hinton_01_neuron_structure.png","path":"img/hinton_01_neuron_structure.png","modified":0,"renderable":0},{"_id":"source/img/hinton_02_perceptron_gragh.png","path":"img/hinton_02_perceptron_gragh.png","modified":0,"renderable":0},{"_id":"source/img/hinton_02_margin.png","path":"img/hinton_02_margin.png","modified":0,"renderable":0},{"_id":"source/img/hinton_06_maanmian.jpg","path":"img/hinton_06_maanmian.jpg","modified":0,"renderable":0},{"_id":"source/img/install_ubuntu_in_dell_weixiaodaizhepibei.jpg","path":"img/install_ubuntu_in_dell_weixiaodaizhepibei.jpg","modified":0,"renderable":0},{"_id":"source/img/install_ubuntu_in_dell_kaiyuandafahao.jpg","path":"img/install_ubuntu_in_dell_kaiyuandafahao.jpg","modified":0,"renderable":0},{"_id":"source/img/hinton_brainsimulator.jpg","path":"img/hinton_brainsimulator.jpg","modified":0,"renderable":0},{"_id":"source/img/install_ubuntu_in_dell_weixiaojiuhao.jpg","path":"img/install_ubuntu_in_dell_weixiaojiuhao.jpg","modified":0,"renderable":0},{"_id":"source/img/jupyternotebook_logo.png","path":"img/jupyternotebook_logo.png","modified":0,"renderable":0},{"_id":"source/img/kmeans_data_demo.png","path":"img/kmeans_data_demo.png","modified":0,"renderable":0},{"_id":"source/img/kmeans_success.png","path":"img/kmeans_success.png","modified":0,"renderable":0},{"_id":"source/img/line_fit_demo.png","path":"img/line_fit_demo.png","modified":0,"renderable":0},{"_id":"source/img/meanshift_basics.jpg","path":"img/meanshift_basics.jpg","modified":0,"renderable":0},{"_id":"source/img/mnist_example.png","path":"img/mnist_example.png","modified":0,"renderable":0},{"_id":"source/img/meanshift_simple_demo.png","path":"img/meanshift_simple_demo.png","modified":0,"renderable":0},{"_id":"source/img/qicizuobiao.png","path":"img/qicizuobiao.png","modified":0,"renderable":0},{"_id":"source/img/pytorch_logo.png","path":"img/pytorch_logo.png","modified":0,"renderable":0},{"_id":"source/img/ransac_step.png","path":"img/ransac_step.png","modified":0,"renderable":0},{"_id":"source/img/residualnet_unit.png","path":"img/residualnet_unit.png","modified":0,"renderable":0},{"_id":"source/img/rotation.png","path":"img/rotation.png","modified":0,"renderable":0},{"_id":"source/img/shell-programming-bash-logo.png","path":"img/shell-programming-bash-logo.png","modified":0,"renderable":0},{"_id":"source/img/sift_dominant_orientation.png","path":"img/sift_dominant_orientation.png","modified":0,"renderable":0},{"_id":"source/img/sift_picture.jpg","path":"img/sift_picture.jpg","modified":0,"renderable":0},{"_id":"source/img/silver_rl_bellman_equation_figure.png","path":"img/silver_rl_bellman_equation_figure.png","modified":0,"renderable":0},{"_id":"source/img/silver_rl_bellman_equation_matrix.png","path":"img/silver_rl_bellman_equation_matrix.png","modified":0,"renderable":0},{"_id":"source/img/silver_rl_bellman_equation_solution.png","path":"img/silver_rl_bellman_equation_solution.png","modified":0,"renderable":0},{"_id":"source/img/silver_rl_dp_policy_evaluating_demo.png","path":"img/silver_rl_dp_policy_evaluating_demo.png","modified":0,"renderable":0},{"_id":"source/img/silver_rl_dp_policy_evaluating_demo_result.png","path":"img/silver_rl_dp_policy_evaluating_demo_result.png","modified":0,"renderable":0},{"_id":"source/img/silver_rl_mdp.png","path":"img/silver_rl_mdp.png","modified":0,"renderable":0},{"_id":"source/img/silver_rl_mdp_optimal_policy.png","path":"img/silver_rl_mdp_optimal_policy.png","modified":0,"renderable":0},{"_id":"source/img/silver_rl_mdp_optimal_vq_relationship.png","path":"img/silver_rl_mdp_optimal_vq_relationship.png","modified":0,"renderable":0},{"_id":"source/img/silver_rl_mdp_optimal_vq_relationship2.png","path":"img/silver_rl_mdp_optimal_vq_relationship2.png","modified":0,"renderable":0},{"_id":"source/img/silver_rl_mdp_vq_relationship.png","path":"img/silver_rl_mdp_vq_relationship.png","modified":0,"renderable":0},{"_id":"source/img/silver_rl_mdp_vq_relationship2.png","path":"img/silver_rl_mdp_vq_relationship2.png","modified":0,"renderable":0},{"_id":"source/img/svd_ranking.png","path":"img/svd_ranking.png","modified":0,"renderable":0},{"_id":"source/img/svd_picture.jpg","path":"img/svd_picture.jpg","modified":0,"renderable":0},{"_id":"source/img/yolo2_bbox_param.png","path":"img/yolo2_bbox_param.png","modified":0,"renderable":0},{"_id":"source/img/yolo2_bbox_location.png","path":"img/yolo2_bbox_location.png","modified":0,"renderable":0},{"_id":"source/img/yolo2_cluster_result.png","path":"img/yolo2_cluster_result.png","modified":0,"renderable":0},{"_id":"source/img/caffe_bn_bp_of_bn.jpg","path":"img/caffe_bn_bp_of_bn.jpg","modified":0,"renderable":0},{"_id":"source/img/caffe_mathfunctions_useuva.png","path":"img/caffe_mathfunctions_useuva.png","modified":0,"renderable":0},{"_id":"source/img/caffe_syncedmem_blob_flow.jpg","path":"img/caffe_syncedmem_blob_flow.jpg","modified":0,"renderable":0},{"_id":"source/img/caffe_syncedmem_transfer.png","path":"img/caffe_syncedmem_transfer.png","modified":0,"renderable":0},{"_id":"source/img/canny_linking.png","path":"img/canny_linking.png","modified":0,"renderable":0},{"_id":"source/img/canny_nms.png","path":"img/canny_nms.png","modified":0,"renderable":0},{"_id":"source/img/case_6_m.png","path":"img/case_6_m.png","modified":0,"renderable":0},{"_id":"source/img/corner_window_fun.png","path":"img/corner_window_fun.png","modified":0,"renderable":0},{"_id":"source/img/cs131_opticalflow_demo.jpg","path":"img/cs131_opticalflow_demo.jpg","modified":0,"renderable":0},{"_id":"source/img/cs131_opticalflow_lkleastsquare.png","path":"img/cs131_opticalflow_lkleastsquare.png","modified":0,"renderable":0},{"_id":"source/img/cs131_opticalflow_lkrelationshipwithharris.png","path":"img/cs131_opticalflow_lkrelationshipwithharris.png","modified":0,"renderable":0},{"_id":"source/img/doc2dash_how_to_add_docset.jpg","path":"img/doc2dash_how_to_add_docset.jpg","modified":0,"renderable":0},{"_id":"source/img/doxygen_picture.png","path":"img/doxygen_picture.png","modified":0,"renderable":0},{"_id":"source/img/edge_camera_man.png","path":"img/edge_camera_man.png","modified":0,"renderable":0},{"_id":"source/img/edge_deriative.png","path":"img/edge_deriative.png","modified":0,"renderable":0},{"_id":"source/img/effectivecpp_06_joke.jpg","path":"img/effectivecpp_06_joke.jpg","modified":0,"renderable":0},{"_id":"source/img/god_use_vpn.png","path":"img/god_use_vpn.png","modified":0,"renderable":0},{"_id":"source/img/harris_non_scale_constant.png","path":"img/harris_non_scale_constant.png","modified":0,"renderable":0},{"_id":"source/img/hinton_02_feed_forward_nn.png","path":"img/hinton_02_feed_forward_nn.png","modified":0,"renderable":0},{"_id":"source/img/hinton_02_perceptron_xor.png","path":"img/hinton_02_perceptron_xor.png","modified":0,"renderable":0},{"_id":"source/img/hinton_02_recurrent_nn.png","path":"img/hinton_02_recurrent_nn.png","modified":0,"renderable":0},{"_id":"source/img/hinton_02_why_training_works_1.png","path":"img/hinton_02_why_training_works_1.png","modified":0,"renderable":0},{"_id":"source/img/meanshift_gradient_of_density.png","path":"img/meanshift_gradient_of_density.png","modified":0,"renderable":0},{"_id":"source/img/meanshift_kernel_function.png","path":"img/meanshift_kernel_function.png","modified":0,"renderable":0},{"_id":"source/img/patch_average_intensity_scale_constant.png","path":"img/patch_average_intensity_scale_constant.png","modified":0,"renderable":0},{"_id":"source/img/qicizuobiao_transform.png","path":"img/qicizuobiao_transform.png","modified":0,"renderable":0},{"_id":"source/img/residualnet_bottleneck_unit.png","path":"img/residualnet_bottleneck_unit.png","modified":0,"renderable":0},{"_id":"source/img/residualnet_improved_structure.png","path":"img/residualnet_improved_structure.png","modified":0,"renderable":0},{"_id":"source/img/rotation_matrix.png","path":"img/rotation_matrix.png","modified":0,"renderable":0},{"_id":"source/img/sift_detection_maximum.png","path":"img/sift_detection_maximum.png","modified":0,"renderable":0},{"_id":"source/img/silver_mdp_value_function.png","path":"img/silver_mdp_value_function.png","modified":0,"renderable":0},{"_id":"source/img/silver_rl_dp_detailed_prioritized_dp.png","path":"img/silver_rl_dp_detailed_prioritized_dp.png","modified":0,"renderable":0},{"_id":"source/img/silver_rl_dp_inplace_value_iteration.png","path":"img/silver_rl_dp_inplace_value_iteration.png","modified":0,"renderable":0},{"_id":"source/img/silver_rl_dp_improve_policy_greedily_proof_2.png","path":"img/silver_rl_dp_improve_policy_greedily_proof_2.png","modified":0,"renderable":0},{"_id":"source/img/silver_rl_dp_realtime_dp.png","path":"img/silver_rl_dp_realtime_dp.png","modified":0,"renderable":0},{"_id":"source/img/silver_rl_dp_value_iteration_demo.png","path":"img/silver_rl_dp_value_iteration_demo.png","modified":0,"renderable":0},{"_id":"source/img/silver_rl_dp_synchronous_value_iteration.png","path":"img/silver_rl_dp_synchronous_value_iteration.png","modified":0,"renderable":0},{"_id":"source/img/silver_rl_mdp_optimal_qq_relationship.png","path":"img/silver_rl_mdp_optimal_qq_relationship.png","modified":0,"renderable":0},{"_id":"source/img/silver_rl_mdp_optimal_vv_relationship.png","path":"img/silver_rl_mdp_optimal_vv_relationship.png","modified":0,"renderable":0},{"_id":"source/img/silver_rl_mdp_qq_relationship.png","path":"img/silver_rl_mdp_qq_relationship.png","modified":0,"renderable":0},{"_id":"source/img/silver_rl_mdp_vv_relationship.png","path":"img/silver_rl_mdp_vv_relationship.png","modified":0,"renderable":0},{"_id":"source/img/svd_superman.png","path":"img/svd_superman.png","modified":0,"renderable":0},{"_id":"source/img/video_linear_alg_essential_linear_equation.png","path":"img/video_linear_alg_essential_linear_equation.png","modified":0,"renderable":0},{"_id":"source/img/yolo1_loss_fun.png","path":"img/yolo1_loss_fun.png","modified":0,"renderable":0},{"_id":"source/img/yolo1_network_arch.png","path":"img/yolo1_network_arch.png","modified":0,"renderable":0},{"_id":"source/img/yolo2_different_methods_comparation.png","path":"img/yolo2_different_methods_comparation.png","modified":0,"renderable":0},{"_id":"source/img/yolo2_different_methods_improvement.png","path":"img/yolo2_different_methods_improvement.png","modified":0,"renderable":0},{"_id":"source/img/camera_translation_rotation.png","path":"img/camera_translation_rotation.png","modified":0,"renderable":0},{"_id":"source/img/corner_judge.png","path":"img/corner_judge.png","modified":0,"renderable":0},{"_id":"source/img/corner_judge_2.png","path":"img/corner_judge_2.png","modified":0,"renderable":0},{"_id":"source/img/cs131_opticalflow_lkequation.png","path":"img/cs131_opticalflow_lkequation.png","modified":0,"renderable":0},{"_id":"source/img/cs131_opticalflow_lkharris.png","path":"img/cs131_opticalflow_lkharris.png","modified":0,"renderable":0},{"_id":"source/img/epipolar_constraint_1.png","path":"img/epipolar_constraint_1.png","modified":0,"renderable":0},{"_id":"source/img/epipolar_fig.png","path":"img/epipolar_fig.png","modified":0,"renderable":0},{"_id":"source/img/flowwarped.png","path":"img/flowwarped.png","modified":0,"renderable":0},{"_id":"source/img/fun_noise.png","path":"img/fun_noise.png","modified":0,"renderable":0},{"_id":"source/img/forwardwarped.png","path":"img/forwardwarped.png","modified":0,"renderable":0},{"_id":"source/img/generic_projection_matrix.png","path":"img/generic_projection_matrix.png","modified":0,"renderable":0},{"_id":"source/img/hinton_01_mnist_example.png","path":"img/hinton_01_mnist_example.png","modified":0,"renderable":0},{"_id":"source/img/hinton_01_sigmoid_function.png","path":"img/hinton_01_sigmoid_function.png","modified":0,"renderable":0},{"_id":"source/img/hinton_02_perceptron_paradigm_for_pattern_recong.png","path":"img/hinton_02_perceptron_paradigm_for_pattern_recong.png","modified":0,"renderable":0},{"_id":"source/img/hinton_02_rnn_app.png","path":"img/hinton_02_rnn_app.png","modified":0,"renderable":0},{"_id":"source/img/hinton_06_adamax.png","path":"img/hinton_06_adamax.png","modified":0,"renderable":0},{"_id":"source/img/ipsec_ios_vpn_setting.png","path":"img/ipsec_ios_vpn_setting.png","modified":0,"renderable":0},{"_id":"source/img/kmeans_object_fun_vs_k.png","path":"img/kmeans_object_fun_vs_k.png","modified":0,"renderable":0},{"_id":"source/img/kmeans_sensitive_to_outlier.png","path":"img/kmeans_sensitive_to_outlier.png","modified":0,"renderable":0},{"_id":"source/img/original_superman.png","path":"img/original_superman.png","modified":0,"renderable":0},{"_id":"source/img/ransac_k.png","path":"img/ransac_k.png","modified":0,"renderable":0},{"_id":"source/img/residualnet_comparison_with_plainnet.png","path":"img/residualnet_comparison_with_plainnet.png","modified":0,"renderable":0},{"_id":"source/img/residualnet_deepnet_problem.png","path":"img/residualnet_deepnet_problem.png","modified":0,"renderable":0},{"_id":"source/img/silver_rl_dp_improve_policy_greedily_proof.png","path":"img/silver_rl_dp_improve_policy_greedily_proof.png","modified":0,"renderable":0},{"_id":"source/img/what_is_corner.png","path":"img/what_is_corner.png","modified":0,"renderable":0},{"_id":"source/img/yolo2_different_methods_comparation_in_table.png","path":"img/yolo2_different_methods_comparation_in_table.png","modified":0,"renderable":0},{"_id":"themes/next/source/css/main.styl","path":"css/main.styl","modified":0,"renderable":1},{"_id":"themes/next/source/images/algolia_logo.svg","path":"images/algolia_logo.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/avatar.gif","path":"images/avatar.gif","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc-nd.svg","path":"images/cc-by-nc-nd.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc-sa.svg","path":"images/cc-by-nc-sa.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc.svg","path":"images/cc-by-nc.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-nd.svg","path":"images/cc-by-nd.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-sa.svg","path":"images/cc-by-sa.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by.svg","path":"images/cc-by.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/loading.gif","path":"images/loading.gif","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-zero.svg","path":"images/cc-zero.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/placeholder.gif","path":"images/placeholder.gif","modified":0,"renderable":1},{"_id":"themes/next/source/images/searchicon.png","path":"images/searchicon.png","modified":0,"renderable":1},{"_id":"themes/next/source/images/quote-l.svg","path":"images/quote-l.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/quote-r.svg","path":"images/quote-r.svg","modified":0,"renderable":1},{"_id":"source/img/caffe_bn_what_is_bn.jpg","path":"img/caffe_bn_what_is_bn.jpg","modified":0,"renderable":0},{"_id":"source/img/caffe_image.jpg","path":"img/caffe_image.jpg","modified":0,"renderable":0},{"_id":"source/img/just-a-joke.png","path":"img/just-a-joke.png","modified":0,"renderable":0},{"_id":"source/img/kmeans_bigger_demo.png","path":"img/kmeans_bigger_demo.png","modified":0,"renderable":0},{"_id":"source/img/pinhole_camera_model.png","path":"img/pinhole_camera_model.png","modified":0,"renderable":0},{"_id":"source/img/resnet-164layer-cifar10-training.jpg","path":"img/resnet-164layer-cifar10-training.jpg","modified":0,"renderable":0},{"_id":"source/img/resnet-164layer-cifar10-testing.jpg","path":"img/resnet-164layer-cifar10-testing.jpg","modified":0,"renderable":0},{"_id":"source/img/silver_rl_dp_synchronous_dp_algorithms.png","path":"img/silver_rl_dp_synchronous_dp_algorithms.png","modified":0,"renderable":0},{"_id":"source/img/yolo2_dartnet_19_structure.png","path":"img/yolo2_dartnet_19_structure.png","modified":0,"renderable":0},{"_id":"source/avatar/liumengli.jpg","path":"avatar/liumengli.jpg","modified":0,"renderable":0},{"_id":"source/img/cs131_opticalflow_assignment_crossfade.png","path":"img/cs131_opticalflow_assignment_crossfade.png","modified":0,"renderable":0},{"_id":"source/img/cs131_opticalflow_pyramid.png","path":"img/cs131_opticalflow_pyramid.png","modified":0,"renderable":0},{"_id":"source/img/mathfunctions_time_distribution.png","path":"img/mathfunctions_time_distribution.png","modified":0,"renderable":0},{"_id":"source/img/ubuntu_exfat.png","path":"img/ubuntu_exfat.png","modified":0,"renderable":0},{"_id":"source/img/useful_tools_colored_man_pages.jpg","path":"img/useful_tools_colored_man_pages.jpg","modified":0,"renderable":0},{"_id":"themes/next/source/js/src/algolia-search.js","path":"js/src/algolia-search.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/affix.js","path":"js/src/affix.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/hook-duoshuo.js","path":"js/src/hook-duoshuo.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/bootstrap.js","path":"js/src/bootstrap.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/motion.js","path":"js/src/motion.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/post-details.js","path":"js/src/post-details.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/scrollspy.js","path":"js/src/scrollspy.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/utils.js","path":"js/src/utils.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/canvas-nest/canvas-nest.min.js","path":"lib/canvas-nest/canvas-nest.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.css","path":"lib/algolia-instant-search/instantsearch.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fastclick/bower.json","path":"lib/fastclick/bower.json","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fastclick/LICENSE","path":"lib/fastclick/LICENSE","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fastclick/README.md","path":"lib/fastclick/README.md","modified":0,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/CONTRIBUTING.md","path":"lib/jquery_lazyload/CONTRIBUTING.md","modified":0,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/README.md","path":"lib/jquery_lazyload/README.md","modified":0,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/bower.json","path":"lib/jquery_lazyload/bower.json","modified":0,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/jquery.lazyload.js","path":"lib/jquery_lazyload/jquery.lazyload.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/jquery.scrollstop.js","path":"lib/jquery_lazyload/jquery.scrollstop.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/HELP-US-OUT.txt","path":"lib/font-awesome/HELP-US-OUT.txt","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/bower.json","path":"lib/font-awesome/bower.json","modified":0,"renderable":1},{"_id":"themes/next/source/lib/velocity/bower.json","path":"lib/velocity/bower.json","modified":0,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.min.js","path":"lib/velocity/velocity.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.ui.js","path":"lib/velocity/velocity.ui.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.ui.min.js","path":"lib/velocity/velocity.ui.min.js","modified":0,"renderable":1},{"_id":"source/img/bash-programming-comparing-string.jpg","path":"img/bash-programming-comparing-string.jpg","modified":0,"renderable":0},{"_id":"source/img/hinton_06_nesterov_momentum.png","path":"img/hinton_06_nesterov_momentum.png","modified":0,"renderable":0},{"_id":"source/img/kmeans_demo.png","path":"img/kmeans_demo.png","modified":0,"renderable":0},{"_id":"source/img/svd_flower.png","path":"img/svd_flower.png","modified":0,"renderable":0},{"_id":"source/img/video_linear_alg_essential.png","path":"img/video_linear_alg_essential.png","modified":0,"renderable":0},{"_id":"themes/next/source/lib/jquery/index.js","path":"lib/jquery/index.js","modified":0,"renderable":1},{"_id":"source/img/dog_x.png","path":"img/dog_x.png","modified":0,"renderable":0},{"_id":"source/img/paper-inq-result.png","path":"img/paper-inq-result.png","modified":0,"renderable":0},{"_id":"source/img/ransac_line_fit.png","path":"img/ransac_line_fit.png","modified":0,"renderable":0},{"_id":"source/img/regex_picture.jpg","path":"img/regex_picture.jpg","modified":0,"renderable":0},{"_id":"source/img/sift_dog.png","path":"img/sift_dog.png","modified":0,"renderable":0},{"_id":"themes/next/source/js/src/schemes/pisces.js","path":"js/src/schemes/pisces.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/blank.gif","path":"lib/fancybox/source/blank.gif","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_loading.gif","path":"lib/fancybox/source/fancybox_loading.gif","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_loading@2x.gif","path":"lib/fancybox/source/fancybox_loading@2x.gif","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_overlay.png","path":"lib/fancybox/source/fancybox_overlay.png","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_sprite.png","path":"lib/fancybox/source/fancybox_sprite.png","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_sprite@2x.png","path":"lib/fancybox/source/fancybox_sprite@2x.png","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.css","path":"lib/fancybox/source/jquery.fancybox.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.pack.js","path":"lib/fancybox/source/jquery.fancybox.pack.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.js","path":"lib/fancybox/source/jquery.fancybox.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fastclick/lib/fastclick.js","path":"lib/fastclick/lib/fastclick.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fastclick/lib/fastclick.min.js","path":"lib/fastclick/lib/fastclick.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css","path":"lib/font-awesome/css/font-awesome.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css.map","path":"lib/font-awesome/css/font-awesome.css.map","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.min.css","path":"lib/font-awesome/css/font-awesome.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.pack.js","path":"lib/ua-parser-js/dist/ua-parser.pack.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.min.js","path":"lib/ua-parser-js/dist/ua-parser.min.js","modified":0,"renderable":1},{"_id":"source/img/hinton_06_learningrate.png","path":"img/hinton_06_learningrate.png","modified":0,"renderable":0},{"_id":"source/img/sift_experiment_1.png","path":"img/sift_experiment_1.png","modified":0,"renderable":0},{"_id":"source/img/yolo1_detection_system.png","path":"img/yolo1_detection_system.png","modified":0,"renderable":0},{"_id":"source/img/warpctc_intro.png","path":"img/warpctc_intro.png","modified":0,"renderable":0},{"_id":"themes/next/source/lib/font-awesome/fonts/FontAwesome.otf","path":"lib/font-awesome/fonts/FontAwesome.otf","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.eot","path":"lib/font-awesome/fonts/fontawesome-webfont.eot","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff","path":"lib/font-awesome/fonts/fontawesome-webfont.woff","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff2","path":"lib/font-awesome/fonts/fontawesome-webfont.woff2","modified":0,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.js","path":"lib/velocity/velocity.js","modified":0,"renderable":1},{"_id":"source/img/camera_model_things_to_remember.png","path":"img/camera_model_things_to_remember.png","modified":0,"renderable":0},{"_id":"source/img/hinton_06_rmsprop_improvement.png","path":"img/hinton_06_rmsprop_improvement.png","modified":0,"renderable":0},{"_id":"themes/next/source/lib/fancybox/source/helpers/fancybox_buttons.png","path":"lib/fancybox/source/helpers/fancybox_buttons.png","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-buttons.js","path":"lib/fancybox/source/helpers/jquery.fancybox-buttons.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-buttons.css","path":"lib/fancybox/source/helpers/jquery.fancybox-buttons.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-media.js","path":"lib/fancybox/source/helpers/jquery.fancybox-media.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-thumbs.css","path":"lib/fancybox/source/helpers/jquery.fancybox-thumbs.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-thumbs.js","path":"lib/fancybox/source/helpers/jquery.fancybox-thumbs.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.ttf","path":"lib/font-awesome/fonts/fontawesome-webfont.ttf","modified":0,"renderable":1},{"_id":"source/img/cs131_linear_algebra.jpg","path":"img/cs131_linear_algebra.jpg","modified":0,"renderable":0},{"_id":"source/img/hinton_02_weight_space_hyperplane.png","path":"img/hinton_02_weight_space_hyperplane.png","modified":0,"renderable":0},{"_id":"source/img/hinton_06_summary.png","path":"img/hinton_06_summary.png","modified":0,"renderable":0},{"_id":"source/img/hinton_06_tricks_for_adaptive_lr.png","path":"img/hinton_06_tricks_for_adaptive_lr.png","modified":0,"renderable":0},{"_id":"source/img/shell-programming-if-operators.jpg","path":"img/shell-programming-if-operators.jpg","modified":0,"renderable":0},{"_id":"source/img/yolo2_result.png","path":"img/yolo2_result.png","modified":0,"renderable":0},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.js","path":"lib/algolia-instant-search/instantsearch.min.js","modified":0,"renderable":1},{"_id":"source/img/convolution.png","path":"img/convolution.png","modified":0,"renderable":0},{"_id":"source/img/dog_different_size.png","path":"img/dog_different_size.png","modified":0,"renderable":0},{"_id":"source/img/shell-programming-wild-cards.jpg","path":"img/shell-programming-wild-cards.jpg","modified":0,"renderable":0},{"_id":"source/img/shell-programming-system-variables.jpg","path":"img/shell-programming-system-variables.jpg","modified":0,"renderable":0},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.svg","path":"lib/font-awesome/fonts/fontawesome-webfont.svg","modified":0,"renderable":1},{"_id":"source/img/doc2dash_pytorch_example.jpg","path":"img/doc2dash_pytorch_example.jpg","modified":0,"renderable":0},{"_id":"source/img/focal_loss_vs_ce_loss.jpg","path":"img/focal_loss_vs_ce_loss.jpg","modified":0,"renderable":0},{"_id":"source/img/kmeans_scaling_up.png","path":"img/kmeans_scaling_up.png","modified":0,"renderable":0},{"_id":"source/img/contours_evaluation_optimizers.gif","path":"img/contours_evaluation_optimizers.gif","modified":0,"renderable":0},{"_id":"source/img/mathfunctions_im2col.png","path":"img/mathfunctions_im2col.png","modified":0,"renderable":0},{"_id":"source/img/yolo1_basic_idea.png","path":"img/yolo1_basic_idea.png","modified":0,"renderable":0},{"_id":"source/img/kmeans_algorithm.png","path":"img/kmeans_algorithm.png","modified":0,"renderable":0},{"_id":"source/img/sift_experiment_2.png","path":"img/sift_experiment_2.png","modified":0,"renderable":0},{"_id":"source/img/caffe_mathfunctions_uvarequirement.png","path":"img/caffe_mathfunctions_uvarequirement.png","modified":0,"renderable":0},{"_id":"source/img/focal_loss_different_model_comparison.jpg","path":"img/focal_loss_different_model_comparison.jpg","modified":0,"renderable":0},{"_id":"source/img/projective_geometry_property_1.png","path":"img/projective_geometry_property_1.png","modified":0,"renderable":0},{"_id":"source/img/kmeans_image_seg_via_intensity.png","path":"img/kmeans_image_seg_via_intensity.png","modified":0,"renderable":0},{"_id":"source/img/projective_geometry_property_2.png","path":"img/projective_geometry_property_2.png","modified":0,"renderable":0},{"_id":"source/img/caffe_mathfunctions_whatisuva.png","path":"img/caffe_mathfunctions_whatisuva.png","modified":0,"renderable":0},{"_id":"source/img/camera_geometry_application.png","path":"img/camera_geometry_application.png","modified":0,"renderable":0},{"_id":"source/img/image_matching_hard.png","path":"img/image_matching_hard.png","modified":0,"renderable":0},{"_id":"source/img/paper-inq-quantize-set.png","path":"img/paper-inq-quantize-set.png","modified":1,"renderable":0},{"_id":"source/img/paper-inq-algorithm-demo.png","path":"img/paper-inq-algorithm-demo.png","modified":1,"renderable":0},{"_id":"source/img/paper-inq-different-quantize.png","path":"img/paper-inq-different-quantize.png","modified":1,"renderable":0}],"Cache":[{"_id":"source/.DS_Store","hash":"ec25431005a9ac1a881b1a21f470db7671cd677a","modified":1494125054000},{"_id":"themes/next/.bowerrc","hash":"3228a58ed0ece9f85e1e3136352094080b8dece1","modified":1515738141279},{"_id":"themes/next/.gitignore","hash":"5f09fca02e030b7676c1d312cd88ce8fbccf381c","modified":1515738141282},{"_id":"themes/next/.editorconfig","hash":"792fd2bd8174ece1a75d5fd24ab16594886f3a7f","modified":1515738141280},{"_id":"themes/next/.hound.yml","hash":"b76daa84c9ca3ad292c78412603370a367cc2bc3","modified":1515738141282},{"_id":"themes/next/.javascript_ignore","hash":"f9ea3c5395f8feb225a24e2c32baa79afda30c16","modified":1515738141282},{"_id":"themes/next/.jshintrc","hash":"9928f81bd822f6a8d67fdbc909b517178533bca9","modified":1515738141283},{"_id":"themes/next/README.en.md","hash":"3b0c7998cf17f9cf9e1a5bfcd65679a43a00c817","modified":1515738141283},{"_id":"themes/next/README.md","hash":"500b5606eb6a09c979d16128f8b00f4bf9bc95ac","modified":1515738141284},{"_id":"themes/next/_config.yml","hash":"c679ad2e7bb223fd92cf4d660137dc9dca55a561","modified":1515738141284},{"_id":"themes/next/bower.json","hash":"5abc236d9cc2512f5457ed57c1fba76669eb7399","modified":1515738141285},{"_id":"themes/next/gulpfile.coffee","hash":"61ef0606a8134894d7ac796bc8d0fa4ba6a94483","modified":1515738141285},{"_id":"themes/next/package.json","hash":"877cb98025e59015532c4c9a04a33e2af4ad56f9","modified":1515738141316},{"_id":"source/about/index.md","hash":"ecb4aea7dbbad7cdf296c67222526f5011739793","modified":1515738140485},{"_id":"source/_posts/build-caffe-ubuntu.md","hash":"e08f1d5390815fb605c46e2afe73dfed154a6558","modified":1515738140456},{"_id":"source/_posts/caffe-batch-norm.md","hash":"999f47cdb07486066c6cb9b61d63816f6c3133a9","modified":1515738140457},{"_id":"source/_posts/caffe-syncedmem.md","hash":"81c1a9091d2695ecdd23a7bc36c72de05217696c","modified":1516866266577},{"_id":"source/_posts/cs131-camera.md","hash":"9da13519e583f76d96d6bbe8cd37a032b03d7cdb","modified":1515738140459},{"_id":"source/_posts/cs131-filter-svd.md","hash":"d5ce30bc545a605ee347da5a58bea06db82dbe22","modified":1515738140459},{"_id":"source/_posts/cs131-edge-detection.md","hash":"a02d1bef37b25b7b0fd159019533bd02637be85b","modified":1515738140459},{"_id":"source/_posts/cs131-finding-features.md","hash":"1390fe3ca5f3c751c869dfc4a15602d800b81c09","modified":1515738140460},{"_id":"source/_posts/cs131-kmeans.md","hash":"6bbf310095da3bdf051e605922083e66a33ad17c","modified":1515738140460},{"_id":"source/_posts/cs131-mean-shift.md","hash":"d8b930ef584ec81102059e8fffee7f590a00da32","modified":1515738140461},{"_id":"source/_posts/cs131-linear-alg.md","hash":"7b56db3f15a3d743b35db4e05e91d057f5d065a4","modified":1515738140460},{"_id":"source/_posts/cs131-opticalflow.md","hash":"57339fa60ba747f0bb37e2af1b663e803b65e505","modified":1515738140461},{"_id":"source/_posts/cs131-sift.md","hash":"c2d4392bb366299e96cfca061390bf79d8174e3c","modified":1515738140462},{"_id":"source/_posts/debugging-with-ipdb.md","hash":"7cbee0319a9d7c170657f643b97273906c04c514","modified":1515738140462},{"_id":"source/_posts/digitalocean-shadowsocks.md","hash":"428cd1c58559e1e1dbc806d25ad8873bb655a82a","modified":1515738140463},{"_id":"source/_posts/effective-cpp-01.md","hash":"4abab2c9963faac02536630230ba45dfa638d169","modified":1515738140464},{"_id":"source/_posts/effective-cpp-03.md","hash":"a77eb64d1d0da11e5fd69e5d6cd9237e6709bef4","modified":1515738140464},{"_id":"source/_posts/doc2dash-usage.md","hash":"06ec8f2abc8e0e33061b72ba714ca8036e9b9a84","modified":1515738140463},{"_id":"source/_posts/effective-cpp-04.md","hash":"56e86176464d3526e2bc81730586797c0df17641","modified":1515738140465},{"_id":"source/_posts/effective-cpp-06.md","hash":"a97c471b35cd485babdaa22b726879380be07a8d","modified":1515738140467},{"_id":"source/_posts/effective-cpp-02.md","hash":"89c73725e3c1685a5e7bcd4a5f49258a0534ef84","modified":1515738140464},{"_id":"source/_posts/effective-cpp-07.md","hash":"9e1bdeada5048e08c7531bd0e8b114e880bdf03b","modified":1515738140468},{"_id":"source/_posts/effective-cpp-05.md","hash":"6aaaad76185c4c262e7ce6f5ae2bd8f4f4e1692e","modified":1515738140467},{"_id":"source/_posts/gsl-with-vs.md","hash":"73f407dc72fd5b71305fa83717e53bcbda6246e9","modified":1515738140470},{"_id":"source/_posts/focal-loss-paper.md","hash":"c407b7bc9071772269f49994168f30bb458e4dd1","modified":1515738140469},{"_id":"source/_posts/effective-cpp-08.md","hash":"cf3ad741018efe0911b74f229dcbc9616e86b082","modified":1515738140469},{"_id":"source/_posts/hello-world.md","hash":"387bcaf830e035ba5b4ad5646a9c1a981912a821","modified":1515738140470},{"_id":"source/_posts/fuck-gfw.md","hash":"90086691982d3249a50e58b5e9e1eab118914b53","modified":1515738140470},{"_id":"source/_posts/hinton-nnml-02.md","hash":"47d42cb75ce78cd13cff05471495ced79d284305","modified":1515738140471},{"_id":"source/_posts/inq-paper.md","hash":"07fc99f3f5a2d744f53c2459e87ac70c0c525494","modified":1516871078894},{"_id":"source/_posts/hinton-nnml-01.md","hash":"6476c725e2436ecc85324bf0ea67e04cc3497614","modified":1515738140471},{"_id":"source/_posts/hinton-nnml-06.md","hash":"92f039cb1e302c7be3909455efb4572a1da3f625","modified":1515738140472},{"_id":"source/_posts/learn-pyqt.md","hash":"d56d516a5f1327d60b77c9e4e81aaca94b801363","modified":1515738140473},{"_id":"source/_posts/install-ubuntu-in-dell.md","hash":"514d8fccdd74b7ad2099ea37021b7afe9f4e6409","modified":1515738140472},{"_id":"source/_posts/jupyternotebook-remote-useage.md","hash":"7b918275be8efb37a05afa7491e7f8ab44bebf2d","modified":1515738140473},{"_id":"source/_posts/mathfunctions-in-caffe.md","hash":"d81f68345b9d1467338a9a902eef6af2b62dbcde","modified":1515738140474},{"_id":"source/_posts/pytorch-tutor-01.md","hash":"cfe0effb4933e2daba0ae90882f8ca785fecc0a5","modified":1515738140479},{"_id":"source/_posts/python-reg-exp.md","hash":"6e9499e3e519b41e5fff1627d04e9f3ee2003432","modified":1515738140476},{"_id":"source/_posts/pytorch-mnist-example.md","hash":"c1268451de8a6d6f9b7d4dbd27547c408db97f06","modified":1515738140478},{"_id":"source/_posts/python-iter-generator.md","hash":"1555669f41aec84762e9d345fed081640ffbf532","modified":1515738140474},{"_id":"source/_posts/residualnet-paper.md","hash":"83b8830740628e82ad4c3c6ded7e564d5e4e0d02","modified":1515738140479},{"_id":"source/_posts/residualnet-paper2-identitymapping.md","hash":"1a7f16cc067e358c15af4a62ba3c80978fc71e89","modified":1515738140480},{"_id":"source/_posts/silver-rl-dp.md","hash":"e71580ef3c92cf2dd905923857c105ad37cc2b96","modified":1515738140481},{"_id":"source/_posts/shell-programming.md","hash":"38e8fecf7c36701bb8dea89552ab8d0b2abc83af","modified":1515738140480},{"_id":"source/_posts/silver-rl-mdp.md","hash":"ae6f085514a39cbe86add05f700e74645a701d57","modified":1515738140482},{"_id":"source/_posts/ubuntu-cannot-mount-exfat-disk.md","hash":"0d7e85f790b76454d056d29fb3c31eb9dcb52e93","modified":1515738140482},{"_id":"source/_posts/use-doxygen.md","hash":"065620944cfe7c382ddd26f7e48cdc8b868f7336","modified":1515738140482},{"_id":"source/_posts/useful-tools-list.md","hash":"1eaf43fb0585d98fcf60ba702657d37f63f968ab","modified":1515738140483},{"_id":"source/_posts/video-linear-alg-essential-property.md","hash":"816cafa7a5b551bfa2ac3ee2e402cc77dd64d821","modified":1515738140483},{"_id":"source/_posts/warpctc-caffe.md","hash":"e0b82a11af55cafd21e5ee59f33be9361db2a3e3","modified":1515738140484},{"_id":"source/_posts/yolo-cfg-parser.md","hash":"4f2a9ada97562a75545b599bfa016a393203960d","modified":1515738140484},{"_id":"source/_posts/yolo-paper.md","hash":"085514ac80a19eb9b403396316dd67d8e7478e4c","modified":1515738140485},{"_id":"source/tags/index.md","hash":"373c64088bf05b888f592ce18dd880fdbc87140b","modified":1515738141224},{"_id":"source/img/caffe_mathfunctions_gpuisnuclearweapon.jpg","hash":"0de63da28688d902aa01233db9255068a81f7031","modified":1515738140506},{"_id":"source/img/camera_skew.png","hash":"c02198d706e8366ffd307fccc931f0400317642c","modified":1515738140581},{"_id":"source/img/captcha_test_accuracy.png","hash":"74880dfb9c94598cad889c939017bbc9e8576250","modified":1515738140589},{"_id":"source/img/captcha_train_loss.png","hash":"7bb018f8aec8b99b91679e18db2916dfea5d5495","modified":1515738140590},{"_id":"source/img/case_1_m.png","hash":"6d4b596285b981d003a660d3ce03a95a74946576","modified":1515738140591},{"_id":"source/img/case_2_m.png","hash":"ced9ffe517ff6250f0f6a4e402897592233018ba","modified":1515738140592},{"_id":"source/img/case_3_m.png","hash":"b2d5bc2d469bbf0e4a4534c191feb4a30132e5cc","modified":1515738140593},{"_id":"source/img/case_4_m.png","hash":"85dee8dc61da967d28caa56a10d949f9de5b2e24","modified":1515738140594},{"_id":"source/img/case_m_5.png","hash":"dca097cea37695ed4b237671b017c2f04940de8e","modified":1515738140596},{"_id":"source/img/corner_type_1.png","hash":"4c431c0faf73514d013771932114831969ab3038","modified":1515738140625},{"_id":"source/img/cs131_opticalflow_brightnessconstancy_assumption.png","hash":"a80570ae3e40181cd337fd8a210f9455326094d4","modified":1515738140648},{"_id":"source/img/effective_cpp_07_joke.jpg","hash":"08743427f366b31f4d155e80394b78076852773f","modified":1515738140701},{"_id":"source/img/effectivecpp_04_cwithclass.jpg","hash":"0f8efb48280903e335e75869cb1534664f87a608","modified":1515738140702},{"_id":"source/img/effectivecpp_02_pointers.png","hash":"11c3156dfc9b34242a0a53785da8e1c544bd8702","modified":1515738140702},{"_id":"source/img/effectivecpp_01_cpp_rely_on_renpin.jpg","hash":"a9608a9d7d702c0f94912d77a5e340bdbf791093","modified":1515738140702},{"_id":"source/img/effectivecpp_diamond.png","hash":"93e89a2b9476a7cf1aa87e91d54866564eadd5cc","modified":1515738140704},{"_id":"source/img/effectivecpp_08_memory_leak_everywherre.jpg","hash":"ad604af84e93047fd6527c0899b4c47a67910df5","modified":1515738140704},{"_id":"source/img/effectivecpp_strategy_pattern.png","hash":"4b3cbc58261b48a08a53e2a402e0c33697726d1d","modified":1515738140705},{"_id":"source/img/gsl_picture.jpg","hash":"1f4b216b84bae09a72bac183d04503ecf5215a0c","modified":1515738140769},{"_id":"source/img/helloworld_hexo.png","hash":"f4227d5040c36734b9fb0d5f57eb1f6669199b87","modified":1515738140771},{"_id":"source/img/hinton_01_neuron_commucation.png","hash":"92bd15934436c01e73af2a84c82cc9f29eec6cec","modified":1515738140775},{"_id":"source/img/hinton_01_neuron_structure.png","hash":"270884f27a9a78e6285ca96902d8e96cc7ae05e7","modified":1515738140776},{"_id":"source/img/hinton_02_perceptron_gragh.png","hash":"542614250f7f25345633c3cacf780370c9edc70b","modified":1515738140792},{"_id":"source/img/hinton_02_margin.png","hash":"38eb68b50c0dbcebcb217edafee7aff6c5742d1d","modified":1515738140788},{"_id":"source/img/hinton_06_maanmian.jpg","hash":"85a0994193b98d60aafc366a8558064b7c323983","modified":1515738140864},{"_id":"source/img/install_ubuntu_in_dell_weixiaodaizhepibei.jpg","hash":"f8b3e8d9d6254897e7ef25bd237f4cfeac0a19d9","modified":1515738140914},{"_id":"source/img/install_ubuntu_in_dell_kaiyuandafahao.jpg","hash":"6a46ba1d17ae795fae0d82939b4138cec87db7a2","modified":1515738140913},{"_id":"source/img/hinton_brainsimulator.jpg","hash":"f823531a7ff9b1d12be0e1e7dfa337c83caaf805","modified":1515738140893},{"_id":"source/img/install_ubuntu_in_dell_weixiaojiuhao.jpg","hash":"c730683b4d25f4c69b15dc08875a397063fc8722","modified":1515738140914},{"_id":"source/img/jupyternotebook_logo.png","hash":"d6e1c6cf1171c28573949ef19f9cadfc19092a07","modified":1515738140918},{"_id":"source/img/kmeans_data_demo.png","hash":"f7cf89ccebbfd8d9b3ea5ab2b82f0fe52722755a","modified":1515738140937},{"_id":"source/img/kmeans_success.png","hash":"0fa428daf7f61a07b3d0c1946de4d6aa98fe0647","modified":1515738140970},{"_id":"source/img/line_fit_demo.png","hash":"0916a1148ce54c4b0f9bfc2dde33bd35782bf724","modified":1515738140970},{"_id":"source/img/meanshift_basics.jpg","hash":"6461e3234fbae264333a7c1a9cf74b1faf1fe7b5","modified":1515738140982},{"_id":"source/img/mnist_example.png","hash":"998011e1ab53d491adec736cebc319511764561e","modified":1515738140987},{"_id":"source/img/meanshift_simple_demo.png","hash":"201a77852c5d0c90c1fa0369d07599cb5237e2e5","modified":1515738140987},{"_id":"source/img/qicizuobiao.png","hash":"2bd2071a724832d96284a6d5a685991ab087f801","modified":1515738141021},{"_id":"source/img/pytorch_logo.png","hash":"6fd1856de312e104cc62ad804b6006d205b74c74","modified":1515738141019},{"_id":"source/img/ransac_step.png","hash":"eb9e2d50c4920cfd4516c9e7df2c981d8544b118","modified":1515738141031},{"_id":"source/img/residualnet_unit.png","hash":"d86018307f8101c6d010174d81490ebf1277b09a","modified":1515738141049},{"_id":"source/img/rotation.png","hash":"11557a269f79012091059c15602438a69a211721","modified":1515738141055},{"_id":"source/img/shell-programming-bash-logo.png","hash":"8501d934280e5144af436b03b0b815474dfd79b0","modified":1515738141058},{"_id":"source/img/sift_dominant_orientation.png","hash":"9a5a1538b25d9b7edcc18e23ad8a41fdaea341c8","modified":1515738141096},{"_id":"source/img/sift_picture.jpg","hash":"7c6be74bdd2c1ee630ba913e39012e8d515dc920","modified":1515738141116},{"_id":"source/img/silver_rl_bellman_equation_figure.png","hash":"9e2cade1db517dc26ba4d845dbca0633006ebee2","modified":1515738141119},{"_id":"source/img/silver_rl_bellman_equation_matrix.png","hash":"26f1c7be02d145832d2d5537b588a5bd12aad13a","modified":1515738141120},{"_id":"source/img/silver_rl_bellman_equation_solution.png","hash":"63ca3871282327e8815e7873780721075e34ea2a","modified":1515738141122},{"_id":"source/img/silver_rl_dp_policy_evaluating_demo.png","hash":"87a5e83dd518ce1b2ced9fd33534da4905689878","modified":1515738141132},{"_id":"source/img/silver_rl_dp_policy_evaluating_demo_result.png","hash":"13e05cadf85bdb48873c76a308b6f96d3e9b4bed","modified":1515738141134},{"_id":"source/img/silver_rl_mdp.png","hash":"b021de45efc46674613a7848ef36104e5f7e0021","modified":1515738141144},{"_id":"source/img/silver_rl_mdp_optimal_policy.png","hash":"a0c1ff0323b1af537c2be1ebc9d312e6e8a65933","modified":1515738141145},{"_id":"source/img/silver_rl_mdp_optimal_vq_relationship.png","hash":"c63650bcd9ff2f206f5af900b68f503ee94eb0d7","modified":1515738141148},{"_id":"source/img/silver_rl_mdp_optimal_vq_relationship2.png","hash":"20b91ee1732002cc624b1c8c0a213616e45068a9","modified":1515738141150},{"_id":"source/img/silver_rl_mdp_vq_relationship.png","hash":"4e125735fa32b5d1131fbbaa3d903200376fef67","modified":1515738141152},{"_id":"source/img/silver_rl_mdp_vq_relationship2.png","hash":"33d92cd984cda6d2299ac0b9d29067ee6c6ff9bb","modified":1515738141153},{"_id":"source/img/svd_ranking.png","hash":"6cae2c6139567934a59a8e93d9672633d1016e1a","modified":1515738141160},{"_id":"source/img/svd_picture.jpg","hash":"10cae458d21313eb9409e87c7a680c79d188f395","modified":1515738141159},{"_id":"source/img/yolo2_bbox_param.png","hash":"5be107b1b67e823d3a2dd05f385dc98d3588dabf","modified":1515738141206},{"_id":"source/img/yolo2_bbox_location.png","hash":"0c2f9c8b0bf4993abf1596869028fc69c2774dfd","modified":1515738141205},{"_id":"source/img/yolo2_cluster_result.png","hash":"1378adcf8ae8e6db2e7f1f81a9e6d6c8ec849fbb","modified":1515738141208},{"_id":"themes/next/languages/de.yml","hash":"1fdea1f84b7f691f5b4dd4d2b43eeb27b10fa0c8","modified":1515738141286},{"_id":"themes/next/.github/CONTRIBUTING.md","hash":"5ab257af816986cd0e53f9527a92d5934ac70ae9","modified":1515738141281},{"_id":"themes/next/.github/ISSUE_TEMPLATE.md","hash":"c2024ded82143807c28a299c5fe6b927ef3525ff","modified":1515738141281},{"_id":"themes/next/languages/default.yml","hash":"767470a80dc257e23e14c3a78e8c52a46c9d6209","modified":1515738141287},{"_id":"themes/next/languages/fr-FR.yml","hash":"9fca01ef917d33ae2ae6bc04561ec6799dff5351","modified":1515738141288},{"_id":"themes/next/languages/en.yml","hash":"40057d6608e825d06e0864bac4dcd27ed88ada87","modified":1515738141287},{"_id":"themes/next/languages/id.yml","hash":"34396bef27c4ab9e9a3c5d3e3aa94b0e3b3a7b0d","modified":1515738141288},{"_id":"themes/next/languages/ko.yml","hash":"b6bc5d6b0c000deb44099b42d3aebb8c49dbfca9","modified":1515738141289},{"_id":"themes/next/languages/pt.yml","hash":"6b660b117314cad93f08757601df3adb04c68beb","modified":1515738141290},{"_id":"themes/next/languages/ja.yml","hash":"49f12149edcc1892b26a6207328cda64da20116d","modified":1515738141289},{"_id":"themes/next/languages/pt-BR.yml","hash":"7742ba4c0d682cbe1d38305332ebc928abd754b5","modified":1515738141289},{"_id":"themes/next/languages/ru.yml","hash":"257d11e626cbe4b9b78785a764190b9278f95c28","modified":1515738141290},{"_id":"themes/next/languages/zh-Hans.yml","hash":"f6c9fafa0f5f0050cd07ca2cf5e38fbae3e28145","modified":1515738141290},{"_id":"themes/next/languages/zh-hk.yml","hash":"34c84c6d04447a25bd5eac576922a13947c000e2","modified":1515738141291},{"_id":"themes/next/languages/zh-tw.yml","hash":"c97a5c41149de9b17f33439b0ecf0eff6fdae50e","modified":1515738141293},{"_id":"themes/next/layout/_layout.swig","hash":"7a1e4443c3ba1e08c20e64ddbf0b8255d034dab0","modified":1515738141296},{"_id":"themes/next/layout/category.swig","hash":"6422d196ceaff4220d54b8af770e7e957f3364ad","modified":1515738141314},{"_id":"themes/next/layout/archive.swig","hash":"b5b59d70fc1563f482fa07afd435752774ad5981","modified":1515738141314},{"_id":"themes/next/layout/page.swig","hash":"3727fab9dadb967e9c2204edca787dc72264674a","modified":1515738141314},{"_id":"themes/next/layout/index.swig","hash":"427d0b95b854e311ae363088ab39a393bf8fdc8b","modified":1515738141314},{"_id":"themes/next/scripts/merge-configs.js","hash":"0c56be2e85c694247cfa327ea6d627b99ca265e8","modified":1515738141316},{"_id":"themes/next/layout/post.swig","hash":"e2e512142961ddfe77eba29eaa88f4a2ee43ae18","modified":1515738141315},{"_id":"themes/next/layout/schedule.swig","hash":"1f1cdc268f4ef773fd3ae693bbdf7d0b2f45c3a3","modified":1515738141315},{"_id":"themes/next/layout/tag.swig","hash":"07cf49c49c39a14dfbe9ce8e7d7eea3d4d0a4911","modified":1515738141315},{"_id":"themes/next/test/intern.js","hash":"11fa8a4f5c3b4119a179ae0a2584c8187f907a73","modified":1515738141442},{"_id":"themes/next/test/.jshintrc","hash":"19f93d13d1689fe033c82eb2d5f3ce30b6543cc0","modified":1515738141441},{"_id":"themes/next/test/helpers.js","hash":"a1f5de25154c3724ffc24a91ddc576cdbd60864f","modified":1515738141442},{"_id":"source/img/caffe_bn_bp_of_bn.jpg","hash":"e16455064f7de834d18e74b45b18dc86da509c22","modified":1515738140498},{"_id":"source/img/caffe_mathfunctions_useuva.png","hash":"0eabfe2a025f3466bd353d99402bef456c6601f7","modified":1515738140508},{"_id":"source/img/caffe_syncedmem_blob_flow.jpg","hash":"7c240c8e7d2a9cc67bcabb0f24aafa1394a90ec1","modified":1515738140543},{"_id":"source/img/caffe_syncedmem_transfer.png","hash":"77a0421f7f3b5e866d8bbd391dbb6920e8892ade","modified":1515738140545},{"_id":"source/img/canny_linking.png","hash":"8545dcacf874990d7d1b1ae074abbf4eb694f382","modified":1515738140587},{"_id":"source/img/canny_nms.png","hash":"363582d281cb85ba31ebf20dff0b1e7e7c2588cb","modified":1515738140589},{"_id":"source/img/case_6_m.png","hash":"bbd8245d6e3478b0287629f778aa8489489f3a37","modified":1515738140595},{"_id":"source/img/corner_window_fun.png","hash":"81b549c79bbe2bec390aab961ea503d9e80980a5","modified":1515738140626},{"_id":"source/img/cs131_opticalflow_demo.jpg","hash":"1fd9ea1700ad63bdff80c64c15e4782267ce8a1d","modified":1515738140649},{"_id":"source/img/cs131_opticalflow_lkleastsquare.png","hash":"2c10891de2393d326d2c14698feb9dbf32f46802","modified":1515738140654},{"_id":"source/img/cs131_opticalflow_lkrelationshipwithharris.png","hash":"9f369b094c44815373463074924559381f479055","modified":1515738140656},{"_id":"source/img/doc2dash_how_to_add_docset.jpg","hash":"67d4b405e4c9de0b21df1ee874d20bcbece0d3ca","modified":1515738140668},{"_id":"source/img/doxygen_picture.png","hash":"f4236c94b6b661d6d54343d58e5fefbdc2d98160","modified":1515738140697},{"_id":"source/img/edge_camera_man.png","hash":"99c2057614bdc949df7310fded651a0ea80e4e5e","modified":1515738140698},{"_id":"source/img/edge_deriative.png","hash":"a26ae2c627cdf6b95419c89d52689ad06fdb7d0d","modified":1515738140699},{"_id":"source/img/effectivecpp_06_joke.jpg","hash":"7bbd12277769af9f7f43bc2792df18a6b65e114c","modified":1515738140703},{"_id":"source/img/god_use_vpn.png","hash":"9eac5df14cdc99596cd1872c4f958e60a943d90e","modified":1515738140766},{"_id":"source/img/harris_non_scale_constant.png","hash":"e36afe38ba3f1e428c435b00c12d030c43edb108","modified":1515738140770},{"_id":"source/img/hinton_02_feed_forward_nn.png","hash":"1a76251378e9133204889641bb2d132a061069eb","modified":1515738140782},{"_id":"source/img/hinton_02_perceptron_xor.png","hash":"6f771f53c343e41c919763be0d28445eea9bca94","modified":1515738140809},{"_id":"source/img/hinton_02_recurrent_nn.png","hash":"f8f5606d55a8d381fad8ee4db97ab1e41db63bcf","modified":1515738140812},{"_id":"source/img/hinton_02_why_training_works_1.png","hash":"b60f818d861af5c6b91617d78d85cdc381894dfe","modified":1515738140849},{"_id":"source/img/meanshift_gradient_of_density.png","hash":"1a33449962e301dc741e56673579e055f3d41aab","modified":1515738140983},{"_id":"source/img/meanshift_kernel_function.png","hash":"8cababe376a596d11fce090968d0b3046c9fbaa6","modified":1515738140986},{"_id":"source/img/patch_average_intensity_scale_constant.png","hash":"bd0609e8de60de54e12c47ef7416c972f3582a0b","modified":1515738140992},{"_id":"source/img/qicizuobiao_transform.png","hash":"06b6ea9933a6d47c79b700f5c2d3c83e9cd4602f","modified":1515738141023},{"_id":"source/img/residualnet_bottleneck_unit.png","hash":"e0bc9a473bc68919afc7cd7b8fcd19262f223ae4","modified":1515738141038},{"_id":"source/img/residualnet_improved_structure.png","hash":"5a3aba104935d6be97dc2b5567c83e3a71cace88","modified":1515738141048},{"_id":"source/img/rotation_matrix.png","hash":"35aed3946385abfcd448fe99c796db440ea0ddb6","modified":1515738141057},{"_id":"source/img/sift_detection_maximum.png","hash":"eaa0c3e9bda32792b35008184e794bd3e8966cf4","modified":1515738141090},{"_id":"source/img/silver_mdp_value_function.png","hash":"9a297ebb51eced462f17529af9e797370b2b837d","modified":1515738141118},{"_id":"source/img/silver_rl_dp_detailed_prioritized_dp.png","hash":"5290a941f15980888f15c2fddaa049f6b0685f15","modified":1515738141123},{"_id":"source/img/silver_rl_dp_inplace_value_iteration.png","hash":"7c49c42afa727f6d5903848aeede42ee32c84704","modified":1515738141131},{"_id":"source/img/silver_rl_dp_improve_policy_greedily_proof_2.png","hash":"1b94a0f3bebb5b840430afab177e6cce03f766e9","modified":1515738141129},{"_id":"source/img/silver_rl_dp_realtime_dp.png","hash":"b50c4ebd39926014f294ff4e899eb88bbf2024fc","modified":1515738141136},{"_id":"source/img/silver_rl_dp_value_iteration_demo.png","hash":"24e1a868e5b085367fa9c06cc3f152ebf8d3ef95","modified":1515738141143},{"_id":"source/img/silver_rl_dp_synchronous_value_iteration.png","hash":"7a62cc20abef6add247d0ad8fe5061fb393f212c","modified":1515738141140},{"_id":"source/img/silver_rl_mdp_optimal_qq_relationship.png","hash":"2ee8fad62930a985f6edff667fc443915d5ddff3","modified":1515738141148},{"_id":"source/img/silver_rl_mdp_optimal_vv_relationship.png","hash":"8fb74930631641a34ddc58e534f9b370666c4604","modified":1515738141151},{"_id":"source/img/silver_rl_mdp_qq_relationship.png","hash":"6924625a90af23d3ac82a68c6cc667bc58513164","modified":1515738141152},{"_id":"source/img/silver_rl_mdp_vv_relationship.png","hash":"0f8f68c65bcad5d07cf4a5a4df12cc45a92dd771","modified":1515738141154},{"_id":"source/img/svd_superman.png","hash":"a4c3b97c4acfac14b978a6a00d163e2436227eda","modified":1515738141162},{"_id":"source/img/video_linear_alg_essential_linear_equation.png","hash":"678acc30007fe3cb83323a41b7152f8f2cf06e19","modified":1515738141176},{"_id":"source/img/yolo1_loss_fun.png","hash":"b3019b1551e05b6fc109dfa788408582d0855aa1","modified":1515738141203},{"_id":"source/img/yolo1_network_arch.png","hash":"eff2b8461119c0a1f32268ebe8a8bfb3e3a15b99","modified":1515738141204},{"_id":"source/img/yolo2_different_methods_comparation.png","hash":"abc4705828afe401a773a2ce29e4d2fc8474237a","modified":1515738141213},{"_id":"source/img/yolo2_different_methods_improvement.png","hash":"05ee7120be804c38a2867508e39aaaaeff7601ed","modified":1515738141218},{"_id":"themes/next/source/fonts/.gitkeep","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1515738141356},{"_id":"source/_posts/.vscode/tags","hash":"9a081dd0c4bcf261823f304d3359c4eb49fa5a8f","modified":1515738140456},{"_id":"source/img/camera_translation_rotation.png","hash":"bd89b487aff4a1c8aa5faff2dd8280557b480478","modified":1515738140585},{"_id":"source/img/corner_judge.png","hash":"4020cd84f4ea052bcc10ce09f25bd1bad8f5c293","modified":1515738140622},{"_id":"source/img/corner_judge_2.png","hash":"d6daba0908de0a3686bc925c31f7c34adb4068f2","modified":1515738140624},{"_id":"source/img/cs131_opticalflow_lkequation.png","hash":"ef0e3ea55870aa40e8abafcac5474371200cc6ed","modified":1515738140651},{"_id":"source/img/cs131_opticalflow_lkharris.png","hash":"d5f8d63186a593dbd437f888123c594d6668f528","modified":1515738140653},{"_id":"source/img/epipolar_constraint_1.png","hash":"25afbc27d8c7d984106b8fd65fff9aa4a97f624b","modified":1515738140707},{"_id":"source/img/epipolar_fig.png","hash":"ae34928aff5f8125b99f237c7e0dd1ee0b006839","modified":1515738140709},{"_id":"source/img/flowwarped.png","hash":"0794e55fa4ffeb0dc577744ba79efd8412870c5f","modified":1515738140711},{"_id":"source/img/fun_noise.png","hash":"b1e7adbf4ffd2801b0685be5213579bf0f7d16e3","modified":1515738140755},{"_id":"source/img/forwardwarped.png","hash":"13d49285cd7901316fd360c97d654a92a8ca1667","modified":1515738140746},{"_id":"source/img/generic_projection_matrix.png","hash":"4bf8906146b76f3fc74891042de14982eece2346","modified":1515738140765},{"_id":"source/img/hinton_01_mnist_example.png","hash":"6b2b2ea6952bb910fc5f1b32a86eece597835201","modified":1515738140774},{"_id":"source/img/hinton_01_sigmoid_function.png","hash":"57d2d037299607eb36d1cbcbbb48d1571814a448","modified":1515738140779},{"_id":"source/img/hinton_02_perceptron_paradigm_for_pattern_recong.png","hash":"8bc1af8fa449ed6ff9ec79a58916fe9e31156db9","modified":1515738140805},{"_id":"source/img/hinton_02_rnn_app.png","hash":"bc1a0a9bc95f2b1148ce46fb1a65eba8c526ab86","modified":1515738140820},{"_id":"source/img/hinton_06_adamax.png","hash":"0400a21629be95ec17fa7f0f887484d2a577adfc","modified":1515738140858},{"_id":"source/img/ipsec_ios_vpn_setting.png","hash":"a8cbce62cec6b3fe876ab1c8dd0efb06f69225ee","modified":1515738140918},{"_id":"source/img/kmeans_object_fun_vs_k.png","hash":"f543e4d170edba374ceea98bb8d3904c4a557a16","modified":1515738140955},{"_id":"source/img/kmeans_sensitive_to_outlier.png","hash":"a933295f1867a5684dd984d327f588de1a0f9fe8","modified":1515738140969},{"_id":"source/img/original_superman.png","hash":"310f3412935a64309e53d8e1d8403dd1c52206c8","modified":1515738140990},{"_id":"source/img/ransac_k.png","hash":"abe2b6c3ff381e1c113c96b4839e579a59da8da0","modified":1515738141026},{"_id":"source/img/residualnet_comparison_with_plainnet.png","hash":"1e838d861721ee62022309681e671d9b2cbe127e","modified":1515738141041},{"_id":"source/img/residualnet_deepnet_problem.png","hash":"dfa7ac480ee5ddd4d705dbd5b5ca75ee83b760f3","modified":1515738141046},{"_id":"source/img/silver_rl_dp_improve_policy_greedily_proof.png","hash":"d665413bc7cd0e317e29f8ea91a3ba0a0fde2446","modified":1515738141127},{"_id":"source/img/what_is_corner.png","hash":"18bd784f1845b9252f72cb54e0128157db10a940","modified":1515738141185},{"_id":"source/img/yolo2_different_methods_comparation_in_table.png","hash":"7d0294948cc1680535e3dd04c179583005e07efa","modified":1515738141217},{"_id":"themes/next/layout/_custom/header.swig","hash":"adc83b19e793491b1c6ea0fd8b46cd9f32e592fc","modified":1515738141295},{"_id":"themes/next/layout/_custom/sidebar.swig","hash":"adc83b19e793491b1c6ea0fd8b46cd9f32e592fc","modified":1515738141295},{"_id":"themes/next/layout/_macro/post-collapse.swig","hash":"5864f5567ba5efeabcf6ea355013c0b603ee07f2","modified":1515738141296},{"_id":"themes/next/layout/_macro/reward.swig","hash":"37e5b7c42ec17b9b6b786c5512bcc481a21c974e","modified":1515738141297},{"_id":"themes/next/layout/_macro/post.swig","hash":"e6016def9b512188f4c2725399c9adc7bc41cdae","modified":1515738141297},{"_id":"themes/next/layout/_macro/sidebar.swig","hash":"43d8830bb19da4fc7a5773866be19fa066b62645","modified":1515738141297},{"_id":"themes/next/layout/_partials/comments.swig","hash":"57377fdc1c435962e24bb6823ddd4c8f8fe73110","modified":1515738141298},{"_id":"themes/next/layout/_macro/wechat-subscriber.swig","hash":"14e785adeb0e671ba0ff9a553e6f0d8def6c670c","modified":1515738141298},{"_id":"themes/next/layout/_partials/footer.swig","hash":"7172c6053118b7c291a56a7860128a652ae66b83","modified":1515738141298},{"_id":"themes/next/layout/_partials/duoshuo-hot-articles.swig","hash":"5d4638c46aef65bf32a01681495b62416ccc98db","modified":1515738141298},{"_id":"themes/next/layout/_partials/head.swig","hash":"ca56f92e2fa82b03853869f5073ee1a5626a4796","modified":1515738141299},{"_id":"themes/next/layout/_partials/pagination.swig","hash":"9e8e21d194ef44d271b1cca0bc1448c14d7edf4f","modified":1515738141300},{"_id":"themes/next/layout/_partials/header.swig","hash":"adab5c3f7b173f1b45454787f39dde07aea03483","modified":1515738141299},{"_id":"themes/next/layout/_partials/page-header.swig","hash":"39d613e5a9f8389d4ea52d6082502af8e833b9f2","modified":1515738141299},{"_id":"themes/next/layout/_partials/search.swig","hash":"1431719d1dbba3f5ee385eebc46376d1a960b2d5","modified":1515738141300},{"_id":"themes/next/layout/_scripts/boostrap.swig","hash":"03aaebe9d50f6acb007ec38cc04acd1cfceb404d","modified":1515738141302},{"_id":"themes/next/layout/_scripts/baidu-push.swig","hash":"c057b17f79e8261680fbae8dc4e81317a127c799","modified":1515738141302},{"_id":"themes/next/layout/_scripts/commons.swig","hash":"766b2bdda29523ed6cd8d7aa197f996022f8fd94","modified":1515738141302},{"_id":"themes/next/layout/_scripts/vendors.swig","hash":"4512867d80d9eddfc3a0f5fea3c456f33aa9d522","modified":1515738141313},{"_id":"themes/next/scripts/tags/button.js","hash":"62e6dbeb53d07627a048132c79630b45d9a8f2cc","modified":1515738141316},{"_id":"themes/next/scripts/tags/center-quote.js","hash":"535fc542781021c4326dec24d8495cbb1387634a","modified":1515738141317},{"_id":"themes/next/scripts/tags/full-image.js","hash":"8eeb3fb89540299bdbb799edfdfdac3743b50596","modified":1515738141317},{"_id":"themes/next/scripts/tags/group-pictures.js","hash":"49252824cd53184dc9b97b2f2d87ff28e1b3ef27","modified":1515738141317},{"_id":"themes/next/scripts/tags/note.js","hash":"6752925eedbdb939d8ec4d11bdfb75199f18dd70","modified":1515738141318},{"_id":"themes/next/source/css/main.styl","hash":"20702c48d6053c92c5bcdbc68e8d0ef1369848a0","modified":1515738141355},{"_id":"themes/next/source/images/algolia_logo.svg","hash":"90035272fa31a3f65b3c0e2cb8a633876ef457dc","modified":1515738141356},{"_id":"themes/next/source/images/avatar.gif","hash":"264082bb3a1af70d5499c7d22b0902cb454b6d12","modified":1515738141357},{"_id":"themes/next/source/images/cc-by-nc-nd.svg","hash":"c6524ece3f8039a5f612feaf865d21ec8a794564","modified":1515738141357},{"_id":"themes/next/source/images/cc-by-nc-sa.svg","hash":"3031be41e8753c70508aa88e84ed8f4f653f157e","modified":1515738141358},{"_id":"themes/next/source/images/cc-by-nc.svg","hash":"8d39b39d88f8501c0d27f8df9aae47136ebc59b7","modified":1515738141359},{"_id":"themes/next/source/images/cc-by-nd.svg","hash":"c563508ce9ced1e66948024ba1153400ac0e0621","modified":1515738141361},{"_id":"themes/next/source/images/cc-by-sa.svg","hash":"aa4742d733c8af8d38d4c183b8adbdcab045872e","modified":1515738141362},{"_id":"themes/next/source/images/cc-by.svg","hash":"28a0a4fe355a974a5e42f68031652b76798d4f7e","modified":1515738141363},{"_id":"themes/next/source/images/loading.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1515738141365},{"_id":"themes/next/source/images/cc-zero.svg","hash":"87669bf8ac268a91d027a0a4802c92a1473e9030","modified":1515738141364},{"_id":"themes/next/source/images/placeholder.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1515738141365},{"_id":"themes/next/source/images/searchicon.png","hash":"67727a6a969be0b2659b908518fa6706eed307b8","modified":1515738141367},{"_id":"themes/next/source/images/quote-l.svg","hash":"94e870b4c8c48da61d09522196d4dd40e277a98f","modified":1515738141366},{"_id":"themes/next/source/images/quote-r.svg","hash":"e60ae504f9d99b712c793c3740c6b100d057d4ec","modified":1515738141366},{"_id":"source/img/caffe_bn_what_is_bn.jpg","hash":"1b142a77b6478e2c4588189f6b5a49046181c09e","modified":1515738140503},{"_id":"source/img/caffe_image.jpg","hash":"a183ae9970b3fddd258210175a9f2f3ac62306be","modified":1515738140506},{"_id":"source/img/just-a-joke.png","hash":"704a94cdda9d91c3a247b6fa0bb76e850c247f43","modified":1515738140922},{"_id":"source/img/kmeans_bigger_demo.png","hash":"926e9588bdb1c1bba243b0702ccfa959a4f46715","modified":1515738140936},{"_id":"source/img/pinhole_camera_model.png","hash":"b72eab7554feea7be265c28d0b11efc3156dae40","modified":1515738140994},{"_id":"source/img/resnet-164layer-cifar10-training.jpg","hash":"cc95edb475c1657ada111dacb8824edbad7f3937","modified":1515738141051},{"_id":"source/img/resnet-164layer-cifar10-testing.jpg","hash":"9e93b700ba1e13d33b855dacdaa00f18dc8fdc18","modified":1515738141050},{"_id":"source/img/silver_rl_dp_synchronous_dp_algorithms.png","hash":"5ad9c56b5c6e5ed4a802ee9e74df6c3b5f4fc87a","modified":1515738141139},{"_id":"source/img/yolo2_dartnet_19_structure.png","hash":"4f2a9c8c3d17fe4dacb97d3964dd568493c45422","modified":1515738141211},{"_id":"themes/next/layout/_scripts/schemes/mist.swig","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1515738141303},{"_id":"themes/next/layout/_scripts/schemes/muse.swig","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1515738141303},{"_id":"themes/next/source/css/_mixins/Mist.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1515738141348},{"_id":"themes/next/source/css/_mixins/Muse.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1515738141348},{"_id":"themes/next/source/css/_mixins/custom.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1515738141348},{"_id":"themes/next/source/css/_variables/Muse.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1515738141355},{"_id":"themes/next/source/css/_variables/custom.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1515738141355},{"_id":"source/avatar/liumengli.jpg","hash":"4069caff1851227a11b74759a75bec586c568cd3","modified":1515738140489},{"_id":"source/img/cs131_opticalflow_assignment_crossfade.png","hash":"cfdb82236a04bbadffce1e86f5fc530f31ab404a","modified":1515738140646},{"_id":"source/img/cs131_opticalflow_pyramid.png","hash":"fa06334cf6cb2ae8524478a83da662d931bbbb76","modified":1515738140667},{"_id":"source/img/mathfunctions_time_distribution.png","hash":"de743cc640396c72f76ebded8086f567d12eca53","modified":1515738140980},{"_id":"source/img/ubuntu_exfat.png","hash":"7651aac6370650a2038383cc327f596bbaf9da90","modified":1515738141165},{"_id":"source/img/useful_tools_colored_man_pages.jpg","hash":"3a5796889bdd602b146661232f4861f59b37962b","modified":1515738141169},{"_id":"themes/next/layout/_components/algolia-search/assets.swig","hash":"28ff4ed6714c59124569ffcbd10f1173d53ca923","modified":1515738141294},{"_id":"themes/next/layout/_components/algolia-search/dom.swig","hash":"636f1181dd5887a70b4a08ca8f655d4e46635792","modified":1515738141294},{"_id":"themes/next/layout/_partials/head/custom-head.swig","hash":"9e1b9666efa77f4cf8d8261bcfa445a9ac608e53","modified":1515738141299},{"_id":"themes/next/layout/_partials/head/external-fonts.swig","hash":"7ce76358411184482bb0934e70037949dd0da8ca","modified":1515738141299},{"_id":"themes/next/layout/_partials/search/localsearch.swig","hash":"ff5523d5dacaa77a55a24e50e6e6530c3b98bfad","modified":1515738141300},{"_id":"themes/next/layout/_partials/search/swiftype.swig","hash":"959b7e04a96a5596056e4009b73b6489c117597e","modified":1515738141300},{"_id":"themes/next/layout/_partials/search/tinysou.swig","hash":"eefe2388ff3d424694045eda21346989b123977c","modified":1515738141301},{"_id":"themes/next/layout/_partials/share/baidushare.swig","hash":"1f1107468aaf03f7d0dcd7eb2b653e2813a675b4","modified":1515738141301},{"_id":"themes/next/layout/_partials/share/add-this.swig","hash":"23e23dc0f76ef3c631f24c65277adf7ea517b383","modified":1515738141301},{"_id":"themes/next/layout/_partials/share/duoshuo_share.swig","hash":"89c5a5240ecb223acfe1d12377df5562a943fd5d","modified":1515738141302},{"_id":"themes/next/layout/_partials/share/jiathis.swig","hash":"63315fcf210799f894208c9f512737096df84962","modified":1515738141302},{"_id":"themes/next/layout/_scripts/pages/post-details.swig","hash":"069d1357c717572256e5cdee09574ebce529cbae","modified":1515738141303},{"_id":"themes/next/layout/_scripts/schemes/pisces.swig","hash":"a44acf9b0d0f44ef3dfc767376a95c984cc127de","modified":1515738141303},{"_id":"themes/next/layout/_scripts/third-party/analytics.swig","hash":"394d9fff7951287cc90f52acc2d4cbfd1bae079d","modified":1515738141303},{"_id":"themes/next/layout/_scripts/third-party/comments.swig","hash":"ac88399b75144f35a806aad3b64797f3ac73a48e","modified":1515738141306},{"_id":"themes/next/layout/_scripts/third-party/lean-analytics.swig","hash":"92dc60821307fc9769bea9b2d60adaeb798342af","modified":1515738141311},{"_id":"themes/next/layout/_scripts/third-party/localsearch.swig","hash":"b460e27db3dcd4ab40b17d8926a5c4e624f293a9","modified":1515738141312},{"_id":"themes/next/layout/_scripts/third-party/mathjax.swig","hash":"6d25596d6a7c57700d37b607f8d9a62d89708683","modified":1515738141312},{"_id":"themes/next/layout/_scripts/third-party/schedule.swig","hash":"22369026c87fc23893c35a7f250b42f3bb1b60f1","modified":1515738141313},{"_id":"themes/next/layout/_scripts/third-party/tinysou.swig","hash":"cb3a5d36dbe1630bab84e03a52733a46df7c219b","modified":1515738141313},{"_id":"themes/next/source/css/_custom/custom.styl","hash":"328d9a9696cc2ccf59c67d3c26000d569f46344c","modified":1515738141348},{"_id":"themes/next/source/css/_mixins/Pisces.styl","hash":"715d5b40dc52f319fe4bff0325beb874774d9bd9","modified":1515738141348},{"_id":"themes/next/source/css/_mixins/base.styl","hash":"78a83c38f69a8747bb74e420e6c9eeef1ea76525","modified":1515738141348},{"_id":"themes/next/source/css/_variables/Mist.styl","hash":"c8d35a6b9e3bff6d8fdb66de853065af9d37562d","modified":1515738141355},{"_id":"themes/next/source/css/_variables/Pisces.styl","hash":"c459aa6d607d8bcb747544e74f6ad0b8374aa3b1","modified":1515738141355},{"_id":"themes/next/source/css/_variables/base.styl","hash":"fc185c6cec79593775d1c2440dbe2a71cfbe2e99","modified":1515738141355},{"_id":"themes/next/source/js/src/algolia-search.js","hash":"96b29f69b8b916b22f62c9959a117b5a968200a5","modified":1515738141368},{"_id":"themes/next/source/js/src/affix.js","hash":"978e0422b5bf1b560236d8d10ebc1adcf66392e3","modified":1515738141368},{"_id":"themes/next/source/js/src/hook-duoshuo.js","hash":"a6119070c0119f33e08b29da7d2cce2635eb40a0","modified":1515738141369},{"_id":"themes/next/source/js/src/bootstrap.js","hash":"39bf93769d9080fa01a9a875183b43198f79bc19","modified":1515738141368},{"_id":"themes/next/source/js/src/motion.js","hash":"269414e84df544a4ccb88519f6abae4943db3c67","modified":1515738141369},{"_id":"themes/next/source/js/src/post-details.js","hash":"2038f54e289b6da5def09689e69f623187147be5","modified":1515738141370},{"_id":"themes/next/source/js/src/scrollspy.js","hash":"fe4da1b9fe73518226446f5f27d2831e4426fc35","modified":1515738141371},{"_id":"themes/next/source/js/src/utils.js","hash":"384e17ff857f073060f5bf8c6e4f4b7353236331","modified":1515738141371},{"_id":"themes/next/source/lib/canvas-nest/canvas-nest.min.js","hash":"0387e75e23b1db108a755073fe52a0d03eb391a7","modified":1515738141380},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.css","hash":"90ef19edc982645b118b095615838d9c5eaba0de","modified":1515738141373},{"_id":"themes/next/source/lib/fastclick/.bower.json","hash":"93ebd5b35e632f714dcf1753e1f6db77ec74449b","modified":1515738141395},{"_id":"themes/next/source/lib/fastclick/bower.json","hash":"13379463c7463b4b96d13556b46faa4cc38d81e6","modified":1515738141401},{"_id":"themes/next/source/lib/fastclick/LICENSE","hash":"dcd5b6b43095d9e90353a28b09cb269de8d4838e","modified":1515738141398},{"_id":"themes/next/source/lib/fastclick/README.md","hash":"1decd8e1adad2cd6db0ab50cf56de6035156f4ea","modified":1515738141399},{"_id":"themes/next/source/lib/jquery/.bower.json","hash":"91745c2cc6c946c7275f952b2b0760b880cea69e","modified":1515738141429},{"_id":"themes/next/source/lib/jquery_lazyload/.bower.json","hash":"b7638afc93e9cd350d0783565ee9a7da6805ad8e","modified":1515738141431},{"_id":"themes/next/source/lib/jquery_lazyload/CONTRIBUTING.md","hash":"4891864c24c28efecd81a6a8d3f261145190f901","modified":1515738141432},{"_id":"themes/next/source/lib/jquery_lazyload/README.md","hash":"895d50fa29759af7835256522e9dd7dac597765c","modified":1515738141432},{"_id":"themes/next/source/lib/jquery_lazyload/bower.json","hash":"65bc85d12197e71c40a55c0cd7f6823995a05222","modified":1515738141432},{"_id":"themes/next/source/lib/font-awesome/.gitignore","hash":"69d152fa46b517141ec3b1114dd6134724494d83","modified":1515738141406},{"_id":"themes/next/source/lib/jquery_lazyload/jquery.lazyload.js","hash":"481fd478650e12b67c201a0ea41e92743f8b45a3","modified":1515738141433},{"_id":"themes/next/source/lib/jquery_lazyload/jquery.scrollstop.js","hash":"0e9a81785a011c98be5ea821a8ed7d411818cfd1","modified":1515738141433},{"_id":"themes/next/source/lib/font-awesome/.bower.json","hash":"c1072942459fa0880e8a33a1bd929176b62b4171","modified":1515738141405},{"_id":"themes/next/source/lib/font-awesome/HELP-US-OUT.txt","hash":"4f7bf961f1bed448f6ba99aeb9219fabf930ba96","modified":1515738141410},{"_id":"themes/next/source/lib/font-awesome/.npmignore","hash":"dcf470ab3a358103bb896a539cc03caeda10fa8b","modified":1515738141409},{"_id":"themes/next/source/lib/font-awesome/bower.json","hash":"279a8a718ab6c930a67c41237f0aac166c1b9440","modified":1515738141411},{"_id":"themes/next/source/lib/velocity/.bower.json","hash":"05f960846f1c7a93dab1d3f9a1121e86812e8c88","modified":1515738141435},{"_id":"themes/next/source/lib/velocity/bower.json","hash":"2ec99573e84c7117368beccb9e94b6bf35d2db03","modified":1515738141435},{"_id":"themes/next/source/lib/velocity/velocity.min.js","hash":"2f1afadc12e4cf59ef3b405308d21baa97e739c6","modified":1515738141439},{"_id":"themes/next/source/lib/velocity/velocity.ui.js","hash":"6a1d101eab3de87527bb54fcc8c7b36b79d8f0df","modified":1515738141440},{"_id":"themes/next/source/lib/velocity/velocity.ui.min.js","hash":"ed5e534cd680a25d8d14429af824f38a2c7d9908","modified":1515738141440},{"_id":"source/img/bash-programming-comparing-string.jpg","hash":"a55e9ef5954a7961eccdaff14dd06c923a259cd3","modified":1515738140496},{"_id":"source/img/hinton_06_nesterov_momentum.png","hash":"5833f7accf4067323716b32a794f468eddbfc654","modified":1515738140868},{"_id":"source/img/kmeans_demo.png","hash":"b10c3c2d5eaf201c0111cde24cc8d3c4533134dd","modified":1515738140942},{"_id":"source/img/svd_flower.png","hash":"e1924f7eaf2a920c769fd63ba27baf210adf59c7","modified":1515738141158},{"_id":"source/img/video_linear_alg_essential.png","hash":"e11c2c04b910a7b9f1a5ccaee3ae9dbb67fca666","modified":1515738141174},{"_id":"themes/next/source/lib/jquery/index.js","hash":"41b4bfbaa96be6d1440db6e78004ade1c134e276","modified":1515738141431},{"_id":"source/img/dog_x.png","hash":"5672ad3c046997781703816e4d58d3961fe26eeb","modified":1515738140696},{"_id":"source/img/paper-inq-result.png","hash":"704d4b7ef16f3d6f4fc7929c9b020c993f2c8fbd","modified":1516865936558},{"_id":"source/img/ransac_line_fit.png","hash":"f3711368fa15222a40f134c267842999574277e2","modified":1515738141031},{"_id":"source/img/regex_picture.jpg","hash":"1744ddfe9d73a4af202364cbea16ba0f2211dc50","modified":1515738141036},{"_id":"source/img/sift_dog.png","hash":"60371c0dbb5dfae010c84c59227dac9d878d3530","modified":1515738141095},{"_id":"themes/next/layout/_scripts/third-party/analytics/application-insights.swig","hash":"60426bf73f8a89ba61fb1be2df3ad5398e32c4ef","modified":1515738141303},{"_id":"themes/next/layout/_scripts/third-party/analytics/baidu-analytics.swig","hash":"deda6a814ed48debc694c4e0c466f06c127163d0","modified":1515738141304},{"_id":"themes/next/layout/_scripts/third-party/analytics/busuanzi-counter.swig","hash":"4fcbf57c4918528ab51d3d042cff92cf5aefb599","modified":1515738141304},{"_id":"themes/next/layout/_scripts/third-party/analytics/cnzz-analytics.swig","hash":"8160b27bee0aa372c7dc7c8476c05bae57f58d0f","modified":1515738141304},{"_id":"themes/next/layout/_scripts/third-party/analytics/facebook-sdk.swig","hash":"394d008e5e94575280407ad8a1607a028026cbc3","modified":1515738141304},{"_id":"themes/next/layout/_scripts/third-party/analytics/google-analytics.swig","hash":"30a23fa7e816496fdec0e932aa42e2d13098a9c2","modified":1515738141305},{"_id":"themes/next/layout/_scripts/third-party/analytics/tencent-analytics.swig","hash":"3658414379e0e8a34c45c40feadc3edc8dc55f88","modified":1515738141305},{"_id":"themes/next/layout/_scripts/third-party/comments/disqus.swig","hash":"fb1d04ede838b52ca7541973f86c3810f1ad396e","modified":1515738141307},{"_id":"themes/next/layout/_scripts/third-party/comments/duoshuo.swig","hash":"a356b2185d40914447fde817eb3d358ab6b3e4c3","modified":1515738141307},{"_id":"themes/next/layout/_scripts/third-party/comments/gentie.swig","hash":"03592d1d731592103a41ebb87437fe4b0a4c78ca","modified":1515738141308},{"_id":"themes/next/layout/_scripts/third-party/comments/hypercomments.swig","hash":"3e8dc5c6c912628a37e3b5f886bec7b2e5ed14ea","modified":1515738141308},{"_id":"themes/next/layout/_scripts/third-party/comments/livere.swig","hash":"3603a438495bde6fb869447f8f7613cac36d366e","modified":1515738141309},{"_id":"themes/next/layout/_scripts/third-party/comments/youyan.swig","hash":"ea8078fa9e10be2bb042749d8b6a97adc38f914c","modified":1515738141310},{"_id":"themes/next/source/css/_common/components/back-to-top.styl","hash":"b49efc66bd055a2d0be7deabfcb02ee72a9a28c8","modified":1515738141318},{"_id":"themes/next/source/css/_common/components/comments.styl","hash":"471f1627891aca5c0e1973e09fbcb01e1510d193","modified":1515738141320},{"_id":"themes/next/source/css/_common/components/buttons.styl","hash":"0dfb4b3ba3180d7285e66f270e1d3fa0f132c3d2","modified":1515738141319},{"_id":"themes/next/source/css/_common/components/components.styl","hash":"10994990d6e0b4d965a728a22cf7f6ee29cae9f6","modified":1515738141320},{"_id":"themes/next/source/css/_common/components/pagination.styl","hash":"711c8830886619d4f4a0598b0cde5499dce50c62","modified":1515738141331},{"_id":"themes/next/source/css/_common/components/tag-cloud.styl","hash":"dd8a3b22fc2f222ac6e6c05bd8a773fb039169c0","modified":1515738141338},{"_id":"themes/next/source/css/_common/scaffolding/helpers.styl","hash":"54c90cf7bdbf5c596179d8dae6e671bad1292662","modified":1515738141346},{"_id":"themes/next/source/css/_common/scaffolding/base.styl","hash":"5304f99581da3a31de3ecec959b7adf9002fde83","modified":1515738141346},{"_id":"themes/next/source/css/_common/outline/outline.styl","hash":"2186be20e317505cd31886f1291429cc21f76703","modified":1515738141345},{"_id":"themes/next/source/css/_common/scaffolding/normalize.styl","hash":"ece571f38180febaf02ace8187ead8318a300ea7","modified":1515738141346},{"_id":"themes/next/source/css/_common/scaffolding/tables.styl","hash":"64f5d56c08d74a338813df1265580ca0cbf0190b","modified":1515738141347},{"_id":"themes/next/source/css/_common/scaffolding/scaffolding.styl","hash":"013619c472c7e4b08311c464fcbe9fcf5edde603","modified":1515738141347},{"_id":"themes/next/source/css/_schemes/Mist/_base.styl","hash":"c2d079788d6fc2e9a191ccdae94e50d55bf849dc","modified":1515738141349},{"_id":"themes/next/source/css/_schemes/Mist/_header.styl","hash":"5ae7906dc7c1d9468c7f4b4a6feddddc555797a1","modified":1515738141349},{"_id":"themes/next/source/css/_schemes/Mist/_logo.styl","hash":"38e5df90c8689a71c978fd83ba74af3d4e4e5386","modified":1515738141349},{"_id":"themes/next/source/css/_schemes/Mist/_menu.styl","hash":"b0dcca862cd0cc6e732e33d975b476d744911742","modified":1515738141349},{"_id":"themes/next/source/css/_schemes/Mist/_posts-expanded.styl","hash":"fda14bc35be2e1b332809b55b3d07155a833dbf4","modified":1515738141350},{"_id":"themes/next/source/css/_schemes/Mist/_search.styl","hash":"1452cbe674cc1d008e1e9640eb4283841058fc64","modified":1515738141350},{"_id":"themes/next/source/css/_schemes/Mist/index.styl","hash":"9a5581a770af8964064fef7afd3e16963e45547f","modified":1515738141350},{"_id":"themes/next/source/css/_schemes/Muse/_layout.styl","hash":"0efa036a15c18f5abb058b7c0fad1dd9ac5eed4c","modified":1515738141351},{"_id":"themes/next/source/css/_schemes/Muse/_logo.styl","hash":"8829bc556ca38bfec4add4f15a2f028092ac6d46","modified":1515738141351},{"_id":"themes/next/source/css/_schemes/Muse/_menu.styl","hash":"82bbaa6322764779a1ac2e2c8390ce901c7972e2","modified":1515738141351},{"_id":"themes/next/source/css/_schemes/Muse/_search.styl","hash":"1452cbe674cc1d008e1e9640eb4283841058fc64","modified":1515738141351},{"_id":"themes/next/source/css/_schemes/Muse/index.styl","hash":"a0e2030a606c934fb2c5c7373aaae04a1caac4c5","modified":1515738141351},{"_id":"themes/next/source/css/_schemes/Pisces/_brand.styl","hash":"ff9f163bb05c0709577040a875924d36c9ab99d6","modified":1515738141353},{"_id":"themes/next/source/css/_schemes/Pisces/_layout.styl","hash":"dcf9fe43b2ef78b923118ba39efedb38760e76b1","modified":1515738141353},{"_id":"themes/next/source/css/_schemes/Pisces/_menu.styl","hash":"1408209dfb9a22a0982a30bdbd14842c2b53f264","modified":1515738141353},{"_id":"themes/next/source/css/_schemes/Pisces/_posts.styl","hash":"2f878213cb24c5ddc18877f6d15ec5c5f57745ac","modified":1515738141354},{"_id":"themes/next/source/css/_schemes/Pisces/_sidebar.styl","hash":"9b63bd8effc7cf4b96acdea4d73add7df934a222","modified":1515738141354},{"_id":"themes/next/source/css/_schemes/Pisces/index.styl","hash":"69ecd6c97e7cdfd822ac8102b45ad0ede85050db","modified":1515738141354},{"_id":"themes/next/source/js/src/schemes/pisces.js","hash":"9ccee9189c910b8a264802d7b2ec305d12dedcd0","modified":1515738141370},{"_id":"themes/next/source/lib/fancybox/source/blank.gif","hash":"2daeaa8b5f19f0bc209d976c02bd6acb51b00b0a","modified":1515738141381},{"_id":"themes/next/source/lib/fancybox/source/fancybox_loading.gif","hash":"1a755fb2599f3a313cc6cfdb14df043f8c14a99c","modified":1515738141381},{"_id":"themes/next/source/lib/fancybox/source/fancybox_loading@2x.gif","hash":"273b123496a42ba45c3416adb027cd99745058b0","modified":1515738141382},{"_id":"themes/next/source/lib/fancybox/source/fancybox_overlay.png","hash":"b3a4ee645ba494f52840ef8412015ba0f465dbe0","modified":1515738141382},{"_id":"themes/next/source/lib/fancybox/source/fancybox_sprite.png","hash":"17df19f97628e77be09c352bf27425faea248251","modified":1515738141383},{"_id":"themes/next/source/lib/fancybox/source/fancybox_sprite@2x.png","hash":"30c58913f327e28f466a00f4c1ac8001b560aed8","modified":1515738141383},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.css","hash":"5f163444617b6cf267342f06ac166a237bb62df9","modified":1515738141387},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.pack.js","hash":"53360764b429c212f424399384417ccc233bb3be","modified":1515738141391},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.js","hash":"1cf3d47b5ccb7cb6e9019c64f2a88d03a64853e4","modified":1515738141388},{"_id":"themes/next/source/lib/fastclick/lib/fastclick.js","hash":"06cef196733a710e77ad7e386ced6963f092dc55","modified":1515738141403},{"_id":"themes/next/source/lib/fastclick/lib/fastclick.min.js","hash":"2cae0f5a6c5d6f3cb993015e6863f9483fc4de18","modified":1515738141404},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css","hash":"4eda182cbcc046dbf449aef97c02c230cf80a494","modified":1515738141415},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css.map","hash":"0189d278706509412bac4745f96c83984e1d59f4","modified":1515738141416},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.min.css","hash":"fb5b49426dee7f1508500e698d1b3c6b04c8fcce","modified":1515738141417},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.pack.js","hash":"214dad442a92d36af77ed0ca1d9092b16687f02f","modified":1515738141434},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.min.js","hash":"38628e75e4412cc6f11074e03e1c6d257aae495b","modified":1515738141434},{"_id":"source/img/hinton_06_learningrate.png","hash":"a5bda32c1be2372d1fd48bb3ecfb70ea730865a5","modified":1515738140863},{"_id":"source/img/sift_experiment_1.png","hash":"87873fe348c7cb572b8d499730cf2ebc86a1e4ef","modified":1515738141101},{"_id":"source/img/yolo1_detection_system.png","hash":"3406908556a3d80fc565a94f6e394bfba6c69413","modified":1515738141202},{"_id":"source/img/warpctc_intro.png","hash":"2521cbdd73abfb274c8cb3cdd3bb91f86a4d3866","modified":1515738141182},{"_id":"themes/next/source/lib/font-awesome/fonts/FontAwesome.otf","hash":"1b22f17fdc38070de50e6d1ab3a32da71aa2d819","modified":1515738141419},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.eot","hash":"965ce8f688fedbeed504efd498bc9c1622d12362","modified":1515738141420},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff","hash":"6d7e6a5fc802b13694d8820fc0138037c0977d2e","modified":1515738141426},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff2","hash":"97e438cc545714309882fbceadbf344fcaddcec5","modified":1515738141427},{"_id":"themes/next/source/lib/velocity/velocity.js","hash":"9f08181baea0cc0e906703b7e5df9111b9ef3373","modified":1515738141437},{"_id":"source/img/camera_model_things_to_remember.png","hash":"687a27be2aa5a335814315d19421f29fa79fee06","modified":1515738140580},{"_id":"source/img/hinton_06_rmsprop_improvement.png","hash":"f76c1f27120142ab5168c03fc5e74b54aaf71147","modified":1515738140876},{"_id":"themes/next/source/css/_common/components/footer/footer.styl","hash":"8994ffcce84deac0471532f270f97c44fea54dc0","modified":1515738141320},{"_id":"themes/next/source/css/_common/components/header/header.styl","hash":"ae1ca14e51de67b07dba8f61ec79ee0e2e344574","modified":1515738141322},{"_id":"themes/next/source/css/_common/components/header/headerband.styl","hash":"d27448f199fc2f9980b601bc22b87f08b5d64dd1","modified":1515738141323},{"_id":"themes/next/source/css/_common/components/header/menu.styl","hash":"c890ce7fe933abad7baf39764a01894924854e92","modified":1515738141323},{"_id":"themes/next/source/css/_common/components/highlight/diff.styl","hash":"96f32ea6c3265a3889e6abe57587f6e2a2a40dfb","modified":1515738141324},{"_id":"themes/next/source/css/_common/components/header/site-nav.styl","hash":"49c2b2c14a1e7fcc810c6be4b632975d0204c281","modified":1515738141324},{"_id":"themes/next/source/css/_common/components/highlight/highlight.styl","hash":"4b7f81e1006e7acee3d1c840ccba155239f830cc","modified":1515738141325},{"_id":"themes/next/source/css/_common/components/highlight/theme.styl","hash":"b76387934fb6bb75212b23c1a194486892cc495e","modified":1515738141326},{"_id":"themes/next/source/css/_common/components/pages/archive.styl","hash":"7778920dd105fa4de3a7ab206eeba30b1a7bac45","modified":1515738141329},{"_id":"themes/next/source/css/_common/components/pages/categories.styl","hash":"4eff5b252d7b614e500fc7d52c97ce325e57d3ab","modified":1515738141329},{"_id":"themes/next/source/css/_common/components/header/site-meta.styl","hash":"6c00f6e0978f4d8f9a846a15579963728aaa6a17","modified":1515738141323},{"_id":"themes/next/source/css/_common/components/pages/pages.styl","hash":"2039590632bba3943c39319d80ef630af7928185","modified":1515738141330},{"_id":"themes/next/source/css/_common/components/pages/schedule.styl","hash":"a82afbb72d83ee394aedc7b37ac0008a9823b4f4","modified":1515738141331},{"_id":"themes/next/source/css/_common/components/pages/post-detail.styl","hash":"9bf4362a4d0ae151ada84b219d39fbe5bb8c790e","modified":1515738141330},{"_id":"themes/next/source/css/_common/components/post/post-button.styl","hash":"fdfadbb4483043c7e0afd541ee9712389e633517","modified":1515738141332},{"_id":"themes/next/source/css/_common/components/post/post-eof.styl","hash":"2cdc094ecf907a02fce25ad4a607cd5c40da0f2b","modified":1515738141332},{"_id":"themes/next/source/css/_common/components/post/post-expand.styl","hash":"b25132fe6a7ad67059a2c3afc60feabb479bdd75","modified":1515738141332},{"_id":"themes/next/source/css/_common/components/post/post-collapse.styl","hash":"8fae54591877a73dff0b29b2be2e8935e3c63575","modified":1515738141332},{"_id":"themes/next/source/css/_common/components/post/post-gallery.styl","hash":"387ce23bba52b22a586b2dfb4ec618fe1ffd3926","modified":1515738141333},{"_id":"themes/next/source/css/_common/components/post/post-nav.styl","hash":"c6dab7661a6b8c678b21b7eb273cef7100f970f6","modified":1515738141333},{"_id":"themes/next/source/css/_common/components/post/post-tags.styl","hash":"a352ae5b1f8857393bf770d2e638bf15f0c9585d","modified":1515738141334},{"_id":"themes/next/source/css/_common/components/post/post-meta.styl","hash":"7f1aab694caf603809e33cff82beea84cd0128fd","modified":1515738141333},{"_id":"themes/next/source/css/_common/components/post/post-reward.styl","hash":"e792c8dc41561c96d128e9b421187f1c3dc978a0","modified":1515738141333},{"_id":"themes/next/source/css/_common/components/post/post-title.styl","hash":"963105a531403d7aad6d9e5e23e3bfabb8ec065a","modified":1515738141334},{"_id":"themes/next/source/css/_common/components/post/post-type.styl","hash":"10251257aceecb117233c9554dcf8ecfef8e2104","modified":1515738141334},{"_id":"themes/next/source/css/_common/components/post/post.styl","hash":"bfd806d0a9f21446a22df82ac02e37d0075cc3b5","modified":1515738141335},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-author-links.styl","hash":"2e7ec9aaa3293941106b1bdd09055246aa3c3dc6","modified":1515738141335},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-author.styl","hash":"920343e41c124221a17f050bbb989494d44f7a24","modified":1515738141335},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-blogroll.styl","hash":"c44f6a553ec7ea5508f2054a13be33a62a15d3a9","modified":1515738141336},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-feed-link.styl","hash":"9486ddd2cb255227db102d09a7df4cae0fabad72","modified":1515738141336},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-nav.styl","hash":"45fa7193435a8eae9960267438750b4c9fa9587f","modified":1515738141336},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-toc.styl","hash":"7690b9596ec3a49befbe529a5a2649abec0faf76","modified":1515738141337},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-toggle.styl","hash":"2d3abbc85b979a648e0e579e45f16a6eba49d1e7","modified":1515738141337},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar.styl","hash":"234facd038f144bd0fe09a31ed1357c5d74c517f","modified":1515738141337},{"_id":"themes/next/source/css/_common/components/sidebar/site-state.styl","hash":"3623e7fa4324ec1307370f33d8f287a9e20a5578","modified":1515738141337},{"_id":"themes/next/source/css/_common/components/tags/blockquote-center.styl","hash":"c2abe4d87148e23e15d49ee225bc650de60baf46","modified":1515738141340},{"_id":"themes/next/source/css/_common/components/tags/full-image.styl","hash":"b8969e1654eec89a0fd10d88b337fee9cb03cd44","modified":1515738141340},{"_id":"themes/next/source/css/_common/components/tags/group-pictures.styl","hash":"4851b981020c5cbc354a1af9b831a2dcb3cf9d39","modified":1515738141341},{"_id":"themes/next/source/css/_common/components/tags/note.styl","hash":"74d0ba86f698165d13402670382a822c8736a556","modified":1515738141341},{"_id":"themes/next/source/css/_common/components/tags/tags.styl","hash":"3eb73cee103b810fa56901577ecb9c9bb1793cff","modified":1515738141342},{"_id":"themes/next/source/css/_common/components/third-party/algolia-search.styl","hash":"eba491ae624b4c843c8be4c94a044085dad4ba0f","modified":1515738141342},{"_id":"themes/next/source/css/_common/components/third-party/baidushare.styl","hash":"93b08815c4d17e2b96fef8530ec1f1064dede6ef","modified":1515738141343},{"_id":"themes/next/source/css/_common/components/third-party/busuanzi-counter.styl","hash":"b03f891883446f3a5548b7cc90d29c77e62f1053","modified":1515738141343},{"_id":"themes/next/source/css/_common/components/third-party/duoshuo.styl","hash":"2340dd9b3202c61d73cc708b790fac5adddbfc7f","modified":1515738141344},{"_id":"themes/next/source/css/_common/components/third-party/gentie.styl","hash":"586a3ec0f1015e7207cd6a2474362e068c341744","modified":1515738141344},{"_id":"themes/next/source/css/_common/components/third-party/jiathis.styl","hash":"327b5f63d55ec26f7663185c1a778440588d9803","modified":1515738141344},{"_id":"themes/next/source/css/_common/components/third-party/third-party.styl","hash":"42348219db93a85d2ee23cb06cebd4d8ab121726","modified":1515738141345},{"_id":"themes/next/source/css/_common/components/third-party/localsearch.styl","hash":"637c6b32c58ecf40041be6e911471cd82671919b","modified":1515738141345},{"_id":"themes/next/source/css/_schemes/Mist/outline/outline.styl","hash":"5dc4859c66305f871e56cba78f64bfe3bf1b5f01","modified":1515738141350},{"_id":"themes/next/source/css/_schemes/Mist/sidebar/sidebar-blogroll.styl","hash":"8b8e8cbce98a9296c8fd77f512ae85d945f65d40","modified":1515738141350},{"_id":"themes/next/source/css/_schemes/Muse/sidebar/sidebar-blogroll.styl","hash":"8b8e8cbce98a9296c8fd77f512ae85d945f65d40","modified":1515738141352},{"_id":"themes/next/source/lib/fancybox/source/helpers/fancybox_buttons.png","hash":"e385b139516c6813dcd64b8fc431c364ceafe5f3","modified":1515738141384},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-buttons.js","hash":"91e41741c2e93f732c82aaacec4cfc6e3f3ec876","modified":1515738141384},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-buttons.css","hash":"1a9d8e5c22b371fcc69d4dbbb823d9c39f04c0c8","modified":1515738141384},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-media.js","hash":"3bdf69ed2469e4fb57f5a95f17300eef891ff90d","modified":1515738141385},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-thumbs.css","hash":"4ac329c16a5277592fc12a37cca3d72ca4ec292f","modified":1515738141386},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-thumbs.js","hash":"53e194f4a72e649c04fb586dd57762b8c022800b","modified":1515738141387},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.ttf","hash":"61d8d967807ef12598d81582fa95b9f600c3ee01","modified":1515738141425},{"_id":"source/img/cs131_linear_algebra.jpg","hash":"dd19d6b5b0e026d3c5b1ead3951b6f5073081fa1","modified":1515738140638},{"_id":"source/img/hinton_02_weight_space_hyperplane.png","hash":"314f5c910d1af6909a5082f8ec29b166ffb39547","modified":1515738140845},{"_id":"source/img/hinton_06_summary.png","hash":"b7d44b6002db12d33f2b178b3e1582873f86a5fb","modified":1515738140884},{"_id":"source/img/hinton_06_tricks_for_adaptive_lr.png","hash":"39f3a8ca2940809101656e31f39ef9e402cb9020","modified":1515738140891},{"_id":"source/img/shell-programming-if-operators.jpg","hash":"c09d8ef7ac6057bb185a277ad20da9fc1d6cc8f3","modified":1515738141067},{"_id":"source/img/yolo2_result.png","hash":"6241714e33480315ec130b1dcd17db56f098db9b","modified":1515738141223},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.js","hash":"9ccc6f8144f54e86df9a3fd33a18368d81cf3a4f","modified":1515738141379},{"_id":"source/img/convolution.png","hash":"af8e2638d7e484d8267a49ed11d3f3239fa19107","modified":1515738140618},{"_id":"source/img/dog_different_size.png","hash":"c8d097879dcfffa7cf6138b01b22d4792d56522c","modified":1515738140688},{"_id":"source/img/shell-programming-wild-cards.jpg","hash":"cc83510fd340cb6c52322243329f1fbdbb88b206","modified":1515738141087},{"_id":"source/img/shell-programming-system-variables.jpg","hash":"1fff44fae6634a8f565e48bd2cb93edfddb3ca9d","modified":1515738141077},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.svg","hash":"c0522272bbaef2acb3d341912754d6ea2d0ecfc0","modified":1515738141422},{"_id":"source/img/doc2dash_pytorch_example.jpg","hash":"1759b9f8332ce27123acd68a7bd2782df8d116d9","modified":1515738140677},{"_id":"source/img/focal_loss_vs_ce_loss.jpg","hash":"970dfffff9326032aef74e41e625076dbbd81d36","modified":1515738140737},{"_id":"source/img/kmeans_scaling_up.png","hash":"df16837a020113c80b953765d2b21932dd275d23","modified":1515738140967},{"_id":"source/img/contours_evaluation_optimizers.gif","hash":"0d4d768dcedf08df014f790f86d4f771451b7825","modified":1515738140605},{"_id":"source/img/mathfunctions_im2col.png","hash":"70a0a79092024dddbc2d0ab462fa006166a4e8a4","modified":1515738140977},{"_id":"source/img/yolo1_basic_idea.png","hash":"831008124b5bd8e25e5b8986cf40fb75767b1d03","modified":1515738141197},{"_id":"source/img/kmeans_algorithm.png","hash":"89598b129b98c9da0eb309911b84eb2e49ff38c6","modified":1515738140932},{"_id":"source/img/sift_experiment_2.png","hash":"c1e482a1e85026664f33ce2dd7e72e3e3a58c0fc","modified":1515738141115},{"_id":"source/img/caffe_mathfunctions_uvarequirement.png","hash":"935d56ac8e8689f8aebaf5fa61d1e93a81ec0209","modified":1515738140518},{"_id":"source/img/focal_loss_different_model_comparison.jpg","hash":"14f51cb35df20ca182a069f519da9f7cb98a13d6","modified":1515738140727},{"_id":"source/img/projective_geometry_property_1.png","hash":"ee1d6866a25eb2822cd8adb7b8666ae653a8bf45","modified":1515738141000},{"_id":"source/img/kmeans_image_seg_via_intensity.png","hash":"9a3c60d288ba39bbe659cf84ed941c53ea501eeb","modified":1515738140953},{"_id":"source/img/projective_geometry_property_2.png","hash":"1d0b53aa8901c0dc46673762820683b3afd4202b","modified":1515738141018},{"_id":"source/img/caffe_mathfunctions_whatisuva.png","hash":"a26ef96adcdcf217927372d898ef336766bec67e","modified":1515738140539},{"_id":"source/img/camera_geometry_application.png","hash":"cb34e60657f460f4cf120b740180740d721256e4","modified":1515738140568},{"_id":"source/img/image_matching_hard.png","hash":"4a09523ad90bb2d41a391edb8fd69bfed2cdf8e9","modified":1515738140912},{"_id":"source/img/paper-inq-quantize-set.png","hash":"7038d10998546436dd75632ecabd146c943d0dc6","modified":1516867397638},{"_id":"source/img/paper-inq-different-quantize.png","hash":"1d704fafbcfbf6186cf9983e3c31c7b3a4db3ac9","modified":1516870232724},{"_id":"source/img/paper-inq-algorithm-demo.png","hash":"af7996102c20fdb841d3d4eb09cf34661f8f9bc3","modified":1516869926582},{"_id":"public/search.xml","hash":"64321e6928a6c44c2c29756f428b6ca62e02f4b2","modified":1516871122541},{"_id":"public/about/index.html","hash":"00ad8e2f659bb69ef55d8f44350f871f437908aa","modified":1516871126902},{"_id":"public/tags/index.html","hash":"591282a25aab9c0cb1ecf428277de891ab942ea1","modified":1516871126947},{"_id":"public/2018/01/12/caffe-syncedmem/index.html","hash":"e49076e50f3d7eab1ede388350e6c94dcdc5aa28","modified":1516871126948},{"_id":"public/2017/11/10/shell-programming/index.html","hash":"74d7bb24df02573f6870c8e6a4a905ea052d887f","modified":1516871126948},{"_id":"public/2018/01/08/caffe-batch-norm/index.html","hash":"af0c252aecb0b29af076e9fb8f01546bd599defe","modified":1516871126948},{"_id":"public/2017/10/22/useful-tools-list/index.html","hash":"31428018e511a12bbf0782c2ee4bff1d147b60e6","modified":1516871126948},{"_id":"public/2017/10/27/fuck-gfw/index.html","hash":"86bb8c8613be6e3ea557dcfd71614b660dfdfbb0","modified":1516871126948},{"_id":"public/2018/01/25/inq-paper/index.html","hash":"66ae4817115a21af8d9c4816d2c78fd7a2cdca5b","modified":1516871127064},{"_id":"public/2017/08/14/focal-loss-paper/index.html","hash":"b5fac19eb8a5865c423e64a77e282034523feff6","modified":1516871127065},{"_id":"public/2017/08/21/debugging-with-ipdb/index.html","hash":"9b654a6c450c029a02e4552a16142ff7db4ad57c","modified":1516871127065},{"_id":"public/2017/08/10/install-ubuntu-in-dell/index.html","hash":"8ff4f9459be7ad5b56cb8017e027f752b2ff0c21","modified":1516871127065},{"_id":"public/2017/06/25/hinton-nnml-06/index.html","hash":"56aa90a2b67c8ad65df6882f79bfe057c05f3717","modified":1516871127065},{"_id":"public/2017/08/26/doc2dash-usage/index.html","hash":"acecc41b327a55cab296574d34053eb7f7653137","modified":1516871127065},{"_id":"public/2017/06/17/effective-cpp-06/index.html","hash":"02553498884a6738d2abc5249e5f9eda38ca5898","modified":1516871127065},{"_id":"public/2017/06/06/silver-rl-dp/index.html","hash":"acca84356e9a1f9fd927571b27c73759c2db7640","modified":1516871127065},{"_id":"public/2017/08/29/learn-pyqt/index.html","hash":"dc87c93227a663ec0ae11524b4e42f6089a117d8","modified":1516871127065},{"_id":"public/2017/07/03/effective-cpp-08/index.html","hash":"5dba45c04a13667a04f6d0cc149d00ac82104acc","modified":1516871127065},{"_id":"public/2017/06/23/effective-cpp-07/index.html","hash":"b764b0daeac7beae6ed2bfb8b52a694bc7e702fc","modified":1516871127065},{"_id":"public/2017/05/25/hinton-nnml-02/index.html","hash":"53c515e6575bef606f98ca321808ff6adcf6923b","modified":1516871127065},{"_id":"public/2017/05/03/cs131-opticalflow/index.html","hash":"7cb26af8d4298565529a5d21407d712ee07a9636","modified":1516871127065},{"_id":"public/2017/05/04/ubuntu-cannot-mount-exfat-disk/index.html","hash":"75b913649d002cd02a57d0c89b5e00acccaded89","modified":1516871127065},{"_id":"public/2017/05/31/silver-rl-mdp/index.html","hash":"4f5a035a378b6389ed3872edc49da3ac194344ca","modified":1516871127065},{"_id":"public/2017/05/03/hinton-nnml-01/index.html","hash":"bf923d12ea11dc6e875ef22de3ede05cea8f54c7","modified":1516871127065},{"_id":"public/2017/05/01/effective-cpp-05/index.html","hash":"ec51efbebf59f4b315123bb8f70703f8d1264b72","modified":1516871127065},{"_id":"public/2017/04/29/effective-cpp-04/index.html","hash":"58d932b2f6feb6fdc07592070a1c9fef1f6b98c9","modified":1516871127065},{"_id":"public/2017/04/25/effective-cpp-03/index.html","hash":"d9fbc92e235ad6f0c90ce4a5a1929999113ea2b5","modified":1516871127066},{"_id":"public/2017/04/24/effective-cpp-02/index.html","hash":"1e7ce7ca7b72aaf03b9b8b71c761068a02f3562e","modified":1516871127066},{"_id":"public/2017/04/21/python-iter-generator/index.html","hash":"87b79e2db26c8a75269754ca2df34ca3bfd0e8d6","modified":1516871127066},{"_id":"public/2017/03/06/yolo-cfg-parser/index.html","hash":"8048b071e52a85f3e5bb21f7918450a118c570ea","modified":1516871127066},{"_id":"public/2017/04/20/effective-cpp-01/index.html","hash":"1a7a5cda04d22f1aa11aeebc333642cb9edff3d3","modified":1516871127066},{"_id":"public/2017/03/08/mathfunctions-in-caffe/index.html","hash":"232133d2fe9390227ea85748483a6e7a02d5364c","modified":1516871127066},{"_id":"public/2017/03/07/residualnet-paper2-identitymapping/index.html","hash":"3eeb0792a39722606aaddf085db5810962b7b366","modified":1516871127066},{"_id":"public/2017/03/05/residualnet-paper/index.html","hash":"0a94cf967d29ba798bdf5d163ab774727e1f3040","modified":1516871127066},{"_id":"public/2017/03/04/pytorch-mnist-example/index.html","hash":"8bd0dd73d080a3ea6ca103d71672dca69fca554b","modified":1516871127066},{"_id":"public/2017/02/25/pytorch-tutor-01/index.html","hash":"87c5372c6e8f2375b1409fda43a9d7a0de8c9fdc","modified":1516871127066},{"_id":"public/2017/02/22/warpctc-caffe/index.html","hash":"8dad54a1d808151e92e0615b6296ca20d63bb8ba","modified":1516871127066},{"_id":"public/2017/02/12/cs131-mean-shift/index.html","hash":"0855e460e8a089ef0103beff2a0311c4c22da41d","modified":1516871127066},{"_id":"public/2017/02/05/cs131-kmeans/index.html","hash":"a33713c14f639fa2ad1bebb50e83456f8b783ab3","modified":1516871127066},{"_id":"public/2017/02/09/build-caffe-ubuntu/index.html","hash":"e1d7ae8198450d1ab18d62af954a8dd5df9e02dc","modified":1516871127066},{"_id":"public/2017/02/26/jupyternotebook-remote-useage/index.html","hash":"4402661b46410faebf9f81e556ebbee40d5a1199","modified":1516871127066},{"_id":"public/2017/02/02/cs131-camera/index.html","hash":"f7fb6f8a71a1ea0007e2911a3e613b111a8d903b","modified":1516871127066},{"_id":"public/2017/02/04/yolo-paper/index.html","hash":"89cf2bf6d065487cc77862cd64046e1e9d381eda","modified":1516871127066},{"_id":"public/2017/02/05/video-linear-alg-essential-property/index.html","hash":"15b40478a44ab70e7d661288667c51ac717b1a81","modified":1516871127066},{"_id":"public/2017/01/25/cs131-finding-features/index.html","hash":"f47c3c8647cde981cc057bc609422317c550931b","modified":1516871127066},{"_id":"public/2017/01/23/cs131-filter-svd/index.html","hash":"a2ba6078d0015094d2c3db180bff835c3aa9ce3a","modified":1516871127066},{"_id":"public/2017/02/08/digitalocean-shadowsocks/index.html","hash":"72371fee3793219036b30faf88f2de0f15f7c855","modified":1516871127067},{"_id":"public/2017/01/24/cs131-edge-detection/index.html","hash":"91e84e399fd7cb82d23385c259c6e4ad6c0cb473","modified":1516871127067},{"_id":"public/2017/01/30/cs131-sift/index.html","hash":"deca9e67d5e0b30b713d95ebcfa0e05e39c51aba","modified":1516871127067},{"_id":"public/2016/12/16/use-doxygen/index.html","hash":"17dcda35a05f765193458d28ee9e528f1873c9f9","modified":1516871127067},{"_id":"public/2017/01/22/cs131-linear-alg/index.html","hash":"0938955c726535d9cea29ff24263621902953024","modified":1516871127067},{"_id":"public/index.html","hash":"65e601702cbc37362527ddea5a769b9d65e4b823","modified":1516871127067},{"_id":"public/2014/07/17/python-reg-exp/index.html","hash":"f7f7c03640552107957d1267cade5a327c261e31","modified":1516871127067},{"_id":"public/page/2/index.html","hash":"6762da67a7594d99a68cb10baa102b4a0146510c","modified":1516871127067},{"_id":"public/page/4/index.html","hash":"7ed5f044e9847e32e4abd6cf5b21b1f1fec70a2c","modified":1516871127067},{"_id":"public/2016/12/16/hello-world/index.html","hash":"a95bda3eca935468a7f9bd42f8732fce7f01c408","modified":1516871127067},{"_id":"public/page/5/index.html","hash":"fa03813c67cdde05b4ecc133e14e8d248aab87ba","modified":1516871127067},{"_id":"public/archives/page/2/index.html","hash":"96e07f8a5a8592dd6127cb8f2ce769acd4c8cc19","modified":1516871127067},{"_id":"public/2016/12/16/gsl-with-vs/index.html","hash":"bc66f7e820e8c3e22f08b40d12dc8369ca5d5c66","modified":1516871127067},{"_id":"public/archives/page/3/index.html","hash":"397065e9ef95069843bd808d194b11bda321eabd","modified":1516871127067},{"_id":"public/archives/page/5/index.html","hash":"b5b4272db04fac585eef82cd860b8570e5f80f89","modified":1516871127067},{"_id":"public/archives/page/4/index.html","hash":"4449525b454b328166f39dd2cbfa1c94f8fb1daf","modified":1516871127067},{"_id":"public/archives/2014/index.html","hash":"a6fb026f749e2090e72ccbe0b652dc6a2f167557","modified":1516871127067},{"_id":"public/archives/2016/index.html","hash":"0e4c349dc825f1c7d120abe10fdcb371d32c9594","modified":1516871127067},{"_id":"public/archives/2014/07/index.html","hash":"63d688398b09e296aeae5b8c7be72668b31ea09f","modified":1516871127067},{"_id":"public/page/3/index.html","hash":"798647ca9a4c0864a9a980e0914d308768cef334","modified":1516871127068},{"_id":"public/archives/index.html","hash":"3e1679645db94593bf6b3d1c00390522518c83d8","modified":1516871127068},{"_id":"public/archives/2016/12/index.html","hash":"cc428ad79fcb2bed4a99a68aa19271f362cc32d5","modified":1516871127068},{"_id":"public/archives/2017/index.html","hash":"584b06cee48af6b4d474375350b47f52544cf16c","modified":1516871127068},{"_id":"public/archives/2017/page/4/index.html","hash":"05c63a94e1e05715f4d3760d91422a939449a141","modified":1516871127068},{"_id":"public/archives/2017/page/3/index.html","hash":"f596eae116261b705eb951497f2e85badbe3b4a7","modified":1516871127068},{"_id":"public/archives/2017/page/2/index.html","hash":"7c12cebbb588c6669adf7f8128c4ee6e6884bd46","modified":1516871127068},{"_id":"public/archives/2017/page/5/index.html","hash":"69764ed30bee6f98adddedf2cf0d7b081ac09d66","modified":1516871127068},{"_id":"public/archives/2017/01/index.html","hash":"aec5f6035e56dae56f9874bd1b4bc4665450c70c","modified":1516871127068},{"_id":"public/archives/2017/02/index.html","hash":"d681bbd6d84b5baca6ce356283549aca23584f05","modified":1516871127068},{"_id":"public/archives/2017/03/index.html","hash":"bfc0aac27a0f0027b90c096b4bbf4ff5e6e454b2","modified":1516871127068},{"_id":"public/archives/2017/04/index.html","hash":"31090fb89955c950c2c30d932d93696505d68ec5","modified":1516871127068},{"_id":"public/archives/2017/05/index.html","hash":"4d6eb6bfc82ec18cec0f693366eb5aa316454c28","modified":1516871127068},{"_id":"public/archives/2017/07/index.html","hash":"2899c5b210e416b36a9a4d98e5031fec0ad36bf0","modified":1516871127068},{"_id":"public/archives/2017/06/index.html","hash":"827e82552947c9296f131f90a65fdd50f2e2d507","modified":1516871127068},{"_id":"public/archives/2017/10/index.html","hash":"14416628c5a4f8cf2e55c213af669f904c311bc8","modified":1516871127068},{"_id":"public/archives/2017/08/index.html","hash":"b715fc2bc5033b55ff980b71600b2aa335c0ef83","modified":1516871127068},{"_id":"public/archives/2017/11/index.html","hash":"45ad46d0dcd16eec569a0a47d47dc8e41deca313","modified":1516871127068},{"_id":"public/archives/2018/index.html","hash":"378d11f4433d561a66352dd449f31536fb4d3e8d","modified":1516871127068},{"_id":"public/tags/tool/index.html","hash":"8d341d0b8a48c83872237dde2f828fdce2a4bb4b","modified":1516871127068},{"_id":"public/archives/2018/01/index.html","hash":"723488e46705006e815f4296447ce6845b6d0d69","modified":1516871127068},{"_id":"public/tags/cafe/index.html","hash":"e9a215896bd5a4ac4ee4aebfd73f316b515c435d","modified":1516871127068},{"_id":"public/tags/caffe/index.html","hash":"24b8229aebe431757e68b3adb2fb341a2e1f1cb7","modified":1516871127068},{"_id":"public/tags/cs131/index.html","hash":"6048c5b8e7de13348ad2c2bcc5d5b3ab7cd0dcb9","modified":1516871127068},{"_id":"public/tags/公开课/index.html","hash":"00d03e07baceaa93559426baa6a42944f623ec40","modified":1516871127069},{"_id":"public/tags/python/index.html","hash":"6071ae4902f74dac7cfc651a5b85576f41ebb645","modified":1516871127069},{"_id":"public/tags/公开课/page/2/index.html","hash":"2bf74e122aa3e5bde8af44d2b3afe871d526fcfd","modified":1516871127069},{"_id":"public/tags/cpp/index.html","hash":"9e819ceccda11026f585a63648a065c0bfe21c15","modified":1516871127069},{"_id":"public/tags/paper/index.html","hash":"ab525c11c2acc400d091d7ad40d237c84e87749a","modified":1516871127069},{"_id":"public/tags/gsl/index.html","hash":"9e6446447c90a8bf77ddef9ea015c8ba258bea97","modified":1516871127069},{"_id":"public/tags/qt/index.html","hash":"7fcb8408f181d5dc3b588e102b2bfdf1a0f41710","modified":1516871127069},{"_id":"public/tags/deep-learning/index.html","hash":"8b1b5b6ea48c528ae8afd4356623f9423f5c6ddf","modified":1516871127069},{"_id":"public/tags/pytorch/index.html","hash":"2b18ea60e9e85d9c7ca26fa0798d4d66255d6c43","modified":1516871127069},{"_id":"public/tags/扯淡/index.html","hash":"3b4919d18d74533cf53c988bfe5fc6e4c655ad66","modified":1516871127069},{"_id":"public/tags/ubuntu/index.html","hash":"1058185b17f6232400c83aedf04885688d54d784","modified":1516871127069},{"_id":"public/tags/reinforcement-learning/index.html","hash":"f2f05a19730c4bc41d51ac1e7fce27422c61b392","modified":1516871127069},{"_id":"public/tags/linux/index.html","hash":"31ae91be03937f02cd0703bdb723c7e250f4723c","modified":1516871127069},{"_id":"public/tags/shell/index.html","hash":"a364cf9950dd610aa1a233aafd1b867aac70beca","modified":1516871127069},{"_id":"public/tags/doxygen/index.html","hash":"6bde107a106dbe9bb4f6b4326765358d74594c62","modified":1516871127069},{"_id":"public/tags/math/index.html","hash":"4b91df898e166fac70525e9a22904372141c95b7","modified":1516871127069},{"_id":"public/tags/yolo/index.html","hash":"8a39e19f34f31e28ae1151f98e3af7144152813a","modified":1516871127069},{"_id":"public/page/6/index.html","hash":"4685236b155ae1401c2e4c00496496a6030d7c5d","modified":1516871127080},{"_id":"public/archives/page/6/index.html","hash":"f74c8a97e332c032c157f49fc95c73aa30602289","modified":1516871127080},{"_id":"public/tags/quantize/index.html","hash":"611b7da1d3b1f07ddeff8c850ac0da40fd2253a2","modified":1516871127080},{"_id":"public/img/paper-inq-quantize-set.png","hash":"7038d10998546436dd75632ecabd146c943d0dc6","modified":1516871127083},{"_id":"public/img/paper-inq-different-quantize.png","hash":"1d704fafbcfbf6186cf9983e3c31c7b3a4db3ac9","modified":1516871127086},{"_id":"public/img/paper-inq-result.png","hash":"704d4b7ef16f3d6f4fc7929c9b020c993f2c8fbd","modified":1516871127121},{"_id":"public/img/paper-inq-algorithm-demo.png","hash":"af7996102c20fdb841d3d4eb09cf34661f8f9bc3","modified":1516871127126}],"Category":[],"Data":[],"Page":[{"date":"2017-03-05T04:29:26.000Z","comments":0,"_content":"欢迎来到我的博客。\n\n我在北京理工大学[复杂系统控制与决策国家重点实验室](http://csicdgz.bit.edu.cn/index.htm)组合导航与智能导航研究室取得工学硕士学位，于2017年4月加入[中国科学院计算技术研究所](http://www.ict.cas.cn)泛在计算系统研究中心。\n\n在计算所，我所在小组致力于神经网络硬件加速与底层软件工具搭建。我的工作与深度学习紧密相关，我们希望能够在视觉任务和NLP等方向之外，将深度学习更好地和物联网(IoT)结合，创造更智能的生活。\n\n如果您对我的博客内容感兴趣，并想与我进一步交流，请通过电子邮件联系我：xmfbit[A.T.]gmail.com(A.T. => @)\n","source":"about/index.md","raw":"---\ndate: 2017-03-05 12:29:26\ncomments: false\n---\n欢迎来到我的博客。\n\n我在北京理工大学[复杂系统控制与决策国家重点实验室](http://csicdgz.bit.edu.cn/index.htm)组合导航与智能导航研究室取得工学硕士学位，于2017年4月加入[中国科学院计算技术研究所](http://www.ict.cas.cn)泛在计算系统研究中心。\n\n在计算所，我所在小组致力于神经网络硬件加速与底层软件工具搭建。我的工作与深度学习紧密相关，我们希望能够在视觉任务和NLP等方向之外，将深度学习更好地和物联网(IoT)结合，创造更智能的生活。\n\n如果您对我的博客内容感兴趣，并想与我进一步交流，请通过电子邮件联系我：xmfbit[A.T.]gmail.com(A.T. => @)\n","updated":"2018-01-12T06:22:20.485Z","path":"about/index.html","title":"","layout":"page","_id":"cjcu6vcr20000qu46tqlq3lgz","content":"<p>欢迎来到我的博客。</p>\n<p>我在北京理工大学<a href=\"http://csicdgz.bit.edu.cn/index.htm\" target=\"_blank\" rel=\"external\">复杂系统控制与决策国家重点实验室</a>组合导航与智能导航研究室取得工学硕士学位，于2017年4月加入<a href=\"http://www.ict.cas.cn\" target=\"_blank\" rel=\"external\">中国科学院计算技术研究所</a>泛在计算系统研究中心。</p>\n<p>在计算所，我所在小组致力于神经网络硬件加速与底层软件工具搭建。我的工作与深度学习紧密相关，我们希望能够在视觉任务和NLP等方向之外，将深度学习更好地和物联网(IoT)结合，创造更智能的生活。</p>\n<p>如果您对我的博客内容感兴趣，并想与我进一步交流，请通过电子邮件联系我：xmfbit[A.T.]gmail.com(A.T. =&gt; @)</p>\n","excerpt":"","more":"<p>欢迎来到我的博客。</p>\n<p>我在北京理工大学<a href=\"http://csicdgz.bit.edu.cn/index.htm\">复杂系统控制与决策国家重点实验室</a>组合导航与智能导航研究室取得工学硕士学位，于2017年4月加入<a href=\"http://www.ict.cas.cn\">中国科学院计算技术研究所</a>泛在计算系统研究中心。</p>\n<p>在计算所，我所在小组致力于神经网络硬件加速与底层软件工具搭建。我的工作与深度学习紧密相关，我们希望能够在视觉任务和NLP等方向之外，将深度学习更好地和物联网(IoT)结合，创造更智能的生活。</p>\n<p>如果您对我的博客内容感兴趣，并想与我进一步交流，请通过电子邮件联系我：xmfbit[A.T.]gmail.com(A.T. =&gt; @)</p>\n"},{"title":"标签","date":"2016-12-17T12:10:38.000Z","type":"tags","comments":0,"_content":"","source":"tags/index.md","raw":"---\ntitle: 标签\ndate: 2016-12-17 20:10:38\ntype: tags\ncomments: false\n---\n","updated":"2018-01-12T06:22:21.224Z","path":"tags/index.html","layout":"page","_id":"cjcu6vcr90002qu46d2mb1gdo","content":"","excerpt":"","more":""}],"Post":[{"title":"在Ubuntu14.04构建Caffe","date":"2017-02-09T12:59:05.000Z","_content":"Caffe作为较早的一款深度学习框架，很是流行。然而，由于依赖项众多，而且Jia Yangqing已经毕业，所以留下了不少的坑。这篇博客记录了我在一台操作系统为Ubuntu14.04.3的DELL游匣7559笔记本上编译Caffe的过程，主要是在编译python接口时遇到的import error问题的解决和找不到HDF5链接库的问题。\n\n![caffe](/img/caffe_image.jpg)\n\n<!-- more -->\n## 修改Makefile.config\n当从github上clone下来Caffe的源码之后，我们首先需要修改Makefile.config文件，自定义配置。下面是我的配置文件，主要修改了CUDNN部分和Anaconda Python部分。\n\n``` bash\n## Refer to http://caffe.berkeleyvision.org/installation.html\n# Contributions simplifying and improving our build system are welcome!\n\n# cuDNN acceleration switch (uncomment to build with cuDNN).\nUSE_CUDNN := 1    # 这里我们使用cudnn加速\n\n# CPU-only switch (uncomment to build without GPU support).\n# CPU_ONLY := 1\n\n# uncomment to disable IO dependencies and corresponding data layers\n# USE_OPENCV := 0\n# USE_LEVELDB := 0\n# USE_LMDB := 0\n\n# uncomment to allow MDB_NOLOCK when reading LMDB files (only if necessary)\n#\tYou should not set this flag if you will be reading LMDBs with any\n#\tpossibility of simultaneous read and write\n# ALLOW_LMDB_NOLOCK := 1\n\n# Uncomment if you're using OpenCV 3\n# OPENCV_VERSION := 3\n\n# To customize your choice of compiler, uncomment and set the following.\n# N.B. the default for Linux is g++ and the default for OSX is clang++\n# CUSTOM_CXX := g++\n\n# CUDA directory contains bin/ and lib/ directories that we need.\nCUDA_DIR := /usr/local/cuda\n# On Ubuntu 14.04, if cuda tools are installed via\n# \"sudo apt-get install nvidia-cuda-toolkit\" then use this instead:\n# CUDA_DIR := /usr\n\n# CUDA architecture setting: going with all of them.\n# For CUDA < 6.0, comment the *_50 lines for compatibility.\n# 这里可以去掉sm_20和21，因为实在是已经太老了\n# 如果保留的话，编译时nvcc会给出警告\nCUDA_ARCH := -gencode arch=compute_30,code=sm_30 \\\n\t\t-gencode arch=compute_35,code=sm_35 \\\n\t\t-gencode arch=compute_50,code=sm_50 \\\n\t\t-gencode arch=compute_50,code=compute_50\n\n# BLAS choice:\n# atlas for ATLAS (default)\n# mkl for MKL\n# open for OpenBlas\nBLAS := atlas\n# Custom (MKL/ATLAS/OpenBLAS) include and lib directories.\n# Leave commented to accept the defaults for your choice of BLAS\n# (which should work)!\n# BLAS_INCLUDE := /path/to/your/blas\n# BLAS_LIB := /path/to/your/blas\n\n# Homebrew puts openblas in a directory that is not on the standard search path\n# BLAS_INCLUDE := $(shell brew --prefix openblas)/include\n# BLAS_LIB := $(shell brew --prefix openblas)/lib\n\n# This is required only if you will compile the matlab interface.\n# MATLAB directory should contain the mex binary in /bin.\n# MATLAB_DIR := /usr/local\n# MATLAB_DIR := /Applications/MATLAB_R2012b.app\n\n# NOTE: this is required only if you will compile the python interface.\n# We need to be able to find Python.h and numpy/arrayobject.h.\nPYTHON_INCLUDE := /usr/include/python2.7 \\\n\t\t/usr/lib/python2.7/dist-packages/numpy/core/include\n# Anaconda Python distribution is quite popular. Include path:\n# Verify anaconda location, sometimes it's in root.\n# 这里我们使用Anaconda\nANACONDA_HOME := $(HOME)/anaconda2\n PYTHON_INCLUDE := $(ANACONDA_HOME)/include \\\n\t\t $(ANACONDA_HOME)/include/python2.7 \\\n\t\t $(ANACONDA_HOME)/lib/python2.7/site-packages/numpy/core/include\n\n# Uncomment to use Python 3 (default is Python 2)\n# PYTHON_LIBRARIES := boost_python3 python3.5m\n# PYTHON_INCLUDE := /usr/include/python3.5m \\\n#                 /usr/lib/python3.5/dist-packages/numpy/core/include\n\n# We need to be able to find libpythonX.X.so or .dylib.\n#PYTHON_LIB := /usr/lib\n PYTHON_LIB := $(ANACONDA_HOME)/lib\n\n# Homebrew installs numpy in a non standard path (keg only)\n# PYTHON_INCLUDE += $(dir $(shell python -c 'import numpy.core; print(numpy.core.__file__)'))/include\n# PYTHON_LIB += $(shell brew --prefix numpy)/lib\n\n# Uncomment to support layers written in Python (will link against Python libs)\nWITH_PYTHON_LAYER := 1\n\n# Whatever else you find you need goes here.\nINCLUDE_DIRS := $(PYTHON_INCLUDE) /usr/local/include\nLIBRARY_DIRS := $(PYTHON_LIB) /usr/local/lib /usr/lib\n\n# If Homebrew is installed at a non standard location (for example your home directory) and you use it for general dependencies\n# INCLUDE_DIRS += $(shell brew --prefix)/include\n# LIBRARY_DIRS += $(shell brew --prefix)/lib\n\n# NCCL acceleration switch (uncomment to build with NCCL)\n# https://github.com/NVIDIA/nccl (last tested version: v1.2.3-1+cuda8.0)\n# USE_NCCL := 1\n\n# Uncomment to use `pkg-config` to specify OpenCV library paths.\n# (Usually not necessary -- OpenCV libraries are normally installed in one of the above $LIBRARY_DIRS.)\n# USE_PKG_CONFIG := 1\n\n# N.B. both build and distribute dirs are cleared on `make clean`\nBUILD_DIR := build\nDISTRIBUTE_DIR := distribute\n\n# Uncomment for debugging. Does not work on OSX due to https://github.com/BVLC/caffe/issues/171\n# DEBUG := 1\n\n# The ID of the GPU that 'make runtest' will use to run unit tests.\nTEST_GPUID := 0\n\n# enable pretty build (comment to see full commands)\nQ ?= @\n```\n\n对于CUDNN，从Nvidia官方网站上下载后，可不按照官方给定的方法安装。直接将`include`中的头文件放于`/usr/local/cuda-8.0/include`下，将`lib`中的库文件放于`/usr/loca/cuda-8.0/lib64`文件夹下即可。\n\n## 构建\n使用`make -j8`进行编译，并使用`make pycaffe`生成python接口。并在`.bashrc`中添加内容：\n```\nexport PYTHONPATH=/path_to_caffe/python:$PYTHONPATH\n```\n\n结果在`import caffe`时出现问题如下：\n```\nImportError: libcudnn.so.5: cannot open shared object file: No such file or directory\n```\n解决方法如下，详见GitHub issue[讨论](https://github.com/NVIDIA/DIGITS/issues/8)。\n```\nsudo ldconfig /usr/local/cuda/lib64\n```\n\n然而仍有问题，如下：\n```\nImportError: No module named google.protobuf.internal\n```\n解决方法如下，详见G+ caffe-user group的[帖子](https://groups.google.com/forum/#!topic/caffe-users/9Q10WkpCGxs)。\n```\npip install protobuf\n```\n\n不过仍然存在的问题是远程SSH登录时，不能在`ipython`环境下导入caffe，不知为何。\n\n使用`make test; make runtest`进行测试，结果提示HDF5动态链接库出现问题，怀疑与Anaconda中的HDF5冲突有关。错误信息如下：\n\n```\nerror while loading shared libraries: libhdf5_hl.so.10: cannot open shared object file: No such file or directory\n```\n\n解决方法为手动添加符号链接，详见GitHub[讨论帖](https://github.com/BVLC/caffe/issues/1463)。\n\n```\ncd /usr/lib/x86_64-linux-gnu\nsudo ln -s libhdf5.so.7 libhdf5.so.10\nsudo ln -s libhdf5_hl.so.7 libhdf5_hl.so.10\n```\n\n另外，在另一台机器上使用MKL库时，发现会提示找不到相关动态链接库的问题。找到MKL的安装位置，默认应该在目录`/opt/intel/mkl`下。使用`sudo`权限，在目录`/etc/ld.so.conf.d/`下建立一个名为`intel_mkl_setttings.conf`的文件，将MKL安装位置下的链接库目录添加进去，如下所示：\n```\n/opt/intel/mkl/lib/intel64_lin/\n```\n接着，运行`sudo ldconfig`命令，就可以了。\n\n## 测试\n首先，通过`make runtest`看是否全部test可以通过。其次，可以试运行`example`下的LeNet训练。\n```\ncd $CAFFE_ROOT\n./data/mnist/get_mnist.sh\n./examples/mnist/create_mnist.sh\n./examples/mnist/train_lenet.sh\n```\n","source":"_posts/build-caffe-ubuntu.md","raw":"---\ntitle: 在Ubuntu14.04构建Caffe\ndate: 2017-02-09 20:59:05\ntags:\n    - tool\n    - caffe\n---\nCaffe作为较早的一款深度学习框架，很是流行。然而，由于依赖项众多，而且Jia Yangqing已经毕业，所以留下了不少的坑。这篇博客记录了我在一台操作系统为Ubuntu14.04.3的DELL游匣7559笔记本上编译Caffe的过程，主要是在编译python接口时遇到的import error问题的解决和找不到HDF5链接库的问题。\n\n![caffe](/img/caffe_image.jpg)\n\n<!-- more -->\n## 修改Makefile.config\n当从github上clone下来Caffe的源码之后，我们首先需要修改Makefile.config文件，自定义配置。下面是我的配置文件，主要修改了CUDNN部分和Anaconda Python部分。\n\n``` bash\n## Refer to http://caffe.berkeleyvision.org/installation.html\n# Contributions simplifying and improving our build system are welcome!\n\n# cuDNN acceleration switch (uncomment to build with cuDNN).\nUSE_CUDNN := 1    # 这里我们使用cudnn加速\n\n# CPU-only switch (uncomment to build without GPU support).\n# CPU_ONLY := 1\n\n# uncomment to disable IO dependencies and corresponding data layers\n# USE_OPENCV := 0\n# USE_LEVELDB := 0\n# USE_LMDB := 0\n\n# uncomment to allow MDB_NOLOCK when reading LMDB files (only if necessary)\n#\tYou should not set this flag if you will be reading LMDBs with any\n#\tpossibility of simultaneous read and write\n# ALLOW_LMDB_NOLOCK := 1\n\n# Uncomment if you're using OpenCV 3\n# OPENCV_VERSION := 3\n\n# To customize your choice of compiler, uncomment and set the following.\n# N.B. the default for Linux is g++ and the default for OSX is clang++\n# CUSTOM_CXX := g++\n\n# CUDA directory contains bin/ and lib/ directories that we need.\nCUDA_DIR := /usr/local/cuda\n# On Ubuntu 14.04, if cuda tools are installed via\n# \"sudo apt-get install nvidia-cuda-toolkit\" then use this instead:\n# CUDA_DIR := /usr\n\n# CUDA architecture setting: going with all of them.\n# For CUDA < 6.0, comment the *_50 lines for compatibility.\n# 这里可以去掉sm_20和21，因为实在是已经太老了\n# 如果保留的话，编译时nvcc会给出警告\nCUDA_ARCH := -gencode arch=compute_30,code=sm_30 \\\n\t\t-gencode arch=compute_35,code=sm_35 \\\n\t\t-gencode arch=compute_50,code=sm_50 \\\n\t\t-gencode arch=compute_50,code=compute_50\n\n# BLAS choice:\n# atlas for ATLAS (default)\n# mkl for MKL\n# open for OpenBlas\nBLAS := atlas\n# Custom (MKL/ATLAS/OpenBLAS) include and lib directories.\n# Leave commented to accept the defaults for your choice of BLAS\n# (which should work)!\n# BLAS_INCLUDE := /path/to/your/blas\n# BLAS_LIB := /path/to/your/blas\n\n# Homebrew puts openblas in a directory that is not on the standard search path\n# BLAS_INCLUDE := $(shell brew --prefix openblas)/include\n# BLAS_LIB := $(shell brew --prefix openblas)/lib\n\n# This is required only if you will compile the matlab interface.\n# MATLAB directory should contain the mex binary in /bin.\n# MATLAB_DIR := /usr/local\n# MATLAB_DIR := /Applications/MATLAB_R2012b.app\n\n# NOTE: this is required only if you will compile the python interface.\n# We need to be able to find Python.h and numpy/arrayobject.h.\nPYTHON_INCLUDE := /usr/include/python2.7 \\\n\t\t/usr/lib/python2.7/dist-packages/numpy/core/include\n# Anaconda Python distribution is quite popular. Include path:\n# Verify anaconda location, sometimes it's in root.\n# 这里我们使用Anaconda\nANACONDA_HOME := $(HOME)/anaconda2\n PYTHON_INCLUDE := $(ANACONDA_HOME)/include \\\n\t\t $(ANACONDA_HOME)/include/python2.7 \\\n\t\t $(ANACONDA_HOME)/lib/python2.7/site-packages/numpy/core/include\n\n# Uncomment to use Python 3 (default is Python 2)\n# PYTHON_LIBRARIES := boost_python3 python3.5m\n# PYTHON_INCLUDE := /usr/include/python3.5m \\\n#                 /usr/lib/python3.5/dist-packages/numpy/core/include\n\n# We need to be able to find libpythonX.X.so or .dylib.\n#PYTHON_LIB := /usr/lib\n PYTHON_LIB := $(ANACONDA_HOME)/lib\n\n# Homebrew installs numpy in a non standard path (keg only)\n# PYTHON_INCLUDE += $(dir $(shell python -c 'import numpy.core; print(numpy.core.__file__)'))/include\n# PYTHON_LIB += $(shell brew --prefix numpy)/lib\n\n# Uncomment to support layers written in Python (will link against Python libs)\nWITH_PYTHON_LAYER := 1\n\n# Whatever else you find you need goes here.\nINCLUDE_DIRS := $(PYTHON_INCLUDE) /usr/local/include\nLIBRARY_DIRS := $(PYTHON_LIB) /usr/local/lib /usr/lib\n\n# If Homebrew is installed at a non standard location (for example your home directory) and you use it for general dependencies\n# INCLUDE_DIRS += $(shell brew --prefix)/include\n# LIBRARY_DIRS += $(shell brew --prefix)/lib\n\n# NCCL acceleration switch (uncomment to build with NCCL)\n# https://github.com/NVIDIA/nccl (last tested version: v1.2.3-1+cuda8.0)\n# USE_NCCL := 1\n\n# Uncomment to use `pkg-config` to specify OpenCV library paths.\n# (Usually not necessary -- OpenCV libraries are normally installed in one of the above $LIBRARY_DIRS.)\n# USE_PKG_CONFIG := 1\n\n# N.B. both build and distribute dirs are cleared on `make clean`\nBUILD_DIR := build\nDISTRIBUTE_DIR := distribute\n\n# Uncomment for debugging. Does not work on OSX due to https://github.com/BVLC/caffe/issues/171\n# DEBUG := 1\n\n# The ID of the GPU that 'make runtest' will use to run unit tests.\nTEST_GPUID := 0\n\n# enable pretty build (comment to see full commands)\nQ ?= @\n```\n\n对于CUDNN，从Nvidia官方网站上下载后，可不按照官方给定的方法安装。直接将`include`中的头文件放于`/usr/local/cuda-8.0/include`下，将`lib`中的库文件放于`/usr/loca/cuda-8.0/lib64`文件夹下即可。\n\n## 构建\n使用`make -j8`进行编译，并使用`make pycaffe`生成python接口。并在`.bashrc`中添加内容：\n```\nexport PYTHONPATH=/path_to_caffe/python:$PYTHONPATH\n```\n\n结果在`import caffe`时出现问题如下：\n```\nImportError: libcudnn.so.5: cannot open shared object file: No such file or directory\n```\n解决方法如下，详见GitHub issue[讨论](https://github.com/NVIDIA/DIGITS/issues/8)。\n```\nsudo ldconfig /usr/local/cuda/lib64\n```\n\n然而仍有问题，如下：\n```\nImportError: No module named google.protobuf.internal\n```\n解决方法如下，详见G+ caffe-user group的[帖子](https://groups.google.com/forum/#!topic/caffe-users/9Q10WkpCGxs)。\n```\npip install protobuf\n```\n\n不过仍然存在的问题是远程SSH登录时，不能在`ipython`环境下导入caffe，不知为何。\n\n使用`make test; make runtest`进行测试，结果提示HDF5动态链接库出现问题，怀疑与Anaconda中的HDF5冲突有关。错误信息如下：\n\n```\nerror while loading shared libraries: libhdf5_hl.so.10: cannot open shared object file: No such file or directory\n```\n\n解决方法为手动添加符号链接，详见GitHub[讨论帖](https://github.com/BVLC/caffe/issues/1463)。\n\n```\ncd /usr/lib/x86_64-linux-gnu\nsudo ln -s libhdf5.so.7 libhdf5.so.10\nsudo ln -s libhdf5_hl.so.7 libhdf5_hl.so.10\n```\n\n另外，在另一台机器上使用MKL库时，发现会提示找不到相关动态链接库的问题。找到MKL的安装位置，默认应该在目录`/opt/intel/mkl`下。使用`sudo`权限，在目录`/etc/ld.so.conf.d/`下建立一个名为`intel_mkl_setttings.conf`的文件，将MKL安装位置下的链接库目录添加进去，如下所示：\n```\n/opt/intel/mkl/lib/intel64_lin/\n```\n接着，运行`sudo ldconfig`命令，就可以了。\n\n## 测试\n首先，通过`make runtest`看是否全部test可以通过。其次，可以试运行`example`下的LeNet训练。\n```\ncd $CAFFE_ROOT\n./data/mnist/get_mnist.sh\n./examples/mnist/create_mnist.sh\n./examples/mnist/train_lenet.sh\n```\n","slug":"build-caffe-ubuntu","published":1,"updated":"2018-01-12T06:22:20.456Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjcu6vcr50001qu46mldy0kmn","content":"<p>Caffe作为较早的一款深度学习框架，很是流行。然而，由于依赖项众多，而且Jia Yangqing已经毕业，所以留下了不少的坑。这篇博客记录了我在一台操作系统为Ubuntu14.04.3的DELL游匣7559笔记本上编译Caffe的过程，主要是在编译python接口时遇到的import error问题的解决和找不到HDF5链接库的问题。</p>\n<p><img src=\"/img/caffe_image.jpg\" alt=\"caffe\"></p>\n<a id=\"more\"></a>\n<h2 id=\"修改Makefile-config\"><a href=\"#修改Makefile-config\" class=\"headerlink\" title=\"修改Makefile.config\"></a>修改Makefile.config</h2><p>当从github上clone下来Caffe的源码之后，我们首先需要修改Makefile.config文件，自定义配置。下面是我的配置文件，主要修改了CUDNN部分和Anaconda Python部分。</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div><div class=\"line\">47</div><div class=\"line\">48</div><div class=\"line\">49</div><div class=\"line\">50</div><div class=\"line\">51</div><div class=\"line\">52</div><div class=\"line\">53</div><div class=\"line\">54</div><div class=\"line\">55</div><div class=\"line\">56</div><div class=\"line\">57</div><div class=\"line\">58</div><div class=\"line\">59</div><div class=\"line\">60</div><div class=\"line\">61</div><div class=\"line\">62</div><div class=\"line\">63</div><div class=\"line\">64</div><div class=\"line\">65</div><div class=\"line\">66</div><div class=\"line\">67</div><div class=\"line\">68</div><div class=\"line\">69</div><div class=\"line\">70</div><div class=\"line\">71</div><div class=\"line\">72</div><div class=\"line\">73</div><div class=\"line\">74</div><div class=\"line\">75</div><div class=\"line\">76</div><div class=\"line\">77</div><div class=\"line\">78</div><div class=\"line\">79</div><div class=\"line\">80</div><div class=\"line\">81</div><div class=\"line\">82</div><div class=\"line\">83</div><div class=\"line\">84</div><div class=\"line\">85</div><div class=\"line\">86</div><div class=\"line\">87</div><div class=\"line\">88</div><div class=\"line\">89</div><div class=\"line\">90</div><div class=\"line\">91</div><div class=\"line\">92</div><div class=\"line\">93</div><div class=\"line\">94</div><div class=\"line\">95</div><div class=\"line\">96</div><div class=\"line\">97</div><div class=\"line\">98</div><div class=\"line\">99</div><div class=\"line\">100</div><div class=\"line\">101</div><div class=\"line\">102</div><div class=\"line\">103</div><div class=\"line\">104</div><div class=\"line\">105</div><div class=\"line\">106</div><div class=\"line\">107</div><div class=\"line\">108</div><div class=\"line\">109</div><div class=\"line\">110</div><div class=\"line\">111</div><div class=\"line\">112</div><div class=\"line\">113</div><div class=\"line\">114</div><div class=\"line\">115</div><div class=\"line\">116</div><div class=\"line\">117</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\">## Refer to http://caffe.berkeleyvision.org/installation.html</span></div><div class=\"line\"><span class=\"comment\"># Contributions simplifying and improving our build system are welcome!</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># cuDNN acceleration switch (uncomment to build with cuDNN).</span></div><div class=\"line\">USE_CUDNN := 1    <span class=\"comment\"># 这里我们使用cudnn加速</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># CPU-only switch (uncomment to build without GPU support).</span></div><div class=\"line\"><span class=\"comment\"># CPU_ONLY := 1</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># uncomment to disable IO dependencies and corresponding data layers</span></div><div class=\"line\"><span class=\"comment\"># USE_OPENCV := 0</span></div><div class=\"line\"><span class=\"comment\"># USE_LEVELDB := 0</span></div><div class=\"line\"><span class=\"comment\"># USE_LMDB := 0</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># uncomment to allow MDB_NOLOCK when reading LMDB files (only if necessary)</span></div><div class=\"line\"><span class=\"comment\">#\tYou should not set this flag if you will be reading LMDBs with any</span></div><div class=\"line\"><span class=\"comment\">#\tpossibility of simultaneous read and write</span></div><div class=\"line\"><span class=\"comment\"># ALLOW_LMDB_NOLOCK := 1</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># Uncomment if you're using OpenCV 3</span></div><div class=\"line\"><span class=\"comment\"># OPENCV_VERSION := 3</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># To customize your choice of compiler, uncomment and set the following.</span></div><div class=\"line\"><span class=\"comment\"># N.B. the default for Linux is g++ and the default for OSX is clang++</span></div><div class=\"line\"><span class=\"comment\"># CUSTOM_CXX := g++</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># CUDA directory contains bin/ and lib/ directories that we need.</span></div><div class=\"line\">CUDA_DIR := /usr/<span class=\"built_in\">local</span>/cuda</div><div class=\"line\"><span class=\"comment\"># On Ubuntu 14.04, if cuda tools are installed via</span></div><div class=\"line\"><span class=\"comment\"># \"sudo apt-get install nvidia-cuda-toolkit\" then use this instead:</span></div><div class=\"line\"><span class=\"comment\"># CUDA_DIR := /usr</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># CUDA architecture setting: going with all of them.</span></div><div class=\"line\"><span class=\"comment\"># For CUDA &lt; 6.0, comment the *_50 lines for compatibility.</span></div><div class=\"line\"><span class=\"comment\"># 这里可以去掉sm_20和21，因为实在是已经太老了</span></div><div class=\"line\"><span class=\"comment\"># 如果保留的话，编译时nvcc会给出警告</span></div><div class=\"line\">CUDA_ARCH := -gencode arch=compute_30,code=sm_30 \\</div><div class=\"line\">\t\t-gencode arch=compute_35,code=sm_35 \\</div><div class=\"line\">\t\t-gencode arch=compute_50,code=sm_50 \\</div><div class=\"line\">\t\t-gencode arch=compute_50,code=compute_50</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># BLAS choice:</span></div><div class=\"line\"><span class=\"comment\"># atlas for ATLAS (default)</span></div><div class=\"line\"><span class=\"comment\"># mkl for MKL</span></div><div class=\"line\"><span class=\"comment\"># open for OpenBlas</span></div><div class=\"line\">BLAS := atlas</div><div class=\"line\"><span class=\"comment\"># Custom (MKL/ATLAS/OpenBLAS) include and lib directories.</span></div><div class=\"line\"><span class=\"comment\"># Leave commented to accept the defaults for your choice of BLAS</span></div><div class=\"line\"><span class=\"comment\"># (which should work)!</span></div><div class=\"line\"><span class=\"comment\"># BLAS_INCLUDE := /path/to/your/blas</span></div><div class=\"line\"><span class=\"comment\"># BLAS_LIB := /path/to/your/blas</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># Homebrew puts openblas in a directory that is not on the standard search path</span></div><div class=\"line\"><span class=\"comment\"># BLAS_INCLUDE := $(shell brew --prefix openblas)/include</span></div><div class=\"line\"><span class=\"comment\"># BLAS_LIB := $(shell brew --prefix openblas)/lib</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># This is required only if you will compile the matlab interface.</span></div><div class=\"line\"><span class=\"comment\"># MATLAB directory should contain the mex binary in /bin.</span></div><div class=\"line\"><span class=\"comment\"># MATLAB_DIR := /usr/local</span></div><div class=\"line\"><span class=\"comment\"># MATLAB_DIR := /Applications/MATLAB_R2012b.app</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># <span class=\"doctag\">NOTE:</span> this is required only if you will compile the python interface.</span></div><div class=\"line\"><span class=\"comment\"># We need to be able to find Python.h and numpy/arrayobject.h.</span></div><div class=\"line\">PYTHON_INCLUDE := /usr/include/python2.7 \\</div><div class=\"line\">\t\t/usr/lib/python2.7/dist-packages/numpy/core/include</div><div class=\"line\"><span class=\"comment\"># Anaconda Python distribution is quite popular. Include path:</span></div><div class=\"line\"><span class=\"comment\"># Verify anaconda location, sometimes it's in root.</span></div><div class=\"line\"><span class=\"comment\"># 这里我们使用Anaconda</span></div><div class=\"line\">ANACONDA_HOME := $(HOME)/anaconda2</div><div class=\"line\"> PYTHON_INCLUDE := $(ANACONDA_HOME)/include \\</div><div class=\"line\">\t\t $(ANACONDA_HOME)/include/python2.7 \\</div><div class=\"line\">\t\t $(ANACONDA_HOME)/lib/python2.7/site-packages/numpy/core/include</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># Uncomment to use Python 3 (default is Python 2)</span></div><div class=\"line\"><span class=\"comment\"># PYTHON_LIBRARIES := boost_python3 python3.5m</span></div><div class=\"line\"><span class=\"comment\"># PYTHON_INCLUDE := /usr/include/python3.5m \\</span></div><div class=\"line\"><span class=\"comment\">#                 /usr/lib/python3.5/dist-packages/numpy/core/include</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># We need to be able to find libpythonX.X.so or .dylib.</span></div><div class=\"line\"><span class=\"comment\">#PYTHON_LIB := /usr/lib</span></div><div class=\"line\"> PYTHON_LIB := $(ANACONDA_HOME)/lib</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># Homebrew installs numpy in a non standard path (keg only)</span></div><div class=\"line\"><span class=\"comment\"># PYTHON_INCLUDE += $(dir $(shell python -c 'import numpy.core; print(numpy.core.__file__)'))/include</span></div><div class=\"line\"><span class=\"comment\"># PYTHON_LIB += $(shell brew --prefix numpy)/lib</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># Uncomment to support layers written in Python (will link against Python libs)</span></div><div class=\"line\">WITH_PYTHON_LAYER := 1</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># Whatever else you find you need goes here.</span></div><div class=\"line\">INCLUDE_DIRS := $(PYTHON_INCLUDE) /usr/<span class=\"built_in\">local</span>/include</div><div class=\"line\">LIBRARY_DIRS := $(PYTHON_LIB) /usr/<span class=\"built_in\">local</span>/lib /usr/lib</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># If Homebrew is installed at a non standard location (for example your home directory) and you use it for general dependencies</span></div><div class=\"line\"><span class=\"comment\"># INCLUDE_DIRS += $(shell brew --prefix)/include</span></div><div class=\"line\"><span class=\"comment\"># LIBRARY_DIRS += $(shell brew --prefix)/lib</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># NCCL acceleration switch (uncomment to build with NCCL)</span></div><div class=\"line\"><span class=\"comment\"># https://github.com/NVIDIA/nccl (last tested version: v1.2.3-1+cuda8.0)</span></div><div class=\"line\"><span class=\"comment\"># USE_NCCL := 1</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># Uncomment to use `pkg-config` to specify OpenCV library paths.</span></div><div class=\"line\"><span class=\"comment\"># (Usually not necessary -- OpenCV libraries are normally installed in one of the above $LIBRARY_DIRS.)</span></div><div class=\"line\"><span class=\"comment\"># USE_PKG_CONFIG := 1</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># N.B. both build and distribute dirs are cleared on `make clean`</span></div><div class=\"line\">BUILD_DIR := build</div><div class=\"line\">DISTRIBUTE_DIR := distribute</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># Uncomment for debugging. Does not work on OSX due to https://github.com/BVLC/caffe/issues/171</span></div><div class=\"line\"><span class=\"comment\"># DEBUG := 1</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># The ID of the GPU that 'make runtest' will use to run unit tests.</span></div><div class=\"line\">TEST_GPUID := 0</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># enable pretty build (comment to see full commands)</span></div><div class=\"line\">Q ?= @</div></pre></td></tr></table></figure>\n<p>对于CUDNN，从Nvidia官方网站上下载后，可不按照官方给定的方法安装。直接将<code>include</code>中的头文件放于<code>/usr/local/cuda-8.0/include</code>下，将<code>lib</code>中的库文件放于<code>/usr/loca/cuda-8.0/lib64</code>文件夹下即可。</p>\n<h2 id=\"构建\"><a href=\"#构建\" class=\"headerlink\" title=\"构建\"></a>构建</h2><p>使用<code>make -j8</code>进行编译，并使用<code>make pycaffe</code>生成python接口。并在<code>.bashrc</code>中添加内容：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">export PYTHONPATH=/path_to_caffe/python:$PYTHONPATH</div></pre></td></tr></table></figure></p>\n<p>结果在<code>import caffe</code>时出现问题如下：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">ImportError: libcudnn.so.5: cannot open shared object file: No such file or directory</div></pre></td></tr></table></figure></p>\n<p>解决方法如下，详见GitHub issue<a href=\"https://github.com/NVIDIA/DIGITS/issues/8\" target=\"_blank\" rel=\"external\">讨论</a>。<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">sudo ldconfig /usr/local/cuda/lib64</div></pre></td></tr></table></figure></p>\n<p>然而仍有问题，如下：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">ImportError: No module named google.protobuf.internal</div></pre></td></tr></table></figure></p>\n<p>解决方法如下，详见G+ caffe-user group的<a href=\"https://groups.google.com/forum/#!topic/caffe-users/9Q10WkpCGxs\" target=\"_blank\" rel=\"external\">帖子</a>。<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">pip install protobuf</div></pre></td></tr></table></figure></p>\n<p>不过仍然存在的问题是远程SSH登录时，不能在<code>ipython</code>环境下导入caffe，不知为何。</p>\n<p>使用<code>make test; make runtest</code>进行测试，结果提示HDF5动态链接库出现问题，怀疑与Anaconda中的HDF5冲突有关。错误信息如下：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">error while loading shared libraries: libhdf5_hl.so.10: cannot open shared object file: No such file or directory</div></pre></td></tr></table></figure>\n<p>解决方法为手动添加符号链接，详见GitHub<a href=\"https://github.com/BVLC/caffe/issues/1463\" target=\"_blank\" rel=\"external\">讨论帖</a>。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\">cd /usr/lib/x86_64-linux-gnu</div><div class=\"line\">sudo ln -s libhdf5.so.7 libhdf5.so.10</div><div class=\"line\">sudo ln -s libhdf5_hl.so.7 libhdf5_hl.so.10</div></pre></td></tr></table></figure>\n<p>另外，在另一台机器上使用MKL库时，发现会提示找不到相关动态链接库的问题。找到MKL的安装位置，默认应该在目录<code>/opt/intel/mkl</code>下。使用<code>sudo</code>权限，在目录<code>/etc/ld.so.conf.d/</code>下建立一个名为<code>intel_mkl_setttings.conf</code>的文件，将MKL安装位置下的链接库目录添加进去，如下所示：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">/opt/intel/mkl/lib/intel64_lin/</div></pre></td></tr></table></figure></p>\n<p>接着，运行<code>sudo ldconfig</code>命令，就可以了。</p>\n<h2 id=\"测试\"><a href=\"#测试\" class=\"headerlink\" title=\"测试\"></a>测试</h2><p>首先，通过<code>make runtest</code>看是否全部test可以通过。其次，可以试运行<code>example</code>下的LeNet训练。<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\">cd $CAFFE_ROOT</div><div class=\"line\">./data/mnist/get_mnist.sh</div><div class=\"line\">./examples/mnist/create_mnist.sh</div><div class=\"line\">./examples/mnist/train_lenet.sh</div></pre></td></tr></table></figure></p>\n","excerpt":"<p>Caffe作为较早的一款深度学习框架，很是流行。然而，由于依赖项众多，而且Jia Yangqing已经毕业，所以留下了不少的坑。这篇博客记录了我在一台操作系统为Ubuntu14.04.3的DELL游匣7559笔记本上编译Caffe的过程，主要是在编译python接口时遇到的import error问题的解决和找不到HDF5链接库的问题。</p>\n<p><img src=\"/img/caffe_image.jpg\" alt=\"caffe\"></p>","more":"<h2 id=\"修改Makefile-config\"><a href=\"#修改Makefile-config\" class=\"headerlink\" title=\"修改Makefile.config\"></a>修改Makefile.config</h2><p>当从github上clone下来Caffe的源码之后，我们首先需要修改Makefile.config文件，自定义配置。下面是我的配置文件，主要修改了CUDNN部分和Anaconda Python部分。</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div><div class=\"line\">47</div><div class=\"line\">48</div><div class=\"line\">49</div><div class=\"line\">50</div><div class=\"line\">51</div><div class=\"line\">52</div><div class=\"line\">53</div><div class=\"line\">54</div><div class=\"line\">55</div><div class=\"line\">56</div><div class=\"line\">57</div><div class=\"line\">58</div><div class=\"line\">59</div><div class=\"line\">60</div><div class=\"line\">61</div><div class=\"line\">62</div><div class=\"line\">63</div><div class=\"line\">64</div><div class=\"line\">65</div><div class=\"line\">66</div><div class=\"line\">67</div><div class=\"line\">68</div><div class=\"line\">69</div><div class=\"line\">70</div><div class=\"line\">71</div><div class=\"line\">72</div><div class=\"line\">73</div><div class=\"line\">74</div><div class=\"line\">75</div><div class=\"line\">76</div><div class=\"line\">77</div><div class=\"line\">78</div><div class=\"line\">79</div><div class=\"line\">80</div><div class=\"line\">81</div><div class=\"line\">82</div><div class=\"line\">83</div><div class=\"line\">84</div><div class=\"line\">85</div><div class=\"line\">86</div><div class=\"line\">87</div><div class=\"line\">88</div><div class=\"line\">89</div><div class=\"line\">90</div><div class=\"line\">91</div><div class=\"line\">92</div><div class=\"line\">93</div><div class=\"line\">94</div><div class=\"line\">95</div><div class=\"line\">96</div><div class=\"line\">97</div><div class=\"line\">98</div><div class=\"line\">99</div><div class=\"line\">100</div><div class=\"line\">101</div><div class=\"line\">102</div><div class=\"line\">103</div><div class=\"line\">104</div><div class=\"line\">105</div><div class=\"line\">106</div><div class=\"line\">107</div><div class=\"line\">108</div><div class=\"line\">109</div><div class=\"line\">110</div><div class=\"line\">111</div><div class=\"line\">112</div><div class=\"line\">113</div><div class=\"line\">114</div><div class=\"line\">115</div><div class=\"line\">116</div><div class=\"line\">117</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\">## Refer to http://caffe.berkeleyvision.org/installation.html</span></div><div class=\"line\"><span class=\"comment\"># Contributions simplifying and improving our build system are welcome!</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># cuDNN acceleration switch (uncomment to build with cuDNN).</span></div><div class=\"line\">USE_CUDNN := 1    <span class=\"comment\"># 这里我们使用cudnn加速</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># CPU-only switch (uncomment to build without GPU support).</span></div><div class=\"line\"><span class=\"comment\"># CPU_ONLY := 1</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># uncomment to disable IO dependencies and corresponding data layers</span></div><div class=\"line\"><span class=\"comment\"># USE_OPENCV := 0</span></div><div class=\"line\"><span class=\"comment\"># USE_LEVELDB := 0</span></div><div class=\"line\"><span class=\"comment\"># USE_LMDB := 0</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># uncomment to allow MDB_NOLOCK when reading LMDB files (only if necessary)</span></div><div class=\"line\"><span class=\"comment\">#\tYou should not set this flag if you will be reading LMDBs with any</span></div><div class=\"line\"><span class=\"comment\">#\tpossibility of simultaneous read and write</span></div><div class=\"line\"><span class=\"comment\"># ALLOW_LMDB_NOLOCK := 1</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># Uncomment if you're using OpenCV 3</span></div><div class=\"line\"><span class=\"comment\"># OPENCV_VERSION := 3</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># To customize your choice of compiler, uncomment and set the following.</span></div><div class=\"line\"><span class=\"comment\"># N.B. the default for Linux is g++ and the default for OSX is clang++</span></div><div class=\"line\"><span class=\"comment\"># CUSTOM_CXX := g++</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># CUDA directory contains bin/ and lib/ directories that we need.</span></div><div class=\"line\">CUDA_DIR := /usr/<span class=\"built_in\">local</span>/cuda</div><div class=\"line\"><span class=\"comment\"># On Ubuntu 14.04, if cuda tools are installed via</span></div><div class=\"line\"><span class=\"comment\"># \"sudo apt-get install nvidia-cuda-toolkit\" then use this instead:</span></div><div class=\"line\"><span class=\"comment\"># CUDA_DIR := /usr</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># CUDA architecture setting: going with all of them.</span></div><div class=\"line\"><span class=\"comment\"># For CUDA &lt; 6.0, comment the *_50 lines for compatibility.</span></div><div class=\"line\"><span class=\"comment\"># 这里可以去掉sm_20和21，因为实在是已经太老了</span></div><div class=\"line\"><span class=\"comment\"># 如果保留的话，编译时nvcc会给出警告</span></div><div class=\"line\">CUDA_ARCH := -gencode arch=compute_30,code=sm_30 \\</div><div class=\"line\">\t\t-gencode arch=compute_35,code=sm_35 \\</div><div class=\"line\">\t\t-gencode arch=compute_50,code=sm_50 \\</div><div class=\"line\">\t\t-gencode arch=compute_50,code=compute_50</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># BLAS choice:</span></div><div class=\"line\"><span class=\"comment\"># atlas for ATLAS (default)</span></div><div class=\"line\"><span class=\"comment\"># mkl for MKL</span></div><div class=\"line\"><span class=\"comment\"># open for OpenBlas</span></div><div class=\"line\">BLAS := atlas</div><div class=\"line\"><span class=\"comment\"># Custom (MKL/ATLAS/OpenBLAS) include and lib directories.</span></div><div class=\"line\"><span class=\"comment\"># Leave commented to accept the defaults for your choice of BLAS</span></div><div class=\"line\"><span class=\"comment\"># (which should work)!</span></div><div class=\"line\"><span class=\"comment\"># BLAS_INCLUDE := /path/to/your/blas</span></div><div class=\"line\"><span class=\"comment\"># BLAS_LIB := /path/to/your/blas</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># Homebrew puts openblas in a directory that is not on the standard search path</span></div><div class=\"line\"><span class=\"comment\"># BLAS_INCLUDE := $(shell brew --prefix openblas)/include</span></div><div class=\"line\"><span class=\"comment\"># BLAS_LIB := $(shell brew --prefix openblas)/lib</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># This is required only if you will compile the matlab interface.</span></div><div class=\"line\"><span class=\"comment\"># MATLAB directory should contain the mex binary in /bin.</span></div><div class=\"line\"><span class=\"comment\"># MATLAB_DIR := /usr/local</span></div><div class=\"line\"><span class=\"comment\"># MATLAB_DIR := /Applications/MATLAB_R2012b.app</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># <span class=\"doctag\">NOTE:</span> this is required only if you will compile the python interface.</span></div><div class=\"line\"><span class=\"comment\"># We need to be able to find Python.h and numpy/arrayobject.h.</span></div><div class=\"line\">PYTHON_INCLUDE := /usr/include/python2.7 \\</div><div class=\"line\">\t\t/usr/lib/python2.7/dist-packages/numpy/core/include</div><div class=\"line\"><span class=\"comment\"># Anaconda Python distribution is quite popular. Include path:</span></div><div class=\"line\"><span class=\"comment\"># Verify anaconda location, sometimes it's in root.</span></div><div class=\"line\"><span class=\"comment\"># 这里我们使用Anaconda</span></div><div class=\"line\">ANACONDA_HOME := $(HOME)/anaconda2</div><div class=\"line\"> PYTHON_INCLUDE := $(ANACONDA_HOME)/include \\</div><div class=\"line\">\t\t $(ANACONDA_HOME)/include/python2.7 \\</div><div class=\"line\">\t\t $(ANACONDA_HOME)/lib/python2.7/site-packages/numpy/core/include</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># Uncomment to use Python 3 (default is Python 2)</span></div><div class=\"line\"><span class=\"comment\"># PYTHON_LIBRARIES := boost_python3 python3.5m</span></div><div class=\"line\"><span class=\"comment\"># PYTHON_INCLUDE := /usr/include/python3.5m \\</span></div><div class=\"line\"><span class=\"comment\">#                 /usr/lib/python3.5/dist-packages/numpy/core/include</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># We need to be able to find libpythonX.X.so or .dylib.</span></div><div class=\"line\"><span class=\"comment\">#PYTHON_LIB := /usr/lib</span></div><div class=\"line\"> PYTHON_LIB := $(ANACONDA_HOME)/lib</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># Homebrew installs numpy in a non standard path (keg only)</span></div><div class=\"line\"><span class=\"comment\"># PYTHON_INCLUDE += $(dir $(shell python -c 'import numpy.core; print(numpy.core.__file__)'))/include</span></div><div class=\"line\"><span class=\"comment\"># PYTHON_LIB += $(shell brew --prefix numpy)/lib</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># Uncomment to support layers written in Python (will link against Python libs)</span></div><div class=\"line\">WITH_PYTHON_LAYER := 1</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># Whatever else you find you need goes here.</span></div><div class=\"line\">INCLUDE_DIRS := $(PYTHON_INCLUDE) /usr/<span class=\"built_in\">local</span>/include</div><div class=\"line\">LIBRARY_DIRS := $(PYTHON_LIB) /usr/<span class=\"built_in\">local</span>/lib /usr/lib</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># If Homebrew is installed at a non standard location (for example your home directory) and you use it for general dependencies</span></div><div class=\"line\"><span class=\"comment\"># INCLUDE_DIRS += $(shell brew --prefix)/include</span></div><div class=\"line\"><span class=\"comment\"># LIBRARY_DIRS += $(shell brew --prefix)/lib</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># NCCL acceleration switch (uncomment to build with NCCL)</span></div><div class=\"line\"><span class=\"comment\"># https://github.com/NVIDIA/nccl (last tested version: v1.2.3-1+cuda8.0)</span></div><div class=\"line\"><span class=\"comment\"># USE_NCCL := 1</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># Uncomment to use `pkg-config` to specify OpenCV library paths.</span></div><div class=\"line\"><span class=\"comment\"># (Usually not necessary -- OpenCV libraries are normally installed in one of the above $LIBRARY_DIRS.)</span></div><div class=\"line\"><span class=\"comment\"># USE_PKG_CONFIG := 1</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># N.B. both build and distribute dirs are cleared on `make clean`</span></div><div class=\"line\">BUILD_DIR := build</div><div class=\"line\">DISTRIBUTE_DIR := distribute</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># Uncomment for debugging. Does not work on OSX due to https://github.com/BVLC/caffe/issues/171</span></div><div class=\"line\"><span class=\"comment\"># DEBUG := 1</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># The ID of the GPU that 'make runtest' will use to run unit tests.</span></div><div class=\"line\">TEST_GPUID := 0</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># enable pretty build (comment to see full commands)</span></div><div class=\"line\">Q ?= @</div></pre></td></tr></table></figure>\n<p>对于CUDNN，从Nvidia官方网站上下载后，可不按照官方给定的方法安装。直接将<code>include</code>中的头文件放于<code>/usr/local/cuda-8.0/include</code>下，将<code>lib</code>中的库文件放于<code>/usr/loca/cuda-8.0/lib64</code>文件夹下即可。</p>\n<h2 id=\"构建\"><a href=\"#构建\" class=\"headerlink\" title=\"构建\"></a>构建</h2><p>使用<code>make -j8</code>进行编译，并使用<code>make pycaffe</code>生成python接口。并在<code>.bashrc</code>中添加内容：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">export PYTHONPATH=/path_to_caffe/python:$PYTHONPATH</div></pre></td></tr></table></figure></p>\n<p>结果在<code>import caffe</code>时出现问题如下：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">ImportError: libcudnn.so.5: cannot open shared object file: No such file or directory</div></pre></td></tr></table></figure></p>\n<p>解决方法如下，详见GitHub issue<a href=\"https://github.com/NVIDIA/DIGITS/issues/8\">讨论</a>。<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">sudo ldconfig /usr/local/cuda/lib64</div></pre></td></tr></table></figure></p>\n<p>然而仍有问题，如下：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">ImportError: No module named google.protobuf.internal</div></pre></td></tr></table></figure></p>\n<p>解决方法如下，详见G+ caffe-user group的<a href=\"https://groups.google.com/forum/#!topic/caffe-users/9Q10WkpCGxs\">帖子</a>。<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">pip install protobuf</div></pre></td></tr></table></figure></p>\n<p>不过仍然存在的问题是远程SSH登录时，不能在<code>ipython</code>环境下导入caffe，不知为何。</p>\n<p>使用<code>make test; make runtest</code>进行测试，结果提示HDF5动态链接库出现问题，怀疑与Anaconda中的HDF5冲突有关。错误信息如下：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">error while loading shared libraries: libhdf5_hl.so.10: cannot open shared object file: No such file or directory</div></pre></td></tr></table></figure>\n<p>解决方法为手动添加符号链接，详见GitHub<a href=\"https://github.com/BVLC/caffe/issues/1463\">讨论帖</a>。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\">cd /usr/lib/x86_64-linux-gnu</div><div class=\"line\">sudo ln -s libhdf5.so.7 libhdf5.so.10</div><div class=\"line\">sudo ln -s libhdf5_hl.so.7 libhdf5_hl.so.10</div></pre></td></tr></table></figure>\n<p>另外，在另一台机器上使用MKL库时，发现会提示找不到相关动态链接库的问题。找到MKL的安装位置，默认应该在目录<code>/opt/intel/mkl</code>下。使用<code>sudo</code>权限，在目录<code>/etc/ld.so.conf.d/</code>下建立一个名为<code>intel_mkl_setttings.conf</code>的文件，将MKL安装位置下的链接库目录添加进去，如下所示：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">/opt/intel/mkl/lib/intel64_lin/</div></pre></td></tr></table></figure></p>\n<p>接着，运行<code>sudo ldconfig</code>命令，就可以了。</p>\n<h2 id=\"测试\"><a href=\"#测试\" class=\"headerlink\" title=\"测试\"></a>测试</h2><p>首先，通过<code>make runtest</code>看是否全部test可以通过。其次，可以试运行<code>example</code>下的LeNet训练。<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\">cd $CAFFE_ROOT</div><div class=\"line\">./data/mnist/get_mnist.sh</div><div class=\"line\">./examples/mnist/create_mnist.sh</div><div class=\"line\">./examples/mnist/train_lenet.sh</div></pre></td></tr></table></figure></p>"},{"title":"Caffe中的BatchNorm实现","date":"2018-01-08T12:12:44.000Z","_content":"这篇博客总结了Caffe中BN的实现。\n<!-- more -->\n\n## BN简介\n\n由于BN技术已经有很广泛的应用，所以这里只对BN做一个简单的介绍。\n\nBN是Batch Normalization的简称，来源于Google研究人员的论文：[Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](https://arxiv.org/pdf/1502.03167.pdf)。对于网络的输入层，我们可以采用减去均值除以方差的方法进行归一化，对于网络中间层，BN可以实现类似的功能。\n\n在BN层中，训练时，会对输入blob各个channel的均值和方差做一统计。在做inference的时候，我们就可以利用均值和方法，对输入$x$做如下的归一化操作。其中，$\\epsilon$是为了防止除数是$0$，$i$是channel的index。\n$$\\hat{x_i} = \\frac{x_i-\\mu_i}{\\sqrt{Var(x_i)+\\epsilon}}$$\n\n不过如果只是做如上的操作，会影响模型的表达能力。例如，Identity Map($y = x$)就不能表示了。所以，作者提出还需要在后面添加一个线性变换，如下所示。其中，$\\gamma$和$\\beta$都是待学习的参数，使用梯度下降进行更新。BN的最终输出就是$y$。\n$$y_i = \\gamma \\hat{x_i} + \\beta$$\n\n如下图所示，展示了BN变换的过程。\n![BN变换](/img/caffe_bn_what_is_bn.jpg)\n\n上面，我们讲的还是inference时候BN变换是什么样子的。那么，训练时候，BN是如何估计样本均值和方差的呢？下面，结合Caffe的代码进行梳理。\n\n## BN in Caffe\n在BVLC的Caffe实现中，BN层需要和Scale层配合使用。在这里，BN层专门用来做“Normalization”操作（确实是人如其名了），而后续的线性变换层，交给Scale层去做。\n\n下面的这段代码取自He Kaiming的Residual Net50的[模型定义文件](https://github.com/KaimingHe/deep-residual-networks/blob/master/prototxt/ResNet-50-deploy.prototxt#L21)。在这里，设置`batch_norm_param`中`use_global_stats`为`true`，是指在inference阶段，我们只使用已经得到的均值和方差统计量，进行归一化处理，而不再更新这两个统计量。后面Scale层设置的`bias_term: true`是不可省略的。这个选项将其配置为线性变换层。\n\n```\nlayer {\n\tbottom: \"conv1\"\n\ttop: \"conv1\"\n\tname: \"bn_conv1\"\n\ttype: \"BatchNorm\"\n\tbatch_norm_param {\n\t\tuse_global_stats: true\n\t}\n}\n\nlayer {\n\tbottom: \"conv1\"\n\ttop: \"conv1\"\n\tname: \"scale_conv1\"\n\ttype: \"Scale\"\n\tscale_param {\n\t\tbias_term: true\n\t}\n}\n```\n这就是Caffe中BN层的固定搭配方法。这里只是简单提到，具体参数的意义待我们深入代码可以分析。\n\n## BatchNorm 层的实现\n上面说过，Caffe中的BN层与原始论文稍有不同，只是做了输入的归一化，而后续的线性变换是交由后续的Scale层实现的。\n### proto定义的相关参数\n我们首先看一下`caffe.proto`中关于BN层参数的描述。保留了原始的英文注释，并添加了中文解释。\n```\nmessage BatchNormParameter {\n  // If false, normalization is performed over the current mini-batch\n  // and global statistics are accumulated (but not yet used) by a moving\n  // average.\n  // If true, those accumulated mean and variance values are used for the\n  // normalization.\n  // By default, it is set to false when the network is in the training\n  // phase and true when the network is in the testing phase.\n  // 设置为False的话，更新全局统计量，对当前的mini-batch进行规范化时，不使用全局统计量，而是\n  // 当前batch的均值和方差。\n  // 设置为True，使用全局统计量做规范化。\n  // 后面在BN的实现代码我们会看到，这个变量默认随着当前网络在train或test phase而变化。\n  // 当train时为false，当test时为true。\n  optional bool use_global_stats = 1;\n  \n  // What fraction of the moving average remains each iteration?\n  // Smaller values make the moving average decay faster, giving more\n  // weight to the recent values.\n  // Each iteration updates the moving average @f$S_{t-1}@f$ with the\n  // current mean @f$ Y_t @f$ by\n  // @f$ S_t = (1-\\beta)Y_t + \\beta \\cdot S_{t-1} @f$, where @f$ \\beta @f$\n  // is the moving_average_fraction parameter.\n  // BN在统计全局均值和方差信息时，使用的是滑动平均法，也就是\n  // St = (1-beta)*Yt + beta*S_{t-1}\n  // 其中St为当前估计出来的全局统计量（均值或方差），Yt为当前batch的均值或方差\n  // beta是滑动因子。其实这是一种很常见的平滑滤波的方法。\n  optional float moving_average_fraction = 2 [default = .999];\n  \n  // Small value to add to the variance estimate so that we don't divide by\n  // zero.\n  // 防止除数为0加上去的eps\n  optional float eps = 3 [default = 1e-5];\n}\n```\n\nOK。现在可以进入BN的代码实现了。阅读大部分代码都没有什么难度，下面主要结合代码讲解`use_global_stats`变量的作用和均值（方差同理）的计算。由于均值和方差的计算原理相近，所以下面只会详细介绍均值的计算。\n\n### SetUp\nBN层的SetUp代码如下。首先，会根据当前处于train还是test决定是否使用全局的统计量。如果prototxt文件中设置了`use_global_stats`标志，则会使用用户给定的配置。所以一般在使用BN时，无需对`use_global_stats`进行配置。\n\n这里有一个地方容易迷惑。BN中要对样本的均值和方差进行统计，即我们需要两个blob来存储。但是从下面的代码可以看到，BN一共有3个blob作为参数。这里做一解释，主要参考了wiki的[moving average条目](https://wiki2.org/en/Moving_average)。\n\n``` cpp\ntemplate <typename Dtype>\nvoid BatchNormLayer<Dtype>::LayerSetUp(const vector<Blob<Dtype>*>& bottom,\n      const vector<Blob<Dtype>*>& top) {\n  BatchNormParameter param = this->layer_param_.batch_norm_param();\n  moving_average_fraction_ = param.moving_average_fraction();\n  // 默认根据当前是否处在TEST模式而决定是否使用全局mean和var\n  use_global_stats_ = this->phase_ == TEST;\n  if (param.has_use_global_stats())\n    use_global_stats_ = param.use_global_stats();\n  // 得到channels数量\n  // 为了防止越界，首先检查输入是否为1D\n  if (bottom[0]->num_axes() == 1)\n    channels_ = 1;\n  else\n    channels_ = bottom[0]->shape(1);\n  eps_ = param.eps();\n  if (this->blobs_.size() > 0) {\n    LOG(INFO) << \"Skipping parameter initialization\";\n  } else {\n    // 参数共3个\n    this->blobs_.resize(3);\n    vector<int> sz;\n    sz.push_back(channels_);\n    // mean 和var都是1D的长度为channels的向量\n    // 因为在规范化过程中，要逐channel进行，即：\n    // for c in range(channels):\n    //     x_hat[c] = (x[c] - mean[c]) / std[c]\n    this->blobs_[0].reset(new Blob<Dtype>(sz));\n    this->blobs_[1].reset(new Blob<Dtype>(sz));\n    // 这里的解释见下\n    sz[0] = 1;\n    this->blobs_[2].reset(new Blob<Dtype>(sz));\n    for (int i = 0; i < 3; ++i) {\n      caffe_set(this->blobs_[i]->count(), Dtype(0),\n                this->blobs_[i]->mutable_cpu_data());\n    }\n  }\n  // Mask statistics from optimization by setting local learning rates\n  // for mean, variance, and the bias correction to zero.\n  // mean 和 std在训练的时候是不需要梯度下降来更新的，这里强制把其learning rate\n  // 设置为0\n  for (int i = 0; i < this->blobs_.size(); ++i) {\n    if (this->layer_param_.param_size() == i) {\n      ParamSpec* fixed_param_spec = this->layer_param_.add_param();\n      fixed_param_spec->set_lr_mult(0.f);\n    } else {\n      CHECK_EQ(this->layer_param_.param(i).lr_mult(), 0.f)\n          << \"Cannot configure batch normalization statistics as layer \"\n          << \"parameters.\";\n    }\n  }\n}\n```\n\n在求取某个流数据（stream）的平均值的时候，常用的一种方法是滑动平均法，也就是使用系数$\\alpha$来做平滑滤波，如下所示：\n$$S_t = \\alpha Y_t + (1-\\alpha) S_{t-1}$$\n\n上面的式子等价于：\n$$S_t = \\frac{\\text{WeightedSum}_n}{\\text{WeightedCount}_n}$$\n\n其中，$$\\text{WeightedSum}_n = Y_t + (1-\\alpha) \\text{WeightedSum}_{n-1}$$\n$$\\text{WeightedCount}_n = 1 + (1-\\alpha) \\text{WeightedCount}_{n-1}$$\n\n而Caffe中BN的实现中，`blobs_[0]`和`blobs_[1]`中存储的实际是$\\text{WeightedSum}\\_n$，而`blos_[2]`中存储的是$\\text{WeightedCount}\\_n$。所以，真正的mean和var是两者相除的结果。即：\n```\nmu = blobs_[0] / blobs_[2]\nvar = blobs_[1] / blobs_[2]\n```\n\n### Forward\n下面是Forward CPU的代码。主要应该注意当前batch的mean和var的求法。\n``` cpp\ntemplate <typename Dtype>\nvoid BatchNormLayer<Dtype>::Forward_cpu(const vector<Blob<Dtype>*>& bottom,\n    const vector<Blob<Dtype>*>& top) {\n  const Dtype* bottom_data = bottom[0]->cpu_data();\n  Dtype* top_data = top[0]->mutable_cpu_data();\n  int num = bottom[0]->shape(0);\n  int spatial_dim = bottom[0]->count()/(bottom[0]->shape(0)*channels_);\n\n  // 如果不是就地操作，首先将bottom的数据复制到top\n  if (bottom[0] != top[0]) {\n    caffe_copy(bottom[0]->count(), bottom_data, top_data);\n  }\n\n  // 如果使用全局统计量，我们需要先计算出真正的mean和var\n  if (use_global_stats_) {\n    // use the stored mean/variance estimates.\n    const Dtype scale_factor = this->blobs_[2]->cpu_data()[0] == 0 ?\n        0 : 1 / this->blobs_[2]->cpu_data()[0];\n    // mean = blobs[0] / blobs[2]\n    caffe_cpu_scale(variance_.count(), scale_factor,\n        this->blobs_[0]->cpu_data(), mean_.mutable_cpu_data());\n    // var = blobs[1] / blobs[2]\n    caffe_cpu_scale(variance_.count(), scale_factor,\n        this->blobs_[1]->cpu_data(), variance_.mutable_cpu_data());\n  } else {\n    // 不使用全局统计量时，我们要根据当前batch的mean和var做规范化\n    // compute mean\n    // spatial_sum_multiplier_是全1向量\n    // batch_sum_multiplier_也是全1向量\n    // gemv做矩阵与向量相乘 y = alpha*A*x + beta*y。\n    // 下面式子是将bottom_data这个矩阵与一个全1向量相乘，\n    // 相当于是在统计行和。\n    // 注意第二个参数channels_ * num指矩阵的行数，第三个参数是矩阵的列数\n    // 所以这是在计算每个channel的feature map的和\n    // 结果out[n][c]是指输入第n个sample的第c个channel的和\n    // 同时，传入了 1. / (num * spatial_dim) 作为因子乘到结果上面，作用见下面\n    caffe_cpu_gemv<Dtype>(CblasNoTrans, channels_ * num, spatial_dim,\n        1. / (num * spatial_dim), bottom_data,\n        spatial_sum_multiplier_.cpu_data(), 0.,\n        num_by_chans_.mutable_cpu_data());\n    // 道理和上面相同，注意下面通过传入CblasTrans，指定了矩阵要转置。所以是在求列和\n    // 这样，就求出了各个channel的和。\n    // 上面不是已经除了 num * spatial_dim 吗？这就是求和元素的总数量\n    // 到此，我们就完成了对当前batch的平均值的求解\n    caffe_cpu_gemv<Dtype>(CblasTrans, num, channels_, 1.,\n        num_by_chans_.cpu_data(), batch_sum_multiplier_.cpu_data(), 0.,\n        mean_.mutable_cpu_data());\n  }\n\n  // subtract mean\n  // gemm是在做矩阵与矩阵相乘 C = alpha*A*B + beta*C\n  // 下面这个是在做broadcasting subtraction\n  caffe_cpu_gemm<Dtype>(CblasNoTrans, CblasNoTrans, num, channels_, 1, 1,\n      batch_sum_multiplier_.cpu_data(), mean_.cpu_data(), 0.,\n      num_by_chans_.mutable_cpu_data());\n  caffe_cpu_gemm<Dtype>(CblasNoTrans, CblasNoTrans, channels_ * num,\n      spatial_dim, 1, -1, num_by_chans_.cpu_data(),\n      spatial_sum_multiplier_.cpu_data(), 1., top_data);\n\n  // 计算当前的var\n  if (!use_global_stats_) {\n    // compute variance using var(X) = E((X-EX)^2)\n    caffe_sqr<Dtype>(top[0]->count(), top_data,\n                     temp_.mutable_cpu_data());  // (X-EX)^2\n    caffe_cpu_gemv<Dtype>(CblasNoTrans, channels_ * num, spatial_dim,\n        1. / (num * spatial_dim), temp_.cpu_data(),\n        spatial_sum_multiplier_.cpu_data(), 0.,\n        num_by_chans_.mutable_cpu_data());\n    caffe_cpu_gemv<Dtype>(CblasTrans, num, channels_, 1.,\n        num_by_chans_.cpu_data(), batch_sum_multiplier_.cpu_data(), 0.,\n        variance_.mutable_cpu_data());  // E((X_EX)^2)\n\n    // compute and save moving average\n    // 做滑动平均，更新全局统计量，这里可以参见上面的式子\n    this->blobs_[2]->mutable_cpu_data()[0] *= moving_average_fraction_;\n    this->blobs_[2]->mutable_cpu_data()[0] += 1;\n    caffe_cpu_axpby(mean_.count(), Dtype(1), mean_.cpu_data(),\n        moving_average_fraction_, this->blobs_[0]->mutable_cpu_data());\n    int m = bottom[0]->count()/channels_;\n    Dtype bias_correction_factor = m > 1 ? Dtype(m)/(m-1) : 1;\n    caffe_cpu_axpby(variance_.count(), bias_correction_factor,\n        variance_.cpu_data(), moving_average_fraction_,\n        this->blobs_[1]->mutable_cpu_data());\n  }\n\n  // normalize variance\n  caffe_add_scalar(variance_.count(), eps_, variance_.mutable_cpu_data());\n  caffe_sqrt(variance_.count(), variance_.cpu_data(),\n             variance_.mutable_cpu_data());\n\n  // replicate variance to input size\n  // 同样是在做broadcasting\n  caffe_cpu_gemm<Dtype>(CblasNoTrans, CblasNoTrans, num, channels_, 1, 1,\n      batch_sum_multiplier_.cpu_data(), variance_.cpu_data(), 0.,\n      num_by_chans_.mutable_cpu_data());\n  caffe_cpu_gemm<Dtype>(CblasNoTrans, CblasNoTrans, channels_ * num,\n      spatial_dim, 1, 1., num_by_chans_.cpu_data(),\n      spatial_sum_multiplier_.cpu_data(), 0., temp_.mutable_cpu_data());\n  caffe_div(temp_.count(), top_data, temp_.cpu_data(), top_data);\n  // TODO(cdoersch): The caching is only needed because later in-place layers\n  //                 might clobber the data.  Can we skip this if they won't?\n  caffe_copy(x_norm_.count(), top_data,\n      x_norm_.mutable_cpu_data());\n}\n```\n由上面的计算过程不难得出，当经过很多轮迭代之后，`blobs_[2]`的值会趋于稳定。下面我们使用$m\\_t$来表示第$t$轮迭代后的`blobs_[2]`的值，也就是$\\text{WeightedCount}\\_n$，使用$\\alpha$表示`moving_average_fraction_`，那么我们有：\n\n$$m_t = 1 + \\alpha m_{t-1}$$\n\n可以求取$m\\_t$的通项后令$t=\\infty$，可以得到，$m\\_{\\infty}=\\frac{1}{1-\\alpha}$。\n\n### Backward\n在做BP的时候，我们需要分情况讨论。\n\n- 当`use_global_stats == true`的时候，BN所做的操作是一个线性变换\n$$BN(x) = \\frac{x-\\mu}{\\sqrt{Var}}$$\n所以\n$$\\frac{\\partial L}{\\partial x} = \\frac{1}{\\sqrt{Var}}\\frac{\\partial L}{\\partial y}$$\n\n对应的代码如下。其中，`temp_`是broadcasting之后的输入`x`的标准差（见上面`Forward`部分的代码最后），做逐元素的除法即可。\n``` cpp\nif (use_global_stats_) {\n  caffe_div(temp_.count(), top_diff, temp_.cpu_data(), bottom_diff);\n  return;\n}\n```\n\n- 当`use_global_stats == false`的时候，BN所做操作虽然也是上述线性变换。但是注意，现在式子里面的$\\mu$和$Var(x)$都是当前batch计算出来的，也就是它们都是输入`x`的函数。所以就麻烦了不少。这里我并没有推导，而是看了[这篇博客](https://kevinzakka.github.io/2016/09/14/batch_normalization/)，里面有详细的推导过程，写的很易懂。我将最后的结果贴在下面，对计算过程感兴趣的可以去原文章查看。\n![BP的推导结果](/img/caffe_bn_bp_of_bn.jpg)\n\n我们使用$y$来代替上面的$\\hat{x_i}$，并且上下同时除以$m$，就可以得到Caffe BN代码中所给的BP式子：\n$$\\frac{\\partial f}{\\partial x_i} = \\frac{\\frac{\\partial f}{\\partial y}-E[\\frac{\\partial f}{\\partial y}]-yE[\\frac{\\partial f}{\\partial y}y]}{\\sqrt{\\sigma^2+\\epsilon}}$$\n``` cpp\n  // if Y = (X-mean(X))/(sqrt(var(X)+eps)), then\n  //\n  // dE(Y)/dX =\n  //   (dE/dY - mean(dE/dY) - mean(dE/dY \\cdot Y) \\cdot Y)\n  //     ./ sqrt(var(X) + eps)\n  //\n  // where \\cdot and ./ are hadamard product and elementwise division,\n  // respectively, dE/dY is the top diff, and mean/var/sum are all computed\n  // along all dimensions except the channels dimension.  In the above\n  // equation, the operations allow for expansion (i.e. broadcast) along all\n  // dimensions except the channels dimension where required.\n```\n下面的代码部分就是实现上面这个式子的内容，注释很详细，要解决的一个比较棘手的问题就是broadcasting，这个有兴趣可以看一下。对Caffe中BN的介绍就到这里。下面介绍与BN经常成对出现的Scale层。\n\n## Scale层的实现\nCaffe中将后续的线性变换使用单独的Scale层实现。Caffe中的Scale可以根据需要配置成不同的模式：\n- 当输入blob为两个时，计算输入blob的逐元素乘的结果（维度不相同时，第二个blob可以做broadcasting）。\n- 当输入blob为一个时，计算输入blob与一个可学习参数`gamma`的按元素相乘结果。\n- 当设置`bias_term: true`时，添加一个偏置项。\n\n用于BN的线性变换的计算方法很直接，这里不再多说了。\n","source":"_posts/caffe-batch-norm.md","raw":"---\ntitle: Caffe中的BatchNorm实现\ndate: 2018-01-08 20:12:44\ntags:\n     - cafe\n---\n这篇博客总结了Caffe中BN的实现。\n<!-- more -->\n\n## BN简介\n\n由于BN技术已经有很广泛的应用，所以这里只对BN做一个简单的介绍。\n\nBN是Batch Normalization的简称，来源于Google研究人员的论文：[Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](https://arxiv.org/pdf/1502.03167.pdf)。对于网络的输入层，我们可以采用减去均值除以方差的方法进行归一化，对于网络中间层，BN可以实现类似的功能。\n\n在BN层中，训练时，会对输入blob各个channel的均值和方差做一统计。在做inference的时候，我们就可以利用均值和方法，对输入$x$做如下的归一化操作。其中，$\\epsilon$是为了防止除数是$0$，$i$是channel的index。\n$$\\hat{x_i} = \\frac{x_i-\\mu_i}{\\sqrt{Var(x_i)+\\epsilon}}$$\n\n不过如果只是做如上的操作，会影响模型的表达能力。例如，Identity Map($y = x$)就不能表示了。所以，作者提出还需要在后面添加一个线性变换，如下所示。其中，$\\gamma$和$\\beta$都是待学习的参数，使用梯度下降进行更新。BN的最终输出就是$y$。\n$$y_i = \\gamma \\hat{x_i} + \\beta$$\n\n如下图所示，展示了BN变换的过程。\n![BN变换](/img/caffe_bn_what_is_bn.jpg)\n\n上面，我们讲的还是inference时候BN变换是什么样子的。那么，训练时候，BN是如何估计样本均值和方差的呢？下面，结合Caffe的代码进行梳理。\n\n## BN in Caffe\n在BVLC的Caffe实现中，BN层需要和Scale层配合使用。在这里，BN层专门用来做“Normalization”操作（确实是人如其名了），而后续的线性变换层，交给Scale层去做。\n\n下面的这段代码取自He Kaiming的Residual Net50的[模型定义文件](https://github.com/KaimingHe/deep-residual-networks/blob/master/prototxt/ResNet-50-deploy.prototxt#L21)。在这里，设置`batch_norm_param`中`use_global_stats`为`true`，是指在inference阶段，我们只使用已经得到的均值和方差统计量，进行归一化处理，而不再更新这两个统计量。后面Scale层设置的`bias_term: true`是不可省略的。这个选项将其配置为线性变换层。\n\n```\nlayer {\n\tbottom: \"conv1\"\n\ttop: \"conv1\"\n\tname: \"bn_conv1\"\n\ttype: \"BatchNorm\"\n\tbatch_norm_param {\n\t\tuse_global_stats: true\n\t}\n}\n\nlayer {\n\tbottom: \"conv1\"\n\ttop: \"conv1\"\n\tname: \"scale_conv1\"\n\ttype: \"Scale\"\n\tscale_param {\n\t\tbias_term: true\n\t}\n}\n```\n这就是Caffe中BN层的固定搭配方法。这里只是简单提到，具体参数的意义待我们深入代码可以分析。\n\n## BatchNorm 层的实现\n上面说过，Caffe中的BN层与原始论文稍有不同，只是做了输入的归一化，而后续的线性变换是交由后续的Scale层实现的。\n### proto定义的相关参数\n我们首先看一下`caffe.proto`中关于BN层参数的描述。保留了原始的英文注释，并添加了中文解释。\n```\nmessage BatchNormParameter {\n  // If false, normalization is performed over the current mini-batch\n  // and global statistics are accumulated (but not yet used) by a moving\n  // average.\n  // If true, those accumulated mean and variance values are used for the\n  // normalization.\n  // By default, it is set to false when the network is in the training\n  // phase and true when the network is in the testing phase.\n  // 设置为False的话，更新全局统计量，对当前的mini-batch进行规范化时，不使用全局统计量，而是\n  // 当前batch的均值和方差。\n  // 设置为True，使用全局统计量做规范化。\n  // 后面在BN的实现代码我们会看到，这个变量默认随着当前网络在train或test phase而变化。\n  // 当train时为false，当test时为true。\n  optional bool use_global_stats = 1;\n  \n  // What fraction of the moving average remains each iteration?\n  // Smaller values make the moving average decay faster, giving more\n  // weight to the recent values.\n  // Each iteration updates the moving average @f$S_{t-1}@f$ with the\n  // current mean @f$ Y_t @f$ by\n  // @f$ S_t = (1-\\beta)Y_t + \\beta \\cdot S_{t-1} @f$, where @f$ \\beta @f$\n  // is the moving_average_fraction parameter.\n  // BN在统计全局均值和方差信息时，使用的是滑动平均法，也就是\n  // St = (1-beta)*Yt + beta*S_{t-1}\n  // 其中St为当前估计出来的全局统计量（均值或方差），Yt为当前batch的均值或方差\n  // beta是滑动因子。其实这是一种很常见的平滑滤波的方法。\n  optional float moving_average_fraction = 2 [default = .999];\n  \n  // Small value to add to the variance estimate so that we don't divide by\n  // zero.\n  // 防止除数为0加上去的eps\n  optional float eps = 3 [default = 1e-5];\n}\n```\n\nOK。现在可以进入BN的代码实现了。阅读大部分代码都没有什么难度，下面主要结合代码讲解`use_global_stats`变量的作用和均值（方差同理）的计算。由于均值和方差的计算原理相近，所以下面只会详细介绍均值的计算。\n\n### SetUp\nBN层的SetUp代码如下。首先，会根据当前处于train还是test决定是否使用全局的统计量。如果prototxt文件中设置了`use_global_stats`标志，则会使用用户给定的配置。所以一般在使用BN时，无需对`use_global_stats`进行配置。\n\n这里有一个地方容易迷惑。BN中要对样本的均值和方差进行统计，即我们需要两个blob来存储。但是从下面的代码可以看到，BN一共有3个blob作为参数。这里做一解释，主要参考了wiki的[moving average条目](https://wiki2.org/en/Moving_average)。\n\n``` cpp\ntemplate <typename Dtype>\nvoid BatchNormLayer<Dtype>::LayerSetUp(const vector<Blob<Dtype>*>& bottom,\n      const vector<Blob<Dtype>*>& top) {\n  BatchNormParameter param = this->layer_param_.batch_norm_param();\n  moving_average_fraction_ = param.moving_average_fraction();\n  // 默认根据当前是否处在TEST模式而决定是否使用全局mean和var\n  use_global_stats_ = this->phase_ == TEST;\n  if (param.has_use_global_stats())\n    use_global_stats_ = param.use_global_stats();\n  // 得到channels数量\n  // 为了防止越界，首先检查输入是否为1D\n  if (bottom[0]->num_axes() == 1)\n    channels_ = 1;\n  else\n    channels_ = bottom[0]->shape(1);\n  eps_ = param.eps();\n  if (this->blobs_.size() > 0) {\n    LOG(INFO) << \"Skipping parameter initialization\";\n  } else {\n    // 参数共3个\n    this->blobs_.resize(3);\n    vector<int> sz;\n    sz.push_back(channels_);\n    // mean 和var都是1D的长度为channels的向量\n    // 因为在规范化过程中，要逐channel进行，即：\n    // for c in range(channels):\n    //     x_hat[c] = (x[c] - mean[c]) / std[c]\n    this->blobs_[0].reset(new Blob<Dtype>(sz));\n    this->blobs_[1].reset(new Blob<Dtype>(sz));\n    // 这里的解释见下\n    sz[0] = 1;\n    this->blobs_[2].reset(new Blob<Dtype>(sz));\n    for (int i = 0; i < 3; ++i) {\n      caffe_set(this->blobs_[i]->count(), Dtype(0),\n                this->blobs_[i]->mutable_cpu_data());\n    }\n  }\n  // Mask statistics from optimization by setting local learning rates\n  // for mean, variance, and the bias correction to zero.\n  // mean 和 std在训练的时候是不需要梯度下降来更新的，这里强制把其learning rate\n  // 设置为0\n  for (int i = 0; i < this->blobs_.size(); ++i) {\n    if (this->layer_param_.param_size() == i) {\n      ParamSpec* fixed_param_spec = this->layer_param_.add_param();\n      fixed_param_spec->set_lr_mult(0.f);\n    } else {\n      CHECK_EQ(this->layer_param_.param(i).lr_mult(), 0.f)\n          << \"Cannot configure batch normalization statistics as layer \"\n          << \"parameters.\";\n    }\n  }\n}\n```\n\n在求取某个流数据（stream）的平均值的时候，常用的一种方法是滑动平均法，也就是使用系数$\\alpha$来做平滑滤波，如下所示：\n$$S_t = \\alpha Y_t + (1-\\alpha) S_{t-1}$$\n\n上面的式子等价于：\n$$S_t = \\frac{\\text{WeightedSum}_n}{\\text{WeightedCount}_n}$$\n\n其中，$$\\text{WeightedSum}_n = Y_t + (1-\\alpha) \\text{WeightedSum}_{n-1}$$\n$$\\text{WeightedCount}_n = 1 + (1-\\alpha) \\text{WeightedCount}_{n-1}$$\n\n而Caffe中BN的实现中，`blobs_[0]`和`blobs_[1]`中存储的实际是$\\text{WeightedSum}\\_n$，而`blos_[2]`中存储的是$\\text{WeightedCount}\\_n$。所以，真正的mean和var是两者相除的结果。即：\n```\nmu = blobs_[0] / blobs_[2]\nvar = blobs_[1] / blobs_[2]\n```\n\n### Forward\n下面是Forward CPU的代码。主要应该注意当前batch的mean和var的求法。\n``` cpp\ntemplate <typename Dtype>\nvoid BatchNormLayer<Dtype>::Forward_cpu(const vector<Blob<Dtype>*>& bottom,\n    const vector<Blob<Dtype>*>& top) {\n  const Dtype* bottom_data = bottom[0]->cpu_data();\n  Dtype* top_data = top[0]->mutable_cpu_data();\n  int num = bottom[0]->shape(0);\n  int spatial_dim = bottom[0]->count()/(bottom[0]->shape(0)*channels_);\n\n  // 如果不是就地操作，首先将bottom的数据复制到top\n  if (bottom[0] != top[0]) {\n    caffe_copy(bottom[0]->count(), bottom_data, top_data);\n  }\n\n  // 如果使用全局统计量，我们需要先计算出真正的mean和var\n  if (use_global_stats_) {\n    // use the stored mean/variance estimates.\n    const Dtype scale_factor = this->blobs_[2]->cpu_data()[0] == 0 ?\n        0 : 1 / this->blobs_[2]->cpu_data()[0];\n    // mean = blobs[0] / blobs[2]\n    caffe_cpu_scale(variance_.count(), scale_factor,\n        this->blobs_[0]->cpu_data(), mean_.mutable_cpu_data());\n    // var = blobs[1] / blobs[2]\n    caffe_cpu_scale(variance_.count(), scale_factor,\n        this->blobs_[1]->cpu_data(), variance_.mutable_cpu_data());\n  } else {\n    // 不使用全局统计量时，我们要根据当前batch的mean和var做规范化\n    // compute mean\n    // spatial_sum_multiplier_是全1向量\n    // batch_sum_multiplier_也是全1向量\n    // gemv做矩阵与向量相乘 y = alpha*A*x + beta*y。\n    // 下面式子是将bottom_data这个矩阵与一个全1向量相乘，\n    // 相当于是在统计行和。\n    // 注意第二个参数channels_ * num指矩阵的行数，第三个参数是矩阵的列数\n    // 所以这是在计算每个channel的feature map的和\n    // 结果out[n][c]是指输入第n个sample的第c个channel的和\n    // 同时，传入了 1. / (num * spatial_dim) 作为因子乘到结果上面，作用见下面\n    caffe_cpu_gemv<Dtype>(CblasNoTrans, channels_ * num, spatial_dim,\n        1. / (num * spatial_dim), bottom_data,\n        spatial_sum_multiplier_.cpu_data(), 0.,\n        num_by_chans_.mutable_cpu_data());\n    // 道理和上面相同，注意下面通过传入CblasTrans，指定了矩阵要转置。所以是在求列和\n    // 这样，就求出了各个channel的和。\n    // 上面不是已经除了 num * spatial_dim 吗？这就是求和元素的总数量\n    // 到此，我们就完成了对当前batch的平均值的求解\n    caffe_cpu_gemv<Dtype>(CblasTrans, num, channels_, 1.,\n        num_by_chans_.cpu_data(), batch_sum_multiplier_.cpu_data(), 0.,\n        mean_.mutable_cpu_data());\n  }\n\n  // subtract mean\n  // gemm是在做矩阵与矩阵相乘 C = alpha*A*B + beta*C\n  // 下面这个是在做broadcasting subtraction\n  caffe_cpu_gemm<Dtype>(CblasNoTrans, CblasNoTrans, num, channels_, 1, 1,\n      batch_sum_multiplier_.cpu_data(), mean_.cpu_data(), 0.,\n      num_by_chans_.mutable_cpu_data());\n  caffe_cpu_gemm<Dtype>(CblasNoTrans, CblasNoTrans, channels_ * num,\n      spatial_dim, 1, -1, num_by_chans_.cpu_data(),\n      spatial_sum_multiplier_.cpu_data(), 1., top_data);\n\n  // 计算当前的var\n  if (!use_global_stats_) {\n    // compute variance using var(X) = E((X-EX)^2)\n    caffe_sqr<Dtype>(top[0]->count(), top_data,\n                     temp_.mutable_cpu_data());  // (X-EX)^2\n    caffe_cpu_gemv<Dtype>(CblasNoTrans, channels_ * num, spatial_dim,\n        1. / (num * spatial_dim), temp_.cpu_data(),\n        spatial_sum_multiplier_.cpu_data(), 0.,\n        num_by_chans_.mutable_cpu_data());\n    caffe_cpu_gemv<Dtype>(CblasTrans, num, channels_, 1.,\n        num_by_chans_.cpu_data(), batch_sum_multiplier_.cpu_data(), 0.,\n        variance_.mutable_cpu_data());  // E((X_EX)^2)\n\n    // compute and save moving average\n    // 做滑动平均，更新全局统计量，这里可以参见上面的式子\n    this->blobs_[2]->mutable_cpu_data()[0] *= moving_average_fraction_;\n    this->blobs_[2]->mutable_cpu_data()[0] += 1;\n    caffe_cpu_axpby(mean_.count(), Dtype(1), mean_.cpu_data(),\n        moving_average_fraction_, this->blobs_[0]->mutable_cpu_data());\n    int m = bottom[0]->count()/channels_;\n    Dtype bias_correction_factor = m > 1 ? Dtype(m)/(m-1) : 1;\n    caffe_cpu_axpby(variance_.count(), bias_correction_factor,\n        variance_.cpu_data(), moving_average_fraction_,\n        this->blobs_[1]->mutable_cpu_data());\n  }\n\n  // normalize variance\n  caffe_add_scalar(variance_.count(), eps_, variance_.mutable_cpu_data());\n  caffe_sqrt(variance_.count(), variance_.cpu_data(),\n             variance_.mutable_cpu_data());\n\n  // replicate variance to input size\n  // 同样是在做broadcasting\n  caffe_cpu_gemm<Dtype>(CblasNoTrans, CblasNoTrans, num, channels_, 1, 1,\n      batch_sum_multiplier_.cpu_data(), variance_.cpu_data(), 0.,\n      num_by_chans_.mutable_cpu_data());\n  caffe_cpu_gemm<Dtype>(CblasNoTrans, CblasNoTrans, channels_ * num,\n      spatial_dim, 1, 1., num_by_chans_.cpu_data(),\n      spatial_sum_multiplier_.cpu_data(), 0., temp_.mutable_cpu_data());\n  caffe_div(temp_.count(), top_data, temp_.cpu_data(), top_data);\n  // TODO(cdoersch): The caching is only needed because later in-place layers\n  //                 might clobber the data.  Can we skip this if they won't?\n  caffe_copy(x_norm_.count(), top_data,\n      x_norm_.mutable_cpu_data());\n}\n```\n由上面的计算过程不难得出，当经过很多轮迭代之后，`blobs_[2]`的值会趋于稳定。下面我们使用$m\\_t$来表示第$t$轮迭代后的`blobs_[2]`的值，也就是$\\text{WeightedCount}\\_n$，使用$\\alpha$表示`moving_average_fraction_`，那么我们有：\n\n$$m_t = 1 + \\alpha m_{t-1}$$\n\n可以求取$m\\_t$的通项后令$t=\\infty$，可以得到，$m\\_{\\infty}=\\frac{1}{1-\\alpha}$。\n\n### Backward\n在做BP的时候，我们需要分情况讨论。\n\n- 当`use_global_stats == true`的时候，BN所做的操作是一个线性变换\n$$BN(x) = \\frac{x-\\mu}{\\sqrt{Var}}$$\n所以\n$$\\frac{\\partial L}{\\partial x} = \\frac{1}{\\sqrt{Var}}\\frac{\\partial L}{\\partial y}$$\n\n对应的代码如下。其中，`temp_`是broadcasting之后的输入`x`的标准差（见上面`Forward`部分的代码最后），做逐元素的除法即可。\n``` cpp\nif (use_global_stats_) {\n  caffe_div(temp_.count(), top_diff, temp_.cpu_data(), bottom_diff);\n  return;\n}\n```\n\n- 当`use_global_stats == false`的时候，BN所做操作虽然也是上述线性变换。但是注意，现在式子里面的$\\mu$和$Var(x)$都是当前batch计算出来的，也就是它们都是输入`x`的函数。所以就麻烦了不少。这里我并没有推导，而是看了[这篇博客](https://kevinzakka.github.io/2016/09/14/batch_normalization/)，里面有详细的推导过程，写的很易懂。我将最后的结果贴在下面，对计算过程感兴趣的可以去原文章查看。\n![BP的推导结果](/img/caffe_bn_bp_of_bn.jpg)\n\n我们使用$y$来代替上面的$\\hat{x_i}$，并且上下同时除以$m$，就可以得到Caffe BN代码中所给的BP式子：\n$$\\frac{\\partial f}{\\partial x_i} = \\frac{\\frac{\\partial f}{\\partial y}-E[\\frac{\\partial f}{\\partial y}]-yE[\\frac{\\partial f}{\\partial y}y]}{\\sqrt{\\sigma^2+\\epsilon}}$$\n``` cpp\n  // if Y = (X-mean(X))/(sqrt(var(X)+eps)), then\n  //\n  // dE(Y)/dX =\n  //   (dE/dY - mean(dE/dY) - mean(dE/dY \\cdot Y) \\cdot Y)\n  //     ./ sqrt(var(X) + eps)\n  //\n  // where \\cdot and ./ are hadamard product and elementwise division,\n  // respectively, dE/dY is the top diff, and mean/var/sum are all computed\n  // along all dimensions except the channels dimension.  In the above\n  // equation, the operations allow for expansion (i.e. broadcast) along all\n  // dimensions except the channels dimension where required.\n```\n下面的代码部分就是实现上面这个式子的内容，注释很详细，要解决的一个比较棘手的问题就是broadcasting，这个有兴趣可以看一下。对Caffe中BN的介绍就到这里。下面介绍与BN经常成对出现的Scale层。\n\n## Scale层的实现\nCaffe中将后续的线性变换使用单独的Scale层实现。Caffe中的Scale可以根据需要配置成不同的模式：\n- 当输入blob为两个时，计算输入blob的逐元素乘的结果（维度不相同时，第二个blob可以做broadcasting）。\n- 当输入blob为一个时，计算输入blob与一个可学习参数`gamma`的按元素相乘结果。\n- 当设置`bias_term: true`时，添加一个偏置项。\n\n用于BN的线性变换的计算方法很直接，这里不再多说了。\n","slug":"caffe-batch-norm","published":1,"updated":"2018-01-12T06:22:20.457Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjcu6vcra0003qu46tagrhuz3","content":"<p>这篇博客总结了Caffe中BN的实现。<br><a id=\"more\"></a></p>\n<h2 id=\"BN简介\"><a href=\"#BN简介\" class=\"headerlink\" title=\"BN简介\"></a>BN简介</h2><p>由于BN技术已经有很广泛的应用，所以这里只对BN做一个简单的介绍。</p>\n<p>BN是Batch Normalization的简称，来源于Google研究人员的论文：<a href=\"https://arxiv.org/pdf/1502.03167.pdf\" target=\"_blank\" rel=\"external\">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</a>。对于网络的输入层，我们可以采用减去均值除以方差的方法进行归一化，对于网络中间层，BN可以实现类似的功能。</p>\n<p>在BN层中，训练时，会对输入blob各个channel的均值和方差做一统计。在做inference的时候，我们就可以利用均值和方法，对输入$x$做如下的归一化操作。其中，$\\epsilon$是为了防止除数是$0$，$i$是channel的index。</p>\n<script type=\"math/tex; mode=display\">\\hat{x_i} = \\frac{x_i-\\mu_i}{\\sqrt{Var(x_i)+\\epsilon}}</script><p>不过如果只是做如上的操作，会影响模型的表达能力。例如，Identity Map($y = x$)就不能表示了。所以，作者提出还需要在后面添加一个线性变换，如下所示。其中，$\\gamma$和$\\beta$都是待学习的参数，使用梯度下降进行更新。BN的最终输出就是$y$。</p>\n<script type=\"math/tex; mode=display\">y_i = \\gamma \\hat{x_i} + \\beta</script><p>如下图所示，展示了BN变换的过程。<br><img src=\"/img/caffe_bn_what_is_bn.jpg\" alt=\"BN变换\"></p>\n<p>上面，我们讲的还是inference时候BN变换是什么样子的。那么，训练时候，BN是如何估计样本均值和方差的呢？下面，结合Caffe的代码进行梳理。</p>\n<h2 id=\"BN-in-Caffe\"><a href=\"#BN-in-Caffe\" class=\"headerlink\" title=\"BN in Caffe\"></a>BN in Caffe</h2><p>在BVLC的Caffe实现中，BN层需要和Scale层配合使用。在这里，BN层专门用来做“Normalization”操作（确实是人如其名了），而后续的线性变换层，交给Scale层去做。</p>\n<p>下面的这段代码取自He Kaiming的Residual Net50的<a href=\"https://github.com/KaimingHe/deep-residual-networks/blob/master/prototxt/ResNet-50-deploy.prototxt#L21\" target=\"_blank\" rel=\"external\">模型定义文件</a>。在这里，设置<code>batch_norm_param</code>中<code>use_global_stats</code>为<code>true</code>，是指在inference阶段，我们只使用已经得到的均值和方差统计量，进行归一化处理，而不再更新这两个统计量。后面Scale层设置的<code>bias_term: true</code>是不可省略的。这个选项将其配置为线性变换层。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div></pre></td><td class=\"code\"><pre><div class=\"line\">layer &#123;</div><div class=\"line\">\tbottom: &quot;conv1&quot;</div><div class=\"line\">\ttop: &quot;conv1&quot;</div><div class=\"line\">\tname: &quot;bn_conv1&quot;</div><div class=\"line\">\ttype: &quot;BatchNorm&quot;</div><div class=\"line\">\tbatch_norm_param &#123;</div><div class=\"line\">\t\tuse_global_stats: true</div><div class=\"line\">\t&#125;</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\">layer &#123;</div><div class=\"line\">\tbottom: &quot;conv1&quot;</div><div class=\"line\">\ttop: &quot;conv1&quot;</div><div class=\"line\">\tname: &quot;scale_conv1&quot;</div><div class=\"line\">\ttype: &quot;Scale&quot;</div><div class=\"line\">\tscale_param &#123;</div><div class=\"line\">\t\tbias_term: true</div><div class=\"line\">\t&#125;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<p>这就是Caffe中BN层的固定搭配方法。这里只是简单提到，具体参数的意义待我们深入代码可以分析。</p>\n<h2 id=\"BatchNorm-层的实现\"><a href=\"#BatchNorm-层的实现\" class=\"headerlink\" title=\"BatchNorm 层的实现\"></a>BatchNorm 层的实现</h2><p>上面说过，Caffe中的BN层与原始论文稍有不同，只是做了输入的归一化，而后续的线性变换是交由后续的Scale层实现的。</p>\n<h3 id=\"proto定义的相关参数\"><a href=\"#proto定义的相关参数\" class=\"headerlink\" title=\"proto定义的相关参数\"></a>proto定义的相关参数</h3><p>我们首先看一下<code>caffe.proto</code>中关于BN层参数的描述。保留了原始的英文注释，并添加了中文解释。<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div></pre></td><td class=\"code\"><pre><div class=\"line\">message BatchNormParameter &#123;</div><div class=\"line\">  // If false, normalization is performed over the current mini-batch</div><div class=\"line\">  // and global statistics are accumulated (but not yet used) by a moving</div><div class=\"line\">  // average.</div><div class=\"line\">  // If true, those accumulated mean and variance values are used for the</div><div class=\"line\">  // normalization.</div><div class=\"line\">  // By default, it is set to false when the network is in the training</div><div class=\"line\">  // phase and true when the network is in the testing phase.</div><div class=\"line\">  // 设置为False的话，更新全局统计量，对当前的mini-batch进行规范化时，不使用全局统计量，而是</div><div class=\"line\">  // 当前batch的均值和方差。</div><div class=\"line\">  // 设置为True，使用全局统计量做规范化。</div><div class=\"line\">  // 后面在BN的实现代码我们会看到，这个变量默认随着当前网络在train或test phase而变化。</div><div class=\"line\">  // 当train时为false，当test时为true。</div><div class=\"line\">  optional bool use_global_stats = 1;</div><div class=\"line\">  </div><div class=\"line\">  // What fraction of the moving average remains each iteration?</div><div class=\"line\">  // Smaller values make the moving average decay faster, giving more</div><div class=\"line\">  // weight to the recent values.</div><div class=\"line\">  // Each iteration updates the moving average @f$S_&#123;t-1&#125;@f$ with the</div><div class=\"line\">  // current mean @f$ Y_t @f$ by</div><div class=\"line\">  // @f$ S_t = (1-\\beta)Y_t + \\beta \\cdot S_&#123;t-1&#125; @f$, where @f$ \\beta @f$</div><div class=\"line\">  // is the moving_average_fraction parameter.</div><div class=\"line\">  // BN在统计全局均值和方差信息时，使用的是滑动平均法，也就是</div><div class=\"line\">  // St = (1-beta)*Yt + beta*S_&#123;t-1&#125;</div><div class=\"line\">  // 其中St为当前估计出来的全局统计量（均值或方差），Yt为当前batch的均值或方差</div><div class=\"line\">  // beta是滑动因子。其实这是一种很常见的平滑滤波的方法。</div><div class=\"line\">  optional float moving_average_fraction = 2 [default = .999];</div><div class=\"line\">  </div><div class=\"line\">  // Small value to add to the variance estimate so that we don&apos;t divide by</div><div class=\"line\">  // zero.</div><div class=\"line\">  // 防止除数为0加上去的eps</div><div class=\"line\">  optional float eps = 3 [default = 1e-5];</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure></p>\n<p>OK。现在可以进入BN的代码实现了。阅读大部分代码都没有什么难度，下面主要结合代码讲解<code>use_global_stats</code>变量的作用和均值（方差同理）的计算。由于均值和方差的计算原理相近，所以下面只会详细介绍均值的计算。</p>\n<h3 id=\"SetUp\"><a href=\"#SetUp\" class=\"headerlink\" title=\"SetUp\"></a>SetUp</h3><p>BN层的SetUp代码如下。首先，会根据当前处于train还是test决定是否使用全局的统计量。如果prototxt文件中设置了<code>use_global_stats</code>标志，则会使用用户给定的配置。所以一般在使用BN时，无需对<code>use_global_stats</code>进行配置。</p>\n<p>这里有一个地方容易迷惑。BN中要对样本的均值和方差进行统计，即我们需要两个blob来存储。但是从下面的代码可以看到，BN一共有3个blob作为参数。这里做一解释，主要参考了wiki的<a href=\"https://wiki2.org/en/Moving_average\" target=\"_blank\" rel=\"external\">moving average条目</a>。</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div><div class=\"line\">47</div><div class=\"line\">48</div><div class=\"line\">49</div><div class=\"line\">50</div><div class=\"line\">51</div><div class=\"line\">52</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">template</span> &lt;<span class=\"keyword\">typename</span> Dtype&gt;</div><div class=\"line\"><span class=\"keyword\">void</span> BatchNormLayer&lt;Dtype&gt;::LayerSetUp(<span class=\"keyword\">const</span> <span class=\"built_in\">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,</div><div class=\"line\">      <span class=\"keyword\">const</span> <span class=\"built_in\">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; top) &#123;</div><div class=\"line\">  BatchNormParameter param = <span class=\"keyword\">this</span>-&gt;layer_param_.batch_norm_param();</div><div class=\"line\">  moving_average_fraction_ = param.moving_average_fraction();</div><div class=\"line\">  <span class=\"comment\">// 默认根据当前是否处在TEST模式而决定是否使用全局mean和var</span></div><div class=\"line\">  use_global_stats_ = <span class=\"keyword\">this</span>-&gt;phase_ == TEST;</div><div class=\"line\">  <span class=\"keyword\">if</span> (param.has_use_global_stats())</div><div class=\"line\">    use_global_stats_ = param.use_global_stats();</div><div class=\"line\">  <span class=\"comment\">// 得到channels数量</span></div><div class=\"line\">  <span class=\"comment\">// 为了防止越界，首先检查输入是否为1D</span></div><div class=\"line\">  <span class=\"keyword\">if</span> (bottom[<span class=\"number\">0</span>]-&gt;num_axes() == <span class=\"number\">1</span>)</div><div class=\"line\">    channels_ = <span class=\"number\">1</span>;</div><div class=\"line\">  <span class=\"keyword\">else</span></div><div class=\"line\">    channels_ = bottom[<span class=\"number\">0</span>]-&gt;shape(<span class=\"number\">1</span>);</div><div class=\"line\">  eps_ = param.eps();</div><div class=\"line\">  <span class=\"keyword\">if</span> (<span class=\"keyword\">this</span>-&gt;blobs_.size() &gt; <span class=\"number\">0</span>) &#123;</div><div class=\"line\">    LOG(INFO) &lt;&lt; <span class=\"string\">\"Skipping parameter initialization\"</span>;</div><div class=\"line\">  &#125; <span class=\"keyword\">else</span> &#123;</div><div class=\"line\">    <span class=\"comment\">// 参数共3个</span></div><div class=\"line\">    <span class=\"keyword\">this</span>-&gt;blobs_.resize(<span class=\"number\">3</span>);</div><div class=\"line\">    <span class=\"built_in\">vector</span>&lt;<span class=\"keyword\">int</span>&gt; sz;</div><div class=\"line\">    sz.push_back(channels_);</div><div class=\"line\">    <span class=\"comment\">// mean 和var都是1D的长度为channels的向量</span></div><div class=\"line\">    <span class=\"comment\">// 因为在规范化过程中，要逐channel进行，即：</span></div><div class=\"line\">    <span class=\"comment\">// for c in range(channels):</span></div><div class=\"line\">    <span class=\"comment\">//     x_hat[c] = (x[c] - mean[c]) / std[c]</span></div><div class=\"line\">    <span class=\"keyword\">this</span>-&gt;blobs_[<span class=\"number\">0</span>].reset(<span class=\"keyword\">new</span> Blob&lt;Dtype&gt;(sz));</div><div class=\"line\">    <span class=\"keyword\">this</span>-&gt;blobs_[<span class=\"number\">1</span>].reset(<span class=\"keyword\">new</span> Blob&lt;Dtype&gt;(sz));</div><div class=\"line\">    <span class=\"comment\">// 这里的解释见下</span></div><div class=\"line\">    sz[<span class=\"number\">0</span>] = <span class=\"number\">1</span>;</div><div class=\"line\">    <span class=\"keyword\">this</span>-&gt;blobs_[<span class=\"number\">2</span>].reset(<span class=\"keyword\">new</span> Blob&lt;Dtype&gt;(sz));</div><div class=\"line\">    <span class=\"keyword\">for</span> (<span class=\"keyword\">int</span> i = <span class=\"number\">0</span>; i &lt; <span class=\"number\">3</span>; ++i) &#123;</div><div class=\"line\">      caffe_set(<span class=\"keyword\">this</span>-&gt;blobs_[i]-&gt;count(), Dtype(<span class=\"number\">0</span>),</div><div class=\"line\">                <span class=\"keyword\">this</span>-&gt;blobs_[i]-&gt;mutable_cpu_data());</div><div class=\"line\">    &#125;</div><div class=\"line\">  &#125;</div><div class=\"line\">  <span class=\"comment\">// Mask statistics from optimization by setting local learning rates</span></div><div class=\"line\">  <span class=\"comment\">// for mean, variance, and the bias correction to zero.</span></div><div class=\"line\">  <span class=\"comment\">// mean 和 std在训练的时候是不需要梯度下降来更新的，这里强制把其learning rate</span></div><div class=\"line\">  <span class=\"comment\">// 设置为0</span></div><div class=\"line\">  <span class=\"keyword\">for</span> (<span class=\"keyword\">int</span> i = <span class=\"number\">0</span>; i &lt; <span class=\"keyword\">this</span>-&gt;blobs_.size(); ++i) &#123;</div><div class=\"line\">    <span class=\"keyword\">if</span> (<span class=\"keyword\">this</span>-&gt;layer_param_.param_size() == i) &#123;</div><div class=\"line\">      ParamSpec* fixed_param_spec = <span class=\"keyword\">this</span>-&gt;layer_param_.add_param();</div><div class=\"line\">      fixed_param_spec-&gt;set_lr_mult(<span class=\"number\">0.f</span>);</div><div class=\"line\">    &#125; <span class=\"keyword\">else</span> &#123;</div><div class=\"line\">      CHECK_EQ(<span class=\"keyword\">this</span>-&gt;layer_param_.param(i).lr_mult(), <span class=\"number\">0.f</span>)</div><div class=\"line\">          &lt;&lt; <span class=\"string\">\"Cannot configure batch normalization statistics as layer \"</span></div><div class=\"line\">          &lt;&lt; <span class=\"string\">\"parameters.\"</span>;</div><div class=\"line\">    &#125;</div><div class=\"line\">  &#125;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<p>在求取某个流数据（stream）的平均值的时候，常用的一种方法是滑动平均法，也就是使用系数$\\alpha$来做平滑滤波，如下所示：</p>\n<script type=\"math/tex; mode=display\">S_t = \\alpha Y_t + (1-\\alpha) S_{t-1}</script><p>上面的式子等价于：</p>\n<script type=\"math/tex; mode=display\">S_t = \\frac{\\text{WeightedSum}_n}{\\text{WeightedCount}_n}</script><p>其中，<script type=\"math/tex\">\\text{WeightedSum}_n = Y_t + (1-\\alpha) \\text{WeightedSum}_{n-1}</script></p>\n<script type=\"math/tex; mode=display\">\\text{WeightedCount}_n = 1 + (1-\\alpha) \\text{WeightedCount}_{n-1}</script><p>而Caffe中BN的实现中，<code>blobs_[0]</code>和<code>blobs_[1]</code>中存储的实际是$\\text{WeightedSum}_n$，而<code>blos_[2]</code>中存储的是$\\text{WeightedCount}_n$。所以，真正的mean和var是两者相除的结果。即：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">mu = blobs_[0] / blobs_[2]</div><div class=\"line\">var = blobs_[1] / blobs_[2]</div></pre></td></tr></table></figure></p>\n<h3 id=\"Forward\"><a href=\"#Forward\" class=\"headerlink\" title=\"Forward\"></a>Forward</h3><p>下面是Forward CPU的代码。主要应该注意当前batch的mean和var的求法。<br><figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div><div class=\"line\">47</div><div class=\"line\">48</div><div class=\"line\">49</div><div class=\"line\">50</div><div class=\"line\">51</div><div class=\"line\">52</div><div class=\"line\">53</div><div class=\"line\">54</div><div class=\"line\">55</div><div class=\"line\">56</div><div class=\"line\">57</div><div class=\"line\">58</div><div class=\"line\">59</div><div class=\"line\">60</div><div class=\"line\">61</div><div class=\"line\">62</div><div class=\"line\">63</div><div class=\"line\">64</div><div class=\"line\">65</div><div class=\"line\">66</div><div class=\"line\">67</div><div class=\"line\">68</div><div class=\"line\">69</div><div class=\"line\">70</div><div class=\"line\">71</div><div class=\"line\">72</div><div class=\"line\">73</div><div class=\"line\">74</div><div class=\"line\">75</div><div class=\"line\">76</div><div class=\"line\">77</div><div class=\"line\">78</div><div class=\"line\">79</div><div class=\"line\">80</div><div class=\"line\">81</div><div class=\"line\">82</div><div class=\"line\">83</div><div class=\"line\">84</div><div class=\"line\">85</div><div class=\"line\">86</div><div class=\"line\">87</div><div class=\"line\">88</div><div class=\"line\">89</div><div class=\"line\">90</div><div class=\"line\">91</div><div class=\"line\">92</div><div class=\"line\">93</div><div class=\"line\">94</div><div class=\"line\">95</div><div class=\"line\">96</div><div class=\"line\">97</div><div class=\"line\">98</div><div class=\"line\">99</div><div class=\"line\">100</div><div class=\"line\">101</div><div class=\"line\">102</div><div class=\"line\">103</div><div class=\"line\">104</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">template</span> &lt;<span class=\"keyword\">typename</span> Dtype&gt;</div><div class=\"line\"><span class=\"keyword\">void</span> BatchNormLayer&lt;Dtype&gt;::Forward_cpu(<span class=\"keyword\">const</span> <span class=\"built_in\">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,</div><div class=\"line\">    <span class=\"keyword\">const</span> <span class=\"built_in\">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; top) &#123;</div><div class=\"line\">  <span class=\"keyword\">const</span> Dtype* bottom_data = bottom[<span class=\"number\">0</span>]-&gt;cpu_data();</div><div class=\"line\">  Dtype* top_data = top[<span class=\"number\">0</span>]-&gt;mutable_cpu_data();</div><div class=\"line\">  <span class=\"keyword\">int</span> num = bottom[<span class=\"number\">0</span>]-&gt;shape(<span class=\"number\">0</span>);</div><div class=\"line\">  <span class=\"keyword\">int</span> spatial_dim = bottom[<span class=\"number\">0</span>]-&gt;count()/(bottom[<span class=\"number\">0</span>]-&gt;shape(<span class=\"number\">0</span>)*channels_);</div><div class=\"line\"></div><div class=\"line\">  <span class=\"comment\">// 如果不是就地操作，首先将bottom的数据复制到top</span></div><div class=\"line\">  <span class=\"keyword\">if</span> (bottom[<span class=\"number\">0</span>] != top[<span class=\"number\">0</span>]) &#123;</div><div class=\"line\">    caffe_copy(bottom[<span class=\"number\">0</span>]-&gt;count(), bottom_data, top_data);</div><div class=\"line\">  &#125;</div><div class=\"line\"></div><div class=\"line\">  <span class=\"comment\">// 如果使用全局统计量，我们需要先计算出真正的mean和var</span></div><div class=\"line\">  <span class=\"keyword\">if</span> (use_global_stats_) &#123;</div><div class=\"line\">    <span class=\"comment\">// use the stored mean/variance estimates.</span></div><div class=\"line\">    <span class=\"keyword\">const</span> Dtype scale_factor = <span class=\"keyword\">this</span>-&gt;blobs_[<span class=\"number\">2</span>]-&gt;cpu_data()[<span class=\"number\">0</span>] == <span class=\"number\">0</span> ?</div><div class=\"line\">        <span class=\"number\">0</span> : <span class=\"number\">1</span> / <span class=\"keyword\">this</span>-&gt;blobs_[<span class=\"number\">2</span>]-&gt;cpu_data()[<span class=\"number\">0</span>];</div><div class=\"line\">    <span class=\"comment\">// mean = blobs[0] / blobs[2]</span></div><div class=\"line\">    caffe_cpu_scale(variance_.count(), scale_factor,</div><div class=\"line\">        <span class=\"keyword\">this</span>-&gt;blobs_[<span class=\"number\">0</span>]-&gt;cpu_data(), mean_.mutable_cpu_data());</div><div class=\"line\">    <span class=\"comment\">// var = blobs[1] / blobs[2]</span></div><div class=\"line\">    caffe_cpu_scale(variance_.count(), scale_factor,</div><div class=\"line\">        <span class=\"keyword\">this</span>-&gt;blobs_[<span class=\"number\">1</span>]-&gt;cpu_data(), variance_.mutable_cpu_data());</div><div class=\"line\">  &#125; <span class=\"keyword\">else</span> &#123;</div><div class=\"line\">    <span class=\"comment\">// 不使用全局统计量时，我们要根据当前batch的mean和var做规范化</span></div><div class=\"line\">    <span class=\"comment\">// compute mean</span></div><div class=\"line\">    <span class=\"comment\">// spatial_sum_multiplier_是全1向量</span></div><div class=\"line\">    <span class=\"comment\">// batch_sum_multiplier_也是全1向量</span></div><div class=\"line\">    <span class=\"comment\">// gemv做矩阵与向量相乘 y = alpha*A*x + beta*y。</span></div><div class=\"line\">    <span class=\"comment\">// 下面式子是将bottom_data这个矩阵与一个全1向量相乘，</span></div><div class=\"line\">    <span class=\"comment\">// 相当于是在统计行和。</span></div><div class=\"line\">    <span class=\"comment\">// 注意第二个参数channels_ * num指矩阵的行数，第三个参数是矩阵的列数</span></div><div class=\"line\">    <span class=\"comment\">// 所以这是在计算每个channel的feature map的和</span></div><div class=\"line\">    <span class=\"comment\">// 结果out[n][c]是指输入第n个sample的第c个channel的和</span></div><div class=\"line\">    <span class=\"comment\">// 同时，传入了 1. / (num * spatial_dim) 作为因子乘到结果上面，作用见下面</span></div><div class=\"line\">    caffe_cpu_gemv&lt;Dtype&gt;(CblasNoTrans, channels_ * num, spatial_dim,</div><div class=\"line\">        <span class=\"number\">1.</span> / (num * spatial_dim), bottom_data,</div><div class=\"line\">        spatial_sum_multiplier_.cpu_data(), <span class=\"number\">0.</span>,</div><div class=\"line\">        num_by_chans_.mutable_cpu_data());</div><div class=\"line\">    <span class=\"comment\">// 道理和上面相同，注意下面通过传入CblasTrans，指定了矩阵要转置。所以是在求列和</span></div><div class=\"line\">    <span class=\"comment\">// 这样，就求出了各个channel的和。</span></div><div class=\"line\">    <span class=\"comment\">// 上面不是已经除了 num * spatial_dim 吗？这就是求和元素的总数量</span></div><div class=\"line\">    <span class=\"comment\">// 到此，我们就完成了对当前batch的平均值的求解</span></div><div class=\"line\">    caffe_cpu_gemv&lt;Dtype&gt;(CblasTrans, num, channels_, <span class=\"number\">1.</span>,</div><div class=\"line\">        num_by_chans_.cpu_data(), batch_sum_multiplier_.cpu_data(), <span class=\"number\">0.</span>,</div><div class=\"line\">        mean_.mutable_cpu_data());</div><div class=\"line\">  &#125;</div><div class=\"line\"></div><div class=\"line\">  <span class=\"comment\">// subtract mean</span></div><div class=\"line\">  <span class=\"comment\">// gemm是在做矩阵与矩阵相乘 C = alpha*A*B + beta*C</span></div><div class=\"line\">  <span class=\"comment\">// 下面这个是在做broadcasting subtraction</span></div><div class=\"line\">  caffe_cpu_gemm&lt;Dtype&gt;(CblasNoTrans, CblasNoTrans, num, channels_, <span class=\"number\">1</span>, <span class=\"number\">1</span>,</div><div class=\"line\">      batch_sum_multiplier_.cpu_data(), mean_.cpu_data(), <span class=\"number\">0.</span>,</div><div class=\"line\">      num_by_chans_.mutable_cpu_data());</div><div class=\"line\">  caffe_cpu_gemm&lt;Dtype&gt;(CblasNoTrans, CblasNoTrans, channels_ * num,</div><div class=\"line\">      spatial_dim, <span class=\"number\">1</span>, <span class=\"number\">-1</span>, num_by_chans_.cpu_data(),</div><div class=\"line\">      spatial_sum_multiplier_.cpu_data(), <span class=\"number\">1.</span>, top_data);</div><div class=\"line\"></div><div class=\"line\">  <span class=\"comment\">// 计算当前的var</span></div><div class=\"line\">  <span class=\"keyword\">if</span> (!use_global_stats_) &#123;</div><div class=\"line\">    <span class=\"comment\">// compute variance using var(X) = E((X-EX)^2)</span></div><div class=\"line\">    caffe_sqr&lt;Dtype&gt;(top[<span class=\"number\">0</span>]-&gt;count(), top_data,</div><div class=\"line\">                     temp_.mutable_cpu_data());  <span class=\"comment\">// (X-EX)^2</span></div><div class=\"line\">    caffe_cpu_gemv&lt;Dtype&gt;(CblasNoTrans, channels_ * num, spatial_dim,</div><div class=\"line\">        <span class=\"number\">1.</span> / (num * spatial_dim), temp_.cpu_data(),</div><div class=\"line\">        spatial_sum_multiplier_.cpu_data(), <span class=\"number\">0.</span>,</div><div class=\"line\">        num_by_chans_.mutable_cpu_data());</div><div class=\"line\">    caffe_cpu_gemv&lt;Dtype&gt;(CblasTrans, num, channels_, <span class=\"number\">1.</span>,</div><div class=\"line\">        num_by_chans_.cpu_data(), batch_sum_multiplier_.cpu_data(), <span class=\"number\">0.</span>,</div><div class=\"line\">        variance_.mutable_cpu_data());  <span class=\"comment\">// E((X_EX)^2)</span></div><div class=\"line\"></div><div class=\"line\">    <span class=\"comment\">// compute and save moving average</span></div><div class=\"line\">    <span class=\"comment\">// 做滑动平均，更新全局统计量，这里可以参见上面的式子</span></div><div class=\"line\">    <span class=\"keyword\">this</span>-&gt;blobs_[<span class=\"number\">2</span>]-&gt;mutable_cpu_data()[<span class=\"number\">0</span>] *= moving_average_fraction_;</div><div class=\"line\">    <span class=\"keyword\">this</span>-&gt;blobs_[<span class=\"number\">2</span>]-&gt;mutable_cpu_data()[<span class=\"number\">0</span>] += <span class=\"number\">1</span>;</div><div class=\"line\">    caffe_cpu_axpby(mean_.count(), Dtype(<span class=\"number\">1</span>), mean_.cpu_data(),</div><div class=\"line\">        moving_average_fraction_, <span class=\"keyword\">this</span>-&gt;blobs_[<span class=\"number\">0</span>]-&gt;mutable_cpu_data());</div><div class=\"line\">    <span class=\"keyword\">int</span> m = bottom[<span class=\"number\">0</span>]-&gt;count()/channels_;</div><div class=\"line\">    Dtype bias_correction_factor = m &gt; <span class=\"number\">1</span> ? Dtype(m)/(m<span class=\"number\">-1</span>) : <span class=\"number\">1</span>;</div><div class=\"line\">    caffe_cpu_axpby(variance_.count(), bias_correction_factor,</div><div class=\"line\">        variance_.cpu_data(), moving_average_fraction_,</div><div class=\"line\">        <span class=\"keyword\">this</span>-&gt;blobs_[<span class=\"number\">1</span>]-&gt;mutable_cpu_data());</div><div class=\"line\">  &#125;</div><div class=\"line\"></div><div class=\"line\">  <span class=\"comment\">// normalize variance</span></div><div class=\"line\">  caffe_add_scalar(variance_.count(), eps_, variance_.mutable_cpu_data());</div><div class=\"line\">  caffe_sqrt(variance_.count(), variance_.cpu_data(),</div><div class=\"line\">             variance_.mutable_cpu_data());</div><div class=\"line\"></div><div class=\"line\">  <span class=\"comment\">// replicate variance to input size</span></div><div class=\"line\">  <span class=\"comment\">// 同样是在做broadcasting</span></div><div class=\"line\">  caffe_cpu_gemm&lt;Dtype&gt;(CblasNoTrans, CblasNoTrans, num, channels_, <span class=\"number\">1</span>, <span class=\"number\">1</span>,</div><div class=\"line\">      batch_sum_multiplier_.cpu_data(), variance_.cpu_data(), <span class=\"number\">0.</span>,</div><div class=\"line\">      num_by_chans_.mutable_cpu_data());</div><div class=\"line\">  caffe_cpu_gemm&lt;Dtype&gt;(CblasNoTrans, CblasNoTrans, channels_ * num,</div><div class=\"line\">      spatial_dim, <span class=\"number\">1</span>, <span class=\"number\">1.</span>, num_by_chans_.cpu_data(),</div><div class=\"line\">      spatial_sum_multiplier_.cpu_data(), <span class=\"number\">0.</span>, temp_.mutable_cpu_data());</div><div class=\"line\">  caffe_div(temp_.count(), top_data, temp_.cpu_data(), top_data);</div><div class=\"line\">  <span class=\"comment\">// TODO(cdoersch): The caching is only needed because later in-place layers</span></div><div class=\"line\">  <span class=\"comment\">//                 might clobber the data.  Can we skip this if they won't?</span></div><div class=\"line\">  caffe_copy(x_norm_.count(), top_data,</div><div class=\"line\">      x_norm_.mutable_cpu_data());</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure></p>\n<p>由上面的计算过程不难得出，当经过很多轮迭代之后，<code>blobs_[2]</code>的值会趋于稳定。下面我们使用$m_t$来表示第$t$轮迭代后的<code>blobs_[2]</code>的值，也就是$\\text{WeightedCount}_n$，使用$\\alpha$表示<code>moving_average_fraction_</code>，那么我们有：</p>\n<script type=\"math/tex; mode=display\">m_t = 1 + \\alpha m_{t-1}</script><p>可以求取$m_t$的通项后令$t=\\infty$，可以得到，$m_{\\infty}=\\frac{1}{1-\\alpha}$。</p>\n<h3 id=\"Backward\"><a href=\"#Backward\" class=\"headerlink\" title=\"Backward\"></a>Backward</h3><p>在做BP的时候，我们需要分情况讨论。</p>\n<ul>\n<li>当<code>use_global_stats == true</code>的时候，BN所做的操作是一个线性变换<script type=\"math/tex; mode=display\">BN(x) = \\frac{x-\\mu}{\\sqrt{Var}}</script>所以<script type=\"math/tex; mode=display\">\\frac{\\partial L}{\\partial x} = \\frac{1}{\\sqrt{Var}}\\frac{\\partial L}{\\partial y}</script></li>\n</ul>\n<p>对应的代码如下。其中，<code>temp_</code>是broadcasting之后的输入<code>x</code>的标准差（见上面<code>Forward</code>部分的代码最后），做逐元素的除法即可。<br><figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">if</span> (use_global_stats_) &#123;</div><div class=\"line\">  caffe_div(temp_.count(), top_diff, temp_.cpu_data(), bottom_diff);</div><div class=\"line\">  <span class=\"keyword\">return</span>;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure></p>\n<ul>\n<li>当<code>use_global_stats == false</code>的时候，BN所做操作虽然也是上述线性变换。但是注意，现在式子里面的$\\mu$和$Var(x)$都是当前batch计算出来的，也就是它们都是输入<code>x</code>的函数。所以就麻烦了不少。这里我并没有推导，而是看了<a href=\"https://kevinzakka.github.io/2016/09/14/batch_normalization/\" target=\"_blank\" rel=\"external\">这篇博客</a>，里面有详细的推导过程，写的很易懂。我将最后的结果贴在下面，对计算过程感兴趣的可以去原文章查看。<br><img src=\"/img/caffe_bn_bp_of_bn.jpg\" alt=\"BP的推导结果\"></li>\n</ul>\n<p>我们使用$y$来代替上面的$\\hat{x_i}$，并且上下同时除以$m$，就可以得到Caffe BN代码中所给的BP式子：</p>\n<script type=\"math/tex; mode=display\">\\frac{\\partial f}{\\partial x_i} = \\frac{\\frac{\\partial f}{\\partial y}-E[\\frac{\\partial f}{\\partial y}]-yE[\\frac{\\partial f}{\\partial y}y]}{\\sqrt{\\sigma^2+\\epsilon}}</script><figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\">// if Y = (X-mean(X))/(sqrt(var(X)+eps)), then</span></div><div class=\"line\"><span class=\"comment\">//</span></div><div class=\"line\"><span class=\"comment\">// dE(Y)/dX =</span></div><div class=\"line\"><span class=\"comment\">//   (dE/dY - mean(dE/dY) - mean(dE/dY \\cdot Y) \\cdot Y)</span></div><div class=\"line\"><span class=\"comment\">//     ./ sqrt(var(X) + eps)</span></div><div class=\"line\"><span class=\"comment\">//</span></div><div class=\"line\"><span class=\"comment\">// where \\cdot and ./ are hadamard product and elementwise division,</span></div><div class=\"line\"><span class=\"comment\">// respectively, dE/dY is the top diff, and mean/var/sum are all computed</span></div><div class=\"line\"><span class=\"comment\">// along all dimensions except the channels dimension.  In the above</span></div><div class=\"line\"><span class=\"comment\">// equation, the operations allow for expansion (i.e. broadcast) along all</span></div><div class=\"line\"><span class=\"comment\">// dimensions except the channels dimension where required.</span></div></pre></td></tr></table></figure>\n<p>下面的代码部分就是实现上面这个式子的内容，注释很详细，要解决的一个比较棘手的问题就是broadcasting，这个有兴趣可以看一下。对Caffe中BN的介绍就到这里。下面介绍与BN经常成对出现的Scale层。</p>\n<h2 id=\"Scale层的实现\"><a href=\"#Scale层的实现\" class=\"headerlink\" title=\"Scale层的实现\"></a>Scale层的实现</h2><p>Caffe中将后续的线性变换使用单独的Scale层实现。Caffe中的Scale可以根据需要配置成不同的模式：</p>\n<ul>\n<li>当输入blob为两个时，计算输入blob的逐元素乘的结果（维度不相同时，第二个blob可以做broadcasting）。</li>\n<li>当输入blob为一个时，计算输入blob与一个可学习参数<code>gamma</code>的按元素相乘结果。</li>\n<li>当设置<code>bias_term: true</code>时，添加一个偏置项。</li>\n</ul>\n<p>用于BN的线性变换的计算方法很直接，这里不再多说了。</p>\n","excerpt":"<p>这篇博客总结了Caffe中BN的实现。<br>","more":"</p>\n<h2 id=\"BN简介\"><a href=\"#BN简介\" class=\"headerlink\" title=\"BN简介\"></a>BN简介</h2><p>由于BN技术已经有很广泛的应用，所以这里只对BN做一个简单的介绍。</p>\n<p>BN是Batch Normalization的简称，来源于Google研究人员的论文：<a href=\"https://arxiv.org/pdf/1502.03167.pdf\">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</a>。对于网络的输入层，我们可以采用减去均值除以方差的方法进行归一化，对于网络中间层，BN可以实现类似的功能。</p>\n<p>在BN层中，训练时，会对输入blob各个channel的均值和方差做一统计。在做inference的时候，我们就可以利用均值和方法，对输入$x$做如下的归一化操作。其中，$\\epsilon$是为了防止除数是$0$，$i$是channel的index。</p>\n<script type=\"math/tex; mode=display\">\\hat{x_i} = \\frac{x_i-\\mu_i}{\\sqrt{Var(x_i)+\\epsilon}}</script><p>不过如果只是做如上的操作，会影响模型的表达能力。例如，Identity Map($y = x$)就不能表示了。所以，作者提出还需要在后面添加一个线性变换，如下所示。其中，$\\gamma$和$\\beta$都是待学习的参数，使用梯度下降进行更新。BN的最终输出就是$y$。</p>\n<script type=\"math/tex; mode=display\">y_i = \\gamma \\hat{x_i} + \\beta</script><p>如下图所示，展示了BN变换的过程。<br><img src=\"/img/caffe_bn_what_is_bn.jpg\" alt=\"BN变换\"></p>\n<p>上面，我们讲的还是inference时候BN变换是什么样子的。那么，训练时候，BN是如何估计样本均值和方差的呢？下面，结合Caffe的代码进行梳理。</p>\n<h2 id=\"BN-in-Caffe\"><a href=\"#BN-in-Caffe\" class=\"headerlink\" title=\"BN in Caffe\"></a>BN in Caffe</h2><p>在BVLC的Caffe实现中，BN层需要和Scale层配合使用。在这里，BN层专门用来做“Normalization”操作（确实是人如其名了），而后续的线性变换层，交给Scale层去做。</p>\n<p>下面的这段代码取自He Kaiming的Residual Net50的<a href=\"https://github.com/KaimingHe/deep-residual-networks/blob/master/prototxt/ResNet-50-deploy.prototxt#L21\">模型定义文件</a>。在这里，设置<code>batch_norm_param</code>中<code>use_global_stats</code>为<code>true</code>，是指在inference阶段，我们只使用已经得到的均值和方差统计量，进行归一化处理，而不再更新这两个统计量。后面Scale层设置的<code>bias_term: true</code>是不可省略的。这个选项将其配置为线性变换层。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div></pre></td><td class=\"code\"><pre><div class=\"line\">layer &#123;</div><div class=\"line\">\tbottom: &quot;conv1&quot;</div><div class=\"line\">\ttop: &quot;conv1&quot;</div><div class=\"line\">\tname: &quot;bn_conv1&quot;</div><div class=\"line\">\ttype: &quot;BatchNorm&quot;</div><div class=\"line\">\tbatch_norm_param &#123;</div><div class=\"line\">\t\tuse_global_stats: true</div><div class=\"line\">\t&#125;</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\">layer &#123;</div><div class=\"line\">\tbottom: &quot;conv1&quot;</div><div class=\"line\">\ttop: &quot;conv1&quot;</div><div class=\"line\">\tname: &quot;scale_conv1&quot;</div><div class=\"line\">\ttype: &quot;Scale&quot;</div><div class=\"line\">\tscale_param &#123;</div><div class=\"line\">\t\tbias_term: true</div><div class=\"line\">\t&#125;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<p>这就是Caffe中BN层的固定搭配方法。这里只是简单提到，具体参数的意义待我们深入代码可以分析。</p>\n<h2 id=\"BatchNorm-层的实现\"><a href=\"#BatchNorm-层的实现\" class=\"headerlink\" title=\"BatchNorm 层的实现\"></a>BatchNorm 层的实现</h2><p>上面说过，Caffe中的BN层与原始论文稍有不同，只是做了输入的归一化，而后续的线性变换是交由后续的Scale层实现的。</p>\n<h3 id=\"proto定义的相关参数\"><a href=\"#proto定义的相关参数\" class=\"headerlink\" title=\"proto定义的相关参数\"></a>proto定义的相关参数</h3><p>我们首先看一下<code>caffe.proto</code>中关于BN层参数的描述。保留了原始的英文注释，并添加了中文解释。<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div></pre></td><td class=\"code\"><pre><div class=\"line\">message BatchNormParameter &#123;</div><div class=\"line\">  // If false, normalization is performed over the current mini-batch</div><div class=\"line\">  // and global statistics are accumulated (but not yet used) by a moving</div><div class=\"line\">  // average.</div><div class=\"line\">  // If true, those accumulated mean and variance values are used for the</div><div class=\"line\">  // normalization.</div><div class=\"line\">  // By default, it is set to false when the network is in the training</div><div class=\"line\">  // phase and true when the network is in the testing phase.</div><div class=\"line\">  // 设置为False的话，更新全局统计量，对当前的mini-batch进行规范化时，不使用全局统计量，而是</div><div class=\"line\">  // 当前batch的均值和方差。</div><div class=\"line\">  // 设置为True，使用全局统计量做规范化。</div><div class=\"line\">  // 后面在BN的实现代码我们会看到，这个变量默认随着当前网络在train或test phase而变化。</div><div class=\"line\">  // 当train时为false，当test时为true。</div><div class=\"line\">  optional bool use_global_stats = 1;</div><div class=\"line\">  </div><div class=\"line\">  // What fraction of the moving average remains each iteration?</div><div class=\"line\">  // Smaller values make the moving average decay faster, giving more</div><div class=\"line\">  // weight to the recent values.</div><div class=\"line\">  // Each iteration updates the moving average @f$S_&#123;t-1&#125;@f$ with the</div><div class=\"line\">  // current mean @f$ Y_t @f$ by</div><div class=\"line\">  // @f$ S_t = (1-\\beta)Y_t + \\beta \\cdot S_&#123;t-1&#125; @f$, where @f$ \\beta @f$</div><div class=\"line\">  // is the moving_average_fraction parameter.</div><div class=\"line\">  // BN在统计全局均值和方差信息时，使用的是滑动平均法，也就是</div><div class=\"line\">  // St = (1-beta)*Yt + beta*S_&#123;t-1&#125;</div><div class=\"line\">  // 其中St为当前估计出来的全局统计量（均值或方差），Yt为当前batch的均值或方差</div><div class=\"line\">  // beta是滑动因子。其实这是一种很常见的平滑滤波的方法。</div><div class=\"line\">  optional float moving_average_fraction = 2 [default = .999];</div><div class=\"line\">  </div><div class=\"line\">  // Small value to add to the variance estimate so that we don&apos;t divide by</div><div class=\"line\">  // zero.</div><div class=\"line\">  // 防止除数为0加上去的eps</div><div class=\"line\">  optional float eps = 3 [default = 1e-5];</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure></p>\n<p>OK。现在可以进入BN的代码实现了。阅读大部分代码都没有什么难度，下面主要结合代码讲解<code>use_global_stats</code>变量的作用和均值（方差同理）的计算。由于均值和方差的计算原理相近，所以下面只会详细介绍均值的计算。</p>\n<h3 id=\"SetUp\"><a href=\"#SetUp\" class=\"headerlink\" title=\"SetUp\"></a>SetUp</h3><p>BN层的SetUp代码如下。首先，会根据当前处于train还是test决定是否使用全局的统计量。如果prototxt文件中设置了<code>use_global_stats</code>标志，则会使用用户给定的配置。所以一般在使用BN时，无需对<code>use_global_stats</code>进行配置。</p>\n<p>这里有一个地方容易迷惑。BN中要对样本的均值和方差进行统计，即我们需要两个blob来存储。但是从下面的代码可以看到，BN一共有3个blob作为参数。这里做一解释，主要参考了wiki的<a href=\"https://wiki2.org/en/Moving_average\">moving average条目</a>。</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div><div class=\"line\">47</div><div class=\"line\">48</div><div class=\"line\">49</div><div class=\"line\">50</div><div class=\"line\">51</div><div class=\"line\">52</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">template</span> &lt;<span class=\"keyword\">typename</span> Dtype&gt;</div><div class=\"line\"><span class=\"keyword\">void</span> BatchNormLayer&lt;Dtype&gt;::LayerSetUp(<span class=\"keyword\">const</span> <span class=\"built_in\">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,</div><div class=\"line\">      <span class=\"keyword\">const</span> <span class=\"built_in\">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; top) &#123;</div><div class=\"line\">  BatchNormParameter param = <span class=\"keyword\">this</span>-&gt;layer_param_.batch_norm_param();</div><div class=\"line\">  moving_average_fraction_ = param.moving_average_fraction();</div><div class=\"line\">  <span class=\"comment\">// 默认根据当前是否处在TEST模式而决定是否使用全局mean和var</span></div><div class=\"line\">  use_global_stats_ = <span class=\"keyword\">this</span>-&gt;phase_ == TEST;</div><div class=\"line\">  <span class=\"keyword\">if</span> (param.has_use_global_stats())</div><div class=\"line\">    use_global_stats_ = param.use_global_stats();</div><div class=\"line\">  <span class=\"comment\">// 得到channels数量</span></div><div class=\"line\">  <span class=\"comment\">// 为了防止越界，首先检查输入是否为1D</span></div><div class=\"line\">  <span class=\"keyword\">if</span> (bottom[<span class=\"number\">0</span>]-&gt;num_axes() == <span class=\"number\">1</span>)</div><div class=\"line\">    channels_ = <span class=\"number\">1</span>;</div><div class=\"line\">  <span class=\"keyword\">else</span></div><div class=\"line\">    channels_ = bottom[<span class=\"number\">0</span>]-&gt;shape(<span class=\"number\">1</span>);</div><div class=\"line\">  eps_ = param.eps();</div><div class=\"line\">  <span class=\"keyword\">if</span> (<span class=\"keyword\">this</span>-&gt;blobs_.size() &gt; <span class=\"number\">0</span>) &#123;</div><div class=\"line\">    LOG(INFO) &lt;&lt; <span class=\"string\">\"Skipping parameter initialization\"</span>;</div><div class=\"line\">  &#125; <span class=\"keyword\">else</span> &#123;</div><div class=\"line\">    <span class=\"comment\">// 参数共3个</span></div><div class=\"line\">    <span class=\"keyword\">this</span>-&gt;blobs_.resize(<span class=\"number\">3</span>);</div><div class=\"line\">    <span class=\"built_in\">vector</span>&lt;<span class=\"keyword\">int</span>&gt; sz;</div><div class=\"line\">    sz.push_back(channels_);</div><div class=\"line\">    <span class=\"comment\">// mean 和var都是1D的长度为channels的向量</span></div><div class=\"line\">    <span class=\"comment\">// 因为在规范化过程中，要逐channel进行，即：</span></div><div class=\"line\">    <span class=\"comment\">// for c in range(channels):</span></div><div class=\"line\">    <span class=\"comment\">//     x_hat[c] = (x[c] - mean[c]) / std[c]</span></div><div class=\"line\">    <span class=\"keyword\">this</span>-&gt;blobs_[<span class=\"number\">0</span>].reset(<span class=\"keyword\">new</span> Blob&lt;Dtype&gt;(sz));</div><div class=\"line\">    <span class=\"keyword\">this</span>-&gt;blobs_[<span class=\"number\">1</span>].reset(<span class=\"keyword\">new</span> Blob&lt;Dtype&gt;(sz));</div><div class=\"line\">    <span class=\"comment\">// 这里的解释见下</span></div><div class=\"line\">    sz[<span class=\"number\">0</span>] = <span class=\"number\">1</span>;</div><div class=\"line\">    <span class=\"keyword\">this</span>-&gt;blobs_[<span class=\"number\">2</span>].reset(<span class=\"keyword\">new</span> Blob&lt;Dtype&gt;(sz));</div><div class=\"line\">    <span class=\"keyword\">for</span> (<span class=\"keyword\">int</span> i = <span class=\"number\">0</span>; i &lt; <span class=\"number\">3</span>; ++i) &#123;</div><div class=\"line\">      caffe_set(<span class=\"keyword\">this</span>-&gt;blobs_[i]-&gt;count(), Dtype(<span class=\"number\">0</span>),</div><div class=\"line\">                <span class=\"keyword\">this</span>-&gt;blobs_[i]-&gt;mutable_cpu_data());</div><div class=\"line\">    &#125;</div><div class=\"line\">  &#125;</div><div class=\"line\">  <span class=\"comment\">// Mask statistics from optimization by setting local learning rates</span></div><div class=\"line\">  <span class=\"comment\">// for mean, variance, and the bias correction to zero.</span></div><div class=\"line\">  <span class=\"comment\">// mean 和 std在训练的时候是不需要梯度下降来更新的，这里强制把其learning rate</span></div><div class=\"line\">  <span class=\"comment\">// 设置为0</span></div><div class=\"line\">  <span class=\"keyword\">for</span> (<span class=\"keyword\">int</span> i = <span class=\"number\">0</span>; i &lt; <span class=\"keyword\">this</span>-&gt;blobs_.size(); ++i) &#123;</div><div class=\"line\">    <span class=\"keyword\">if</span> (<span class=\"keyword\">this</span>-&gt;layer_param_.param_size() == i) &#123;</div><div class=\"line\">      ParamSpec* fixed_param_spec = <span class=\"keyword\">this</span>-&gt;layer_param_.add_param();</div><div class=\"line\">      fixed_param_spec-&gt;set_lr_mult(<span class=\"number\">0.f</span>);</div><div class=\"line\">    &#125; <span class=\"keyword\">else</span> &#123;</div><div class=\"line\">      CHECK_EQ(<span class=\"keyword\">this</span>-&gt;layer_param_.param(i).lr_mult(), <span class=\"number\">0.f</span>)</div><div class=\"line\">          &lt;&lt; <span class=\"string\">\"Cannot configure batch normalization statistics as layer \"</span></div><div class=\"line\">          &lt;&lt; <span class=\"string\">\"parameters.\"</span>;</div><div class=\"line\">    &#125;</div><div class=\"line\">  &#125;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<p>在求取某个流数据（stream）的平均值的时候，常用的一种方法是滑动平均法，也就是使用系数$\\alpha$来做平滑滤波，如下所示：</p>\n<script type=\"math/tex; mode=display\">S_t = \\alpha Y_t + (1-\\alpha) S_{t-1}</script><p>上面的式子等价于：</p>\n<script type=\"math/tex; mode=display\">S_t = \\frac{\\text{WeightedSum}_n}{\\text{WeightedCount}_n}</script><p>其中，<script type=\"math/tex\">\\text{WeightedSum}_n = Y_t + (1-\\alpha) \\text{WeightedSum}_{n-1}</script></p>\n<script type=\"math/tex; mode=display\">\\text{WeightedCount}_n = 1 + (1-\\alpha) \\text{WeightedCount}_{n-1}</script><p>而Caffe中BN的实现中，<code>blobs_[0]</code>和<code>blobs_[1]</code>中存储的实际是$\\text{WeightedSum}_n$，而<code>blos_[2]</code>中存储的是$\\text{WeightedCount}_n$。所以，真正的mean和var是两者相除的结果。即：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">mu = blobs_[0] / blobs_[2]</div><div class=\"line\">var = blobs_[1] / blobs_[2]</div></pre></td></tr></table></figure></p>\n<h3 id=\"Forward\"><a href=\"#Forward\" class=\"headerlink\" title=\"Forward\"></a>Forward</h3><p>下面是Forward CPU的代码。主要应该注意当前batch的mean和var的求法。<br><figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div><div class=\"line\">47</div><div class=\"line\">48</div><div class=\"line\">49</div><div class=\"line\">50</div><div class=\"line\">51</div><div class=\"line\">52</div><div class=\"line\">53</div><div class=\"line\">54</div><div class=\"line\">55</div><div class=\"line\">56</div><div class=\"line\">57</div><div class=\"line\">58</div><div class=\"line\">59</div><div class=\"line\">60</div><div class=\"line\">61</div><div class=\"line\">62</div><div class=\"line\">63</div><div class=\"line\">64</div><div class=\"line\">65</div><div class=\"line\">66</div><div class=\"line\">67</div><div class=\"line\">68</div><div class=\"line\">69</div><div class=\"line\">70</div><div class=\"line\">71</div><div class=\"line\">72</div><div class=\"line\">73</div><div class=\"line\">74</div><div class=\"line\">75</div><div class=\"line\">76</div><div class=\"line\">77</div><div class=\"line\">78</div><div class=\"line\">79</div><div class=\"line\">80</div><div class=\"line\">81</div><div class=\"line\">82</div><div class=\"line\">83</div><div class=\"line\">84</div><div class=\"line\">85</div><div class=\"line\">86</div><div class=\"line\">87</div><div class=\"line\">88</div><div class=\"line\">89</div><div class=\"line\">90</div><div class=\"line\">91</div><div class=\"line\">92</div><div class=\"line\">93</div><div class=\"line\">94</div><div class=\"line\">95</div><div class=\"line\">96</div><div class=\"line\">97</div><div class=\"line\">98</div><div class=\"line\">99</div><div class=\"line\">100</div><div class=\"line\">101</div><div class=\"line\">102</div><div class=\"line\">103</div><div class=\"line\">104</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">template</span> &lt;<span class=\"keyword\">typename</span> Dtype&gt;</div><div class=\"line\"><span class=\"keyword\">void</span> BatchNormLayer&lt;Dtype&gt;::Forward_cpu(<span class=\"keyword\">const</span> <span class=\"built_in\">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,</div><div class=\"line\">    <span class=\"keyword\">const</span> <span class=\"built_in\">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; top) &#123;</div><div class=\"line\">  <span class=\"keyword\">const</span> Dtype* bottom_data = bottom[<span class=\"number\">0</span>]-&gt;cpu_data();</div><div class=\"line\">  Dtype* top_data = top[<span class=\"number\">0</span>]-&gt;mutable_cpu_data();</div><div class=\"line\">  <span class=\"keyword\">int</span> num = bottom[<span class=\"number\">0</span>]-&gt;shape(<span class=\"number\">0</span>);</div><div class=\"line\">  <span class=\"keyword\">int</span> spatial_dim = bottom[<span class=\"number\">0</span>]-&gt;count()/(bottom[<span class=\"number\">0</span>]-&gt;shape(<span class=\"number\">0</span>)*channels_);</div><div class=\"line\"></div><div class=\"line\">  <span class=\"comment\">// 如果不是就地操作，首先将bottom的数据复制到top</span></div><div class=\"line\">  <span class=\"keyword\">if</span> (bottom[<span class=\"number\">0</span>] != top[<span class=\"number\">0</span>]) &#123;</div><div class=\"line\">    caffe_copy(bottom[<span class=\"number\">0</span>]-&gt;count(), bottom_data, top_data);</div><div class=\"line\">  &#125;</div><div class=\"line\"></div><div class=\"line\">  <span class=\"comment\">// 如果使用全局统计量，我们需要先计算出真正的mean和var</span></div><div class=\"line\">  <span class=\"keyword\">if</span> (use_global_stats_) &#123;</div><div class=\"line\">    <span class=\"comment\">// use the stored mean/variance estimates.</span></div><div class=\"line\">    <span class=\"keyword\">const</span> Dtype scale_factor = <span class=\"keyword\">this</span>-&gt;blobs_[<span class=\"number\">2</span>]-&gt;cpu_data()[<span class=\"number\">0</span>] == <span class=\"number\">0</span> ?</div><div class=\"line\">        <span class=\"number\">0</span> : <span class=\"number\">1</span> / <span class=\"keyword\">this</span>-&gt;blobs_[<span class=\"number\">2</span>]-&gt;cpu_data()[<span class=\"number\">0</span>];</div><div class=\"line\">    <span class=\"comment\">// mean = blobs[0] / blobs[2]</span></div><div class=\"line\">    caffe_cpu_scale(variance_.count(), scale_factor,</div><div class=\"line\">        <span class=\"keyword\">this</span>-&gt;blobs_[<span class=\"number\">0</span>]-&gt;cpu_data(), mean_.mutable_cpu_data());</div><div class=\"line\">    <span class=\"comment\">// var = blobs[1] / blobs[2]</span></div><div class=\"line\">    caffe_cpu_scale(variance_.count(), scale_factor,</div><div class=\"line\">        <span class=\"keyword\">this</span>-&gt;blobs_[<span class=\"number\">1</span>]-&gt;cpu_data(), variance_.mutable_cpu_data());</div><div class=\"line\">  &#125; <span class=\"keyword\">else</span> &#123;</div><div class=\"line\">    <span class=\"comment\">// 不使用全局统计量时，我们要根据当前batch的mean和var做规范化</span></div><div class=\"line\">    <span class=\"comment\">// compute mean</span></div><div class=\"line\">    <span class=\"comment\">// spatial_sum_multiplier_是全1向量</span></div><div class=\"line\">    <span class=\"comment\">// batch_sum_multiplier_也是全1向量</span></div><div class=\"line\">    <span class=\"comment\">// gemv做矩阵与向量相乘 y = alpha*A*x + beta*y。</span></div><div class=\"line\">    <span class=\"comment\">// 下面式子是将bottom_data这个矩阵与一个全1向量相乘，</span></div><div class=\"line\">    <span class=\"comment\">// 相当于是在统计行和。</span></div><div class=\"line\">    <span class=\"comment\">// 注意第二个参数channels_ * num指矩阵的行数，第三个参数是矩阵的列数</span></div><div class=\"line\">    <span class=\"comment\">// 所以这是在计算每个channel的feature map的和</span></div><div class=\"line\">    <span class=\"comment\">// 结果out[n][c]是指输入第n个sample的第c个channel的和</span></div><div class=\"line\">    <span class=\"comment\">// 同时，传入了 1. / (num * spatial_dim) 作为因子乘到结果上面，作用见下面</span></div><div class=\"line\">    caffe_cpu_gemv&lt;Dtype&gt;(CblasNoTrans, channels_ * num, spatial_dim,</div><div class=\"line\">        <span class=\"number\">1.</span> / (num * spatial_dim), bottom_data,</div><div class=\"line\">        spatial_sum_multiplier_.cpu_data(), <span class=\"number\">0.</span>,</div><div class=\"line\">        num_by_chans_.mutable_cpu_data());</div><div class=\"line\">    <span class=\"comment\">// 道理和上面相同，注意下面通过传入CblasTrans，指定了矩阵要转置。所以是在求列和</span></div><div class=\"line\">    <span class=\"comment\">// 这样，就求出了各个channel的和。</span></div><div class=\"line\">    <span class=\"comment\">// 上面不是已经除了 num * spatial_dim 吗？这就是求和元素的总数量</span></div><div class=\"line\">    <span class=\"comment\">// 到此，我们就完成了对当前batch的平均值的求解</span></div><div class=\"line\">    caffe_cpu_gemv&lt;Dtype&gt;(CblasTrans, num, channels_, <span class=\"number\">1.</span>,</div><div class=\"line\">        num_by_chans_.cpu_data(), batch_sum_multiplier_.cpu_data(), <span class=\"number\">0.</span>,</div><div class=\"line\">        mean_.mutable_cpu_data());</div><div class=\"line\">  &#125;</div><div class=\"line\"></div><div class=\"line\">  <span class=\"comment\">// subtract mean</span></div><div class=\"line\">  <span class=\"comment\">// gemm是在做矩阵与矩阵相乘 C = alpha*A*B + beta*C</span></div><div class=\"line\">  <span class=\"comment\">// 下面这个是在做broadcasting subtraction</span></div><div class=\"line\">  caffe_cpu_gemm&lt;Dtype&gt;(CblasNoTrans, CblasNoTrans, num, channels_, <span class=\"number\">1</span>, <span class=\"number\">1</span>,</div><div class=\"line\">      batch_sum_multiplier_.cpu_data(), mean_.cpu_data(), <span class=\"number\">0.</span>,</div><div class=\"line\">      num_by_chans_.mutable_cpu_data());</div><div class=\"line\">  caffe_cpu_gemm&lt;Dtype&gt;(CblasNoTrans, CblasNoTrans, channels_ * num,</div><div class=\"line\">      spatial_dim, <span class=\"number\">1</span>, <span class=\"number\">-1</span>, num_by_chans_.cpu_data(),</div><div class=\"line\">      spatial_sum_multiplier_.cpu_data(), <span class=\"number\">1.</span>, top_data);</div><div class=\"line\"></div><div class=\"line\">  <span class=\"comment\">// 计算当前的var</span></div><div class=\"line\">  <span class=\"keyword\">if</span> (!use_global_stats_) &#123;</div><div class=\"line\">    <span class=\"comment\">// compute variance using var(X) = E((X-EX)^2)</span></div><div class=\"line\">    caffe_sqr&lt;Dtype&gt;(top[<span class=\"number\">0</span>]-&gt;count(), top_data,</div><div class=\"line\">                     temp_.mutable_cpu_data());  <span class=\"comment\">// (X-EX)^2</span></div><div class=\"line\">    caffe_cpu_gemv&lt;Dtype&gt;(CblasNoTrans, channels_ * num, spatial_dim,</div><div class=\"line\">        <span class=\"number\">1.</span> / (num * spatial_dim), temp_.cpu_data(),</div><div class=\"line\">        spatial_sum_multiplier_.cpu_data(), <span class=\"number\">0.</span>,</div><div class=\"line\">        num_by_chans_.mutable_cpu_data());</div><div class=\"line\">    caffe_cpu_gemv&lt;Dtype&gt;(CblasTrans, num, channels_, <span class=\"number\">1.</span>,</div><div class=\"line\">        num_by_chans_.cpu_data(), batch_sum_multiplier_.cpu_data(), <span class=\"number\">0.</span>,</div><div class=\"line\">        variance_.mutable_cpu_data());  <span class=\"comment\">// E((X_EX)^2)</span></div><div class=\"line\"></div><div class=\"line\">    <span class=\"comment\">// compute and save moving average</span></div><div class=\"line\">    <span class=\"comment\">// 做滑动平均，更新全局统计量，这里可以参见上面的式子</span></div><div class=\"line\">    <span class=\"keyword\">this</span>-&gt;blobs_[<span class=\"number\">2</span>]-&gt;mutable_cpu_data()[<span class=\"number\">0</span>] *= moving_average_fraction_;</div><div class=\"line\">    <span class=\"keyword\">this</span>-&gt;blobs_[<span class=\"number\">2</span>]-&gt;mutable_cpu_data()[<span class=\"number\">0</span>] += <span class=\"number\">1</span>;</div><div class=\"line\">    caffe_cpu_axpby(mean_.count(), Dtype(<span class=\"number\">1</span>), mean_.cpu_data(),</div><div class=\"line\">        moving_average_fraction_, <span class=\"keyword\">this</span>-&gt;blobs_[<span class=\"number\">0</span>]-&gt;mutable_cpu_data());</div><div class=\"line\">    <span class=\"keyword\">int</span> m = bottom[<span class=\"number\">0</span>]-&gt;count()/channels_;</div><div class=\"line\">    Dtype bias_correction_factor = m &gt; <span class=\"number\">1</span> ? Dtype(m)/(m<span class=\"number\">-1</span>) : <span class=\"number\">1</span>;</div><div class=\"line\">    caffe_cpu_axpby(variance_.count(), bias_correction_factor,</div><div class=\"line\">        variance_.cpu_data(), moving_average_fraction_,</div><div class=\"line\">        <span class=\"keyword\">this</span>-&gt;blobs_[<span class=\"number\">1</span>]-&gt;mutable_cpu_data());</div><div class=\"line\">  &#125;</div><div class=\"line\"></div><div class=\"line\">  <span class=\"comment\">// normalize variance</span></div><div class=\"line\">  caffe_add_scalar(variance_.count(), eps_, variance_.mutable_cpu_data());</div><div class=\"line\">  caffe_sqrt(variance_.count(), variance_.cpu_data(),</div><div class=\"line\">             variance_.mutable_cpu_data());</div><div class=\"line\"></div><div class=\"line\">  <span class=\"comment\">// replicate variance to input size</span></div><div class=\"line\">  <span class=\"comment\">// 同样是在做broadcasting</span></div><div class=\"line\">  caffe_cpu_gemm&lt;Dtype&gt;(CblasNoTrans, CblasNoTrans, num, channels_, <span class=\"number\">1</span>, <span class=\"number\">1</span>,</div><div class=\"line\">      batch_sum_multiplier_.cpu_data(), variance_.cpu_data(), <span class=\"number\">0.</span>,</div><div class=\"line\">      num_by_chans_.mutable_cpu_data());</div><div class=\"line\">  caffe_cpu_gemm&lt;Dtype&gt;(CblasNoTrans, CblasNoTrans, channels_ * num,</div><div class=\"line\">      spatial_dim, <span class=\"number\">1</span>, <span class=\"number\">1.</span>, num_by_chans_.cpu_data(),</div><div class=\"line\">      spatial_sum_multiplier_.cpu_data(), <span class=\"number\">0.</span>, temp_.mutable_cpu_data());</div><div class=\"line\">  caffe_div(temp_.count(), top_data, temp_.cpu_data(), top_data);</div><div class=\"line\">  <span class=\"comment\">// TODO(cdoersch): The caching is only needed because later in-place layers</span></div><div class=\"line\">  <span class=\"comment\">//                 might clobber the data.  Can we skip this if they won't?</span></div><div class=\"line\">  caffe_copy(x_norm_.count(), top_data,</div><div class=\"line\">      x_norm_.mutable_cpu_data());</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure></p>\n<p>由上面的计算过程不难得出，当经过很多轮迭代之后，<code>blobs_[2]</code>的值会趋于稳定。下面我们使用$m_t$来表示第$t$轮迭代后的<code>blobs_[2]</code>的值，也就是$\\text{WeightedCount}_n$，使用$\\alpha$表示<code>moving_average_fraction_</code>，那么我们有：</p>\n<script type=\"math/tex; mode=display\">m_t = 1 + \\alpha m_{t-1}</script><p>可以求取$m_t$的通项后令$t=\\infty$，可以得到，$m_{\\infty}=\\frac{1}{1-\\alpha}$。</p>\n<h3 id=\"Backward\"><a href=\"#Backward\" class=\"headerlink\" title=\"Backward\"></a>Backward</h3><p>在做BP的时候，我们需要分情况讨论。</p>\n<ul>\n<li>当<code>use_global_stats == true</code>的时候，BN所做的操作是一个线性变换<script type=\"math/tex; mode=display\">BN(x) = \\frac{x-\\mu}{\\sqrt{Var}}</script>所以<script type=\"math/tex; mode=display\">\\frac{\\partial L}{\\partial x} = \\frac{1}{\\sqrt{Var}}\\frac{\\partial L}{\\partial y}</script></li>\n</ul>\n<p>对应的代码如下。其中，<code>temp_</code>是broadcasting之后的输入<code>x</code>的标准差（见上面<code>Forward</code>部分的代码最后），做逐元素的除法即可。<br><figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">if</span> (use_global_stats_) &#123;</div><div class=\"line\">  caffe_div(temp_.count(), top_diff, temp_.cpu_data(), bottom_diff);</div><div class=\"line\">  <span class=\"keyword\">return</span>;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure></p>\n<ul>\n<li>当<code>use_global_stats == false</code>的时候，BN所做操作虽然也是上述线性变换。但是注意，现在式子里面的$\\mu$和$Var(x)$都是当前batch计算出来的，也就是它们都是输入<code>x</code>的函数。所以就麻烦了不少。这里我并没有推导，而是看了<a href=\"https://kevinzakka.github.io/2016/09/14/batch_normalization/\">这篇博客</a>，里面有详细的推导过程，写的很易懂。我将最后的结果贴在下面，对计算过程感兴趣的可以去原文章查看。<br><img src=\"/img/caffe_bn_bp_of_bn.jpg\" alt=\"BP的推导结果\"></li>\n</ul>\n<p>我们使用$y$来代替上面的$\\hat{x_i}$，并且上下同时除以$m$，就可以得到Caffe BN代码中所给的BP式子：</p>\n<script type=\"math/tex; mode=display\">\\frac{\\partial f}{\\partial x_i} = \\frac{\\frac{\\partial f}{\\partial y}-E[\\frac{\\partial f}{\\partial y}]-yE[\\frac{\\partial f}{\\partial y}y]}{\\sqrt{\\sigma^2+\\epsilon}}</script><figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\">// if Y = (X-mean(X))/(sqrt(var(X)+eps)), then</span></div><div class=\"line\"><span class=\"comment\">//</span></div><div class=\"line\"><span class=\"comment\">// dE(Y)/dX =</span></div><div class=\"line\"><span class=\"comment\">//   (dE/dY - mean(dE/dY) - mean(dE/dY \\cdot Y) \\cdot Y)</span></div><div class=\"line\"><span class=\"comment\">//     ./ sqrt(var(X) + eps)</span></div><div class=\"line\"><span class=\"comment\">//</span></div><div class=\"line\"><span class=\"comment\">// where \\cdot and ./ are hadamard product and elementwise division,</span></div><div class=\"line\"><span class=\"comment\">// respectively, dE/dY is the top diff, and mean/var/sum are all computed</span></div><div class=\"line\"><span class=\"comment\">// along all dimensions except the channels dimension.  In the above</span></div><div class=\"line\"><span class=\"comment\">// equation, the operations allow for expansion (i.e. broadcast) along all</span></div><div class=\"line\"><span class=\"comment\">// dimensions except the channels dimension where required.</span></div></pre></td></tr></table></figure>\n<p>下面的代码部分就是实现上面这个式子的内容，注释很详细，要解决的一个比较棘手的问题就是broadcasting，这个有兴趣可以看一下。对Caffe中BN的介绍就到这里。下面介绍与BN经常成对出现的Scale层。</p>\n<h2 id=\"Scale层的实现\"><a href=\"#Scale层的实现\" class=\"headerlink\" title=\"Scale层的实现\"></a>Scale层的实现</h2><p>Caffe中将后续的线性变换使用单独的Scale层实现。Caffe中的Scale可以根据需要配置成不同的模式：</p>\n<ul>\n<li>当输入blob为两个时，计算输入blob的逐元素乘的结果（维度不相同时，第二个blob可以做broadcasting）。</li>\n<li>当输入blob为一个时，计算输入blob与一个可学习参数<code>gamma</code>的按元素相乘结果。</li>\n<li>当设置<code>bias_term: true</code>时，添加一个偏置项。</li>\n</ul>\n<p>用于BN的线性变换的计算方法很直接，这里不再多说了。</p>"},{"title":"Caffe 中的 SyncedMem介绍","date":"2018-01-12T06:05:59.000Z","_content":"`Blob`是Caffe中的基本数据结构，类似于TensorFlow和PyTorch中的Tensor。图像读入后作为`Blob`，开始在各个`Layer`之间传递，最终得到输出。下面这张图展示了`Blob`和`Layer`之间的关系：\n <img src=\"/img/caffe_syncedmem_blob_flow.jpg\" width = \"300\" height = \"200\" alt=\"blob的流动\" align=center />\n\nCaffe中的`Blob`在实现的时候，使用了`SyncedMem`管理内存，并在内存（Host）和显存（device）之间同步。这篇博客对Caffe中`SyncedMem`的实现做一总结。\n<!-- more -->\n\n## SyncedMem的作用\n`Blob`是一个多维的数组，可以位于内存，也可以位于显存（当使用GPU时）。一方面，我们需要对底层的内存进行管理，包括何何时开辟内存空间。另一方面，我们的训练数据常常是首先由硬盘读取到内存中，而训练又经常使用GPU，最终结果的保存或可视化又要求数据重新传回内存，所以涉及到Host和Device内存的同步问题。\n\n## 同步的实现思路\n在`SyncedMem`的实现代码中，作者使用一个枚举量`head_`来标记当前的状态。如下所示：\n\n``` cpp\n// in SyncedMem\nenum SyncedHead { UNINITIALIZED, HEAD_AT_CPU, HEAD_AT_GPU, SYNCED };\n// 使用过Git吗？ 在Git中那个标志着repo最新版本状态的变量就叫 HEAD\n// 这里也是一样，标志着最新的数据位于哪里\nSyncedHead head_;\n```\n\n这样，利用`head_`变量，就可以构建一个状态转移图，在不同状态切换时进行必要的同步操作等。\n![状态转换图](/img/caffe_syncedmem_transfer.png)\n\n## 具体实现\n`SyncedMem`的类声明如下：\n\n``` cpp\n/**\n * @brief Manages memory allocation and synchronization between the host (CPU)\n *        and device (GPU).\n *\n * TODO(dox): more thorough description.\n */\nclass SyncedMemory {\n public:\n  SyncedMemory();\n  explicit SyncedMemory(size_t size);\n  ~SyncedMemory();\n  // 获取CPU data指针\n  const void* cpu_data();\n  // 设置CPU data指针\n  void set_cpu_data(void* data);\n  // 获取GPU data指针\n  const void* gpu_data();\n  // 设置GPU data指针\n  void set_gpu_data(void* data);\n  // 获取CPU data指针，并在后续将改变指针所指向内存的值\n  void* mutable_cpu_data();\n  // 获取GPU data指针，并在后续将改变指针所指向内存的值\n  void* mutable_gpu_data();\n  // CPU 和 GPU的同步状态：未初始化，在CPU（未同步），在GPU（未同步），已同步\n  enum SyncedHead { UNINITIALIZED, HEAD_AT_CPU, HEAD_AT_GPU, SYNCED };\n  SyncedHead head() { return head_; }\n  // 内存大小\n  size_t size() { return size_; }\n\n#ifndef CPU_ONLY\n  void async_gpu_push(const cudaStream_t& stream);\n#endif\n\n private:\n  void check_device();\n\n  void to_cpu();\n  void to_gpu();\n  void* cpu_ptr_;\n  void* gpu_ptr_;\n  size_t size_;\n  SyncedHead head_;\n  bool own_cpu_data_;\n  bool cpu_malloc_use_cuda_;\n  bool own_gpu_data_;\n  // GPU设备编号\n  int device_;\n\n  DISABLE_COPY_AND_ASSIGN(SyncedMemory);\n};  // class SyncedMemory\n```\n\n我们以`to_cpu()`为例，看一下是如何在不同状态之间切换的。\n\n``` cpp\ninline void SyncedMemory::to_gpu() {\n  // 检查设备状态（使用条件编译，只在DEBUG中使能）\n  check_device();\n#ifndef CPU_ONLY\n  switch (head_) {\n  case UNINITIALIZED:\n    // 还没有初始化呢~所以内存啥的还没开\n    // 先在GPU上开块显存吧~\n    CUDA_CHECK(cudaMalloc(&gpu_ptr_, size_));\n    caffe_gpu_memset(size_, 0, gpu_ptr_);\n    // 接着，改变状态标志\n    head_ = HEAD_AT_GPU;\n    own_gpu_data_ = true;\n    break;\n  case HEAD_AT_CPU:\n    // 数据在CPU上~如果需要，先在显存上开内存\n    if (gpu_ptr_ == NULL) {\n      CUDA_CHECK(cudaMalloc(&gpu_ptr_, size_));\n      own_gpu_data_ = true;\n    }\n    // 数据拷贝\n    caffe_gpu_memcpy(size_, cpu_ptr_, gpu_ptr_);\n    // 改变状态变量\n    head_ = SYNCED;\n    break;\n  // 已经在GPU或者已经同步了，什么都不做\n  case HEAD_AT_GPU:\n  case SYNCED:\n    break;\n  }\n#else\n  // NO_GPU 是一个宏，打印FATAL ERROR日志信息\n  // 编译选项没有开GPU支持，只能说 无可奉告\n  NO_GPU;\n#endif\n}\n```\n\n","source":"_posts/caffe-syncedmem.md","raw":"---\ntitle: Caffe 中的 SyncedMem介绍\ndate: 2018-01-12 14:05:59\ntags:\n     - caffe\n---\n`Blob`是Caffe中的基本数据结构，类似于TensorFlow和PyTorch中的Tensor。图像读入后作为`Blob`，开始在各个`Layer`之间传递，最终得到输出。下面这张图展示了`Blob`和`Layer`之间的关系：\n <img src=\"/img/caffe_syncedmem_blob_flow.jpg\" width = \"300\" height = \"200\" alt=\"blob的流动\" align=center />\n\nCaffe中的`Blob`在实现的时候，使用了`SyncedMem`管理内存，并在内存（Host）和显存（device）之间同步。这篇博客对Caffe中`SyncedMem`的实现做一总结。\n<!-- more -->\n\n## SyncedMem的作用\n`Blob`是一个多维的数组，可以位于内存，也可以位于显存（当使用GPU时）。一方面，我们需要对底层的内存进行管理，包括何何时开辟内存空间。另一方面，我们的训练数据常常是首先由硬盘读取到内存中，而训练又经常使用GPU，最终结果的保存或可视化又要求数据重新传回内存，所以涉及到Host和Device内存的同步问题。\n\n## 同步的实现思路\n在`SyncedMem`的实现代码中，作者使用一个枚举量`head_`来标记当前的状态。如下所示：\n\n``` cpp\n// in SyncedMem\nenum SyncedHead { UNINITIALIZED, HEAD_AT_CPU, HEAD_AT_GPU, SYNCED };\n// 使用过Git吗？ 在Git中那个标志着repo最新版本状态的变量就叫 HEAD\n// 这里也是一样，标志着最新的数据位于哪里\nSyncedHead head_;\n```\n\n这样，利用`head_`变量，就可以构建一个状态转移图，在不同状态切换时进行必要的同步操作等。\n![状态转换图](/img/caffe_syncedmem_transfer.png)\n\n## 具体实现\n`SyncedMem`的类声明如下：\n\n``` cpp\n/**\n * @brief Manages memory allocation and synchronization between the host (CPU)\n *        and device (GPU).\n *\n * TODO(dox): more thorough description.\n */\nclass SyncedMemory {\n public:\n  SyncedMemory();\n  explicit SyncedMemory(size_t size);\n  ~SyncedMemory();\n  // 获取CPU data指针\n  const void* cpu_data();\n  // 设置CPU data指针\n  void set_cpu_data(void* data);\n  // 获取GPU data指针\n  const void* gpu_data();\n  // 设置GPU data指针\n  void set_gpu_data(void* data);\n  // 获取CPU data指针，并在后续将改变指针所指向内存的值\n  void* mutable_cpu_data();\n  // 获取GPU data指针，并在后续将改变指针所指向内存的值\n  void* mutable_gpu_data();\n  // CPU 和 GPU的同步状态：未初始化，在CPU（未同步），在GPU（未同步），已同步\n  enum SyncedHead { UNINITIALIZED, HEAD_AT_CPU, HEAD_AT_GPU, SYNCED };\n  SyncedHead head() { return head_; }\n  // 内存大小\n  size_t size() { return size_; }\n\n#ifndef CPU_ONLY\n  void async_gpu_push(const cudaStream_t& stream);\n#endif\n\n private:\n  void check_device();\n\n  void to_cpu();\n  void to_gpu();\n  void* cpu_ptr_;\n  void* gpu_ptr_;\n  size_t size_;\n  SyncedHead head_;\n  bool own_cpu_data_;\n  bool cpu_malloc_use_cuda_;\n  bool own_gpu_data_;\n  // GPU设备编号\n  int device_;\n\n  DISABLE_COPY_AND_ASSIGN(SyncedMemory);\n};  // class SyncedMemory\n```\n\n我们以`to_cpu()`为例，看一下是如何在不同状态之间切换的。\n\n``` cpp\ninline void SyncedMemory::to_gpu() {\n  // 检查设备状态（使用条件编译，只在DEBUG中使能）\n  check_device();\n#ifndef CPU_ONLY\n  switch (head_) {\n  case UNINITIALIZED:\n    // 还没有初始化呢~所以内存啥的还没开\n    // 先在GPU上开块显存吧~\n    CUDA_CHECK(cudaMalloc(&gpu_ptr_, size_));\n    caffe_gpu_memset(size_, 0, gpu_ptr_);\n    // 接着，改变状态标志\n    head_ = HEAD_AT_GPU;\n    own_gpu_data_ = true;\n    break;\n  case HEAD_AT_CPU:\n    // 数据在CPU上~如果需要，先在显存上开内存\n    if (gpu_ptr_ == NULL) {\n      CUDA_CHECK(cudaMalloc(&gpu_ptr_, size_));\n      own_gpu_data_ = true;\n    }\n    // 数据拷贝\n    caffe_gpu_memcpy(size_, cpu_ptr_, gpu_ptr_);\n    // 改变状态变量\n    head_ = SYNCED;\n    break;\n  // 已经在GPU或者已经同步了，什么都不做\n  case HEAD_AT_GPU:\n  case SYNCED:\n    break;\n  }\n#else\n  // NO_GPU 是一个宏，打印FATAL ERROR日志信息\n  // 编译选项没有开GPU支持，只能说 无可奉告\n  NO_GPU;\n#endif\n}\n```\n\n","slug":"caffe-syncedmem","published":1,"updated":"2018-01-25T07:44:26.577Z","_id":"cjcu6vcri0005qu46yyrwcoom","comments":1,"layout":"post","photos":[],"link":"","content":"<p><code>Blob</code>是Caffe中的基本数据结构，类似于TensorFlow和PyTorch中的Tensor。图像读入后作为<code>Blob</code>，开始在各个<code>Layer</code>之间传递，最终得到输出。下面这张图展示了<code>Blob</code>和<code>Layer</code>之间的关系：<br> <img src=\"/img/caffe_syncedmem_blob_flow.jpg\" width=\"300\" height=\"200\" alt=\"blob的流动\" align=\"center\"></p>\n<p>Caffe中的<code>Blob</code>在实现的时候，使用了<code>SyncedMem</code>管理内存，并在内存（Host）和显存（device）之间同步。这篇博客对Caffe中<code>SyncedMem</code>的实现做一总结。<br><a id=\"more\"></a></p>\n<h2 id=\"SyncedMem的作用\"><a href=\"#SyncedMem的作用\" class=\"headerlink\" title=\"SyncedMem的作用\"></a>SyncedMem的作用</h2><p><code>Blob</code>是一个多维的数组，可以位于内存，也可以位于显存（当使用GPU时）。一方面，我们需要对底层的内存进行管理，包括何何时开辟内存空间。另一方面，我们的训练数据常常是首先由硬盘读取到内存中，而训练又经常使用GPU，最终结果的保存或可视化又要求数据重新传回内存，所以涉及到Host和Device内存的同步问题。</p>\n<h2 id=\"同步的实现思路\"><a href=\"#同步的实现思路\" class=\"headerlink\" title=\"同步的实现思路\"></a>同步的实现思路</h2><p>在<code>SyncedMem</code>的实现代码中，作者使用一个枚举量<code>head_</code>来标记当前的状态。如下所示：</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\">// in SyncedMem</span></div><div class=\"line\"><span class=\"keyword\">enum</span> SyncedHead &#123; UNINITIALIZED, HEAD_AT_CPU, HEAD_AT_GPU, SYNCED &#125;;</div><div class=\"line\"><span class=\"comment\">// 使用过Git吗？ 在Git中那个标志着repo最新版本状态的变量就叫 HEAD</span></div><div class=\"line\"><span class=\"comment\">// 这里也是一样，标志着最新的数据位于哪里</span></div><div class=\"line\">SyncedHead head_;</div></pre></td></tr></table></figure>\n<p>这样，利用<code>head_</code>变量，就可以构建一个状态转移图，在不同状态切换时进行必要的同步操作等。<br><img src=\"/img/caffe_syncedmem_transfer.png\" alt=\"状态转换图\"></p>\n<h2 id=\"具体实现\"><a href=\"#具体实现\" class=\"headerlink\" title=\"具体实现\"></a>具体实现</h2><p><code>SyncedMem</code>的类声明如下：</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div><div class=\"line\">47</div><div class=\"line\">48</div><div class=\"line\">49</div><div class=\"line\">50</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\">/**</span></div><div class=\"line\"> * @brief Manages memory allocation and synchronization between the host (CPU)</div><div class=\"line\"> *        and device (GPU).</div><div class=\"line\"> *</div><div class=\"line\"> * TODO(dox): more thorough description.</div><div class=\"line\"> */</div><div class=\"line\"><span class=\"keyword\">class</span> SyncedMemory &#123;</div><div class=\"line\"> <span class=\"keyword\">public</span>:</div><div class=\"line\">  SyncedMemory();</div><div class=\"line\">  <span class=\"function\"><span class=\"keyword\">explicit</span> <span class=\"title\">SyncedMemory</span><span class=\"params\">(<span class=\"keyword\">size_t</span> size)</span></span>;</div><div class=\"line\">  ~SyncedMemory();</div><div class=\"line\">  <span class=\"comment\">// 获取CPU data指针</span></div><div class=\"line\">  <span class=\"function\"><span class=\"keyword\">const</span> <span class=\"keyword\">void</span>* <span class=\"title\">cpu_data</span><span class=\"params\">()</span></span>;</div><div class=\"line\">  <span class=\"comment\">// 设置CPU data指针</span></div><div class=\"line\">  <span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">set_cpu_data</span><span class=\"params\">(<span class=\"keyword\">void</span>* data)</span></span>;</div><div class=\"line\">  <span class=\"comment\">// 获取GPU data指针</span></div><div class=\"line\">  <span class=\"function\"><span class=\"keyword\">const</span> <span class=\"keyword\">void</span>* <span class=\"title\">gpu_data</span><span class=\"params\">()</span></span>;</div><div class=\"line\">  <span class=\"comment\">// 设置GPU data指针</span></div><div class=\"line\">  <span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">set_gpu_data</span><span class=\"params\">(<span class=\"keyword\">void</span>* data)</span></span>;</div><div class=\"line\">  <span class=\"comment\">// 获取CPU data指针，并在后续将改变指针所指向内存的值</span></div><div class=\"line\">  <span class=\"function\"><span class=\"keyword\">void</span>* <span class=\"title\">mutable_cpu_data</span><span class=\"params\">()</span></span>;</div><div class=\"line\">  <span class=\"comment\">// 获取GPU data指针，并在后续将改变指针所指向内存的值</span></div><div class=\"line\">  <span class=\"function\"><span class=\"keyword\">void</span>* <span class=\"title\">mutable_gpu_data</span><span class=\"params\">()</span></span>;</div><div class=\"line\">  <span class=\"comment\">// CPU 和 GPU的同步状态：未初始化，在CPU（未同步），在GPU（未同步），已同步</span></div><div class=\"line\">  <span class=\"keyword\">enum</span> SyncedHead &#123; UNINITIALIZED, HEAD_AT_CPU, HEAD_AT_GPU, SYNCED &#125;;</div><div class=\"line\">  <span class=\"function\">SyncedHead <span class=\"title\">head</span><span class=\"params\">()</span> </span>&#123; <span class=\"keyword\">return</span> head_; &#125;</div><div class=\"line\">  <span class=\"comment\">// 内存大小</span></div><div class=\"line\">  <span class=\"keyword\">size_t</span> size() &#123; <span class=\"keyword\">return</span> size_; &#125;</div><div class=\"line\"></div><div class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">ifndef</span> CPU_ONLY</span></div><div class=\"line\">  <span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">async_gpu_push</span><span class=\"params\">(<span class=\"keyword\">const</span> cudaStream_t&amp; stream)</span></span>;</div><div class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">endif</span></span></div><div class=\"line\"></div><div class=\"line\"> <span class=\"keyword\">private</span>:</div><div class=\"line\">  <span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">check_device</span><span class=\"params\">()</span></span>;</div><div class=\"line\"></div><div class=\"line\">  <span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">to_cpu</span><span class=\"params\">()</span></span>;</div><div class=\"line\">  <span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">to_gpu</span><span class=\"params\">()</span></span>;</div><div class=\"line\">  <span class=\"keyword\">void</span>* cpu_ptr_;</div><div class=\"line\">  <span class=\"keyword\">void</span>* gpu_ptr_;</div><div class=\"line\">  <span class=\"keyword\">size_t</span> size_;</div><div class=\"line\">  SyncedHead head_;</div><div class=\"line\">  <span class=\"keyword\">bool</span> own_cpu_data_;</div><div class=\"line\">  <span class=\"keyword\">bool</span> cpu_malloc_use_cuda_;</div><div class=\"line\">  <span class=\"keyword\">bool</span> own_gpu_data_;</div><div class=\"line\">  <span class=\"comment\">// GPU设备编号</span></div><div class=\"line\">  <span class=\"keyword\">int</span> device_;</div><div class=\"line\"></div><div class=\"line\">  DISABLE_COPY_AND_ASSIGN(SyncedMemory);</div><div class=\"line\">&#125;;  <span class=\"comment\">// class SyncedMemory</span></div></pre></td></tr></table></figure>\n<p>我们以<code>to_cpu()</code>为例，看一下是如何在不同状态之间切换的。</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">inline</span> <span class=\"keyword\">void</span> SyncedMemory::to_gpu() &#123;</div><div class=\"line\">  <span class=\"comment\">// 检查设备状态（使用条件编译，只在DEBUG中使能）</span></div><div class=\"line\">  check_device();</div><div class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">ifndef</span> CPU_ONLY</span></div><div class=\"line\">  <span class=\"keyword\">switch</span> (head_) &#123;</div><div class=\"line\">  <span class=\"keyword\">case</span> UNINITIALIZED:</div><div class=\"line\">    <span class=\"comment\">// 还没有初始化呢~所以内存啥的还没开</span></div><div class=\"line\">    <span class=\"comment\">// 先在GPU上开块显存吧~</span></div><div class=\"line\">    CUDA_CHECK(cudaMalloc(&amp;gpu_ptr_, size_));</div><div class=\"line\">    caffe_gpu_memset(size_, <span class=\"number\">0</span>, gpu_ptr_);</div><div class=\"line\">    <span class=\"comment\">// 接着，改变状态标志</span></div><div class=\"line\">    head_ = HEAD_AT_GPU;</div><div class=\"line\">    own_gpu_data_ = <span class=\"literal\">true</span>;</div><div class=\"line\">    <span class=\"keyword\">break</span>;</div><div class=\"line\">  <span class=\"keyword\">case</span> HEAD_AT_CPU:</div><div class=\"line\">    <span class=\"comment\">// 数据在CPU上~如果需要，先在显存上开内存</span></div><div class=\"line\">    <span class=\"keyword\">if</span> (gpu_ptr_ == <span class=\"literal\">NULL</span>) &#123;</div><div class=\"line\">      CUDA_CHECK(cudaMalloc(&amp;gpu_ptr_, size_));</div><div class=\"line\">      own_gpu_data_ = <span class=\"literal\">true</span>;</div><div class=\"line\">    &#125;</div><div class=\"line\">    <span class=\"comment\">// 数据拷贝</span></div><div class=\"line\">    caffe_gpu_memcpy(size_, cpu_ptr_, gpu_ptr_);</div><div class=\"line\">    <span class=\"comment\">// 改变状态变量</span></div><div class=\"line\">    head_ = SYNCED;</div><div class=\"line\">    <span class=\"keyword\">break</span>;</div><div class=\"line\">  <span class=\"comment\">// 已经在GPU或者已经同步了，什么都不做</span></div><div class=\"line\">  <span class=\"keyword\">case</span> HEAD_AT_GPU:</div><div class=\"line\">  <span class=\"keyword\">case</span> SYNCED:</div><div class=\"line\">    <span class=\"keyword\">break</span>;</div><div class=\"line\">  &#125;</div><div class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">else</span></span></div><div class=\"line\">  <span class=\"comment\">// NO_GPU 是一个宏，打印FATAL ERROR日志信息</span></div><div class=\"line\">  <span class=\"comment\">// 编译选项没有开GPU支持，只能说 无可奉告</span></div><div class=\"line\">  NO_GPU;</div><div class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">endif</span></span></div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n","excerpt":"<p><code>Blob</code>是Caffe中的基本数据结构，类似于TensorFlow和PyTorch中的Tensor。图像读入后作为<code>Blob</code>，开始在各个<code>Layer</code>之间传递，最终得到输出。下面这张图展示了<code>Blob</code>和<code>Layer</code>之间的关系：<br> <img src=\"/img/caffe_syncedmem_blob_flow.jpg\" width = \"300\" height = \"200\" alt=\"blob的流动\" align=center /></p>\n<p>Caffe中的<code>Blob</code>在实现的时候，使用了<code>SyncedMem</code>管理内存，并在内存（Host）和显存（device）之间同步。这篇博客对Caffe中<code>SyncedMem</code>的实现做一总结。<br>","more":"</p>\n<h2 id=\"SyncedMem的作用\"><a href=\"#SyncedMem的作用\" class=\"headerlink\" title=\"SyncedMem的作用\"></a>SyncedMem的作用</h2><p><code>Blob</code>是一个多维的数组，可以位于内存，也可以位于显存（当使用GPU时）。一方面，我们需要对底层的内存进行管理，包括何何时开辟内存空间。另一方面，我们的训练数据常常是首先由硬盘读取到内存中，而训练又经常使用GPU，最终结果的保存或可视化又要求数据重新传回内存，所以涉及到Host和Device内存的同步问题。</p>\n<h2 id=\"同步的实现思路\"><a href=\"#同步的实现思路\" class=\"headerlink\" title=\"同步的实现思路\"></a>同步的实现思路</h2><p>在<code>SyncedMem</code>的实现代码中，作者使用一个枚举量<code>head_</code>来标记当前的状态。如下所示：</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\">// in SyncedMem</span></div><div class=\"line\"><span class=\"keyword\">enum</span> SyncedHead &#123; UNINITIALIZED, HEAD_AT_CPU, HEAD_AT_GPU, SYNCED &#125;;</div><div class=\"line\"><span class=\"comment\">// 使用过Git吗？ 在Git中那个标志着repo最新版本状态的变量就叫 HEAD</span></div><div class=\"line\"><span class=\"comment\">// 这里也是一样，标志着最新的数据位于哪里</span></div><div class=\"line\">SyncedHead head_;</div></pre></td></tr></table></figure>\n<p>这样，利用<code>head_</code>变量，就可以构建一个状态转移图，在不同状态切换时进行必要的同步操作等。<br><img src=\"/img/caffe_syncedmem_transfer.png\" alt=\"状态转换图\"></p>\n<h2 id=\"具体实现\"><a href=\"#具体实现\" class=\"headerlink\" title=\"具体实现\"></a>具体实现</h2><p><code>SyncedMem</code>的类声明如下：</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div><div class=\"line\">47</div><div class=\"line\">48</div><div class=\"line\">49</div><div class=\"line\">50</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\">/**</div><div class=\"line\"> * @brief Manages memory allocation and synchronization between the host (CPU)</div><div class=\"line\"> *        and device (GPU).</div><div class=\"line\"> *</div><div class=\"line\"> * TODO(dox): more thorough description.</div><div class=\"line\"> */</span></div><div class=\"line\"><span class=\"keyword\">class</span> SyncedMemory &#123;</div><div class=\"line\"> <span class=\"keyword\">public</span>:</div><div class=\"line\">  SyncedMemory();</div><div class=\"line\">  <span class=\"function\"><span class=\"keyword\">explicit</span> <span class=\"title\">SyncedMemory</span><span class=\"params\">(<span class=\"keyword\">size_t</span> size)</span></span>;</div><div class=\"line\">  ~SyncedMemory();</div><div class=\"line\">  <span class=\"comment\">// 获取CPU data指针</span></div><div class=\"line\">  <span class=\"function\"><span class=\"keyword\">const</span> <span class=\"keyword\">void</span>* <span class=\"title\">cpu_data</span><span class=\"params\">()</span></span>;</div><div class=\"line\">  <span class=\"comment\">// 设置CPU data指针</span></div><div class=\"line\">  <span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">set_cpu_data</span><span class=\"params\">(<span class=\"keyword\">void</span>* data)</span></span>;</div><div class=\"line\">  <span class=\"comment\">// 获取GPU data指针</span></div><div class=\"line\">  <span class=\"function\"><span class=\"keyword\">const</span> <span class=\"keyword\">void</span>* <span class=\"title\">gpu_data</span><span class=\"params\">()</span></span>;</div><div class=\"line\">  <span class=\"comment\">// 设置GPU data指针</span></div><div class=\"line\">  <span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">set_gpu_data</span><span class=\"params\">(<span class=\"keyword\">void</span>* data)</span></span>;</div><div class=\"line\">  <span class=\"comment\">// 获取CPU data指针，并在后续将改变指针所指向内存的值</span></div><div class=\"line\">  <span class=\"function\"><span class=\"keyword\">void</span>* <span class=\"title\">mutable_cpu_data</span><span class=\"params\">()</span></span>;</div><div class=\"line\">  <span class=\"comment\">// 获取GPU data指针，并在后续将改变指针所指向内存的值</span></div><div class=\"line\">  <span class=\"function\"><span class=\"keyword\">void</span>* <span class=\"title\">mutable_gpu_data</span><span class=\"params\">()</span></span>;</div><div class=\"line\">  <span class=\"comment\">// CPU 和 GPU的同步状态：未初始化，在CPU（未同步），在GPU（未同步），已同步</span></div><div class=\"line\">  <span class=\"keyword\">enum</span> SyncedHead &#123; UNINITIALIZED, HEAD_AT_CPU, HEAD_AT_GPU, SYNCED &#125;;</div><div class=\"line\">  <span class=\"function\">SyncedHead <span class=\"title\">head</span><span class=\"params\">()</span> </span>&#123; <span class=\"keyword\">return</span> head_; &#125;</div><div class=\"line\">  <span class=\"comment\">// 内存大小</span></div><div class=\"line\">  <span class=\"keyword\">size_t</span> size() &#123; <span class=\"keyword\">return</span> size_; &#125;</div><div class=\"line\"></div><div class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">ifndef</span> CPU_ONLY</span></div><div class=\"line\">  <span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">async_gpu_push</span><span class=\"params\">(<span class=\"keyword\">const</span> cudaStream_t&amp; stream)</span></span>;</div><div class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">endif</span></span></div><div class=\"line\"></div><div class=\"line\"> <span class=\"keyword\">private</span>:</div><div class=\"line\">  <span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">check_device</span><span class=\"params\">()</span></span>;</div><div class=\"line\"></div><div class=\"line\">  <span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">to_cpu</span><span class=\"params\">()</span></span>;</div><div class=\"line\">  <span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">to_gpu</span><span class=\"params\">()</span></span>;</div><div class=\"line\">  <span class=\"keyword\">void</span>* cpu_ptr_;</div><div class=\"line\">  <span class=\"keyword\">void</span>* gpu_ptr_;</div><div class=\"line\">  <span class=\"keyword\">size_t</span> size_;</div><div class=\"line\">  SyncedHead head_;</div><div class=\"line\">  <span class=\"keyword\">bool</span> own_cpu_data_;</div><div class=\"line\">  <span class=\"keyword\">bool</span> cpu_malloc_use_cuda_;</div><div class=\"line\">  <span class=\"keyword\">bool</span> own_gpu_data_;</div><div class=\"line\">  <span class=\"comment\">// GPU设备编号</span></div><div class=\"line\">  <span class=\"keyword\">int</span> device_;</div><div class=\"line\"></div><div class=\"line\">  DISABLE_COPY_AND_ASSIGN(SyncedMemory);</div><div class=\"line\">&#125;;  <span class=\"comment\">// class SyncedMemory</span></div></pre></td></tr></table></figure>\n<p>我们以<code>to_cpu()</code>为例，看一下是如何在不同状态之间切换的。</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">inline</span> <span class=\"keyword\">void</span> SyncedMemory::to_gpu() &#123;</div><div class=\"line\">  <span class=\"comment\">// 检查设备状态（使用条件编译，只在DEBUG中使能）</span></div><div class=\"line\">  check_device();</div><div class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">ifndef</span> CPU_ONLY</span></div><div class=\"line\">  <span class=\"keyword\">switch</span> (head_) &#123;</div><div class=\"line\">  <span class=\"keyword\">case</span> UNINITIALIZED:</div><div class=\"line\">    <span class=\"comment\">// 还没有初始化呢~所以内存啥的还没开</span></div><div class=\"line\">    <span class=\"comment\">// 先在GPU上开块显存吧~</span></div><div class=\"line\">    CUDA_CHECK(cudaMalloc(&amp;gpu_ptr_, size_));</div><div class=\"line\">    caffe_gpu_memset(size_, <span class=\"number\">0</span>, gpu_ptr_);</div><div class=\"line\">    <span class=\"comment\">// 接着，改变状态标志</span></div><div class=\"line\">    head_ = HEAD_AT_GPU;</div><div class=\"line\">    own_gpu_data_ = <span class=\"literal\">true</span>;</div><div class=\"line\">    <span class=\"keyword\">break</span>;</div><div class=\"line\">  <span class=\"keyword\">case</span> HEAD_AT_CPU:</div><div class=\"line\">    <span class=\"comment\">// 数据在CPU上~如果需要，先在显存上开内存</span></div><div class=\"line\">    <span class=\"keyword\">if</span> (gpu_ptr_ == <span class=\"literal\">NULL</span>) &#123;</div><div class=\"line\">      CUDA_CHECK(cudaMalloc(&amp;gpu_ptr_, size_));</div><div class=\"line\">      own_gpu_data_ = <span class=\"literal\">true</span>;</div><div class=\"line\">    &#125;</div><div class=\"line\">    <span class=\"comment\">// 数据拷贝</span></div><div class=\"line\">    caffe_gpu_memcpy(size_, cpu_ptr_, gpu_ptr_);</div><div class=\"line\">    <span class=\"comment\">// 改变状态变量</span></div><div class=\"line\">    head_ = SYNCED;</div><div class=\"line\">    <span class=\"keyword\">break</span>;</div><div class=\"line\">  <span class=\"comment\">// 已经在GPU或者已经同步了，什么都不做</span></div><div class=\"line\">  <span class=\"keyword\">case</span> HEAD_AT_GPU:</div><div class=\"line\">  <span class=\"keyword\">case</span> SYNCED:</div><div class=\"line\">    <span class=\"keyword\">break</span>;</div><div class=\"line\">  &#125;</div><div class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">else</span></span></div><div class=\"line\">  <span class=\"comment\">// NO_GPU 是一个宏，打印FATAL ERROR日志信息</span></div><div class=\"line\">  <span class=\"comment\">// 编译选项没有开GPU支持，只能说 无可奉告</span></div><div class=\"line\">  NO_GPU;</div><div class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">endif</span></span></div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>"},{"title":"CS131-立体视觉基础","date":"2017-02-02T12:38:55.000Z","_content":"\n数字图像是现实世界的物体通过光学成像设备在感光材料上的投影。在3D到2D的转换过程中，深度信息丢失。如何从单幅或者多幅图像中恢复出有用的3D信息，需要使用立体视觉的知识进行分析。如下图所示。这篇博客对课程讲义中立体视觉部分做一整理，分别介绍了针孔相机模型和对极集合的基础知识。\n![立体视觉应用](/img/camera_geometry_application.png)\n<!-- more -->\n## 针孔相机模型（Pinhole Camera）\n\n针孔相机是简化的相机模型。光线沿直线传播。物体反射的光线，通过针孔，在成像面形成倒立的影像。针孔与成像面的距离，称为焦距。一般而言，针孔越小，影像越清晰，但针孔太小，会导致衍射，反而令影像模糊。\n\n### 投影几何的重要性质\n\n在对针孔相机详细说明之前，首先介绍投影几何（Projective Geometry）的几条性质。\n\n在投影变换前后，长度和角度信息丢失，但直线仍然保持是直线。如下图所示。原来垂直和平行的台球桌面的边不再保持原有的角度和相对长度，但是仍然是直线。\n![性质1](/img/projective_geometry_property_1.png)\n\n另一方面，虽然3D世界中的平行线在投影变换后相交（我们将交点称为Vanishing point），但是3D世界中互相平行的线，在2D平面上的Vanishing point为同一点。而所有平行线的Vanishing point又在同一条直线上。如下图所示（竖直的绿色平行线在投影变换后仍然保持平行，实际上并不想交，但我们这里认为其交于无穷远的某点）。\n![性质2](/img/projective_geometry_property_2.png)\n\n### 针孔相机模型\n如下图所示，为针孔相机成像原理示意图。外部世界的一点$P$发出（或反射）的光线，经过小孔$O$，成像在在像平面$\\Pi^\\prime$的点$P^\\prime$处。通过简单的相似三角形知识，可以得到图上的关系式。\n![针孔相机模型示意图](/img/pinhole_camera_model.png)\n\n由上图可以知道，3D空间的一点$P$与成像平面上的对应点$P^\\prime$坐标之间的数量关系为：\n$$\\left\\{\\begin{matrix}\nx^\\prime = fx/z \\\\\ny^\\prime = fy/z\n\\end{matrix}\\right.$$\n\n可是，由于变量$z$位于分母的位置，也就是说如果直接写成变换矩阵的话，这个矩阵是和变量$z$有关的，我们不想这样，而是想得到一个完全由相机本身确定的变换矩阵。所以，我们借鉴齐次矩阵的思想，将原来的点增加一个维度，转换为齐次坐标。如下图的两个例子所示。\n![齐次坐标](/img/qicizuobiao.png)\n\n这样，我们可以将3D和2D之间的变换写成下面的形式。当实际计算的时候，我们首先将3D点转换为4维向量（在末尾补1），然后左乘变换矩阵。最后将得到的结果化为末尾元素为1的形式，这样，前面两个元素就代表了变换后2D点的坐标。这样做，使得变换矩阵$M$完全由相机参数（即焦距$f$）决定。\n![](/img/qicizuobiao_transform.png)\n\n上面的式子只是理想情况下的相机模型，成立的假设条件较强，主要有以下几点假设：\n1. 内假设（和相机本身有关）\n    - 不同方向上焦距相同；\n    - 光学中心在相平面的坐标原点$(0, 0)$\n    - 没有倾斜（no skew）\n2. 外假设（和相机位姿有关，和相机本身参数无关）\n    - 相机没有旋转（坐标轴与世界坐标系方向重合）\n    - 相机没有平移（相机中心与世界坐标系中心重合）\n\n其中，no skew情形我也不知道中文应该如何翻译。这种情况如图所示，由于剪切变形，成像平面的坐标轴并不是严格正交，使得两个轴之间实际存在一个很小的夹角。\n![what is \"skew\"?](/img/camera_skew.png)\n\n下面，按照slide的顺序逐渐将上述假设移去，分析变换矩阵$M$应该如何修正。\n\n#### 理想情况\n理想情况以上假设全部满足，矩阵$M$如下所示。\n![case 1: M](/img/case_1_m.png)\n\n#### 光学中心不在像平面的坐标原点\n假设光学中心位于像平面内坐标为$(u_0, v_0)$的位置。则需要在理想情况计算的坐标基础上加上这个偏移量。所以，矩阵$M$被修正为：\n![case 2: M](/img/case_2_m.png)\n\n#### 像素非正方形\n由于透镜在$x$和$y$方向上的焦距不同，使得两个方向上的比例因子不同，不能再使用同一个$f$计算。修正如下：\n![case 3: M](/img/case_3_m.png)\n\n#### no skew\n这时候$x$方向上其实有$y$轴坐标的分量，所以修正如下：\n![case 4: M](/img/case_4_m.png)\n\n#### 相机的旋转和平移\n相机外部姿态很可能经过了旋转和平移，从而不与世界坐标系完全重合。如图所示。\n![相机旋转和平移](/img/camera_translation_rotation.png)\n\n所以我们需要先对世界坐标系内的3D点做一个齐次变换，将其变换为一个中间坐标系，这个中间坐标系和相机坐标系是重合的。也就是我们的变换矩阵应该是下面这个形式的，其中，$H$是表征旋转和平移的齐次矩阵，$H\\in \\mathbb{R}^{3\\times 4}$。\n$$P^\\prime = MHP$$\n\n首先考虑较为简单的平移。所做的修正如下所示，也就是将原来的$\\mathbb{0}$矩阵变为了一个平移向量。\n![case 5: M](/img/case_m_5.png)\n\n进一步考虑旋转时，我们将旋转分解为绕三个坐标轴的三次旋转，单次旋转矩阵如下所示：\n![绕单轴的旋转矩阵](/img/rotation_matrix.png)\n\n将它们按照旋转顺序左乘起来，可以得到修正后的变换形式如下：\n![case 6: M](/img/case_6_m.png)\n\n#### 最终形式\n综上所示，变换矩阵的最终形式为：\n![最终的投影变换矩阵](/img/generic_projection_matrix.png)\n\n其中，内参数矩阵中有5个自由度，外参数矩阵中有6个自由度（3平移，3旋转）。\n\n上面的内容总结起来，如下图所示。\n![things to remember](/img/camera_model_things_to_remember.png)\n\n## 对极几何\n\n### 基础概念\n如下图所示，给出了对极几何中的几个基本概念。两个相机的中心分别位于$O$点和$O^{'}$点，被观察物体位于$P$点。\n\n![对极几何概念图示](/img/epipolar_fig.png)\n\n- 极点：$e$和$e^\\prime$点分别是$OO^\\prime$连线与两个成像平面的交点，也是相机中心在另一台相机成像平面上对应的像。\n- 极平面：点$O$，$O^\\prime$，$P$点共同确定的平面（灰色）\n- 极线：极平面与两个成像平面的交线，即$pe$和$p^\\prime e^\\prime$（蓝色）\n- 基线：两个相机中心的连线（黄色）\n\n### 极线约束\n从上图可以看出，如果两台相机已经给定，那么给定物体点在左边相机像平面内的成像点，则右边相机成像平面内的点被唯一确定。如何能够找到这个对应点呢？或者换种说法，在对极几何中，如何描述这种不同相机成像点的约束关系呢？\n\n如下图所示，从上面的讨论我们已经知道，物点$P$在左右两个相机成像平面的坐标可以使用由相机内参数矩阵和外参数矩阵确定的齐次变换矩阵得到（下图在标注时，右侧相机的点应被标注为$p^\\prime$）。为了简化问题，在下面的讨论时，我们假定相机都是理想的针孔相机模型，也就是说，变换矩阵只和外参数矩阵（也即是相机的位姿决定）。再做简化，认为世界坐标系与左侧相机的相机坐标系重合。那么，就得到了图上所示的变换矩阵$M$和$M^\\prime$。其中，$R$是左右两个相机之间的旋转矩阵，$T$是$OO^\\prime$有向线段，表明相机中心的位移。\n![极线约束1](/img/epipolar_constraint_1.png)\n\n（下面的推导参考了[博客：计算机视觉基础4——对极几何](http://www.cnblogs.com/gemstone/articles/2294551.html)）。在下面的推导中，我们使用$p^\\prime$表示在相机$O^\\prime$下的向量$O\\prime P$，符号$p$同理。那么，有如下关系成立：\n$R(p-T) = p^\\prime$\n","source":"_posts/cs131-camera.md","raw":"---\ntitle: CS131-立体视觉基础\ndate: 2017-02-02 20:38:55\ntags:\n     - cs131\n     - 公开课\n---\n\n数字图像是现实世界的物体通过光学成像设备在感光材料上的投影。在3D到2D的转换过程中，深度信息丢失。如何从单幅或者多幅图像中恢复出有用的3D信息，需要使用立体视觉的知识进行分析。如下图所示。这篇博客对课程讲义中立体视觉部分做一整理，分别介绍了针孔相机模型和对极集合的基础知识。\n![立体视觉应用](/img/camera_geometry_application.png)\n<!-- more -->\n## 针孔相机模型（Pinhole Camera）\n\n针孔相机是简化的相机模型。光线沿直线传播。物体反射的光线，通过针孔，在成像面形成倒立的影像。针孔与成像面的距离，称为焦距。一般而言，针孔越小，影像越清晰，但针孔太小，会导致衍射，反而令影像模糊。\n\n### 投影几何的重要性质\n\n在对针孔相机详细说明之前，首先介绍投影几何（Projective Geometry）的几条性质。\n\n在投影变换前后，长度和角度信息丢失，但直线仍然保持是直线。如下图所示。原来垂直和平行的台球桌面的边不再保持原有的角度和相对长度，但是仍然是直线。\n![性质1](/img/projective_geometry_property_1.png)\n\n另一方面，虽然3D世界中的平行线在投影变换后相交（我们将交点称为Vanishing point），但是3D世界中互相平行的线，在2D平面上的Vanishing point为同一点。而所有平行线的Vanishing point又在同一条直线上。如下图所示（竖直的绿色平行线在投影变换后仍然保持平行，实际上并不想交，但我们这里认为其交于无穷远的某点）。\n![性质2](/img/projective_geometry_property_2.png)\n\n### 针孔相机模型\n如下图所示，为针孔相机成像原理示意图。外部世界的一点$P$发出（或反射）的光线，经过小孔$O$，成像在在像平面$\\Pi^\\prime$的点$P^\\prime$处。通过简单的相似三角形知识，可以得到图上的关系式。\n![针孔相机模型示意图](/img/pinhole_camera_model.png)\n\n由上图可以知道，3D空间的一点$P$与成像平面上的对应点$P^\\prime$坐标之间的数量关系为：\n$$\\left\\{\\begin{matrix}\nx^\\prime = fx/z \\\\\ny^\\prime = fy/z\n\\end{matrix}\\right.$$\n\n可是，由于变量$z$位于分母的位置，也就是说如果直接写成变换矩阵的话，这个矩阵是和变量$z$有关的，我们不想这样，而是想得到一个完全由相机本身确定的变换矩阵。所以，我们借鉴齐次矩阵的思想，将原来的点增加一个维度，转换为齐次坐标。如下图的两个例子所示。\n![齐次坐标](/img/qicizuobiao.png)\n\n这样，我们可以将3D和2D之间的变换写成下面的形式。当实际计算的时候，我们首先将3D点转换为4维向量（在末尾补1），然后左乘变换矩阵。最后将得到的结果化为末尾元素为1的形式，这样，前面两个元素就代表了变换后2D点的坐标。这样做，使得变换矩阵$M$完全由相机参数（即焦距$f$）决定。\n![](/img/qicizuobiao_transform.png)\n\n上面的式子只是理想情况下的相机模型，成立的假设条件较强，主要有以下几点假设：\n1. 内假设（和相机本身有关）\n    - 不同方向上焦距相同；\n    - 光学中心在相平面的坐标原点$(0, 0)$\n    - 没有倾斜（no skew）\n2. 外假设（和相机位姿有关，和相机本身参数无关）\n    - 相机没有旋转（坐标轴与世界坐标系方向重合）\n    - 相机没有平移（相机中心与世界坐标系中心重合）\n\n其中，no skew情形我也不知道中文应该如何翻译。这种情况如图所示，由于剪切变形，成像平面的坐标轴并不是严格正交，使得两个轴之间实际存在一个很小的夹角。\n![what is \"skew\"?](/img/camera_skew.png)\n\n下面，按照slide的顺序逐渐将上述假设移去，分析变换矩阵$M$应该如何修正。\n\n#### 理想情况\n理想情况以上假设全部满足，矩阵$M$如下所示。\n![case 1: M](/img/case_1_m.png)\n\n#### 光学中心不在像平面的坐标原点\n假设光学中心位于像平面内坐标为$(u_0, v_0)$的位置。则需要在理想情况计算的坐标基础上加上这个偏移量。所以，矩阵$M$被修正为：\n![case 2: M](/img/case_2_m.png)\n\n#### 像素非正方形\n由于透镜在$x$和$y$方向上的焦距不同，使得两个方向上的比例因子不同，不能再使用同一个$f$计算。修正如下：\n![case 3: M](/img/case_3_m.png)\n\n#### no skew\n这时候$x$方向上其实有$y$轴坐标的分量，所以修正如下：\n![case 4: M](/img/case_4_m.png)\n\n#### 相机的旋转和平移\n相机外部姿态很可能经过了旋转和平移，从而不与世界坐标系完全重合。如图所示。\n![相机旋转和平移](/img/camera_translation_rotation.png)\n\n所以我们需要先对世界坐标系内的3D点做一个齐次变换，将其变换为一个中间坐标系，这个中间坐标系和相机坐标系是重合的。也就是我们的变换矩阵应该是下面这个形式的，其中，$H$是表征旋转和平移的齐次矩阵，$H\\in \\mathbb{R}^{3\\times 4}$。\n$$P^\\prime = MHP$$\n\n首先考虑较为简单的平移。所做的修正如下所示，也就是将原来的$\\mathbb{0}$矩阵变为了一个平移向量。\n![case 5: M](/img/case_m_5.png)\n\n进一步考虑旋转时，我们将旋转分解为绕三个坐标轴的三次旋转，单次旋转矩阵如下所示：\n![绕单轴的旋转矩阵](/img/rotation_matrix.png)\n\n将它们按照旋转顺序左乘起来，可以得到修正后的变换形式如下：\n![case 6: M](/img/case_6_m.png)\n\n#### 最终形式\n综上所示，变换矩阵的最终形式为：\n![最终的投影变换矩阵](/img/generic_projection_matrix.png)\n\n其中，内参数矩阵中有5个自由度，外参数矩阵中有6个自由度（3平移，3旋转）。\n\n上面的内容总结起来，如下图所示。\n![things to remember](/img/camera_model_things_to_remember.png)\n\n## 对极几何\n\n### 基础概念\n如下图所示，给出了对极几何中的几个基本概念。两个相机的中心分别位于$O$点和$O^{'}$点，被观察物体位于$P$点。\n\n![对极几何概念图示](/img/epipolar_fig.png)\n\n- 极点：$e$和$e^\\prime$点分别是$OO^\\prime$连线与两个成像平面的交点，也是相机中心在另一台相机成像平面上对应的像。\n- 极平面：点$O$，$O^\\prime$，$P$点共同确定的平面（灰色）\n- 极线：极平面与两个成像平面的交线，即$pe$和$p^\\prime e^\\prime$（蓝色）\n- 基线：两个相机中心的连线（黄色）\n\n### 极线约束\n从上图可以看出，如果两台相机已经给定，那么给定物体点在左边相机像平面内的成像点，则右边相机成像平面内的点被唯一确定。如何能够找到这个对应点呢？或者换种说法，在对极几何中，如何描述这种不同相机成像点的约束关系呢？\n\n如下图所示，从上面的讨论我们已经知道，物点$P$在左右两个相机成像平面的坐标可以使用由相机内参数矩阵和外参数矩阵确定的齐次变换矩阵得到（下图在标注时，右侧相机的点应被标注为$p^\\prime$）。为了简化问题，在下面的讨论时，我们假定相机都是理想的针孔相机模型，也就是说，变换矩阵只和外参数矩阵（也即是相机的位姿决定）。再做简化，认为世界坐标系与左侧相机的相机坐标系重合。那么，就得到了图上所示的变换矩阵$M$和$M^\\prime$。其中，$R$是左右两个相机之间的旋转矩阵，$T$是$OO^\\prime$有向线段，表明相机中心的位移。\n![极线约束1](/img/epipolar_constraint_1.png)\n\n（下面的推导参考了[博客：计算机视觉基础4——对极几何](http://www.cnblogs.com/gemstone/articles/2294551.html)）。在下面的推导中，我们使用$p^\\prime$表示在相机$O^\\prime$下的向量$O\\prime P$，符号$p$同理。那么，有如下关系成立：\n$R(p-T) = p^\\prime$\n","slug":"cs131-camera","published":1,"updated":"2018-01-12T06:22:20.459Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjcu6vcrl0006qu465yeu34c2","content":"<p>数字图像是现实世界的物体通过光学成像设备在感光材料上的投影。在3D到2D的转换过程中，深度信息丢失。如何从单幅或者多幅图像中恢复出有用的3D信息，需要使用立体视觉的知识进行分析。如下图所示。这篇博客对课程讲义中立体视觉部分做一整理，分别介绍了针孔相机模型和对极集合的基础知识。<br><img src=\"/img/camera_geometry_application.png\" alt=\"立体视觉应用\"><br><a id=\"more\"></a></p>\n<h2 id=\"针孔相机模型（Pinhole-Camera）\"><a href=\"#针孔相机模型（Pinhole-Camera）\" class=\"headerlink\" title=\"针孔相机模型（Pinhole Camera）\"></a>针孔相机模型（Pinhole Camera）</h2><p>针孔相机是简化的相机模型。光线沿直线传播。物体反射的光线，通过针孔，在成像面形成倒立的影像。针孔与成像面的距离，称为焦距。一般而言，针孔越小，影像越清晰，但针孔太小，会导致衍射，反而令影像模糊。</p>\n<h3 id=\"投影几何的重要性质\"><a href=\"#投影几何的重要性质\" class=\"headerlink\" title=\"投影几何的重要性质\"></a>投影几何的重要性质</h3><p>在对针孔相机详细说明之前，首先介绍投影几何（Projective Geometry）的几条性质。</p>\n<p>在投影变换前后，长度和角度信息丢失，但直线仍然保持是直线。如下图所示。原来垂直和平行的台球桌面的边不再保持原有的角度和相对长度，但是仍然是直线。<br><img src=\"/img/projective_geometry_property_1.png\" alt=\"性质1\"></p>\n<p>另一方面，虽然3D世界中的平行线在投影变换后相交（我们将交点称为Vanishing point），但是3D世界中互相平行的线，在2D平面上的Vanishing point为同一点。而所有平行线的Vanishing point又在同一条直线上。如下图所示（竖直的绿色平行线在投影变换后仍然保持平行，实际上并不想交，但我们这里认为其交于无穷远的某点）。<br><img src=\"/img/projective_geometry_property_2.png\" alt=\"性质2\"></p>\n<h3 id=\"针孔相机模型\"><a href=\"#针孔相机模型\" class=\"headerlink\" title=\"针孔相机模型\"></a>针孔相机模型</h3><p>如下图所示，为针孔相机成像原理示意图。外部世界的一点$P$发出（或反射）的光线，经过小孔$O$，成像在在像平面$\\Pi^\\prime$的点$P^\\prime$处。通过简单的相似三角形知识，可以得到图上的关系式。<br><img src=\"/img/pinhole_camera_model.png\" alt=\"针孔相机模型示意图\"></p>\n<p>由上图可以知道，3D空间的一点$P$与成像平面上的对应点$P^\\prime$坐标之间的数量关系为：</p>\n<script type=\"math/tex; mode=display\">\\left\\{\\begin{matrix}\nx^\\prime = fx/z \\\\\ny^\\prime = fy/z\n\\end{matrix}\\right.</script><p>可是，由于变量$z$位于分母的位置，也就是说如果直接写成变换矩阵的话，这个矩阵是和变量$z$有关的，我们不想这样，而是想得到一个完全由相机本身确定的变换矩阵。所以，我们借鉴齐次矩阵的思想，将原来的点增加一个维度，转换为齐次坐标。如下图的两个例子所示。<br><img src=\"/img/qicizuobiao.png\" alt=\"齐次坐标\"></p>\n<p>这样，我们可以将3D和2D之间的变换写成下面的形式。当实际计算的时候，我们首先将3D点转换为4维向量（在末尾补1），然后左乘变换矩阵。最后将得到的结果化为末尾元素为1的形式，这样，前面两个元素就代表了变换后2D点的坐标。这样做，使得变换矩阵$M$完全由相机参数（即焦距$f$）决定。<br><img src=\"/img/qicizuobiao_transform.png\" alt=\"\"></p>\n<p>上面的式子只是理想情况下的相机模型，成立的假设条件较强，主要有以下几点假设：</p>\n<ol>\n<li>内假设（和相机本身有关）<ul>\n<li>不同方向上焦距相同；</li>\n<li>光学中心在相平面的坐标原点$(0, 0)$</li>\n<li>没有倾斜（no skew）</li>\n</ul>\n</li>\n<li>外假设（和相机位姿有关，和相机本身参数无关）<ul>\n<li>相机没有旋转（坐标轴与世界坐标系方向重合）</li>\n<li>相机没有平移（相机中心与世界坐标系中心重合）</li>\n</ul>\n</li>\n</ol>\n<p>其中，no skew情形我也不知道中文应该如何翻译。这种情况如图所示，由于剪切变形，成像平面的坐标轴并不是严格正交，使得两个轴之间实际存在一个很小的夹角。<br><img src=\"/img/camera_skew.png\" alt=\"what is &quot;skew&quot;?\"></p>\n<p>下面，按照slide的顺序逐渐将上述假设移去，分析变换矩阵$M$应该如何修正。</p>\n<h4 id=\"理想情况\"><a href=\"#理想情况\" class=\"headerlink\" title=\"理想情况\"></a>理想情况</h4><p>理想情况以上假设全部满足，矩阵$M$如下所示。<br><img src=\"/img/case_1_m.png\" alt=\"case 1: M\"></p>\n<h4 id=\"光学中心不在像平面的坐标原点\"><a href=\"#光学中心不在像平面的坐标原点\" class=\"headerlink\" title=\"光学中心不在像平面的坐标原点\"></a>光学中心不在像平面的坐标原点</h4><p>假设光学中心位于像平面内坐标为$(u_0, v_0)$的位置。则需要在理想情况计算的坐标基础上加上这个偏移量。所以，矩阵$M$被修正为：<br><img src=\"/img/case_2_m.png\" alt=\"case 2: M\"></p>\n<h4 id=\"像素非正方形\"><a href=\"#像素非正方形\" class=\"headerlink\" title=\"像素非正方形\"></a>像素非正方形</h4><p>由于透镜在$x$和$y$方向上的焦距不同，使得两个方向上的比例因子不同，不能再使用同一个$f$计算。修正如下：<br><img src=\"/img/case_3_m.png\" alt=\"case 3: M\"></p>\n<h4 id=\"no-skew\"><a href=\"#no-skew\" class=\"headerlink\" title=\"no skew\"></a>no skew</h4><p>这时候$x$方向上其实有$y$轴坐标的分量，所以修正如下：<br><img src=\"/img/case_4_m.png\" alt=\"case 4: M\"></p>\n<h4 id=\"相机的旋转和平移\"><a href=\"#相机的旋转和平移\" class=\"headerlink\" title=\"相机的旋转和平移\"></a>相机的旋转和平移</h4><p>相机外部姿态很可能经过了旋转和平移，从而不与世界坐标系完全重合。如图所示。<br><img src=\"/img/camera_translation_rotation.png\" alt=\"相机旋转和平移\"></p>\n<p>所以我们需要先对世界坐标系内的3D点做一个齐次变换，将其变换为一个中间坐标系，这个中间坐标系和相机坐标系是重合的。也就是我们的变换矩阵应该是下面这个形式的，其中，$H$是表征旋转和平移的齐次矩阵，$H\\in \\mathbb{R}^{3\\times 4}$。</p>\n<script type=\"math/tex; mode=display\">P^\\prime = MHP</script><p>首先考虑较为简单的平移。所做的修正如下所示，也就是将原来的$\\mathbb{0}$矩阵变为了一个平移向量。<br><img src=\"/img/case_m_5.png\" alt=\"case 5: M\"></p>\n<p>进一步考虑旋转时，我们将旋转分解为绕三个坐标轴的三次旋转，单次旋转矩阵如下所示：<br><img src=\"/img/rotation_matrix.png\" alt=\"绕单轴的旋转矩阵\"></p>\n<p>将它们按照旋转顺序左乘起来，可以得到修正后的变换形式如下：<br><img src=\"/img/case_6_m.png\" alt=\"case 6: M\"></p>\n<h4 id=\"最终形式\"><a href=\"#最终形式\" class=\"headerlink\" title=\"最终形式\"></a>最终形式</h4><p>综上所示，变换矩阵的最终形式为：<br><img src=\"/img/generic_projection_matrix.png\" alt=\"最终的投影变换矩阵\"></p>\n<p>其中，内参数矩阵中有5个自由度，外参数矩阵中有6个自由度（3平移，3旋转）。</p>\n<p>上面的内容总结起来，如下图所示。<br><img src=\"/img/camera_model_things_to_remember.png\" alt=\"things to remember\"></p>\n<h2 id=\"对极几何\"><a href=\"#对极几何\" class=\"headerlink\" title=\"对极几何\"></a>对极几何</h2><h3 id=\"基础概念\"><a href=\"#基础概念\" class=\"headerlink\" title=\"基础概念\"></a>基础概念</h3><p>如下图所示，给出了对极几何中的几个基本概念。两个相机的中心分别位于$O$点和$O^{‘}$点，被观察物体位于$P$点。</p>\n<p><img src=\"/img/epipolar_fig.png\" alt=\"对极几何概念图示\"></p>\n<ul>\n<li>极点：$e$和$e^\\prime$点分别是$OO^\\prime$连线与两个成像平面的交点，也是相机中心在另一台相机成像平面上对应的像。</li>\n<li>极平面：点$O$，$O^\\prime$，$P$点共同确定的平面（灰色）</li>\n<li>极线：极平面与两个成像平面的交线，即$pe$和$p^\\prime e^\\prime$（蓝色）</li>\n<li>基线：两个相机中心的连线（黄色）</li>\n</ul>\n<h3 id=\"极线约束\"><a href=\"#极线约束\" class=\"headerlink\" title=\"极线约束\"></a>极线约束</h3><p>从上图可以看出，如果两台相机已经给定，那么给定物体点在左边相机像平面内的成像点，则右边相机成像平面内的点被唯一确定。如何能够找到这个对应点呢？或者换种说法，在对极几何中，如何描述这种不同相机成像点的约束关系呢？</p>\n<p>如下图所示，从上面的讨论我们已经知道，物点$P$在左右两个相机成像平面的坐标可以使用由相机内参数矩阵和外参数矩阵确定的齐次变换矩阵得到（下图在标注时，右侧相机的点应被标注为$p^\\prime$）。为了简化问题，在下面的讨论时，我们假定相机都是理想的针孔相机模型，也就是说，变换矩阵只和外参数矩阵（也即是相机的位姿决定）。再做简化，认为世界坐标系与左侧相机的相机坐标系重合。那么，就得到了图上所示的变换矩阵$M$和$M^\\prime$。其中，$R$是左右两个相机之间的旋转矩阵，$T$是$OO^\\prime$有向线段，表明相机中心的位移。<br><img src=\"/img/epipolar_constraint_1.png\" alt=\"极线约束1\"></p>\n<p>（下面的推导参考了<a href=\"http://www.cnblogs.com/gemstone/articles/2294551.html\" target=\"_blank\" rel=\"external\">博客：计算机视觉基础4——对极几何</a>）。在下面的推导中，我们使用$p^\\prime$表示在相机$O^\\prime$下的向量$O\\prime P$，符号$p$同理。那么，有如下关系成立：<br>$R(p-T) = p^\\prime$</p>\n","excerpt":"<p>数字图像是现实世界的物体通过光学成像设备在感光材料上的投影。在3D到2D的转换过程中，深度信息丢失。如何从单幅或者多幅图像中恢复出有用的3D信息，需要使用立体视觉的知识进行分析。如下图所示。这篇博客对课程讲义中立体视觉部分做一整理，分别介绍了针孔相机模型和对极集合的基础知识。<br><img src=\"/img/camera_geometry_application.png\" alt=\"立体视觉应用\"><br>","more":"</p>\n<h2 id=\"针孔相机模型（Pinhole-Camera）\"><a href=\"#针孔相机模型（Pinhole-Camera）\" class=\"headerlink\" title=\"针孔相机模型（Pinhole Camera）\"></a>针孔相机模型（Pinhole Camera）</h2><p>针孔相机是简化的相机模型。光线沿直线传播。物体反射的光线，通过针孔，在成像面形成倒立的影像。针孔与成像面的距离，称为焦距。一般而言，针孔越小，影像越清晰，但针孔太小，会导致衍射，反而令影像模糊。</p>\n<h3 id=\"投影几何的重要性质\"><a href=\"#投影几何的重要性质\" class=\"headerlink\" title=\"投影几何的重要性质\"></a>投影几何的重要性质</h3><p>在对针孔相机详细说明之前，首先介绍投影几何（Projective Geometry）的几条性质。</p>\n<p>在投影变换前后，长度和角度信息丢失，但直线仍然保持是直线。如下图所示。原来垂直和平行的台球桌面的边不再保持原有的角度和相对长度，但是仍然是直线。<br><img src=\"/img/projective_geometry_property_1.png\" alt=\"性质1\"></p>\n<p>另一方面，虽然3D世界中的平行线在投影变换后相交（我们将交点称为Vanishing point），但是3D世界中互相平行的线，在2D平面上的Vanishing point为同一点。而所有平行线的Vanishing point又在同一条直线上。如下图所示（竖直的绿色平行线在投影变换后仍然保持平行，实际上并不想交，但我们这里认为其交于无穷远的某点）。<br><img src=\"/img/projective_geometry_property_2.png\" alt=\"性质2\"></p>\n<h3 id=\"针孔相机模型\"><a href=\"#针孔相机模型\" class=\"headerlink\" title=\"针孔相机模型\"></a>针孔相机模型</h3><p>如下图所示，为针孔相机成像原理示意图。外部世界的一点$P$发出（或反射）的光线，经过小孔$O$，成像在在像平面$\\Pi^\\prime$的点$P^\\prime$处。通过简单的相似三角形知识，可以得到图上的关系式。<br><img src=\"/img/pinhole_camera_model.png\" alt=\"针孔相机模型示意图\"></p>\n<p>由上图可以知道，3D空间的一点$P$与成像平面上的对应点$P^\\prime$坐标之间的数量关系为：</p>\n<script type=\"math/tex; mode=display\">\\left\\{\\begin{matrix}\nx^\\prime = fx/z \\\\\ny^\\prime = fy/z\n\\end{matrix}\\right.</script><p>可是，由于变量$z$位于分母的位置，也就是说如果直接写成变换矩阵的话，这个矩阵是和变量$z$有关的，我们不想这样，而是想得到一个完全由相机本身确定的变换矩阵。所以，我们借鉴齐次矩阵的思想，将原来的点增加一个维度，转换为齐次坐标。如下图的两个例子所示。<br><img src=\"/img/qicizuobiao.png\" alt=\"齐次坐标\"></p>\n<p>这样，我们可以将3D和2D之间的变换写成下面的形式。当实际计算的时候，我们首先将3D点转换为4维向量（在末尾补1），然后左乘变换矩阵。最后将得到的结果化为末尾元素为1的形式，这样，前面两个元素就代表了变换后2D点的坐标。这样做，使得变换矩阵$M$完全由相机参数（即焦距$f$）决定。<br><img src=\"/img/qicizuobiao_transform.png\" alt=\"\"></p>\n<p>上面的式子只是理想情况下的相机模型，成立的假设条件较强，主要有以下几点假设：</p>\n<ol>\n<li>内假设（和相机本身有关）<ul>\n<li>不同方向上焦距相同；</li>\n<li>光学中心在相平面的坐标原点$(0, 0)$</li>\n<li>没有倾斜（no skew）</li>\n</ul>\n</li>\n<li>外假设（和相机位姿有关，和相机本身参数无关）<ul>\n<li>相机没有旋转（坐标轴与世界坐标系方向重合）</li>\n<li>相机没有平移（相机中心与世界坐标系中心重合）</li>\n</ul>\n</li>\n</ol>\n<p>其中，no skew情形我也不知道中文应该如何翻译。这种情况如图所示，由于剪切变形，成像平面的坐标轴并不是严格正交，使得两个轴之间实际存在一个很小的夹角。<br><img src=\"/img/camera_skew.png\" alt=\"what is &quot;skew&quot;?\"></p>\n<p>下面，按照slide的顺序逐渐将上述假设移去，分析变换矩阵$M$应该如何修正。</p>\n<h4 id=\"理想情况\"><a href=\"#理想情况\" class=\"headerlink\" title=\"理想情况\"></a>理想情况</h4><p>理想情况以上假设全部满足，矩阵$M$如下所示。<br><img src=\"/img/case_1_m.png\" alt=\"case 1: M\"></p>\n<h4 id=\"光学中心不在像平面的坐标原点\"><a href=\"#光学中心不在像平面的坐标原点\" class=\"headerlink\" title=\"光学中心不在像平面的坐标原点\"></a>光学中心不在像平面的坐标原点</h4><p>假设光学中心位于像平面内坐标为$(u_0, v_0)$的位置。则需要在理想情况计算的坐标基础上加上这个偏移量。所以，矩阵$M$被修正为：<br><img src=\"/img/case_2_m.png\" alt=\"case 2: M\"></p>\n<h4 id=\"像素非正方形\"><a href=\"#像素非正方形\" class=\"headerlink\" title=\"像素非正方形\"></a>像素非正方形</h4><p>由于透镜在$x$和$y$方向上的焦距不同，使得两个方向上的比例因子不同，不能再使用同一个$f$计算。修正如下：<br><img src=\"/img/case_3_m.png\" alt=\"case 3: M\"></p>\n<h4 id=\"no-skew\"><a href=\"#no-skew\" class=\"headerlink\" title=\"no skew\"></a>no skew</h4><p>这时候$x$方向上其实有$y$轴坐标的分量，所以修正如下：<br><img src=\"/img/case_4_m.png\" alt=\"case 4: M\"></p>\n<h4 id=\"相机的旋转和平移\"><a href=\"#相机的旋转和平移\" class=\"headerlink\" title=\"相机的旋转和平移\"></a>相机的旋转和平移</h4><p>相机外部姿态很可能经过了旋转和平移，从而不与世界坐标系完全重合。如图所示。<br><img src=\"/img/camera_translation_rotation.png\" alt=\"相机旋转和平移\"></p>\n<p>所以我们需要先对世界坐标系内的3D点做一个齐次变换，将其变换为一个中间坐标系，这个中间坐标系和相机坐标系是重合的。也就是我们的变换矩阵应该是下面这个形式的，其中，$H$是表征旋转和平移的齐次矩阵，$H\\in \\mathbb{R}^{3\\times 4}$。</p>\n<script type=\"math/tex; mode=display\">P^\\prime = MHP</script><p>首先考虑较为简单的平移。所做的修正如下所示，也就是将原来的$\\mathbb{0}$矩阵变为了一个平移向量。<br><img src=\"/img/case_m_5.png\" alt=\"case 5: M\"></p>\n<p>进一步考虑旋转时，我们将旋转分解为绕三个坐标轴的三次旋转，单次旋转矩阵如下所示：<br><img src=\"/img/rotation_matrix.png\" alt=\"绕单轴的旋转矩阵\"></p>\n<p>将它们按照旋转顺序左乘起来，可以得到修正后的变换形式如下：<br><img src=\"/img/case_6_m.png\" alt=\"case 6: M\"></p>\n<h4 id=\"最终形式\"><a href=\"#最终形式\" class=\"headerlink\" title=\"最终形式\"></a>最终形式</h4><p>综上所示，变换矩阵的最终形式为：<br><img src=\"/img/generic_projection_matrix.png\" alt=\"最终的投影变换矩阵\"></p>\n<p>其中，内参数矩阵中有5个自由度，外参数矩阵中有6个自由度（3平移，3旋转）。</p>\n<p>上面的内容总结起来，如下图所示。<br><img src=\"/img/camera_model_things_to_remember.png\" alt=\"things to remember\"></p>\n<h2 id=\"对极几何\"><a href=\"#对极几何\" class=\"headerlink\" title=\"对极几何\"></a>对极几何</h2><h3 id=\"基础概念\"><a href=\"#基础概念\" class=\"headerlink\" title=\"基础概念\"></a>基础概念</h3><p>如下图所示，给出了对极几何中的几个基本概念。两个相机的中心分别位于$O$点和$O^{‘}$点，被观察物体位于$P$点。</p>\n<p><img src=\"/img/epipolar_fig.png\" alt=\"对极几何概念图示\"></p>\n<ul>\n<li>极点：$e$和$e^\\prime$点分别是$OO^\\prime$连线与两个成像平面的交点，也是相机中心在另一台相机成像平面上对应的像。</li>\n<li>极平面：点$O$，$O^\\prime$，$P$点共同确定的平面（灰色）</li>\n<li>极线：极平面与两个成像平面的交线，即$pe$和$p^\\prime e^\\prime$（蓝色）</li>\n<li>基线：两个相机中心的连线（黄色）</li>\n</ul>\n<h3 id=\"极线约束\"><a href=\"#极线约束\" class=\"headerlink\" title=\"极线约束\"></a>极线约束</h3><p>从上图可以看出，如果两台相机已经给定，那么给定物体点在左边相机像平面内的成像点，则右边相机成像平面内的点被唯一确定。如何能够找到这个对应点呢？或者换种说法，在对极几何中，如何描述这种不同相机成像点的约束关系呢？</p>\n<p>如下图所示，从上面的讨论我们已经知道，物点$P$在左右两个相机成像平面的坐标可以使用由相机内参数矩阵和外参数矩阵确定的齐次变换矩阵得到（下图在标注时，右侧相机的点应被标注为$p^\\prime$）。为了简化问题，在下面的讨论时，我们假定相机都是理想的针孔相机模型，也就是说，变换矩阵只和外参数矩阵（也即是相机的位姿决定）。再做简化，认为世界坐标系与左侧相机的相机坐标系重合。那么，就得到了图上所示的变换矩阵$M$和$M^\\prime$。其中，$R$是左右两个相机之间的旋转矩阵，$T$是$OO^\\prime$有向线段，表明相机中心的位移。<br><img src=\"/img/epipolar_constraint_1.png\" alt=\"极线约束1\"></p>\n<p>（下面的推导参考了<a href=\"http://www.cnblogs.com/gemstone/articles/2294551.html\">博客：计算机视觉基础4——对极几何</a>）。在下面的推导中，我们使用$p^\\prime$表示在相机$O^\\prime$下的向量$O\\prime P$，符号$p$同理。那么，有如下关系成立：<br>$R(p-T) = p^\\prime$</p>"},{"title":"CS131-线性滤波器和矩阵的SVD分解","date":"2017-01-23T04:19:05.000Z","_content":"\n数字图像可以看做$\\mathbb{R}^2 \\rightarrow \\mathbb{R}^c$的映射，其中$c$是图像的channel数。使用信号与系统的角度来看，如果单独考察某个channel，可以将图像看做是二维离散系统。\n\n![SVD图示](/img/svd_picture.jpg)\n\n<!-- more -->\n## 卷积\n卷积的概念不再详述，利用不同的kernel与原始图像做卷积，就是对图像进行线性滤波的过程。卷积操作时，以某一个点为中心点，最终结果是这个点以及它的邻域点的线性组合，组合系数由kernel决定。一般kernel的大小取成奇数。如下图所示。（图片来自[博客《图像卷积与滤波的一些知识点》](http://blog.csdn.net/zouxy09/article/details/49080029)）\n![卷积操作示意图](/img/convolution.png)\n\n在卷积操作时，常常需要对图像做padding，常用的padding方法有：\n- zero padding，也就是填充0值。\n- edge replication，也就是复制边缘值进行填充。\n- mirror extension，也就是将图像看做是周期性的，相当于使用对侧像素值进行填充。\n\n## 作业1\n### 调整图像灰度值为0到255\n\n计算相应的k和offset值即可。另外MATLAB中的`uint8`函数可以将结果削顶与截底为0到255之间。\n``` matlab\nscale_ratio = 255.0 / (max_val - min_val);\noffset = -min_val * scale_ratio;\nfixedimg = scale_ratio * dark + offset;\n```\n\n### SVD图像压缩\n\n使用SVD进行图像压缩，下图是将所有奇异值按照从大到小的顺序排列的大小示意图。可以看到，第一个奇异值比其他高出一个数量级。\n![SVD值大小示意图](/img/svd_ranking.png)\n\n#### MATLAB实现\n分别使用10，50， 100个分量进行图像压缩，如下图所示。可以看到，k=10时，已经能够复原出原图像的大致轮廓。当k更大时，更多细节被复原出来。\n![不同分量个数的图像压缩](/img/svd_flower.png)\n\nMATLAB代码如下：\n``` matlab\n%% read image\nim = imread('./flower.bmp');\nim_gray = double(rgb2gray(im));\n[u, s, v] = svd(im_gray);\n%% get sigular value\nsigma = diag(s);\ntop_k = sigma(1:10);\nfigure\nplot(1:length(sigma), sigma, 'r-', 'marker', 's', 'markerfacecolor', 'g');\n\nfigure\nsubplot(2, 2, 1);\nimshow(uint8(im_gray));\ntitle('flower.bmp')\nindex = 2;\nfor k = [10, 50, 100]\n    uk = u(:, 1:k);\n    sk = s(1:k, 1:k);\n    vk = v(:, 1:k);\n    im_rec = uk * sk * vk';\n    subplot(2, 2, index);\n    index = index + 1;\n    imshow(uint8(im_rec));\n    title(sprintf('k = %d', k));\nend\n```\n\n#### 图像SVD压缩中的误差分析\n完全是个人随手推导，不严格的说明：\n\n将矩阵分块。由SVD分解公式$\\mathbf{U}\\mathbf{\\Sigma} \\mathbf{V^\\dagger} = \\mathbf{A}$，把$\\mathbf{U}$按列分块，$\\mathbf{V^\\dagger}$按行分块，有下式成立：\n$$\n\\begin{bmatrix}\nu_1 & u_2 &\\vdots  &u_n\n\\end{bmatrix}\n\\begin{bmatrix}\n\\sigma_1 &  &  & \\\\\\\\\n &  \\sigma_2&  & \\\\\\\\\n &  &  \\ddots& \\\\\\\\\n &  &  &\\sigma_m\n\\end{bmatrix}\n\\begin{bmatrix}\nv_1^\\dagger\\\\\\\\\nv_2^\\dagger\\\\\\\\\n\\dots\\\\\\\\\nv_m^\\dagger\n\\end{bmatrix}=\\mathbf{A}\n$$\n\n由于\n$$\n\\begin{bmatrix}\nu_1 & u_2 &\\vdots  &u_n\n\\end{bmatrix}\n\\begin{bmatrix}\n\\sigma_1 &  &  & \\\\\\\\\n &  \\sigma_2&  & \\\\\\\\\n &  &  \\ddots& \\\\\\\\\n &  &  &\\sigma_m\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n\\sigma_1u_1 & \\sigma_2u_2 &\\vdots  &\\sigma_nu_n\n\\end{bmatrix}\n$$\n\n所以，\n\n$$\\mathbf{A} = \\sum_{i = 1}^{r}\\sigma_iu_iv_i^\\dagger$$\n\n上面的式子和式里面只有$r$项，是因为当$k > r$时，$\\sigma_k = 0$。\n\n所以$$\\mathbf{A} - \\hat{\\mathbf{A}} = \\sum_{i = k+1}^{r}\\sigma_iu_iv_i^\\dagger$$\n\n根绝矩阵范数的[性质](https://zh.wikipedia.org/wiki/矩陣範數)，我们有，\n$$\\left\\lVert\\mathbf{A} - \\hat{\\mathbf{A}}\\right\\rVert \\le \\sum_{i=k+1}^{r}\\sigma_i\\left\\lVert u_i\\right\\rVert\\left\\lVert v_i^\\dagger\\right\\rVert$$\n\n由于$u_i$和$v_i$都是标准正交基，所以范数小于1.故，\n\n$$\\left\\lVert\\mathbf{A} - \\hat{\\mathbf{A}}\\right\\rVert \\le \\sum_{i=k+1}^{r}\\sigma_i$$\n\n取无穷范数，可以知道对于误差矩阵中的任意元素（也就是压缩重建之后任意位置的像素灰度值之差），都有：\n\n$$e \\le \\sum_{i=k+1}^{r}\\sigma_i$$\n\n### SVD与矩阵范数\n\n如果某个函数$f$满足以下的性质，就可以作为矩阵的范数。\n- $f(\\mathbf{A}) = \\mathbf{0} \\Leftrightarrow \\mathbf{A} = \\mathbf{0}$\n- $f(c\\mathbf{A}) = c f(\\mathbf{A}), \\forall c \\in \\mathbb{R}$\n- $f(\\mathbf{A+b}) \\le f(\\mathbf{A}) + f(\\mathbf{B})$\n\n其中，矩阵的2范数可以定义为\n$$\\left\\lVert\\mathbf{A}\\right\\rVert_2 = \\max{\\sqrt{(\\mathbf{A}x)^\\dagger\\mathbf{A}x}}\n$$\n\n其中，$x$是单位向量。上式的意义在于表明矩阵的2范数是对于所有向量，经过该矩阵线性变换后摸长最大的那个变换后向量的长度。\n\n下面，给出不严格的说明，证明矩阵的2范数数值上等于其最大的奇异值。\n\n对于空间内的任意单位向量$x$，利用矩阵的SVD分解，有（为了书写简单，矩阵不再单独加粗）：\n$$(Ax)^\\dagger Ax = x^\\dagger V \\Sigma^\\dagger \\Sigma V^\\dagger x$$\n其中，$U^\\dagger U = I$，已经被消去了。\n\n进一步化简，我们将$V^\\dagger x$看做一个整体，令$\\omega = V\\dagger x$，那么有，\n$$(Ax)^\\dagger Ax = (\\Sigma \\omega)^\\dagger \\Sigma \\omega$$\n\n也就是说，矩阵的2范转换为了$\\Sigma \\omega$的幅值的最大值。由于$\\omega$是酉矩阵和一个单位向量的乘积，所以$\\omega$仍然是单位阵。\n\n由于$\\Sigma$是对角阵，所以$\\omega$与其相乘后，相当于每个分量分别被放大了$\\sigma_i$倍。即\n\n$$\\Sigma \\omega =\n\\begin{bmatrix}\n\\sigma_1 \\omega_1\\\\\\\\\n\\sigma_2 \\omega_2\\\\\\\\\n\\cdots\\\\\\\\\n\\sigma_n \\omega_n\n\\end{bmatrix}\n$$\n\n它的幅值平方为\n\n$$\\left\\lVert \\Sigma \\omega \\right \\rVert ^2 = \\sum_{i=1}^{n}\\sigma_i^2 \\omega_i^2 \\le \\sigma_{1} \\sum_{i=1}^{n}\\omega_i^2 = \\sigma_1^2$$\n\n当且仅当，$\\omega_1 = 1$, $\\omega_k = 0, k > 1$时取得等号。\n\n综上所述，矩阵2范数的值等于其最大的奇异值。\n\n矩阵的另一种范数定义方法Frobenius norm定义如下：\n$$\\left\\lVert A \\right\\rVert_{F} = \\sqrt{\\sum_{i=1}^{m}\\sum_{j=1}^{n}\\left\\vert a_{i,j}\\right\\rvert}$$\n\n如果我们两边平方，可以得到，矩阵的F范数实际等于某个矩阵的迹，见下式：\n\n$$\\left\\lVert A\\right \\rVert_F^2 = \\text{trace}(A^\\dagger A)$$\n\n利用矩阵的SVD分解，可以很容易得出，$\\text{trace}(A^\\dagger A) = \\sum_{i=1}^{r}\\sigma_i^2$\n\n说明如下：\n$$\\text{trace}(A^\\dagger A) = \\text{trace}(V\\Sigma^\\dagger\\Sigma V^\\dagger)$$\n\n由于$V^\\dagger = V^{-1}$，而且$\\text{trace}(BAB^{-1}) = \\text{trace}(A)$，所以，\n$$\\text{trace}(A^\\dagger A) = \\text{trace}(\\Sigma^\\dagger \\Sigma) = \\sum_{i=1}^{r}\\sigma_i^2$$\n\n也就是说，矩阵的F范数等于它的奇异值平方和的平方根。\n\n$$\\left\\lVert A\\right\\rVert_F= \\sqrt{\\sum_{i=1}^{r}\\sigma_i^2}$$\n","source":"_posts/cs131-filter-svd.md","raw":"---\ntitle: CS131-线性滤波器和矩阵的SVD分解\ndate: 2017-01-23 12:19:05\ntags:\n     - cs131\n     - 公开课\n---\n\n数字图像可以看做$\\mathbb{R}^2 \\rightarrow \\mathbb{R}^c$的映射，其中$c$是图像的channel数。使用信号与系统的角度来看，如果单独考察某个channel，可以将图像看做是二维离散系统。\n\n![SVD图示](/img/svd_picture.jpg)\n\n<!-- more -->\n## 卷积\n卷积的概念不再详述，利用不同的kernel与原始图像做卷积，就是对图像进行线性滤波的过程。卷积操作时，以某一个点为中心点，最终结果是这个点以及它的邻域点的线性组合，组合系数由kernel决定。一般kernel的大小取成奇数。如下图所示。（图片来自[博客《图像卷积与滤波的一些知识点》](http://blog.csdn.net/zouxy09/article/details/49080029)）\n![卷积操作示意图](/img/convolution.png)\n\n在卷积操作时，常常需要对图像做padding，常用的padding方法有：\n- zero padding，也就是填充0值。\n- edge replication，也就是复制边缘值进行填充。\n- mirror extension，也就是将图像看做是周期性的，相当于使用对侧像素值进行填充。\n\n## 作业1\n### 调整图像灰度值为0到255\n\n计算相应的k和offset值即可。另外MATLAB中的`uint8`函数可以将结果削顶与截底为0到255之间。\n``` matlab\nscale_ratio = 255.0 / (max_val - min_val);\noffset = -min_val * scale_ratio;\nfixedimg = scale_ratio * dark + offset;\n```\n\n### SVD图像压缩\n\n使用SVD进行图像压缩，下图是将所有奇异值按照从大到小的顺序排列的大小示意图。可以看到，第一个奇异值比其他高出一个数量级。\n![SVD值大小示意图](/img/svd_ranking.png)\n\n#### MATLAB实现\n分别使用10，50， 100个分量进行图像压缩，如下图所示。可以看到，k=10时，已经能够复原出原图像的大致轮廓。当k更大时，更多细节被复原出来。\n![不同分量个数的图像压缩](/img/svd_flower.png)\n\nMATLAB代码如下：\n``` matlab\n%% read image\nim = imread('./flower.bmp');\nim_gray = double(rgb2gray(im));\n[u, s, v] = svd(im_gray);\n%% get sigular value\nsigma = diag(s);\ntop_k = sigma(1:10);\nfigure\nplot(1:length(sigma), sigma, 'r-', 'marker', 's', 'markerfacecolor', 'g');\n\nfigure\nsubplot(2, 2, 1);\nimshow(uint8(im_gray));\ntitle('flower.bmp')\nindex = 2;\nfor k = [10, 50, 100]\n    uk = u(:, 1:k);\n    sk = s(1:k, 1:k);\n    vk = v(:, 1:k);\n    im_rec = uk * sk * vk';\n    subplot(2, 2, index);\n    index = index + 1;\n    imshow(uint8(im_rec));\n    title(sprintf('k = %d', k));\nend\n```\n\n#### 图像SVD压缩中的误差分析\n完全是个人随手推导，不严格的说明：\n\n将矩阵分块。由SVD分解公式$\\mathbf{U}\\mathbf{\\Sigma} \\mathbf{V^\\dagger} = \\mathbf{A}$，把$\\mathbf{U}$按列分块，$\\mathbf{V^\\dagger}$按行分块，有下式成立：\n$$\n\\begin{bmatrix}\nu_1 & u_2 &\\vdots  &u_n\n\\end{bmatrix}\n\\begin{bmatrix}\n\\sigma_1 &  &  & \\\\\\\\\n &  \\sigma_2&  & \\\\\\\\\n &  &  \\ddots& \\\\\\\\\n &  &  &\\sigma_m\n\\end{bmatrix}\n\\begin{bmatrix}\nv_1^\\dagger\\\\\\\\\nv_2^\\dagger\\\\\\\\\n\\dots\\\\\\\\\nv_m^\\dagger\n\\end{bmatrix}=\\mathbf{A}\n$$\n\n由于\n$$\n\\begin{bmatrix}\nu_1 & u_2 &\\vdots  &u_n\n\\end{bmatrix}\n\\begin{bmatrix}\n\\sigma_1 &  &  & \\\\\\\\\n &  \\sigma_2&  & \\\\\\\\\n &  &  \\ddots& \\\\\\\\\n &  &  &\\sigma_m\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n\\sigma_1u_1 & \\sigma_2u_2 &\\vdots  &\\sigma_nu_n\n\\end{bmatrix}\n$$\n\n所以，\n\n$$\\mathbf{A} = \\sum_{i = 1}^{r}\\sigma_iu_iv_i^\\dagger$$\n\n上面的式子和式里面只有$r$项，是因为当$k > r$时，$\\sigma_k = 0$。\n\n所以$$\\mathbf{A} - \\hat{\\mathbf{A}} = \\sum_{i = k+1}^{r}\\sigma_iu_iv_i^\\dagger$$\n\n根绝矩阵范数的[性质](https://zh.wikipedia.org/wiki/矩陣範數)，我们有，\n$$\\left\\lVert\\mathbf{A} - \\hat{\\mathbf{A}}\\right\\rVert \\le \\sum_{i=k+1}^{r}\\sigma_i\\left\\lVert u_i\\right\\rVert\\left\\lVert v_i^\\dagger\\right\\rVert$$\n\n由于$u_i$和$v_i$都是标准正交基，所以范数小于1.故，\n\n$$\\left\\lVert\\mathbf{A} - \\hat{\\mathbf{A}}\\right\\rVert \\le \\sum_{i=k+1}^{r}\\sigma_i$$\n\n取无穷范数，可以知道对于误差矩阵中的任意元素（也就是压缩重建之后任意位置的像素灰度值之差），都有：\n\n$$e \\le \\sum_{i=k+1}^{r}\\sigma_i$$\n\n### SVD与矩阵范数\n\n如果某个函数$f$满足以下的性质，就可以作为矩阵的范数。\n- $f(\\mathbf{A}) = \\mathbf{0} \\Leftrightarrow \\mathbf{A} = \\mathbf{0}$\n- $f(c\\mathbf{A}) = c f(\\mathbf{A}), \\forall c \\in \\mathbb{R}$\n- $f(\\mathbf{A+b}) \\le f(\\mathbf{A}) + f(\\mathbf{B})$\n\n其中，矩阵的2范数可以定义为\n$$\\left\\lVert\\mathbf{A}\\right\\rVert_2 = \\max{\\sqrt{(\\mathbf{A}x)^\\dagger\\mathbf{A}x}}\n$$\n\n其中，$x$是单位向量。上式的意义在于表明矩阵的2范数是对于所有向量，经过该矩阵线性变换后摸长最大的那个变换后向量的长度。\n\n下面，给出不严格的说明，证明矩阵的2范数数值上等于其最大的奇异值。\n\n对于空间内的任意单位向量$x$，利用矩阵的SVD分解，有（为了书写简单，矩阵不再单独加粗）：\n$$(Ax)^\\dagger Ax = x^\\dagger V \\Sigma^\\dagger \\Sigma V^\\dagger x$$\n其中，$U^\\dagger U = I$，已经被消去了。\n\n进一步化简，我们将$V^\\dagger x$看做一个整体，令$\\omega = V\\dagger x$，那么有，\n$$(Ax)^\\dagger Ax = (\\Sigma \\omega)^\\dagger \\Sigma \\omega$$\n\n也就是说，矩阵的2范转换为了$\\Sigma \\omega$的幅值的最大值。由于$\\omega$是酉矩阵和一个单位向量的乘积，所以$\\omega$仍然是单位阵。\n\n由于$\\Sigma$是对角阵，所以$\\omega$与其相乘后，相当于每个分量分别被放大了$\\sigma_i$倍。即\n\n$$\\Sigma \\omega =\n\\begin{bmatrix}\n\\sigma_1 \\omega_1\\\\\\\\\n\\sigma_2 \\omega_2\\\\\\\\\n\\cdots\\\\\\\\\n\\sigma_n \\omega_n\n\\end{bmatrix}\n$$\n\n它的幅值平方为\n\n$$\\left\\lVert \\Sigma \\omega \\right \\rVert ^2 = \\sum_{i=1}^{n}\\sigma_i^2 \\omega_i^2 \\le \\sigma_{1} \\sum_{i=1}^{n}\\omega_i^2 = \\sigma_1^2$$\n\n当且仅当，$\\omega_1 = 1$, $\\omega_k = 0, k > 1$时取得等号。\n\n综上所述，矩阵2范数的值等于其最大的奇异值。\n\n矩阵的另一种范数定义方法Frobenius norm定义如下：\n$$\\left\\lVert A \\right\\rVert_{F} = \\sqrt{\\sum_{i=1}^{m}\\sum_{j=1}^{n}\\left\\vert a_{i,j}\\right\\rvert}$$\n\n如果我们两边平方，可以得到，矩阵的F范数实际等于某个矩阵的迹，见下式：\n\n$$\\left\\lVert A\\right \\rVert_F^2 = \\text{trace}(A^\\dagger A)$$\n\n利用矩阵的SVD分解，可以很容易得出，$\\text{trace}(A^\\dagger A) = \\sum_{i=1}^{r}\\sigma_i^2$\n\n说明如下：\n$$\\text{trace}(A^\\dagger A) = \\text{trace}(V\\Sigma^\\dagger\\Sigma V^\\dagger)$$\n\n由于$V^\\dagger = V^{-1}$，而且$\\text{trace}(BAB^{-1}) = \\text{trace}(A)$，所以，\n$$\\text{trace}(A^\\dagger A) = \\text{trace}(\\Sigma^\\dagger \\Sigma) = \\sum_{i=1}^{r}\\sigma_i^2$$\n\n也就是说，矩阵的F范数等于它的奇异值平方和的平方根。\n\n$$\\left\\lVert A\\right\\rVert_F= \\sqrt{\\sum_{i=1}^{r}\\sigma_i^2}$$\n","slug":"cs131-filter-svd","published":1,"updated":"2018-01-12T06:22:20.459Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjcu6vcrp0007qu46luis53hl","content":"<p>数字图像可以看做$\\mathbb{R}^2 \\rightarrow \\mathbb{R}^c$的映射，其中$c$是图像的channel数。使用信号与系统的角度来看，如果单独考察某个channel，可以将图像看做是二维离散系统。</p>\n<p><img src=\"/img/svd_picture.jpg\" alt=\"SVD图示\"></p>\n<a id=\"more\"></a>\n<h2 id=\"卷积\"><a href=\"#卷积\" class=\"headerlink\" title=\"卷积\"></a>卷积</h2><p>卷积的概念不再详述，利用不同的kernel与原始图像做卷积，就是对图像进行线性滤波的过程。卷积操作时，以某一个点为中心点，最终结果是这个点以及它的邻域点的线性组合，组合系数由kernel决定。一般kernel的大小取成奇数。如下图所示。（图片来自<a href=\"http://blog.csdn.net/zouxy09/article/details/49080029\" target=\"_blank\" rel=\"external\">博客《图像卷积与滤波的一些知识点》</a>）<br><img src=\"/img/convolution.png\" alt=\"卷积操作示意图\"></p>\n<p>在卷积操作时，常常需要对图像做padding，常用的padding方法有：</p>\n<ul>\n<li>zero padding，也就是填充0值。</li>\n<li>edge replication，也就是复制边缘值进行填充。</li>\n<li>mirror extension，也就是将图像看做是周期性的，相当于使用对侧像素值进行填充。</li>\n</ul>\n<h2 id=\"作业1\"><a href=\"#作业1\" class=\"headerlink\" title=\"作业1\"></a>作业1</h2><h3 id=\"调整图像灰度值为0到255\"><a href=\"#调整图像灰度值为0到255\" class=\"headerlink\" title=\"调整图像灰度值为0到255\"></a>调整图像灰度值为0到255</h3><p>计算相应的k和offset值即可。另外MATLAB中的<code>uint8</code>函数可以将结果削顶与截底为0到255之间。<br><figure class=\"highlight matlab\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\">scale_ratio = <span class=\"number\">255.0</span> / (max_val - min_val);</div><div class=\"line\">offset = -min_val * scale_ratio;</div><div class=\"line\">fixedimg = scale_ratio * dark + offset;</div></pre></td></tr></table></figure></p>\n<h3 id=\"SVD图像压缩\"><a href=\"#SVD图像压缩\" class=\"headerlink\" title=\"SVD图像压缩\"></a>SVD图像压缩</h3><p>使用SVD进行图像压缩，下图是将所有奇异值按照从大到小的顺序排列的大小示意图。可以看到，第一个奇异值比其他高出一个数量级。<br><img src=\"/img/svd_ranking.png\" alt=\"SVD值大小示意图\"></p>\n<h4 id=\"MATLAB实现\"><a href=\"#MATLAB实现\" class=\"headerlink\" title=\"MATLAB实现\"></a>MATLAB实现</h4><p>分别使用10，50， 100个分量进行图像压缩，如下图所示。可以看到，k=10时，已经能够复原出原图像的大致轮廓。当k更大时，更多细节被复原出来。<br><img src=\"/img/svd_flower.png\" alt=\"不同分量个数的图像压缩\"></p>\n<p>MATLAB代码如下：<br><figure class=\"highlight matlab\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\">%% read image</span></div><div class=\"line\">im = imread(<span class=\"string\">'./flower.bmp'</span>);</div><div class=\"line\">im_gray = double(rgb2gray(im));</div><div class=\"line\">[u, s, v] = svd(im_gray);</div><div class=\"line\"><span class=\"comment\">%% get sigular value</span></div><div class=\"line\">sigma = <span class=\"built_in\">diag</span>(s);</div><div class=\"line\">top_k = sigma(<span class=\"number\">1</span>:<span class=\"number\">10</span>);</div><div class=\"line\">figure</div><div class=\"line\">plot(<span class=\"number\">1</span>:<span class=\"built_in\">length</span>(sigma), sigma, <span class=\"string\">'r-'</span>, <span class=\"string\">'marker'</span>, <span class=\"string\">'s'</span>, <span class=\"string\">'markerfacecolor'</span>, <span class=\"string\">'g'</span>);</div><div class=\"line\"></div><div class=\"line\">figure</div><div class=\"line\">subplot(<span class=\"number\">2</span>, <span class=\"number\">2</span>, <span class=\"number\">1</span>);</div><div class=\"line\">imshow(uint8(im_gray));</div><div class=\"line\">title(<span class=\"string\">'flower.bmp'</span>)</div><div class=\"line\">index = <span class=\"number\">2</span>;</div><div class=\"line\"><span class=\"keyword\">for</span> k = [<span class=\"number\">10</span>, <span class=\"number\">50</span>, <span class=\"number\">100</span>]</div><div class=\"line\">    uk = u(:, <span class=\"number\">1</span>:k);</div><div class=\"line\">    sk = s(<span class=\"number\">1</span>:k, <span class=\"number\">1</span>:k);</div><div class=\"line\">    vk = v(:, <span class=\"number\">1</span>:k);</div><div class=\"line\">    im_rec = uk * sk * vk';</div><div class=\"line\">    subplot(<span class=\"number\">2</span>, <span class=\"number\">2</span>, index);</div><div class=\"line\">    index = index + <span class=\"number\">1</span>;</div><div class=\"line\">    imshow(uint8(im_rec));</div><div class=\"line\">    title(sprintf(<span class=\"string\">'k = %d'</span>, k));</div><div class=\"line\"><span class=\"keyword\">end</span></div></pre></td></tr></table></figure></p>\n<h4 id=\"图像SVD压缩中的误差分析\"><a href=\"#图像SVD压缩中的误差分析\" class=\"headerlink\" title=\"图像SVD压缩中的误差分析\"></a>图像SVD压缩中的误差分析</h4><p>完全是个人随手推导，不严格的说明：</p>\n<p>将矩阵分块。由SVD分解公式$\\mathbf{U}\\mathbf{\\Sigma} \\mathbf{V^\\dagger} = \\mathbf{A}$，把$\\mathbf{U}$按列分块，$\\mathbf{V^\\dagger}$按行分块，有下式成立：</p>\n<script type=\"math/tex; mode=display\">\n\\begin{bmatrix}\nu_1 & u_2 &\\vdots  &u_n\n\\end{bmatrix}\n\\begin{bmatrix}\n\\sigma_1 &  &  & \\\\\\\\\n &  \\sigma_2&  & \\\\\\\\\n &  &  \\ddots& \\\\\\\\\n &  &  &\\sigma_m\n\\end{bmatrix}\n\\begin{bmatrix}\nv_1^\\dagger\\\\\\\\\nv_2^\\dagger\\\\\\\\\n\\dots\\\\\\\\\nv_m^\\dagger\n\\end{bmatrix}=\\mathbf{A}</script><p>由于</p>\n<script type=\"math/tex; mode=display\">\n\\begin{bmatrix}\nu_1 & u_2 &\\vdots  &u_n\n\\end{bmatrix}\n\\begin{bmatrix}\n\\sigma_1 &  &  & \\\\\\\\\n &  \\sigma_2&  & \\\\\\\\\n &  &  \\ddots& \\\\\\\\\n &  &  &\\sigma_m\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n\\sigma_1u_1 & \\sigma_2u_2 &\\vdots  &\\sigma_nu_n\n\\end{bmatrix}</script><p>所以，</p>\n<script type=\"math/tex; mode=display\">\\mathbf{A} = \\sum_{i = 1}^{r}\\sigma_iu_iv_i^\\dagger</script><p>上面的式子和式里面只有$r$项，是因为当$k &gt; r$时，$\\sigma_k = 0$。</p>\n<p>所以<script type=\"math/tex\">\\mathbf{A} - \\hat{\\mathbf{A}} = \\sum_{i = k+1}^{r}\\sigma_iu_iv_i^\\dagger</script></p>\n<p>根绝矩阵范数的<a href=\"https://zh.wikipedia.org/wiki/矩陣範數\" target=\"_blank\" rel=\"external\">性质</a>，我们有，</p>\n<script type=\"math/tex; mode=display\">\\left\\lVert\\mathbf{A} - \\hat{\\mathbf{A}}\\right\\rVert \\le \\sum_{i=k+1}^{r}\\sigma_i\\left\\lVert u_i\\right\\rVert\\left\\lVert v_i^\\dagger\\right\\rVert</script><p>由于$u_i$和$v_i$都是标准正交基，所以范数小于1.故，</p>\n<script type=\"math/tex; mode=display\">\\left\\lVert\\mathbf{A} - \\hat{\\mathbf{A}}\\right\\rVert \\le \\sum_{i=k+1}^{r}\\sigma_i</script><p>取无穷范数，可以知道对于误差矩阵中的任意元素（也就是压缩重建之后任意位置的像素灰度值之差），都有：</p>\n<script type=\"math/tex; mode=display\">e \\le \\sum_{i=k+1}^{r}\\sigma_i</script><h3 id=\"SVD与矩阵范数\"><a href=\"#SVD与矩阵范数\" class=\"headerlink\" title=\"SVD与矩阵范数\"></a>SVD与矩阵范数</h3><p>如果某个函数$f$满足以下的性质，就可以作为矩阵的范数。</p>\n<ul>\n<li>$f(\\mathbf{A}) = \\mathbf{0} \\Leftrightarrow \\mathbf{A} = \\mathbf{0}$</li>\n<li>$f(c\\mathbf{A}) = c f(\\mathbf{A}), \\forall c \\in \\mathbb{R}$</li>\n<li>$f(\\mathbf{A+b}) \\le f(\\mathbf{A}) + f(\\mathbf{B})$</li>\n</ul>\n<p>其中，矩阵的2范数可以定义为</p>\n<script type=\"math/tex; mode=display\">\\left\\lVert\\mathbf{A}\\right\\rVert_2 = \\max{\\sqrt{(\\mathbf{A}x)^\\dagger\\mathbf{A}x}}</script><p>其中，$x$是单位向量。上式的意义在于表明矩阵的2范数是对于所有向量，经过该矩阵线性变换后摸长最大的那个变换后向量的长度。</p>\n<p>下面，给出不严格的说明，证明矩阵的2范数数值上等于其最大的奇异值。</p>\n<p>对于空间内的任意单位向量$x$，利用矩阵的SVD分解，有（为了书写简单，矩阵不再单独加粗）：</p>\n<script type=\"math/tex; mode=display\">(Ax)^\\dagger Ax = x^\\dagger V \\Sigma^\\dagger \\Sigma V^\\dagger x</script><p>其中，$U^\\dagger U = I$，已经被消去了。</p>\n<p>进一步化简，我们将$V^\\dagger x$看做一个整体，令$\\omega = V\\dagger x$，那么有，</p>\n<script type=\"math/tex; mode=display\">(Ax)^\\dagger Ax = (\\Sigma \\omega)^\\dagger \\Sigma \\omega</script><p>也就是说，矩阵的2范转换为了$\\Sigma \\omega$的幅值的最大值。由于$\\omega$是酉矩阵和一个单位向量的乘积，所以$\\omega$仍然是单位阵。</p>\n<p>由于$\\Sigma$是对角阵，所以$\\omega$与其相乘后，相当于每个分量分别被放大了$\\sigma_i$倍。即</p>\n<script type=\"math/tex; mode=display\">\\Sigma \\omega =\n\\begin{bmatrix}\n\\sigma_1 \\omega_1\\\\\\\\\n\\sigma_2 \\omega_2\\\\\\\\\n\\cdots\\\\\\\\\n\\sigma_n \\omega_n\n\\end{bmatrix}</script><p>它的幅值平方为</p>\n<script type=\"math/tex; mode=display\">\\left\\lVert \\Sigma \\omega \\right \\rVert ^2 = \\sum_{i=1}^{n}\\sigma_i^2 \\omega_i^2 \\le \\sigma_{1} \\sum_{i=1}^{n}\\omega_i^2 = \\sigma_1^2</script><p>当且仅当，$\\omega_1 = 1$, $\\omega_k = 0, k &gt; 1$时取得等号。</p>\n<p>综上所述，矩阵2范数的值等于其最大的奇异值。</p>\n<p>矩阵的另一种范数定义方法Frobenius norm定义如下：</p>\n<script type=\"math/tex; mode=display\">\\left\\lVert A \\right\\rVert_{F} = \\sqrt{\\sum_{i=1}^{m}\\sum_{j=1}^{n}\\left\\vert a_{i,j}\\right\\rvert}</script><p>如果我们两边平方，可以得到，矩阵的F范数实际等于某个矩阵的迹，见下式：</p>\n<script type=\"math/tex; mode=display\">\\left\\lVert A\\right \\rVert_F^2 = \\text{trace}(A^\\dagger A)</script><p>利用矩阵的SVD分解，可以很容易得出，$\\text{trace}(A^\\dagger A) = \\sum_{i=1}^{r}\\sigma_i^2$</p>\n<p>说明如下：</p>\n<script type=\"math/tex; mode=display\">\\text{trace}(A^\\dagger A) = \\text{trace}(V\\Sigma^\\dagger\\Sigma V^\\dagger)</script><p>由于$V^\\dagger = V^{-1}$，而且$\\text{trace}(BAB^{-1}) = \\text{trace}(A)$，所以，</p>\n<script type=\"math/tex; mode=display\">\\text{trace}(A^\\dagger A) = \\text{trace}(\\Sigma^\\dagger \\Sigma) = \\sum_{i=1}^{r}\\sigma_i^2</script><p>也就是说，矩阵的F范数等于它的奇异值平方和的平方根。</p>\n<script type=\"math/tex; mode=display\">\\left\\lVert A\\right\\rVert_F= \\sqrt{\\sum_{i=1}^{r}\\sigma_i^2}</script>","excerpt":"<p>数字图像可以看做$\\mathbb{R}^2 \\rightarrow \\mathbb{R}^c$的映射，其中$c$是图像的channel数。使用信号与系统的角度来看，如果单独考察某个channel，可以将图像看做是二维离散系统。</p>\n<p><img src=\"/img/svd_picture.jpg\" alt=\"SVD图示\"></p>","more":"<h2 id=\"卷积\"><a href=\"#卷积\" class=\"headerlink\" title=\"卷积\"></a>卷积</h2><p>卷积的概念不再详述，利用不同的kernel与原始图像做卷积，就是对图像进行线性滤波的过程。卷积操作时，以某一个点为中心点，最终结果是这个点以及它的邻域点的线性组合，组合系数由kernel决定。一般kernel的大小取成奇数。如下图所示。（图片来自<a href=\"http://blog.csdn.net/zouxy09/article/details/49080029\">博客《图像卷积与滤波的一些知识点》</a>）<br><img src=\"/img/convolution.png\" alt=\"卷积操作示意图\"></p>\n<p>在卷积操作时，常常需要对图像做padding，常用的padding方法有：</p>\n<ul>\n<li>zero padding，也就是填充0值。</li>\n<li>edge replication，也就是复制边缘值进行填充。</li>\n<li>mirror extension，也就是将图像看做是周期性的，相当于使用对侧像素值进行填充。</li>\n</ul>\n<h2 id=\"作业1\"><a href=\"#作业1\" class=\"headerlink\" title=\"作业1\"></a>作业1</h2><h3 id=\"调整图像灰度值为0到255\"><a href=\"#调整图像灰度值为0到255\" class=\"headerlink\" title=\"调整图像灰度值为0到255\"></a>调整图像灰度值为0到255</h3><p>计算相应的k和offset值即可。另外MATLAB中的<code>uint8</code>函数可以将结果削顶与截底为0到255之间。<br><figure class=\"highlight matlab\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\">scale_ratio = <span class=\"number\">255.0</span> / (max_val - min_val);</div><div class=\"line\">offset = -min_val * scale_ratio;</div><div class=\"line\">fixedimg = scale_ratio * dark + offset;</div></pre></td></tr></table></figure></p>\n<h3 id=\"SVD图像压缩\"><a href=\"#SVD图像压缩\" class=\"headerlink\" title=\"SVD图像压缩\"></a>SVD图像压缩</h3><p>使用SVD进行图像压缩，下图是将所有奇异值按照从大到小的顺序排列的大小示意图。可以看到，第一个奇异值比其他高出一个数量级。<br><img src=\"/img/svd_ranking.png\" alt=\"SVD值大小示意图\"></p>\n<h4 id=\"MATLAB实现\"><a href=\"#MATLAB实现\" class=\"headerlink\" title=\"MATLAB实现\"></a>MATLAB实现</h4><p>分别使用10，50， 100个分量进行图像压缩，如下图所示。可以看到，k=10时，已经能够复原出原图像的大致轮廓。当k更大时，更多细节被复原出来。<br><img src=\"/img/svd_flower.png\" alt=\"不同分量个数的图像压缩\"></p>\n<p>MATLAB代码如下：<br><figure class=\"highlight matlab\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\">%% read image</span></div><div class=\"line\">im = imread(<span class=\"string\">'./flower.bmp'</span>);</div><div class=\"line\">im_gray = double(rgb2gray(im));</div><div class=\"line\">[u, s, v] = svd(im_gray);</div><div class=\"line\"><span class=\"comment\">%% get sigular value</span></div><div class=\"line\">sigma = <span class=\"built_in\">diag</span>(s);</div><div class=\"line\">top_k = sigma(<span class=\"number\">1</span>:<span class=\"number\">10</span>);</div><div class=\"line\">figure</div><div class=\"line\">plot(<span class=\"number\">1</span>:<span class=\"built_in\">length</span>(sigma), sigma, <span class=\"string\">'r-'</span>, <span class=\"string\">'marker'</span>, <span class=\"string\">'s'</span>, <span class=\"string\">'markerfacecolor'</span>, <span class=\"string\">'g'</span>);</div><div class=\"line\"></div><div class=\"line\">figure</div><div class=\"line\">subplot(<span class=\"number\">2</span>, <span class=\"number\">2</span>, <span class=\"number\">1</span>);</div><div class=\"line\">imshow(uint8(im_gray));</div><div class=\"line\">title(<span class=\"string\">'flower.bmp'</span>)</div><div class=\"line\">index = <span class=\"number\">2</span>;</div><div class=\"line\"><span class=\"keyword\">for</span> k = [<span class=\"number\">10</span>, <span class=\"number\">50</span>, <span class=\"number\">100</span>]</div><div class=\"line\">    uk = u(:, <span class=\"number\">1</span>:k);</div><div class=\"line\">    sk = s(<span class=\"number\">1</span>:k, <span class=\"number\">1</span>:k);</div><div class=\"line\">    vk = v(:, <span class=\"number\">1</span>:k);</div><div class=\"line\">    im_rec = uk * sk * vk';</div><div class=\"line\">    subplot(<span class=\"number\">2</span>, <span class=\"number\">2</span>, index);</div><div class=\"line\">    index = index + <span class=\"number\">1</span>;</div><div class=\"line\">    imshow(uint8(im_rec));</div><div class=\"line\">    title(sprintf(<span class=\"string\">'k = %d'</span>, k));</div><div class=\"line\"><span class=\"keyword\">end</span></div></pre></td></tr></table></figure></p>\n<h4 id=\"图像SVD压缩中的误差分析\"><a href=\"#图像SVD压缩中的误差分析\" class=\"headerlink\" title=\"图像SVD压缩中的误差分析\"></a>图像SVD压缩中的误差分析</h4><p>完全是个人随手推导，不严格的说明：</p>\n<p>将矩阵分块。由SVD分解公式$\\mathbf{U}\\mathbf{\\Sigma} \\mathbf{V^\\dagger} = \\mathbf{A}$，把$\\mathbf{U}$按列分块，$\\mathbf{V^\\dagger}$按行分块，有下式成立：</p>\n<script type=\"math/tex; mode=display\">\n\\begin{bmatrix}\nu_1 & u_2 &\\vdots  &u_n\n\\end{bmatrix}\n\\begin{bmatrix}\n\\sigma_1 &  &  & \\\\\\\\\n &  \\sigma_2&  & \\\\\\\\\n &  &  \\ddots& \\\\\\\\\n &  &  &\\sigma_m\n\\end{bmatrix}\n\\begin{bmatrix}\nv_1^\\dagger\\\\\\\\\nv_2^\\dagger\\\\\\\\\n\\dots\\\\\\\\\nv_m^\\dagger\n\\end{bmatrix}=\\mathbf{A}</script><p>由于</p>\n<script type=\"math/tex; mode=display\">\n\\begin{bmatrix}\nu_1 & u_2 &\\vdots  &u_n\n\\end{bmatrix}\n\\begin{bmatrix}\n\\sigma_1 &  &  & \\\\\\\\\n &  \\sigma_2&  & \\\\\\\\\n &  &  \\ddots& \\\\\\\\\n &  &  &\\sigma_m\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n\\sigma_1u_1 & \\sigma_2u_2 &\\vdots  &\\sigma_nu_n\n\\end{bmatrix}</script><p>所以，</p>\n<script type=\"math/tex; mode=display\">\\mathbf{A} = \\sum_{i = 1}^{r}\\sigma_iu_iv_i^\\dagger</script><p>上面的式子和式里面只有$r$项，是因为当$k &gt; r$时，$\\sigma_k = 0$。</p>\n<p>所以<script type=\"math/tex\">\\mathbf{A} - \\hat{\\mathbf{A}} = \\sum_{i = k+1}^{r}\\sigma_iu_iv_i^\\dagger</script></p>\n<p>根绝矩阵范数的<a href=\"https://zh.wikipedia.org/wiki/矩陣範數\">性质</a>，我们有，</p>\n<script type=\"math/tex; mode=display\">\\left\\lVert\\mathbf{A} - \\hat{\\mathbf{A}}\\right\\rVert \\le \\sum_{i=k+1}^{r}\\sigma_i\\left\\lVert u_i\\right\\rVert\\left\\lVert v_i^\\dagger\\right\\rVert</script><p>由于$u_i$和$v_i$都是标准正交基，所以范数小于1.故，</p>\n<script type=\"math/tex; mode=display\">\\left\\lVert\\mathbf{A} - \\hat{\\mathbf{A}}\\right\\rVert \\le \\sum_{i=k+1}^{r}\\sigma_i</script><p>取无穷范数，可以知道对于误差矩阵中的任意元素（也就是压缩重建之后任意位置的像素灰度值之差），都有：</p>\n<script type=\"math/tex; mode=display\">e \\le \\sum_{i=k+1}^{r}\\sigma_i</script><h3 id=\"SVD与矩阵范数\"><a href=\"#SVD与矩阵范数\" class=\"headerlink\" title=\"SVD与矩阵范数\"></a>SVD与矩阵范数</h3><p>如果某个函数$f$满足以下的性质，就可以作为矩阵的范数。</p>\n<ul>\n<li>$f(\\mathbf{A}) = \\mathbf{0} \\Leftrightarrow \\mathbf{A} = \\mathbf{0}$</li>\n<li>$f(c\\mathbf{A}) = c f(\\mathbf{A}), \\forall c \\in \\mathbb{R}$</li>\n<li>$f(\\mathbf{A+b}) \\le f(\\mathbf{A}) + f(\\mathbf{B})$</li>\n</ul>\n<p>其中，矩阵的2范数可以定义为</p>\n<script type=\"math/tex; mode=display\">\\left\\lVert\\mathbf{A}\\right\\rVert_2 = \\max{\\sqrt{(\\mathbf{A}x)^\\dagger\\mathbf{A}x}}</script><p>其中，$x$是单位向量。上式的意义在于表明矩阵的2范数是对于所有向量，经过该矩阵线性变换后摸长最大的那个变换后向量的长度。</p>\n<p>下面，给出不严格的说明，证明矩阵的2范数数值上等于其最大的奇异值。</p>\n<p>对于空间内的任意单位向量$x$，利用矩阵的SVD分解，有（为了书写简单，矩阵不再单独加粗）：</p>\n<script type=\"math/tex; mode=display\">(Ax)^\\dagger Ax = x^\\dagger V \\Sigma^\\dagger \\Sigma V^\\dagger x</script><p>其中，$U^\\dagger U = I$，已经被消去了。</p>\n<p>进一步化简，我们将$V^\\dagger x$看做一个整体，令$\\omega = V\\dagger x$，那么有，</p>\n<script type=\"math/tex; mode=display\">(Ax)^\\dagger Ax = (\\Sigma \\omega)^\\dagger \\Sigma \\omega</script><p>也就是说，矩阵的2范转换为了$\\Sigma \\omega$的幅值的最大值。由于$\\omega$是酉矩阵和一个单位向量的乘积，所以$\\omega$仍然是单位阵。</p>\n<p>由于$\\Sigma$是对角阵，所以$\\omega$与其相乘后，相当于每个分量分别被放大了$\\sigma_i$倍。即</p>\n<script type=\"math/tex; mode=display\">\\Sigma \\omega =\n\\begin{bmatrix}\n\\sigma_1 \\omega_1\\\\\\\\\n\\sigma_2 \\omega_2\\\\\\\\\n\\cdots\\\\\\\\\n\\sigma_n \\omega_n\n\\end{bmatrix}</script><p>它的幅值平方为</p>\n<script type=\"math/tex; mode=display\">\\left\\lVert \\Sigma \\omega \\right \\rVert ^2 = \\sum_{i=1}^{n}\\sigma_i^2 \\omega_i^2 \\le \\sigma_{1} \\sum_{i=1}^{n}\\omega_i^2 = \\sigma_1^2</script><p>当且仅当，$\\omega_1 = 1$, $\\omega_k = 0, k &gt; 1$时取得等号。</p>\n<p>综上所述，矩阵2范数的值等于其最大的奇异值。</p>\n<p>矩阵的另一种范数定义方法Frobenius norm定义如下：</p>\n<script type=\"math/tex; mode=display\">\\left\\lVert A \\right\\rVert_{F} = \\sqrt{\\sum_{i=1}^{m}\\sum_{j=1}^{n}\\left\\vert a_{i,j}\\right\\rvert}</script><p>如果我们两边平方，可以得到，矩阵的F范数实际等于某个矩阵的迹，见下式：</p>\n<script type=\"math/tex; mode=display\">\\left\\lVert A\\right \\rVert_F^2 = \\text{trace}(A^\\dagger A)</script><p>利用矩阵的SVD分解，可以很容易得出，$\\text{trace}(A^\\dagger A) = \\sum_{i=1}^{r}\\sigma_i^2$</p>\n<p>说明如下：</p>\n<script type=\"math/tex; mode=display\">\\text{trace}(A^\\dagger A) = \\text{trace}(V\\Sigma^\\dagger\\Sigma V^\\dagger)</script><p>由于$V^\\dagger = V^{-1}$，而且$\\text{trace}(BAB^{-1}) = \\text{trace}(A)$，所以，</p>\n<script type=\"math/tex; mode=display\">\\text{trace}(A^\\dagger A) = \\text{trace}(\\Sigma^\\dagger \\Sigma) = \\sum_{i=1}^{r}\\sigma_i^2</script><p>也就是说，矩阵的F范数等于它的奇异值平方和的平方根。</p>\n<script type=\"math/tex; mode=display\">\\left\\lVert A\\right\\rVert_F= \\sqrt{\\sum_{i=1}^{r}\\sigma_i^2}</script>"},{"title":"CS131-边缘检测","date":"2017-01-24T02:42:47.000Z","_content":"边缘(Edge)在哺乳动物的视觉中有重要意义。在由若干卷积层构成的深度神经网络中，较低层的卷积层就被训练成为对特定形状的边缘做出响应。边缘检测也是计算机视觉和图像处理领域中一个重要的问题。\n\n![边缘检测图示](/img/edge_camera_man.png)\n<!-- more -->\n## 边缘的产生\n若仍采取将图像视作某一函数的观点，边缘是指这个函数中的不连续点。边缘检测是后续进行目标检测和形状识别的基础，也能够在立体视觉中恢复视角等。边缘的来源主要有以下几点：\n- 物体表面不平造成灰度值的不连续；\n- 深度值不同造成灰度值不连续；\n- 物体表面颜色的突变造成灰度值不连续\n\n## 朴素思想\n利用边缘是图像中不连续点的这一性质，可以通过计算图像的一阶导数，再找到一阶导数的极大值，即认为是边缘点。如下图所示，在白黑交界处，图像一阶导数的值非常大，表明此处灰度值变化剧烈，是边缘。\n![边缘点处导数很大](/img/edge_deriative.png)\n\n问题转换为如何求取图像的一阶导数（或梯度）。由于图像是离散的二元函数，所以下文不再区分求导与差分。\n\n在$x$方向上，令$g_x = \\frac{\\partial f}{\\partial x}$；在$y$方向上，令$g_y = \\frac{\\partial f}{\\partial y}$。梯度的大小和方向为\n$$g = \\lbrack g_x, g_y\\rbrack, \\theta = \\arctan(g_y/g_x)$$\n\n通过和Sobel算子等做卷积，可以求取两个正交方向上图像的一阶导数，并计算梯度，之后检测梯度的局部极大值就能够找出边缘点了。\n\n只是这种方法很容易受到噪声影响。如图所示，真实的边缘点被湮没在了噪声中。\n![噪声影响湮没了边缘点](/img/fun_noise.png)\n\n## 改进1：先平滑\n改进措施1，可以首先对图像进行高斯平滑，再按照上面的方法求取边缘点。根据卷积的性质，有：\n\n$$\\frac{d}{dx}(f\\ast g) = f\\ast\\frac{d}{dx}g$$\n\n所以我们可以先求取高斯核的一阶导数，再和原始图像直接做一次卷积就可以一举两得了。这样，引出了DoG(Deriative of Gaussian)。$x$方向的DoG如图所示。\n![x方向的DoG](/img/dog_x.png)\n\n进行高斯平滑不可避免会使图像中原本的细节部分模糊，所以需要在克服噪声和引入模糊之间做好折中。\n![不同](/img/dog_different_size.png)\n\n## 改进2：Canny检测子\n改进措施2，使用Canny检测子进行检测。Canny检测方法同样基于梯度，其基本原理如下：\n- 使用DoG计算梯度幅值和方向。\n- 非极大值抑制，这个过程需要根据梯度方向做线性插值。如图，沿着点$q$的梯度方向找到了$p$和$r$两个点。这两个点的梯度幅值需要根据其临近的两点做插值得到。\n- 利用梯度方向和边缘线互相垂直这一性质，如图，若已经确定点$p$为边缘点，则向它的梯度方向正交方向上寻找下一个边缘点（点$r$或$s$）。这一步也叫edge linking。\n\n![nms示意图](/img/canny_nms.png)\n![linking示意图](/img/canny_linking.png)\n\n同时，为了提高算法性能，Canny中采用了迟滞阈值的方法，设定`low`和`high`两个阈值，来判定某个点是否属于**强**或**弱**边缘点。在做edge linking的时候，从强边缘点开始，如果遇到了弱边缘点，则继续，直到某点的梯度幅值甚至比`low`还要小，则在此停止。\n\n## 改进3：RANSAC方法\n有的时候，我们并不是想要找到所有的边缘点，可能只是想找到图像中水平方向的某些边缘。这时候可以考虑采用RANSAC方法。\n\nRANSAC方法的思想在于，认为已有的feature大部分都是**好的**。这样，每次随机抽取出若干feature，建立model，再在整个feature集合上进行验证。那么由那些好的feature得到的model一定是得分较高的。（世界上还是好人多啊！）这样就剔除了离群点的影响。\n\n以直线拟合为例，在下图中，给出了使用RANSAC方法拟合直线的步骤。如图1所示，由于离群点的存在，如果直接使用最小二乘法进行拟合，拟合结果效果会很不理想。由于确定一条直线需要两个点，所以从点集中选取两个点，并计算拟合直线。并计算点集中的点在这条直线附近的个数，作为对模型好坏的判定，这些点是新的内点。找出最优的那条直线，使用其所有内点再进行拟合，重复上述操作，直至迭代终止。\n![ransac step](/img/ransac_step.png)\n\n上述RANSAC方法进行直线拟合的过程可以总结如下：\n![ransac line fit alg](/img/ransac_line_fit.png)\n\n按照上述思想，我分别使用最小二乘法和RANSAC方法尝试进行直线拟合。在下面的代码中，我首先产生了正常受到一定高斯噪声污染的数据（图中的红色点），这些点的真值都落在直线$y = 2x+1$上。而后，我随机变化了斜率和截距，以期产生一些离群点（图中的蓝色点）。当然，由于随机性，这种方法生成的点有可能仍然是内点。\n\n而后，我分别使用上述两种方法进行拟合。可以从结果图中看出，RANSAC（绿色线）能够有效避免离群点的干扰，获得更好的拟合效果。在某次实验中，两种方法的拟合结果如下：\n``` bash\nleast square: a = 3.319566, b = -1.446528\nransac method: a = 1.899640, b= 1.298608\n```\n![demo result](/img/line_fit_demo.png)\n\n实验使用的MATLAB代码如下：\n``` matlab\n%% generate data\nx = 0:1:10;\ny_gt = 2*x+1;\ny = y_gt + randn(size(y_gt));\nscatter(x, y, [], [1,0,0]);\nhold on\nout_x = 0:1:10;\nout_y = 5*rand(size(out_x)).*out_x + 4*rand(size(out_x));\nscatter(out_x, out_y, [], [0,0,1]);\nX = [x, out_x]';\nY = [y, out_y]';\nX = [X, ones(length(X), 1)];\n[a, b] = ls_fit(X, Y);\nplot(x, a*x+b, 'linestyle', '--', 'color', 'r');\n\n[ra, rb] = ransac_fit(X, Y, 100, 2, 0.5, 3);\nplot(x, ra*x+rb, 'linestyle', '-.', 'color', 'g');\nfprintf('least square: a = %f, b = %f\\n',a, b);\nfprintf('ransac method: a = %f, b= %f\\n', ra, rb)\nfunction [a, b] = ransac_fit(X, Y, k, n, t ,d)\n% ransac fit\n% k -- maximum iteration number\n% n -- smallest point numer required\n% t -- threshold to identify a point is fit well\n% d -- the number of nearby points to assert a model is fine\ndata = [X, Y];\nN = size(data, 1);\nbest_good_cnt = -1;\nbest_a = 0;\nbest_b = 0;\nfor i = 1:k\n    % sample point\n    idx = randsample(N, n);\n    data_sampled = data(idx, :);\n    % fit with least square\n    [a, b] = ls_fit(data_sampled(:, 1:2), data_sampled(:, 3));\n    % test model\n    not_sampled = ones(N, 1);\n    not_sampled(idx) = 0;\n    not_sampled_data = data(not_sampled == 1, :);\n    distance = abs(not_sampled_data(:, 1:2) * [a; b] - not_sampled_data(:, 3)) / sqrt(a^2+1);\n    inner_flag = distance < t;\n    good_cnt = sum(inner_flag);\n    if good_cnt >= d && good_cnt > best_good_cnt\n        best_good_cnt = good_cnt;\n        data_refine = data(find(inner_flag), :);\n        [a, b] = ls_fit(data_refine(:, 1:2), data_refine(:, 3));\n        best_a = a;\n        best_b = b;\n    end\n    fprintf('iteration %d, best_a = %f, best_b = %f\\n', i, best_a, best_b);\nend\na = best_a;\nb = best_b;\nend\n\nfunction [a, b] = ls_fit(X, Y)\n% least square fit\nA = X'*X\\X'*Y;\na = A(1);\nb = A(2);\nend\n```\n\n我们对RANSAC稍作分析，可以大概了解试验次数$k$的确定方法。\n\n仍然使用上述直线拟合的例子。如果所有点中内点所占的比例为$\\omega$，每次挑选$n$个点尝试（上述demo代码中取$n=2$）。那么每次挑选的两个点全部是内点的概率为$\\omega^n$。当选取的$n$个点全部为内点时，视为有效实验。那么，重复$k$次实验，有效实验次数为0的概率为$(1-\\omega^n)^k$。由于底数小于1，所以我们只需尽量增大$k$，就能够降低这种倒霉的概率。下图是不同$n$和$\\omega$情况下为了使得实验成功的概率大于0.99所需的$k$的分布。\n![k](/img/ransac_k.png)\n\nRANSAC方法的有点在于能够较为鲁棒地估计模型的参数，而且实现简单。缺点在于当离群点比例较大时，为保证实验成功所需的$k$值较大。这时候，可能Hough变换等基于投票的方法更适合用于图像中的直线检测问题。\n","source":"_posts/cs131-edge-detection.md","raw":"---\ntitle: CS131-边缘检测\ndate: 2017-01-24 10:42:47\ntags:\n     - cs131\n     - 公开课\n---\n边缘(Edge)在哺乳动物的视觉中有重要意义。在由若干卷积层构成的深度神经网络中，较低层的卷积层就被训练成为对特定形状的边缘做出响应。边缘检测也是计算机视觉和图像处理领域中一个重要的问题。\n\n![边缘检测图示](/img/edge_camera_man.png)\n<!-- more -->\n## 边缘的产生\n若仍采取将图像视作某一函数的观点，边缘是指这个函数中的不连续点。边缘检测是后续进行目标检测和形状识别的基础，也能够在立体视觉中恢复视角等。边缘的来源主要有以下几点：\n- 物体表面不平造成灰度值的不连续；\n- 深度值不同造成灰度值不连续；\n- 物体表面颜色的突变造成灰度值不连续\n\n## 朴素思想\n利用边缘是图像中不连续点的这一性质，可以通过计算图像的一阶导数，再找到一阶导数的极大值，即认为是边缘点。如下图所示，在白黑交界处，图像一阶导数的值非常大，表明此处灰度值变化剧烈，是边缘。\n![边缘点处导数很大](/img/edge_deriative.png)\n\n问题转换为如何求取图像的一阶导数（或梯度）。由于图像是离散的二元函数，所以下文不再区分求导与差分。\n\n在$x$方向上，令$g_x = \\frac{\\partial f}{\\partial x}$；在$y$方向上，令$g_y = \\frac{\\partial f}{\\partial y}$。梯度的大小和方向为\n$$g = \\lbrack g_x, g_y\\rbrack, \\theta = \\arctan(g_y/g_x)$$\n\n通过和Sobel算子等做卷积，可以求取两个正交方向上图像的一阶导数，并计算梯度，之后检测梯度的局部极大值就能够找出边缘点了。\n\n只是这种方法很容易受到噪声影响。如图所示，真实的边缘点被湮没在了噪声中。\n![噪声影响湮没了边缘点](/img/fun_noise.png)\n\n## 改进1：先平滑\n改进措施1，可以首先对图像进行高斯平滑，再按照上面的方法求取边缘点。根据卷积的性质，有：\n\n$$\\frac{d}{dx}(f\\ast g) = f\\ast\\frac{d}{dx}g$$\n\n所以我们可以先求取高斯核的一阶导数，再和原始图像直接做一次卷积就可以一举两得了。这样，引出了DoG(Deriative of Gaussian)。$x$方向的DoG如图所示。\n![x方向的DoG](/img/dog_x.png)\n\n进行高斯平滑不可避免会使图像中原本的细节部分模糊，所以需要在克服噪声和引入模糊之间做好折中。\n![不同](/img/dog_different_size.png)\n\n## 改进2：Canny检测子\n改进措施2，使用Canny检测子进行检测。Canny检测方法同样基于梯度，其基本原理如下：\n- 使用DoG计算梯度幅值和方向。\n- 非极大值抑制，这个过程需要根据梯度方向做线性插值。如图，沿着点$q$的梯度方向找到了$p$和$r$两个点。这两个点的梯度幅值需要根据其临近的两点做插值得到。\n- 利用梯度方向和边缘线互相垂直这一性质，如图，若已经确定点$p$为边缘点，则向它的梯度方向正交方向上寻找下一个边缘点（点$r$或$s$）。这一步也叫edge linking。\n\n![nms示意图](/img/canny_nms.png)\n![linking示意图](/img/canny_linking.png)\n\n同时，为了提高算法性能，Canny中采用了迟滞阈值的方法，设定`low`和`high`两个阈值，来判定某个点是否属于**强**或**弱**边缘点。在做edge linking的时候，从强边缘点开始，如果遇到了弱边缘点，则继续，直到某点的梯度幅值甚至比`low`还要小，则在此停止。\n\n## 改进3：RANSAC方法\n有的时候，我们并不是想要找到所有的边缘点，可能只是想找到图像中水平方向的某些边缘。这时候可以考虑采用RANSAC方法。\n\nRANSAC方法的思想在于，认为已有的feature大部分都是**好的**。这样，每次随机抽取出若干feature，建立model，再在整个feature集合上进行验证。那么由那些好的feature得到的model一定是得分较高的。（世界上还是好人多啊！）这样就剔除了离群点的影响。\n\n以直线拟合为例，在下图中，给出了使用RANSAC方法拟合直线的步骤。如图1所示，由于离群点的存在，如果直接使用最小二乘法进行拟合，拟合结果效果会很不理想。由于确定一条直线需要两个点，所以从点集中选取两个点，并计算拟合直线。并计算点集中的点在这条直线附近的个数，作为对模型好坏的判定，这些点是新的内点。找出最优的那条直线，使用其所有内点再进行拟合，重复上述操作，直至迭代终止。\n![ransac step](/img/ransac_step.png)\n\n上述RANSAC方法进行直线拟合的过程可以总结如下：\n![ransac line fit alg](/img/ransac_line_fit.png)\n\n按照上述思想，我分别使用最小二乘法和RANSAC方法尝试进行直线拟合。在下面的代码中，我首先产生了正常受到一定高斯噪声污染的数据（图中的红色点），这些点的真值都落在直线$y = 2x+1$上。而后，我随机变化了斜率和截距，以期产生一些离群点（图中的蓝色点）。当然，由于随机性，这种方法生成的点有可能仍然是内点。\n\n而后，我分别使用上述两种方法进行拟合。可以从结果图中看出，RANSAC（绿色线）能够有效避免离群点的干扰，获得更好的拟合效果。在某次实验中，两种方法的拟合结果如下：\n``` bash\nleast square: a = 3.319566, b = -1.446528\nransac method: a = 1.899640, b= 1.298608\n```\n![demo result](/img/line_fit_demo.png)\n\n实验使用的MATLAB代码如下：\n``` matlab\n%% generate data\nx = 0:1:10;\ny_gt = 2*x+1;\ny = y_gt + randn(size(y_gt));\nscatter(x, y, [], [1,0,0]);\nhold on\nout_x = 0:1:10;\nout_y = 5*rand(size(out_x)).*out_x + 4*rand(size(out_x));\nscatter(out_x, out_y, [], [0,0,1]);\nX = [x, out_x]';\nY = [y, out_y]';\nX = [X, ones(length(X), 1)];\n[a, b] = ls_fit(X, Y);\nplot(x, a*x+b, 'linestyle', '--', 'color', 'r');\n\n[ra, rb] = ransac_fit(X, Y, 100, 2, 0.5, 3);\nplot(x, ra*x+rb, 'linestyle', '-.', 'color', 'g');\nfprintf('least square: a = %f, b = %f\\n',a, b);\nfprintf('ransac method: a = %f, b= %f\\n', ra, rb)\nfunction [a, b] = ransac_fit(X, Y, k, n, t ,d)\n% ransac fit\n% k -- maximum iteration number\n% n -- smallest point numer required\n% t -- threshold to identify a point is fit well\n% d -- the number of nearby points to assert a model is fine\ndata = [X, Y];\nN = size(data, 1);\nbest_good_cnt = -1;\nbest_a = 0;\nbest_b = 0;\nfor i = 1:k\n    % sample point\n    idx = randsample(N, n);\n    data_sampled = data(idx, :);\n    % fit with least square\n    [a, b] = ls_fit(data_sampled(:, 1:2), data_sampled(:, 3));\n    % test model\n    not_sampled = ones(N, 1);\n    not_sampled(idx) = 0;\n    not_sampled_data = data(not_sampled == 1, :);\n    distance = abs(not_sampled_data(:, 1:2) * [a; b] - not_sampled_data(:, 3)) / sqrt(a^2+1);\n    inner_flag = distance < t;\n    good_cnt = sum(inner_flag);\n    if good_cnt >= d && good_cnt > best_good_cnt\n        best_good_cnt = good_cnt;\n        data_refine = data(find(inner_flag), :);\n        [a, b] = ls_fit(data_refine(:, 1:2), data_refine(:, 3));\n        best_a = a;\n        best_b = b;\n    end\n    fprintf('iteration %d, best_a = %f, best_b = %f\\n', i, best_a, best_b);\nend\na = best_a;\nb = best_b;\nend\n\nfunction [a, b] = ls_fit(X, Y)\n% least square fit\nA = X'*X\\X'*Y;\na = A(1);\nb = A(2);\nend\n```\n\n我们对RANSAC稍作分析，可以大概了解试验次数$k$的确定方法。\n\n仍然使用上述直线拟合的例子。如果所有点中内点所占的比例为$\\omega$，每次挑选$n$个点尝试（上述demo代码中取$n=2$）。那么每次挑选的两个点全部是内点的概率为$\\omega^n$。当选取的$n$个点全部为内点时，视为有效实验。那么，重复$k$次实验，有效实验次数为0的概率为$(1-\\omega^n)^k$。由于底数小于1，所以我们只需尽量增大$k$，就能够降低这种倒霉的概率。下图是不同$n$和$\\omega$情况下为了使得实验成功的概率大于0.99所需的$k$的分布。\n![k](/img/ransac_k.png)\n\nRANSAC方法的有点在于能够较为鲁棒地估计模型的参数，而且实现简单。缺点在于当离群点比例较大时，为保证实验成功所需的$k$值较大。这时候，可能Hough变换等基于投票的方法更适合用于图像中的直线检测问题。\n","slug":"cs131-edge-detection","published":1,"updated":"2018-01-12T06:22:20.459Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjcu6vcrv0009qu461gjqi831","content":"<p>边缘(Edge)在哺乳动物的视觉中有重要意义。在由若干卷积层构成的深度神经网络中，较低层的卷积层就被训练成为对特定形状的边缘做出响应。边缘检测也是计算机视觉和图像处理领域中一个重要的问题。</p>\n<p><img src=\"/img/edge_camera_man.png\" alt=\"边缘检测图示\"><br><a id=\"more\"></a></p>\n<h2 id=\"边缘的产生\"><a href=\"#边缘的产生\" class=\"headerlink\" title=\"边缘的产生\"></a>边缘的产生</h2><p>若仍采取将图像视作某一函数的观点，边缘是指这个函数中的不连续点。边缘检测是后续进行目标检测和形状识别的基础，也能够在立体视觉中恢复视角等。边缘的来源主要有以下几点：</p>\n<ul>\n<li>物体表面不平造成灰度值的不连续；</li>\n<li>深度值不同造成灰度值不连续；</li>\n<li>物体表面颜色的突变造成灰度值不连续</li>\n</ul>\n<h2 id=\"朴素思想\"><a href=\"#朴素思想\" class=\"headerlink\" title=\"朴素思想\"></a>朴素思想</h2><p>利用边缘是图像中不连续点的这一性质，可以通过计算图像的一阶导数，再找到一阶导数的极大值，即认为是边缘点。如下图所示，在白黑交界处，图像一阶导数的值非常大，表明此处灰度值变化剧烈，是边缘。<br><img src=\"/img/edge_deriative.png\" alt=\"边缘点处导数很大\"></p>\n<p>问题转换为如何求取图像的一阶导数（或梯度）。由于图像是离散的二元函数，所以下文不再区分求导与差分。</p>\n<p>在$x$方向上，令$g_x = \\frac{\\partial f}{\\partial x}$；在$y$方向上，令$g_y = \\frac{\\partial f}{\\partial y}$。梯度的大小和方向为</p>\n<script type=\"math/tex; mode=display\">g = \\lbrack g_x, g_y\\rbrack, \\theta = \\arctan(g_y/g_x)</script><p>通过和Sobel算子等做卷积，可以求取两个正交方向上图像的一阶导数，并计算梯度，之后检测梯度的局部极大值就能够找出边缘点了。</p>\n<p>只是这种方法很容易受到噪声影响。如图所示，真实的边缘点被湮没在了噪声中。<br><img src=\"/img/fun_noise.png\" alt=\"噪声影响湮没了边缘点\"></p>\n<h2 id=\"改进1：先平滑\"><a href=\"#改进1：先平滑\" class=\"headerlink\" title=\"改进1：先平滑\"></a>改进1：先平滑</h2><p>改进措施1，可以首先对图像进行高斯平滑，再按照上面的方法求取边缘点。根据卷积的性质，有：</p>\n<script type=\"math/tex; mode=display\">\\frac{d}{dx}(f\\ast g) = f\\ast\\frac{d}{dx}g</script><p>所以我们可以先求取高斯核的一阶导数，再和原始图像直接做一次卷积就可以一举两得了。这样，引出了DoG(Deriative of Gaussian)。$x$方向的DoG如图所示。<br><img src=\"/img/dog_x.png\" alt=\"x方向的DoG\"></p>\n<p>进行高斯平滑不可避免会使图像中原本的细节部分模糊，所以需要在克服噪声和引入模糊之间做好折中。<br><img src=\"/img/dog_different_size.png\" alt=\"不同\"></p>\n<h2 id=\"改进2：Canny检测子\"><a href=\"#改进2：Canny检测子\" class=\"headerlink\" title=\"改进2：Canny检测子\"></a>改进2：Canny检测子</h2><p>改进措施2，使用Canny检测子进行检测。Canny检测方法同样基于梯度，其基本原理如下：</p>\n<ul>\n<li>使用DoG计算梯度幅值和方向。</li>\n<li>非极大值抑制，这个过程需要根据梯度方向做线性插值。如图，沿着点$q$的梯度方向找到了$p$和$r$两个点。这两个点的梯度幅值需要根据其临近的两点做插值得到。</li>\n<li>利用梯度方向和边缘线互相垂直这一性质，如图，若已经确定点$p$为边缘点，则向它的梯度方向正交方向上寻找下一个边缘点（点$r$或$s$）。这一步也叫edge linking。</li>\n</ul>\n<p><img src=\"/img/canny_nms.png\" alt=\"nms示意图\"><br><img src=\"/img/canny_linking.png\" alt=\"linking示意图\"></p>\n<p>同时，为了提高算法性能，Canny中采用了迟滞阈值的方法，设定<code>low</code>和<code>high</code>两个阈值，来判定某个点是否属于<strong>强</strong>或<strong>弱</strong>边缘点。在做edge linking的时候，从强边缘点开始，如果遇到了弱边缘点，则继续，直到某点的梯度幅值甚至比<code>low</code>还要小，则在此停止。</p>\n<h2 id=\"改进3：RANSAC方法\"><a href=\"#改进3：RANSAC方法\" class=\"headerlink\" title=\"改进3：RANSAC方法\"></a>改进3：RANSAC方法</h2><p>有的时候，我们并不是想要找到所有的边缘点，可能只是想找到图像中水平方向的某些边缘。这时候可以考虑采用RANSAC方法。</p>\n<p>RANSAC方法的思想在于，认为已有的feature大部分都是<strong>好的</strong>。这样，每次随机抽取出若干feature，建立model，再在整个feature集合上进行验证。那么由那些好的feature得到的model一定是得分较高的。（世界上还是好人多啊！）这样就剔除了离群点的影响。</p>\n<p>以直线拟合为例，在下图中，给出了使用RANSAC方法拟合直线的步骤。如图1所示，由于离群点的存在，如果直接使用最小二乘法进行拟合，拟合结果效果会很不理想。由于确定一条直线需要两个点，所以从点集中选取两个点，并计算拟合直线。并计算点集中的点在这条直线附近的个数，作为对模型好坏的判定，这些点是新的内点。找出最优的那条直线，使用其所有内点再进行拟合，重复上述操作，直至迭代终止。<br><img src=\"/img/ransac_step.png\" alt=\"ransac step\"></p>\n<p>上述RANSAC方法进行直线拟合的过程可以总结如下：<br><img src=\"/img/ransac_line_fit.png\" alt=\"ransac line fit alg\"></p>\n<p>按照上述思想，我分别使用最小二乘法和RANSAC方法尝试进行直线拟合。在下面的代码中，我首先产生了正常受到一定高斯噪声污染的数据（图中的红色点），这些点的真值都落在直线$y = 2x+1$上。而后，我随机变化了斜率和截距，以期产生一些离群点（图中的蓝色点）。当然，由于随机性，这种方法生成的点有可能仍然是内点。</p>\n<p>而后，我分别使用上述两种方法进行拟合。可以从结果图中看出，RANSAC（绿色线）能够有效避免离群点的干扰，获得更好的拟合效果。在某次实验中，两种方法的拟合结果如下：<br><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">least square: a = 3.319566, b = -1.446528</div><div class=\"line\">ransac method: a = 1.899640, b= 1.298608</div></pre></td></tr></table></figure></p>\n<p><img src=\"/img/line_fit_demo.png\" alt=\"demo result\"></p>\n<p>实验使用的MATLAB代码如下：<br><figure class=\"highlight matlab\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div><div class=\"line\">47</div><div class=\"line\">48</div><div class=\"line\">49</div><div class=\"line\">50</div><div class=\"line\">51</div><div class=\"line\">52</div><div class=\"line\">53</div><div class=\"line\">54</div><div class=\"line\">55</div><div class=\"line\">56</div><div class=\"line\">57</div><div class=\"line\">58</div><div class=\"line\">59</div><div class=\"line\">60</div><div class=\"line\">61</div><div class=\"line\">62</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\">%% generate data</span></div><div class=\"line\">x = <span class=\"number\">0</span>:<span class=\"number\">1</span>:<span class=\"number\">10</span>;</div><div class=\"line\">y_gt = <span class=\"number\">2</span>*x+<span class=\"number\">1</span>;</div><div class=\"line\">y = y_gt + <span class=\"built_in\">randn</span>(<span class=\"built_in\">size</span>(y_gt));</div><div class=\"line\">scatter(x, y, [], [<span class=\"number\">1</span>,<span class=\"number\">0</span>,<span class=\"number\">0</span>]);</div><div class=\"line\">hold on</div><div class=\"line\">out_x = <span class=\"number\">0</span>:<span class=\"number\">1</span>:<span class=\"number\">10</span>;</div><div class=\"line\">out_y = <span class=\"number\">5</span>*<span class=\"built_in\">rand</span>(<span class=\"built_in\">size</span>(out_x)).*out_x + <span class=\"number\">4</span>*<span class=\"built_in\">rand</span>(<span class=\"built_in\">size</span>(out_x));</div><div class=\"line\">scatter(out_x, out_y, [], [<span class=\"number\">0</span>,<span class=\"number\">0</span>,<span class=\"number\">1</span>]);</div><div class=\"line\">X = [x, out_x]';</div><div class=\"line\">Y = [y, out_y]';</div><div class=\"line\">X = [X, ones(length(X), <span class=\"number\">1</span>)];</div><div class=\"line\">[a, b] = ls_fit(X, Y);</div><div class=\"line\">plot(x, a*x+b, <span class=\"string\">'linestyle'</span>, <span class=\"string\">'--'</span>, <span class=\"string\">'color'</span>, <span class=\"string\">'r'</span>);</div><div class=\"line\"></div><div class=\"line\">[ra, rb] = ransac_fit(X, Y, <span class=\"number\">100</span>, <span class=\"number\">2</span>, <span class=\"number\">0.5</span>, <span class=\"number\">3</span>);</div><div class=\"line\">plot(x, ra*x+rb, <span class=\"string\">'linestyle'</span>, <span class=\"string\">'-.'</span>, <span class=\"string\">'color'</span>, <span class=\"string\">'g'</span>);</div><div class=\"line\">fprintf(<span class=\"string\">'least square: a = %f, b = %f\\n'</span>,a, b);</div><div class=\"line\">fprintf(<span class=\"string\">'ransac method: a = %f, b= %f\\n'</span>, ra, rb)</div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">function</span> <span class=\"params\">[a, b]</span> = <span class=\"title\">ransac_fit</span><span class=\"params\">(X, Y, k, n, t ,d)</span></span></div><div class=\"line\"><span class=\"comment\">% ransac fit</span></div><div class=\"line\"><span class=\"comment\">% k -- maximum iteration number</span></div><div class=\"line\"><span class=\"comment\">% n -- smallest point numer required</span></div><div class=\"line\"><span class=\"comment\">% t -- threshold to identify a point is fit well</span></div><div class=\"line\"><span class=\"comment\">% d -- the number of nearby points to assert a model is fine</span></div><div class=\"line\">data = [X, Y];</div><div class=\"line\">N = <span class=\"built_in\">size</span>(data, <span class=\"number\">1</span>);</div><div class=\"line\">best_good_cnt = <span class=\"number\">-1</span>;</div><div class=\"line\">best_a = <span class=\"number\">0</span>;</div><div class=\"line\">best_b = <span class=\"number\">0</span>;</div><div class=\"line\"><span class=\"keyword\">for</span> <span class=\"built_in\">i</span> = <span class=\"number\">1</span>:k</div><div class=\"line\">    <span class=\"comment\">% sample point</span></div><div class=\"line\">    idx = randsample(N, n);</div><div class=\"line\">    data_sampled = data(idx, :);</div><div class=\"line\">    <span class=\"comment\">% fit with least square</span></div><div class=\"line\">    [a, b] = ls_fit(data_sampled(:, <span class=\"number\">1</span>:<span class=\"number\">2</span>), data_sampled(:, <span class=\"number\">3</span>));</div><div class=\"line\">    <span class=\"comment\">% test model</span></div><div class=\"line\">    not_sampled = <span class=\"built_in\">ones</span>(N, <span class=\"number\">1</span>);</div><div class=\"line\">    not_sampled(idx) = <span class=\"number\">0</span>;</div><div class=\"line\">    not_sampled_data = data(not_sampled == <span class=\"number\">1</span>, :);</div><div class=\"line\">    distance = <span class=\"built_in\">abs</span>(not_sampled_data(:, <span class=\"number\">1</span>:<span class=\"number\">2</span>) * [a; b] - not_sampled_data(:, <span class=\"number\">3</span>)) / <span class=\"built_in\">sqrt</span>(a^<span class=\"number\">2</span>+<span class=\"number\">1</span>);</div><div class=\"line\">    inner_flag = distance &lt; t;</div><div class=\"line\">    good_cnt = sum(inner_flag);</div><div class=\"line\">    <span class=\"keyword\">if</span> good_cnt &gt;= d &amp;&amp; good_cnt &gt; best_good_cnt</div><div class=\"line\">        best_good_cnt = good_cnt;</div><div class=\"line\">        data_refine = data(<span class=\"built_in\">find</span>(inner_flag), :);</div><div class=\"line\">        [a, b] = ls_fit(data_refine(:, <span class=\"number\">1</span>:<span class=\"number\">2</span>), data_refine(:, <span class=\"number\">3</span>));</div><div class=\"line\">        best_a = a;</div><div class=\"line\">        best_b = b;</div><div class=\"line\">    <span class=\"keyword\">end</span></div><div class=\"line\">    fprintf(<span class=\"string\">'iteration %d, best_a = %f, best_b = %f\\n'</span>, <span class=\"built_in\">i</span>, best_a, best_b);</div><div class=\"line\"><span class=\"keyword\">end</span></div><div class=\"line\">a = best_a;</div><div class=\"line\">b = best_b;</div><div class=\"line\"><span class=\"keyword\">end</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">function</span> <span class=\"params\">[a, b]</span> = <span class=\"title\">ls_fit</span><span class=\"params\">(X, Y)</span></span></div><div class=\"line\"><span class=\"comment\">% least square fit</span></div><div class=\"line\">A = X'*X\\X'*Y;</div><div class=\"line\">a = A(<span class=\"number\">1</span>);</div><div class=\"line\">b = A(<span class=\"number\">2</span>);</div><div class=\"line\"><span class=\"keyword\">end</span></div></pre></td></tr></table></figure></p>\n<p>我们对RANSAC稍作分析，可以大概了解试验次数$k$的确定方法。</p>\n<p>仍然使用上述直线拟合的例子。如果所有点中内点所占的比例为$\\omega$，每次挑选$n$个点尝试（上述demo代码中取$n=2$）。那么每次挑选的两个点全部是内点的概率为$\\omega^n$。当选取的$n$个点全部为内点时，视为有效实验。那么，重复$k$次实验，有效实验次数为0的概率为$(1-\\omega^n)^k$。由于底数小于1，所以我们只需尽量增大$k$，就能够降低这种倒霉的概率。下图是不同$n$和$\\omega$情况下为了使得实验成功的概率大于0.99所需的$k$的分布。<br><img src=\"/img/ransac_k.png\" alt=\"k\"></p>\n<p>RANSAC方法的有点在于能够较为鲁棒地估计模型的参数，而且实现简单。缺点在于当离群点比例较大时，为保证实验成功所需的$k$值较大。这时候，可能Hough变换等基于投票的方法更适合用于图像中的直线检测问题。</p>\n","excerpt":"<p>边缘(Edge)在哺乳动物的视觉中有重要意义。在由若干卷积层构成的深度神经网络中，较低层的卷积层就被训练成为对特定形状的边缘做出响应。边缘检测也是计算机视觉和图像处理领域中一个重要的问题。</p>\n<p><img src=\"/img/edge_camera_man.png\" alt=\"边缘检测图示\"><br>","more":"</p>\n<h2 id=\"边缘的产生\"><a href=\"#边缘的产生\" class=\"headerlink\" title=\"边缘的产生\"></a>边缘的产生</h2><p>若仍采取将图像视作某一函数的观点，边缘是指这个函数中的不连续点。边缘检测是后续进行目标检测和形状识别的基础，也能够在立体视觉中恢复视角等。边缘的来源主要有以下几点：</p>\n<ul>\n<li>物体表面不平造成灰度值的不连续；</li>\n<li>深度值不同造成灰度值不连续；</li>\n<li>物体表面颜色的突变造成灰度值不连续</li>\n</ul>\n<h2 id=\"朴素思想\"><a href=\"#朴素思想\" class=\"headerlink\" title=\"朴素思想\"></a>朴素思想</h2><p>利用边缘是图像中不连续点的这一性质，可以通过计算图像的一阶导数，再找到一阶导数的极大值，即认为是边缘点。如下图所示，在白黑交界处，图像一阶导数的值非常大，表明此处灰度值变化剧烈，是边缘。<br><img src=\"/img/edge_deriative.png\" alt=\"边缘点处导数很大\"></p>\n<p>问题转换为如何求取图像的一阶导数（或梯度）。由于图像是离散的二元函数，所以下文不再区分求导与差分。</p>\n<p>在$x$方向上，令$g_x = \\frac{\\partial f}{\\partial x}$；在$y$方向上，令$g_y = \\frac{\\partial f}{\\partial y}$。梯度的大小和方向为</p>\n<script type=\"math/tex; mode=display\">g = \\lbrack g_x, g_y\\rbrack, \\theta = \\arctan(g_y/g_x)</script><p>通过和Sobel算子等做卷积，可以求取两个正交方向上图像的一阶导数，并计算梯度，之后检测梯度的局部极大值就能够找出边缘点了。</p>\n<p>只是这种方法很容易受到噪声影响。如图所示，真实的边缘点被湮没在了噪声中。<br><img src=\"/img/fun_noise.png\" alt=\"噪声影响湮没了边缘点\"></p>\n<h2 id=\"改进1：先平滑\"><a href=\"#改进1：先平滑\" class=\"headerlink\" title=\"改进1：先平滑\"></a>改进1：先平滑</h2><p>改进措施1，可以首先对图像进行高斯平滑，再按照上面的方法求取边缘点。根据卷积的性质，有：</p>\n<script type=\"math/tex; mode=display\">\\frac{d}{dx}(f\\ast g) = f\\ast\\frac{d}{dx}g</script><p>所以我们可以先求取高斯核的一阶导数，再和原始图像直接做一次卷积就可以一举两得了。这样，引出了DoG(Deriative of Gaussian)。$x$方向的DoG如图所示。<br><img src=\"/img/dog_x.png\" alt=\"x方向的DoG\"></p>\n<p>进行高斯平滑不可避免会使图像中原本的细节部分模糊，所以需要在克服噪声和引入模糊之间做好折中。<br><img src=\"/img/dog_different_size.png\" alt=\"不同\"></p>\n<h2 id=\"改进2：Canny检测子\"><a href=\"#改进2：Canny检测子\" class=\"headerlink\" title=\"改进2：Canny检测子\"></a>改进2：Canny检测子</h2><p>改进措施2，使用Canny检测子进行检测。Canny检测方法同样基于梯度，其基本原理如下：</p>\n<ul>\n<li>使用DoG计算梯度幅值和方向。</li>\n<li>非极大值抑制，这个过程需要根据梯度方向做线性插值。如图，沿着点$q$的梯度方向找到了$p$和$r$两个点。这两个点的梯度幅值需要根据其临近的两点做插值得到。</li>\n<li>利用梯度方向和边缘线互相垂直这一性质，如图，若已经确定点$p$为边缘点，则向它的梯度方向正交方向上寻找下一个边缘点（点$r$或$s$）。这一步也叫edge linking。</li>\n</ul>\n<p><img src=\"/img/canny_nms.png\" alt=\"nms示意图\"><br><img src=\"/img/canny_linking.png\" alt=\"linking示意图\"></p>\n<p>同时，为了提高算法性能，Canny中采用了迟滞阈值的方法，设定<code>low</code>和<code>high</code>两个阈值，来判定某个点是否属于<strong>强</strong>或<strong>弱</strong>边缘点。在做edge linking的时候，从强边缘点开始，如果遇到了弱边缘点，则继续，直到某点的梯度幅值甚至比<code>low</code>还要小，则在此停止。</p>\n<h2 id=\"改进3：RANSAC方法\"><a href=\"#改进3：RANSAC方法\" class=\"headerlink\" title=\"改进3：RANSAC方法\"></a>改进3：RANSAC方法</h2><p>有的时候，我们并不是想要找到所有的边缘点，可能只是想找到图像中水平方向的某些边缘。这时候可以考虑采用RANSAC方法。</p>\n<p>RANSAC方法的思想在于，认为已有的feature大部分都是<strong>好的</strong>。这样，每次随机抽取出若干feature，建立model，再在整个feature集合上进行验证。那么由那些好的feature得到的model一定是得分较高的。（世界上还是好人多啊！）这样就剔除了离群点的影响。</p>\n<p>以直线拟合为例，在下图中，给出了使用RANSAC方法拟合直线的步骤。如图1所示，由于离群点的存在，如果直接使用最小二乘法进行拟合，拟合结果效果会很不理想。由于确定一条直线需要两个点，所以从点集中选取两个点，并计算拟合直线。并计算点集中的点在这条直线附近的个数，作为对模型好坏的判定，这些点是新的内点。找出最优的那条直线，使用其所有内点再进行拟合，重复上述操作，直至迭代终止。<br><img src=\"/img/ransac_step.png\" alt=\"ransac step\"></p>\n<p>上述RANSAC方法进行直线拟合的过程可以总结如下：<br><img src=\"/img/ransac_line_fit.png\" alt=\"ransac line fit alg\"></p>\n<p>按照上述思想，我分别使用最小二乘法和RANSAC方法尝试进行直线拟合。在下面的代码中，我首先产生了正常受到一定高斯噪声污染的数据（图中的红色点），这些点的真值都落在直线$y = 2x+1$上。而后，我随机变化了斜率和截距，以期产生一些离群点（图中的蓝色点）。当然，由于随机性，这种方法生成的点有可能仍然是内点。</p>\n<p>而后，我分别使用上述两种方法进行拟合。可以从结果图中看出，RANSAC（绿色线）能够有效避免离群点的干扰，获得更好的拟合效果。在某次实验中，两种方法的拟合结果如下：<br><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">least square: a = 3.319566, b = -1.446528</div><div class=\"line\">ransac method: a = 1.899640, b= 1.298608</div></pre></td></tr></table></figure></p>\n<p><img src=\"/img/line_fit_demo.png\" alt=\"demo result\"></p>\n<p>实验使用的MATLAB代码如下：<br><figure class=\"highlight matlab\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div><div class=\"line\">47</div><div class=\"line\">48</div><div class=\"line\">49</div><div class=\"line\">50</div><div class=\"line\">51</div><div class=\"line\">52</div><div class=\"line\">53</div><div class=\"line\">54</div><div class=\"line\">55</div><div class=\"line\">56</div><div class=\"line\">57</div><div class=\"line\">58</div><div class=\"line\">59</div><div class=\"line\">60</div><div class=\"line\">61</div><div class=\"line\">62</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\">%% generate data</span></div><div class=\"line\">x = <span class=\"number\">0</span>:<span class=\"number\">1</span>:<span class=\"number\">10</span>;</div><div class=\"line\">y_gt = <span class=\"number\">2</span>*x+<span class=\"number\">1</span>;</div><div class=\"line\">y = y_gt + <span class=\"built_in\">randn</span>(<span class=\"built_in\">size</span>(y_gt));</div><div class=\"line\">scatter(x, y, [], [<span class=\"number\">1</span>,<span class=\"number\">0</span>,<span class=\"number\">0</span>]);</div><div class=\"line\">hold on</div><div class=\"line\">out_x = <span class=\"number\">0</span>:<span class=\"number\">1</span>:<span class=\"number\">10</span>;</div><div class=\"line\">out_y = <span class=\"number\">5</span>*<span class=\"built_in\">rand</span>(<span class=\"built_in\">size</span>(out_x)).*out_x + <span class=\"number\">4</span>*<span class=\"built_in\">rand</span>(<span class=\"built_in\">size</span>(out_x));</div><div class=\"line\">scatter(out_x, out_y, [], [<span class=\"number\">0</span>,<span class=\"number\">0</span>,<span class=\"number\">1</span>]);</div><div class=\"line\">X = [x, out_x]';</div><div class=\"line\">Y = [y, out_y]';</div><div class=\"line\">X = [X, ones(length(X), <span class=\"number\">1</span>)];</div><div class=\"line\">[a, b] = ls_fit(X, Y);</div><div class=\"line\">plot(x, a*x+b, <span class=\"string\">'linestyle'</span>, <span class=\"string\">'--'</span>, <span class=\"string\">'color'</span>, <span class=\"string\">'r'</span>);</div><div class=\"line\"></div><div class=\"line\">[ra, rb] = ransac_fit(X, Y, <span class=\"number\">100</span>, <span class=\"number\">2</span>, <span class=\"number\">0.5</span>, <span class=\"number\">3</span>);</div><div class=\"line\">plot(x, ra*x+rb, <span class=\"string\">'linestyle'</span>, <span class=\"string\">'-.'</span>, <span class=\"string\">'color'</span>, <span class=\"string\">'g'</span>);</div><div class=\"line\">fprintf(<span class=\"string\">'least square: a = %f, b = %f\\n'</span>,a, b);</div><div class=\"line\">fprintf(<span class=\"string\">'ransac method: a = %f, b= %f\\n'</span>, ra, rb)</div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">function</span> <span class=\"params\">[a, b]</span> = <span class=\"title\">ransac_fit</span><span class=\"params\">(X, Y, k, n, t ,d)</span></span></div><div class=\"line\"><span class=\"comment\">% ransac fit</span></div><div class=\"line\"><span class=\"comment\">% k -- maximum iteration number</span></div><div class=\"line\"><span class=\"comment\">% n -- smallest point numer required</span></div><div class=\"line\"><span class=\"comment\">% t -- threshold to identify a point is fit well</span></div><div class=\"line\"><span class=\"comment\">% d -- the number of nearby points to assert a model is fine</span></div><div class=\"line\">data = [X, Y];</div><div class=\"line\">N = <span class=\"built_in\">size</span>(data, <span class=\"number\">1</span>);</div><div class=\"line\">best_good_cnt = <span class=\"number\">-1</span>;</div><div class=\"line\">best_a = <span class=\"number\">0</span>;</div><div class=\"line\">best_b = <span class=\"number\">0</span>;</div><div class=\"line\"><span class=\"keyword\">for</span> <span class=\"built_in\">i</span> = <span class=\"number\">1</span>:k</div><div class=\"line\">    <span class=\"comment\">% sample point</span></div><div class=\"line\">    idx = randsample(N, n);</div><div class=\"line\">    data_sampled = data(idx, :);</div><div class=\"line\">    <span class=\"comment\">% fit with least square</span></div><div class=\"line\">    [a, b] = ls_fit(data_sampled(:, <span class=\"number\">1</span>:<span class=\"number\">2</span>), data_sampled(:, <span class=\"number\">3</span>));</div><div class=\"line\">    <span class=\"comment\">% test model</span></div><div class=\"line\">    not_sampled = <span class=\"built_in\">ones</span>(N, <span class=\"number\">1</span>);</div><div class=\"line\">    not_sampled(idx) = <span class=\"number\">0</span>;</div><div class=\"line\">    not_sampled_data = data(not_sampled == <span class=\"number\">1</span>, :);</div><div class=\"line\">    distance = <span class=\"built_in\">abs</span>(not_sampled_data(:, <span class=\"number\">1</span>:<span class=\"number\">2</span>) * [a; b] - not_sampled_data(:, <span class=\"number\">3</span>)) / <span class=\"built_in\">sqrt</span>(a^<span class=\"number\">2</span>+<span class=\"number\">1</span>);</div><div class=\"line\">    inner_flag = distance &lt; t;</div><div class=\"line\">    good_cnt = sum(inner_flag);</div><div class=\"line\">    <span class=\"keyword\">if</span> good_cnt &gt;= d &amp;&amp; good_cnt &gt; best_good_cnt</div><div class=\"line\">        best_good_cnt = good_cnt;</div><div class=\"line\">        data_refine = data(<span class=\"built_in\">find</span>(inner_flag), :);</div><div class=\"line\">        [a, b] = ls_fit(data_refine(:, <span class=\"number\">1</span>:<span class=\"number\">2</span>), data_refine(:, <span class=\"number\">3</span>));</div><div class=\"line\">        best_a = a;</div><div class=\"line\">        best_b = b;</div><div class=\"line\">    <span class=\"keyword\">end</span></div><div class=\"line\">    fprintf(<span class=\"string\">'iteration %d, best_a = %f, best_b = %f\\n'</span>, <span class=\"built_in\">i</span>, best_a, best_b);</div><div class=\"line\"><span class=\"keyword\">end</span></div><div class=\"line\">a = best_a;</div><div class=\"line\">b = best_b;</div><div class=\"line\"><span class=\"keyword\">end</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">function</span> <span class=\"params\">[a, b]</span> = <span class=\"title\">ls_fit</span><span class=\"params\">(X, Y)</span></span></div><div class=\"line\"><span class=\"comment\">% least square fit</span></div><div class=\"line\">A = X'*X\\X'*Y;</div><div class=\"line\">a = A(<span class=\"number\">1</span>);</div><div class=\"line\">b = A(<span class=\"number\">2</span>);</div><div class=\"line\"><span class=\"keyword\">end</span></div></pre></td></tr></table></figure></p>\n<p>我们对RANSAC稍作分析，可以大概了解试验次数$k$的确定方法。</p>\n<p>仍然使用上述直线拟合的例子。如果所有点中内点所占的比例为$\\omega$，每次挑选$n$个点尝试（上述demo代码中取$n=2$）。那么每次挑选的两个点全部是内点的概率为$\\omega^n$。当选取的$n$个点全部为内点时，视为有效实验。那么，重复$k$次实验，有效实验次数为0的概率为$(1-\\omega^n)^k$。由于底数小于1，所以我们只需尽量增大$k$，就能够降低这种倒霉的概率。下图是不同$n$和$\\omega$情况下为了使得实验成功的概率大于0.99所需的$k$的分布。<br><img src=\"/img/ransac_k.png\" alt=\"k\"></p>\n<p>RANSAC方法的有点在于能够较为鲁棒地估计模型的参数，而且实现简单。缺点在于当离群点比例较大时，为保证实验成功所需的$k$值较大。这时候，可能Hough变换等基于投票的方法更适合用于图像中的直线检测问题。</p>"},{"title":"CS131-描述图像的特征(Harris 角点)","date":"2017-01-25T02:51:47.000Z","_content":"\nfeature是对图像的描述。比如，图像整体灰度值的均值和方差就可以作为feature。如果我们想在一副图像中检测足球时，可以使用滑动窗方法，逐一检查窗口内的灰度值分布是否和一张给定的典型黑白格子足球相似。可想而知，这种方法的性能一定让人捉急。而在image matching问题中，常常需要将不同视角的同一目标物进行matching，进而计算相机转过的角度。这在SLAM问题中很有意义。如下图所示，同一处景观，在不同摄影师的镜头下，不仅视角不同，而且明暗变化也有很多差别，右侧的图暖色调更浓。这也告诉我们，上面提到的只使用全局图像灰度值来做feature的做法有多么不靠谱。\n\n那么，让我们脱离全局特征，转而将注意力集中在局部特征上。这是因为使用局部特征能够更好地处理图像中的遮挡、变形等情况，而且我们的研究对象常常是图像中的部分区域而不是图像整体。更特殊地，在这一讲中，我们主要探究key point作为local feature的描述，并介绍Harris角点检测方法。在下一节中介绍SIFT特征。\n\n![image matching example](/img/image_matching_hard.png)\n\n<!-- more -->\n\n## Harris角点\n角点，即corner，和edge类似，区别在于其在两个方向上都有较为剧烈的灰度变化（而edge只在某一个方向上灰度值变化剧烈）。如图所示。\n![what is corner](/img/what_is_corner.png)\n\n[Harris角点](http://www.bmva.org/bmvc/1988/avc-88-023.pdf)得名于其发明者Harris，是一种常见的角点检测方法。\n给定观察窗口大小，计算平移后窗口内各个像素差值的加权平方和，如下式。\n$$E(u,v) = \\sum_x\\sum_yw(x,y)[I(x+u, y+v) - I(x,y)]^2$$\n\n其中，窗口加权函数$w$可以取做门限函数或gaussian函数。如图所示。\n![window function](/img/corner_window_fun.png)\n\n使用泰勒级数展开，并忽略非线性项，我们有\n$$I(x+u,y+v) = I(x,y) + I_x(x,y)u+I_y(x,y)v$$\n\n所以上式可以写成（线性二次型写成了矩阵形式），\n$$E(u,v) = \\sum_{x,y}w(I_xu+I_yv)^2 = \\begin{bmatrix}u&v\\end{bmatrix}M\\begin{bmatrix}u\\\\\\\\v\\end{bmatrix}$$\n\n其中，\n$$M = w\\begin{bmatrix}I_x^2& I_xI_y\\\\\\\\I_xI_y&I_y^2\\end{bmatrix}$$\n\n当使用门限函数时，权值$w_{i,j} = 1$，则，\n$$M = \\begin{bmatrix}\\sum I_xI_x& \\sum I_xI_y\\\\\\\\\\sum I_xI_y&\\sum I_yI_y\\end{bmatrix} = \\sum \\begin{bmatrix}I_x \\\\\\\\I_y\\end{bmatrix}\\begin{bmatrix}I_x &I_y\\end{bmatrix}$$\n\n当corner与xy坐标轴对齐时候，如下图所示。由于在黑色观察窗口内，只有上侧和左侧存在边缘，且在上边缘，$I_y$很大，而$I_x=0$，在左侧边缘，$I_x$很大而$I_y = 0$，所以，矩阵\n$$M = \\begin{bmatrix}\\lambda_1 & 0 \\\\\\\\ 0&\\lambda_2 \\end{bmatrix}$$\n![M为对角阵](/img/corner_type_1.png)\n\n当corner与坐标轴没有对齐时，经过旋转变换就可以将其转换到与坐标轴对齐的角度，而这种旋转操作可以使用矩阵的相似化来表示（其实是二次型的化简，可以使用合同变换，而旋转变换的矩阵是酉矩阵，转置即为矩阵的逆，所以也是相似变换）。也就是说，矩阵$M$相似于某个对角阵。\n$$M = R^{-1}\\Sigma R, \\text{其中}\\Sigma = \\begin{bmatrix}\\lambda_1&0\\\\\\\\0&\\lambda_2\\end{bmatrix}$$\n\n所以，可以根据下面这张图利用矩阵$M$的特征值来判定角点和边缘点。当两个特征值都较大时为角点；当某个分量近似为0而另一个分量较大时，可以判定为边缘点（因为某个方向的导数为0）；当两个特征值都近似为0时，说明是普通点（flat point）。（课件原图如此，空缺的问号处应分别为$\\lambda_1$和$\\lambda_2$）。\n![使用M矩阵特征值判定](/img/corner_judge.png)\n\n 然而矩阵的特征值计算较为复杂，所以使用下面的方法进行近似计算。\n $$\\theta = \\det(M)-\\alpha\\text{trace}(M)^2 = \\lambda_1\\lambda_2-\\alpha(\\lambda_1+\\lambda_2)^2$$\n![使用theta判定](/img/corner_judge_2.png)\n\n为了减弱噪声的影响，常常使用gaussian窗函数。如下式所示：\n$$w(x,y) = \\exp(-(x^2+y^2)/2\\sigma^2)$$\n","source":"_posts/cs131-finding-features.md","raw":"---\ntitle: CS131-描述图像的特征(Harris 角点)\ndate: 2017-01-25 10:51:47\ntags:\n    - cs131\n    - 公开课\n---\n\nfeature是对图像的描述。比如，图像整体灰度值的均值和方差就可以作为feature。如果我们想在一副图像中检测足球时，可以使用滑动窗方法，逐一检查窗口内的灰度值分布是否和一张给定的典型黑白格子足球相似。可想而知，这种方法的性能一定让人捉急。而在image matching问题中，常常需要将不同视角的同一目标物进行matching，进而计算相机转过的角度。这在SLAM问题中很有意义。如下图所示，同一处景观，在不同摄影师的镜头下，不仅视角不同，而且明暗变化也有很多差别，右侧的图暖色调更浓。这也告诉我们，上面提到的只使用全局图像灰度值来做feature的做法有多么不靠谱。\n\n那么，让我们脱离全局特征，转而将注意力集中在局部特征上。这是因为使用局部特征能够更好地处理图像中的遮挡、变形等情况，而且我们的研究对象常常是图像中的部分区域而不是图像整体。更特殊地，在这一讲中，我们主要探究key point作为local feature的描述，并介绍Harris角点检测方法。在下一节中介绍SIFT特征。\n\n![image matching example](/img/image_matching_hard.png)\n\n<!-- more -->\n\n## Harris角点\n角点，即corner，和edge类似，区别在于其在两个方向上都有较为剧烈的灰度变化（而edge只在某一个方向上灰度值变化剧烈）。如图所示。\n![what is corner](/img/what_is_corner.png)\n\n[Harris角点](http://www.bmva.org/bmvc/1988/avc-88-023.pdf)得名于其发明者Harris，是一种常见的角点检测方法。\n给定观察窗口大小，计算平移后窗口内各个像素差值的加权平方和，如下式。\n$$E(u,v) = \\sum_x\\sum_yw(x,y)[I(x+u, y+v) - I(x,y)]^2$$\n\n其中，窗口加权函数$w$可以取做门限函数或gaussian函数。如图所示。\n![window function](/img/corner_window_fun.png)\n\n使用泰勒级数展开，并忽略非线性项，我们有\n$$I(x+u,y+v) = I(x,y) + I_x(x,y)u+I_y(x,y)v$$\n\n所以上式可以写成（线性二次型写成了矩阵形式），\n$$E(u,v) = \\sum_{x,y}w(I_xu+I_yv)^2 = \\begin{bmatrix}u&v\\end{bmatrix}M\\begin{bmatrix}u\\\\\\\\v\\end{bmatrix}$$\n\n其中，\n$$M = w\\begin{bmatrix}I_x^2& I_xI_y\\\\\\\\I_xI_y&I_y^2\\end{bmatrix}$$\n\n当使用门限函数时，权值$w_{i,j} = 1$，则，\n$$M = \\begin{bmatrix}\\sum I_xI_x& \\sum I_xI_y\\\\\\\\\\sum I_xI_y&\\sum I_yI_y\\end{bmatrix} = \\sum \\begin{bmatrix}I_x \\\\\\\\I_y\\end{bmatrix}\\begin{bmatrix}I_x &I_y\\end{bmatrix}$$\n\n当corner与xy坐标轴对齐时候，如下图所示。由于在黑色观察窗口内，只有上侧和左侧存在边缘，且在上边缘，$I_y$很大，而$I_x=0$，在左侧边缘，$I_x$很大而$I_y = 0$，所以，矩阵\n$$M = \\begin{bmatrix}\\lambda_1 & 0 \\\\\\\\ 0&\\lambda_2 \\end{bmatrix}$$\n![M为对角阵](/img/corner_type_1.png)\n\n当corner与坐标轴没有对齐时，经过旋转变换就可以将其转换到与坐标轴对齐的角度，而这种旋转操作可以使用矩阵的相似化来表示（其实是二次型的化简，可以使用合同变换，而旋转变换的矩阵是酉矩阵，转置即为矩阵的逆，所以也是相似变换）。也就是说，矩阵$M$相似于某个对角阵。\n$$M = R^{-1}\\Sigma R, \\text{其中}\\Sigma = \\begin{bmatrix}\\lambda_1&0\\\\\\\\0&\\lambda_2\\end{bmatrix}$$\n\n所以，可以根据下面这张图利用矩阵$M$的特征值来判定角点和边缘点。当两个特征值都较大时为角点；当某个分量近似为0而另一个分量较大时，可以判定为边缘点（因为某个方向的导数为0）；当两个特征值都近似为0时，说明是普通点（flat point）。（课件原图如此，空缺的问号处应分别为$\\lambda_1$和$\\lambda_2$）。\n![使用M矩阵特征值判定](/img/corner_judge.png)\n\n 然而矩阵的特征值计算较为复杂，所以使用下面的方法进行近似计算。\n $$\\theta = \\det(M)-\\alpha\\text{trace}(M)^2 = \\lambda_1\\lambda_2-\\alpha(\\lambda_1+\\lambda_2)^2$$\n![使用theta判定](/img/corner_judge_2.png)\n\n为了减弱噪声的影响，常常使用gaussian窗函数。如下式所示：\n$$w(x,y) = \\exp(-(x^2+y^2)/2\\sigma^2)$$\n","slug":"cs131-finding-features","published":1,"updated":"2018-01-12T06:22:20.460Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjcu6vcrw000aqu46c2kjkmv8","content":"<p>feature是对图像的描述。比如，图像整体灰度值的均值和方差就可以作为feature。如果我们想在一副图像中检测足球时，可以使用滑动窗方法，逐一检查窗口内的灰度值分布是否和一张给定的典型黑白格子足球相似。可想而知，这种方法的性能一定让人捉急。而在image matching问题中，常常需要将不同视角的同一目标物进行matching，进而计算相机转过的角度。这在SLAM问题中很有意义。如下图所示，同一处景观，在不同摄影师的镜头下，不仅视角不同，而且明暗变化也有很多差别，右侧的图暖色调更浓。这也告诉我们，上面提到的只使用全局图像灰度值来做feature的做法有多么不靠谱。</p>\n<p>那么，让我们脱离全局特征，转而将注意力集中在局部特征上。这是因为使用局部特征能够更好地处理图像中的遮挡、变形等情况，而且我们的研究对象常常是图像中的部分区域而不是图像整体。更特殊地，在这一讲中，我们主要探究key point作为local feature的描述，并介绍Harris角点检测方法。在下一节中介绍SIFT特征。</p>\n<p><img src=\"/img/image_matching_hard.png\" alt=\"image matching example\"></p>\n<a id=\"more\"></a>\n<h2 id=\"Harris角点\"><a href=\"#Harris角点\" class=\"headerlink\" title=\"Harris角点\"></a>Harris角点</h2><p>角点，即corner，和edge类似，区别在于其在两个方向上都有较为剧烈的灰度变化（而edge只在某一个方向上灰度值变化剧烈）。如图所示。<br><img src=\"/img/what_is_corner.png\" alt=\"what is corner\"></p>\n<p><a href=\"http://www.bmva.org/bmvc/1988/avc-88-023.pdf\" target=\"_blank\" rel=\"external\">Harris角点</a>得名于其发明者Harris，是一种常见的角点检测方法。<br>给定观察窗口大小，计算平移后窗口内各个像素差值的加权平方和，如下式。</p>\n<script type=\"math/tex; mode=display\">E(u,v) = \\sum_x\\sum_yw(x,y)[I(x+u, y+v) - I(x,y)]^2</script><p>其中，窗口加权函数$w$可以取做门限函数或gaussian函数。如图所示。<br><img src=\"/img/corner_window_fun.png\" alt=\"window function\"></p>\n<p>使用泰勒级数展开，并忽略非线性项，我们有</p>\n<script type=\"math/tex; mode=display\">I(x+u,y+v) = I(x,y) + I_x(x,y)u+I_y(x,y)v</script><p>所以上式可以写成（线性二次型写成了矩阵形式），</p>\n<script type=\"math/tex; mode=display\">E(u,v) = \\sum_{x,y}w(I_xu+I_yv)^2 = \\begin{bmatrix}u&v\\end{bmatrix}M\\begin{bmatrix}u\\\\\\\\v\\end{bmatrix}</script><p>其中，</p>\n<script type=\"math/tex; mode=display\">M = w\\begin{bmatrix}I_x^2& I_xI_y\\\\\\\\I_xI_y&I_y^2\\end{bmatrix}</script><p>当使用门限函数时，权值$w_{i,j} = 1$，则，</p>\n<script type=\"math/tex; mode=display\">M = \\begin{bmatrix}\\sum I_xI_x& \\sum I_xI_y\\\\\\\\\\sum I_xI_y&\\sum I_yI_y\\end{bmatrix} = \\sum \\begin{bmatrix}I_x \\\\\\\\I_y\\end{bmatrix}\\begin{bmatrix}I_x &I_y\\end{bmatrix}</script><p>当corner与xy坐标轴对齐时候，如下图所示。由于在黑色观察窗口内，只有上侧和左侧存在边缘，且在上边缘，$I_y$很大，而$I_x=0$，在左侧边缘，$I_x$很大而$I_y = 0$，所以，矩阵</p>\n<script type=\"math/tex; mode=display\">M = \\begin{bmatrix}\\lambda_1 & 0 \\\\\\\\ 0&\\lambda_2 \\end{bmatrix}</script><p><img src=\"/img/corner_type_1.png\" alt=\"M为对角阵\"></p>\n<p>当corner与坐标轴没有对齐时，经过旋转变换就可以将其转换到与坐标轴对齐的角度，而这种旋转操作可以使用矩阵的相似化来表示（其实是二次型的化简，可以使用合同变换，而旋转变换的矩阵是酉矩阵，转置即为矩阵的逆，所以也是相似变换）。也就是说，矩阵$M$相似于某个对角阵。</p>\n<script type=\"math/tex; mode=display\">M = R^{-1}\\Sigma R, \\text{其中}\\Sigma = \\begin{bmatrix}\\lambda_1&0\\\\\\\\0&\\lambda_2\\end{bmatrix}</script><p>所以，可以根据下面这张图利用矩阵$M$的特征值来判定角点和边缘点。当两个特征值都较大时为角点；当某个分量近似为0而另一个分量较大时，可以判定为边缘点（因为某个方向的导数为0）；当两个特征值都近似为0时，说明是普通点（flat point）。（课件原图如此，空缺的问号处应分别为$\\lambda_1$和$\\lambda_2$）。<br><img src=\"/img/corner_judge.png\" alt=\"使用M矩阵特征值判定\"></p>\n<p> 然而矩阵的特征值计算较为复杂，所以使用下面的方法进行近似计算。</p>\n<script type=\"math/tex; mode=display\">\\theta = \\det(M)-\\alpha\\text{trace}(M)^2 = \\lambda_1\\lambda_2-\\alpha(\\lambda_1+\\lambda_2)^2</script><p><img src=\"/img/corner_judge_2.png\" alt=\"使用theta判定\"></p>\n<p>为了减弱噪声的影响，常常使用gaussian窗函数。如下式所示：</p>\n<script type=\"math/tex; mode=display\">w(x,y) = \\exp(-(x^2+y^2)/2\\sigma^2)</script>","excerpt":"<p>feature是对图像的描述。比如，图像整体灰度值的均值和方差就可以作为feature。如果我们想在一副图像中检测足球时，可以使用滑动窗方法，逐一检查窗口内的灰度值分布是否和一张给定的典型黑白格子足球相似。可想而知，这种方法的性能一定让人捉急。而在image matching问题中，常常需要将不同视角的同一目标物进行matching，进而计算相机转过的角度。这在SLAM问题中很有意义。如下图所示，同一处景观，在不同摄影师的镜头下，不仅视角不同，而且明暗变化也有很多差别，右侧的图暖色调更浓。这也告诉我们，上面提到的只使用全局图像灰度值来做feature的做法有多么不靠谱。</p>\n<p>那么，让我们脱离全局特征，转而将注意力集中在局部特征上。这是因为使用局部特征能够更好地处理图像中的遮挡、变形等情况，而且我们的研究对象常常是图像中的部分区域而不是图像整体。更特殊地，在这一讲中，我们主要探究key point作为local feature的描述，并介绍Harris角点检测方法。在下一节中介绍SIFT特征。</p>\n<p><img src=\"/img/image_matching_hard.png\" alt=\"image matching example\"></p>","more":"<h2 id=\"Harris角点\"><a href=\"#Harris角点\" class=\"headerlink\" title=\"Harris角点\"></a>Harris角点</h2><p>角点，即corner，和edge类似，区别在于其在两个方向上都有较为剧烈的灰度变化（而edge只在某一个方向上灰度值变化剧烈）。如图所示。<br><img src=\"/img/what_is_corner.png\" alt=\"what is corner\"></p>\n<p><a href=\"http://www.bmva.org/bmvc/1988/avc-88-023.pdf\">Harris角点</a>得名于其发明者Harris，是一种常见的角点检测方法。<br>给定观察窗口大小，计算平移后窗口内各个像素差值的加权平方和，如下式。</p>\n<script type=\"math/tex; mode=display\">E(u,v) = \\sum_x\\sum_yw(x,y)[I(x+u, y+v) - I(x,y)]^2</script><p>其中，窗口加权函数$w$可以取做门限函数或gaussian函数。如图所示。<br><img src=\"/img/corner_window_fun.png\" alt=\"window function\"></p>\n<p>使用泰勒级数展开，并忽略非线性项，我们有</p>\n<script type=\"math/tex; mode=display\">I(x+u,y+v) = I(x,y) + I_x(x,y)u+I_y(x,y)v</script><p>所以上式可以写成（线性二次型写成了矩阵形式），</p>\n<script type=\"math/tex; mode=display\">E(u,v) = \\sum_{x,y}w(I_xu+I_yv)^2 = \\begin{bmatrix}u&v\\end{bmatrix}M\\begin{bmatrix}u\\\\\\\\v\\end{bmatrix}</script><p>其中，</p>\n<script type=\"math/tex; mode=display\">M = w\\begin{bmatrix}I_x^2& I_xI_y\\\\\\\\I_xI_y&I_y^2\\end{bmatrix}</script><p>当使用门限函数时，权值$w_{i,j} = 1$，则，</p>\n<script type=\"math/tex; mode=display\">M = \\begin{bmatrix}\\sum I_xI_x& \\sum I_xI_y\\\\\\\\\\sum I_xI_y&\\sum I_yI_y\\end{bmatrix} = \\sum \\begin{bmatrix}I_x \\\\\\\\I_y\\end{bmatrix}\\begin{bmatrix}I_x &I_y\\end{bmatrix}</script><p>当corner与xy坐标轴对齐时候，如下图所示。由于在黑色观察窗口内，只有上侧和左侧存在边缘，且在上边缘，$I_y$很大，而$I_x=0$，在左侧边缘，$I_x$很大而$I_y = 0$，所以，矩阵</p>\n<script type=\"math/tex; mode=display\">M = \\begin{bmatrix}\\lambda_1 & 0 \\\\\\\\ 0&\\lambda_2 \\end{bmatrix}</script><p><img src=\"/img/corner_type_1.png\" alt=\"M为对角阵\"></p>\n<p>当corner与坐标轴没有对齐时，经过旋转变换就可以将其转换到与坐标轴对齐的角度，而这种旋转操作可以使用矩阵的相似化来表示（其实是二次型的化简，可以使用合同变换，而旋转变换的矩阵是酉矩阵，转置即为矩阵的逆，所以也是相似变换）。也就是说，矩阵$M$相似于某个对角阵。</p>\n<script type=\"math/tex; mode=display\">M = R^{-1}\\Sigma R, \\text{其中}\\Sigma = \\begin{bmatrix}\\lambda_1&0\\\\\\\\0&\\lambda_2\\end{bmatrix}</script><p>所以，可以根据下面这张图利用矩阵$M$的特征值来判定角点和边缘点。当两个特征值都较大时为角点；当某个分量近似为0而另一个分量较大时，可以判定为边缘点（因为某个方向的导数为0）；当两个特征值都近似为0时，说明是普通点（flat point）。（课件原图如此，空缺的问号处应分别为$\\lambda_1$和$\\lambda_2$）。<br><img src=\"/img/corner_judge.png\" alt=\"使用M矩阵特征值判定\"></p>\n<p> 然而矩阵的特征值计算较为复杂，所以使用下面的方法进行近似计算。</p>\n<script type=\"math/tex; mode=display\">\\theta = \\det(M)-\\alpha\\text{trace}(M)^2 = \\lambda_1\\lambda_2-\\alpha(\\lambda_1+\\lambda_2)^2</script><p><img src=\"/img/corner_judge_2.png\" alt=\"使用theta判定\"></p>\n<p>为了减弱噪声的影响，常常使用gaussian窗函数。如下式所示：</p>\n<script type=\"math/tex; mode=display\">w(x,y) = \\exp(-(x^2+y^2)/2\\sigma^2)</script>"},{"title":"CS131-KMeans聚类","date":"2017-02-05T15:07:00.000Z","_content":"[K-Means聚类](https://zh.wikipedia.org/wiki/K-平均算法)是把n个点（可以是样本的一次观察或一个实例）划分到$k$个聚类中，使得每个点都属于离他最近的均值（此即聚类中心）对应的聚类，以之作为聚类的标准，满足最小化聚类中心和属于该中心的数据点之间的平方距离和（Sum of Square Distance，SSD）。\n$$\\text{SSD} = \\sum_{i=1}^{k}\\sum_{x\\in c_i}(x-c_i)^2$$\n![K-Means Demo](/img/kmeans_demo.png)\n\n<!-- more -->\n## 目标函数\nK-Means方法实际上需要确定两个参数，$c^\\ast$和$\\delta^\\ast$。其中$c\\_{i}^\\ast$代表各个聚类中心的位置，$\\delta\\_{ij}^\\ast$的取值为$\\lbrace 0,1\\rbrace$，代表点$x_j$是否被分到了第$i$个聚类中心。\n\n那么，目标函数可以写成如下的形式。\n$$c^\\ast, \\delta^\\ast = \\arg\\min_{c,\\delta} \\frac{1}{N}\\sum_{j=1}^{N}\\sum_{i=1}^{k}\\delta_{i,j}(c_i-x_j)^2$$\n\n然而这样一来，造成了一种很尴尬的局面。一方面，要想优化$c\\_i$，需要我们给定每个点所属的类；另一方面，优化$\\delta\\_{i,j}$，需要我们给定聚类中心。由于这两个优化变量之间是纠缠在一起的，K-Means算法如何能够达到收敛就变成了一个讨论“先有鸡还是先有蛋”的问题了。\n\n实际使用中，K-Means的迭代过程，实际是EM算法的一个特例。\n\n## 算法流程\nK-Means算法的流程如下所示。\n![K-Means算法流程](/img/kmeans_algorithm.png)\n\n假设我们有$N$个样本点，$\\lbrace x_1, \\dots, x_N\\rbrace, x_i\\in\\mathbb{R}^D$，并给出聚类数目$k$。\n\n首先，随机选取一系列的聚类中心点$\\mu_i, i = 1,\\dots, k$。接着，我们按照距离最近的原则为每个数据点指定相应的聚类中心并计算新的数据点均值更新聚类中心。如此这般，直到收敛。\n\n## 算法细节\n### 初始化\n上面的K-Means方法对初始值很敏感。朴素的初始化方法是直接在数据点中随机抽取$k$个作为聚类初始中心点。不够好的初始值可能造成收敛速度很慢或者聚类失败。下面介绍两种改进方法。\n- kmeans++方法，尽可能地使初始聚类中心分散开（spread-out）。\n  这种方法首先随机产生一个初始聚类中心点，然后按照概率$P = \\omega(x-c_i)^2$选取其他的聚类中心点。其中$\\omega$是归一化系数。\n\n- 多次初始化，保留最好的结果。\n\n### K值的选取\n在上面的讨论中，我们一直把$k$当做已经给定的参数。但是在实际操作中，聚类类别数通常都是未知的。如何确定超参数$k$呢？\n\n我们可以使用不同的$k$值进行试验，并作出不同试验收敛后目标函数随$k$的变化。如下图所示，分别使用$1$到$4$之间的几个数字作为$k$值，曲线在$k=2$处有一个较为明显的弯曲点（elbow point）。故确定$k$取值为2。\n![参数K的确定](/img/kmeans_object_fun_vs_k.png)\n\n### 距离的度量\n目标函数是各个cluster中的点与其中心点的距离均值。其中就涉及到了“距离”的量度。下面是几种较为常用的距离度量方法。\n- 欧几里得距离（最为常用）\n- 余弦距离（向量的夹角）\n- 核函数（[Kernel K-Means](http://www.public.asu.edu/~jye02/CLASSES/Fall-2005/PAPERS/kdd_spectral_kernelkmeans.pdf)）\n\n### 迭代终止条件\n当判断算法已经收敛时，退出迭代。常用的迭代终止条件如下：\n- 达到了预先给定的最大迭代次数\n- 在将数据点assign到不同聚类中心时，和上轮结果没有变化（说明已经收敛）\n- 目标函数（平均的距离）下降小于阈值\n\n## 基于K-Means的图像分割\n图像分割中可以使用K-Means算法。我们首先将一副图像转换为特征空间（Feature Space）。例如使用灰度或者RGB的值作为特征向量，就得到了1D或者3D的特征空间。之后，可以基于Feature Space上各点的相似度，将它们聚类。这样，就完成了对原始图像的分割操作。如下所示。\n![图像分割结果1](/img/kmeans_image_seg_via_intensity.png)\n\n然而，上述方法有一个缺点。那就是在真实图像中，属于同一个物体的像素点常常在空间上是相关的（Spatial Coherent）。而上述方法没有考虑到这一点。我们可以根据颜色亮度和空间上的距离构建Feature Space，获得更加合理的分割效果。\n\n在2012年PAMI上有一篇文章[SLIC Superpixels Compared to State-of-the-art Superpixel Methods](https://infoscience.epfl.ch/record/177415/files/Superpixel_PAMI2011-2.pdf)介绍了使用K-Means进行超像素分割的相关内容，可以参考。这里不再赘述了。\n\n## 优点和不足\n作为非监督学习中的经典算法，K-Means的一大优点便是实现简单，速度较快，最小化条件方差（conditional variance，good represention of data，这里没有具体解释，不太懂）。\n\n它的缺点主要有：\n- 对outlier比较敏感。如下图所示。由于outlier的存在，右边的cluster硬是被拉长了。这方面，K-Medians更有鲁棒性。\n![outlier是个大麻烦](/img/kmeans_sensitive_to_outlier.png)\n- 每个点只能确定地属于或不属于某一个cluster。可以参考高斯混合模型和Fuzzy K-Means的soft assignment。\n- 在round shape情况下性能比较好（想想圆形和方形各点的欧几里得距离）。而且每个cluster最好数据点数目和分布的密度相近。\n- 如果数据点有非凸形状分布，算法表现糟糕。可以参考谱聚类（Spectral Clustering）或者kernel K-Means。\n\n针对K-Means，也有不少相关改进工作，参考下面这幅图吧。\n![K-Means Scaling Up](/img/kmeans_scaling_up.png)\n\n## MATLAB实验\n下面是我自己写的简单的K-Means demo。首先，在数据产生环节，确定$K = 3$个cluster的中心位置，并随机产生$N$个数据点，并使用`scatter`函数做出散点图。\n\n代码中的主要部分为`my_kmeans`函数的实现（为了不与内建的kmeans函数重名，故加上了`my`前缀）。在此函数内部，首先随机产生$K$个聚类中心，并对数据点进行初始的指定。接着，在迭代过程中，不断计算均值来更新聚类中心和assignment，当达到最大迭代次数或者相邻两次迭代中assignment的值不再变化为止，并计算对应的目标函数$J$的值。\n\n注意到该函数初始确定聚类中心时有一段注释的代码，该段代码用于debug时，指定聚类中心，去除随机性，以验证后续K-Means迭代的正确性。\n\n``` matlab\n%% generate data\nK = 3;   % number of clusters\npos = [-5, 5; 0, 1; 3, 6];  % position of cluster centers\nN = 20;    % number of data points\nR = 3;     % radius of clusters\ndata = zeros(N, 2);    % data\nclass = zeros(N, 1);   % index of cluster\n\nfor i = 1:N\n    idx = randi(3, 1);\n    dr = R*rand();\n    data(i, :) = pos(idx, :) + [dr*cos(rand()*2*pi), dr*sin(rand()*2*pi)];\n    class(i) = idx;\nend\n\n%% visualization data points\nfigure\nhold on\ncolor = [1,0,0; 0,1,0; 0,0,1];\nfor i = 1:K\n    x = data(class == i, 1);\n    y = data(class == i, 2);\n    scatter(x, y, 150, repmat(color(i,:), [length(x), 1]), 'filled');\nend\n\n%% K-Means\nbest_J = 1E100;\nbest_idx = 0;\nfor times = 1:5  % 5 times experiments to choose the best result\n    [mu, assignment, J] = my_kmeans(data, K);\n    if best_J > J\n        best_idx = times;\n        best_J = J;\n    end\n    fprintf('%d experiment: J = %f\\n', times, J);\n    disp(mu);\nend\nfprintf('best: %d experiment: J = %f\\n', best_idx, best_J);\n\n%% basic functions\nfunction J = ssd(X, mu, assignment)\n% sum of square distance\n% X -- data, N*D matrix\n% mu -- centers of clusters, K*D matrix\n% assignment -- current assignment of data to clusters\nJ = 0;\nK = size(mu, 1);\nfor k = 1:K\n    x_k = X(assignment == k, :);\n    mu_k = mu(k, :);\n    err2 = bsxfun(@minus, x_k, mu_k).^2;\n    J = J + sum(err2(:));\nend\nJ = J / size(X, 1);\nend\n\nfunction mu = compute_mu(X, assignment, K)\nmu = zeros(K, size(X, 2));\nfor k = 1:K\n    x_k = X(assignment == k, :);\n    mu(k, :) = mean(x_k, 1);\nend\nend\n\nfunction assignment = assign(X, mu)\n% assign data points to clusters\nN = size(X, 1);\nassignment = zeros(N, 1);\nfor i = 1:N\n    x = X(i, :);\n    err2 = bsxfun(@minus, x, mu).^2;\n    dis = sum(err2, 2);\n    [~, idx] = min(dis);\n    assignment(i) = idx;\nend\nend\n\nfunction [mu, assignment, J] = my_kmeans(X, K)\nN = size(X, 1);\nassignment = zeros(N, 1);\nidx = randsample(N, K);\nmu = X(idx, :);\n\n% for i = 1:K\n%     for j = 1:N\n%         if assignment_gt(j) == i\n%             mu(i,:) = X(j,:);\n%             break;\n%         end\n%     end\n% end\nfigure\nhold on\ncolor = [1,0,0; 0,1,0; 0,0,1];\nscatter(mu(:,1), mu(:,2), 200, color, 'd');\nfor iter = 1:20\n    assignment_prev = assignment;\n    assignment = assign(X, mu);\n    if assignment == assignment_prev\n        break;\n    end\n    mu_prev = mu;\n    mu = compute_mu(X, assignment, K);\n    scatter(mu(:, 1), mu(:, 2), 200, color, 'd');\n    MU = zeros(2*K, 2);\n    MU(1:2:end, :) = mu_prev;\n    MU(2:2:end, :) = mu;\n    mu_x = reshape(MU(:, 1), [], K);\n    mu_y = reshape(MU(:, 2), [], K);\n    plot(mu_x, mu_y, 'k-.');\n\nend\nfor i = 1:K\n    x = X(assignment == i, 1);\n    y = X(assignment == i, 2);\n    scatter(x, y, 150, repmat(color(i,:), [length(x), 1]), 'filled');\nend\nJ = ssd(X, mu, assignment);\nend\n```\n\n在demo中，随机选取初始化的聚类中心，重复进行$5$次实验。下图是产生的原始数据的散点图。用不同的颜色标明了cluster。\n![K-Means聚类](/img/kmeans_data_demo.png)\n\n下图是某次的实验结果，其中菱形和它们之间的连线指示了聚类中心的变化。注意，颜色只是用来区别不同的cluster，和上图并没有对应关系。可以看到，初始化时绿色标注的cluster的中心（也即是上图中的红色cluster）虽然偏离了很远，但是在迭代过程中也能实现纠正。\n\n![K-Means聚类](/img/kmeans_success.png)\n\n再换个大点的数据集来做，效果貌似还不错~\n![大一些](/img/kmeans_bigger_demo.png)\n## PS\n这里插一句与本文内容完全无关的一个tip：在Markdown中使用LaTex时，如果在内联形式（也就是行内使用两个美元符号插入公式）时，如果有多个下划线，那么Markdown有时会将下划线之间的部分认为是自己的斜体标记，如下所示：\n```\n$c_{i}^\\ast$ XXX $\\delta_{ij}^\\ast$\n```\n它的显示效果为$c_{i}^\\ast$ XXX $\\delta_{ij}^\\ast$。\n\n这时候，只需在下划线前加入反斜线进行转义即可（虽然略显麻烦）。如下所示：\n```\n$c\\_{i}^\\ast$ XXX $\\delta\\_{ij}^\\ast$\n```\n它的显示效果为$c\\_{i}^\\ast$ XXX $\\delta\\_{ij}^\\ast$。\n\n具体分析可以参见[博客](http://lukang.me/2014/mathjax-for-hexo.html)。\n","source":"_posts/cs131-kmeans.md","raw":"---\ntitle: CS131-KMeans聚类\ndate: 2017-02-05 23:07:00\ntags:\n    - cs131\n    - 公开课\n---\n[K-Means聚类](https://zh.wikipedia.org/wiki/K-平均算法)是把n个点（可以是样本的一次观察或一个实例）划分到$k$个聚类中，使得每个点都属于离他最近的均值（此即聚类中心）对应的聚类，以之作为聚类的标准，满足最小化聚类中心和属于该中心的数据点之间的平方距离和（Sum of Square Distance，SSD）。\n$$\\text{SSD} = \\sum_{i=1}^{k}\\sum_{x\\in c_i}(x-c_i)^2$$\n![K-Means Demo](/img/kmeans_demo.png)\n\n<!-- more -->\n## 目标函数\nK-Means方法实际上需要确定两个参数，$c^\\ast$和$\\delta^\\ast$。其中$c\\_{i}^\\ast$代表各个聚类中心的位置，$\\delta\\_{ij}^\\ast$的取值为$\\lbrace 0,1\\rbrace$，代表点$x_j$是否被分到了第$i$个聚类中心。\n\n那么，目标函数可以写成如下的形式。\n$$c^\\ast, \\delta^\\ast = \\arg\\min_{c,\\delta} \\frac{1}{N}\\sum_{j=1}^{N}\\sum_{i=1}^{k}\\delta_{i,j}(c_i-x_j)^2$$\n\n然而这样一来，造成了一种很尴尬的局面。一方面，要想优化$c\\_i$，需要我们给定每个点所属的类；另一方面，优化$\\delta\\_{i,j}$，需要我们给定聚类中心。由于这两个优化变量之间是纠缠在一起的，K-Means算法如何能够达到收敛就变成了一个讨论“先有鸡还是先有蛋”的问题了。\n\n实际使用中，K-Means的迭代过程，实际是EM算法的一个特例。\n\n## 算法流程\nK-Means算法的流程如下所示。\n![K-Means算法流程](/img/kmeans_algorithm.png)\n\n假设我们有$N$个样本点，$\\lbrace x_1, \\dots, x_N\\rbrace, x_i\\in\\mathbb{R}^D$，并给出聚类数目$k$。\n\n首先，随机选取一系列的聚类中心点$\\mu_i, i = 1,\\dots, k$。接着，我们按照距离最近的原则为每个数据点指定相应的聚类中心并计算新的数据点均值更新聚类中心。如此这般，直到收敛。\n\n## 算法细节\n### 初始化\n上面的K-Means方法对初始值很敏感。朴素的初始化方法是直接在数据点中随机抽取$k$个作为聚类初始中心点。不够好的初始值可能造成收敛速度很慢或者聚类失败。下面介绍两种改进方法。\n- kmeans++方法，尽可能地使初始聚类中心分散开（spread-out）。\n  这种方法首先随机产生一个初始聚类中心点，然后按照概率$P = \\omega(x-c_i)^2$选取其他的聚类中心点。其中$\\omega$是归一化系数。\n\n- 多次初始化，保留最好的结果。\n\n### K值的选取\n在上面的讨论中，我们一直把$k$当做已经给定的参数。但是在实际操作中，聚类类别数通常都是未知的。如何确定超参数$k$呢？\n\n我们可以使用不同的$k$值进行试验，并作出不同试验收敛后目标函数随$k$的变化。如下图所示，分别使用$1$到$4$之间的几个数字作为$k$值，曲线在$k=2$处有一个较为明显的弯曲点（elbow point）。故确定$k$取值为2。\n![参数K的确定](/img/kmeans_object_fun_vs_k.png)\n\n### 距离的度量\n目标函数是各个cluster中的点与其中心点的距离均值。其中就涉及到了“距离”的量度。下面是几种较为常用的距离度量方法。\n- 欧几里得距离（最为常用）\n- 余弦距离（向量的夹角）\n- 核函数（[Kernel K-Means](http://www.public.asu.edu/~jye02/CLASSES/Fall-2005/PAPERS/kdd_spectral_kernelkmeans.pdf)）\n\n### 迭代终止条件\n当判断算法已经收敛时，退出迭代。常用的迭代终止条件如下：\n- 达到了预先给定的最大迭代次数\n- 在将数据点assign到不同聚类中心时，和上轮结果没有变化（说明已经收敛）\n- 目标函数（平均的距离）下降小于阈值\n\n## 基于K-Means的图像分割\n图像分割中可以使用K-Means算法。我们首先将一副图像转换为特征空间（Feature Space）。例如使用灰度或者RGB的值作为特征向量，就得到了1D或者3D的特征空间。之后，可以基于Feature Space上各点的相似度，将它们聚类。这样，就完成了对原始图像的分割操作。如下所示。\n![图像分割结果1](/img/kmeans_image_seg_via_intensity.png)\n\n然而，上述方法有一个缺点。那就是在真实图像中，属于同一个物体的像素点常常在空间上是相关的（Spatial Coherent）。而上述方法没有考虑到这一点。我们可以根据颜色亮度和空间上的距离构建Feature Space，获得更加合理的分割效果。\n\n在2012年PAMI上有一篇文章[SLIC Superpixels Compared to State-of-the-art Superpixel Methods](https://infoscience.epfl.ch/record/177415/files/Superpixel_PAMI2011-2.pdf)介绍了使用K-Means进行超像素分割的相关内容，可以参考。这里不再赘述了。\n\n## 优点和不足\n作为非监督学习中的经典算法，K-Means的一大优点便是实现简单，速度较快，最小化条件方差（conditional variance，good represention of data，这里没有具体解释，不太懂）。\n\n它的缺点主要有：\n- 对outlier比较敏感。如下图所示。由于outlier的存在，右边的cluster硬是被拉长了。这方面，K-Medians更有鲁棒性。\n![outlier是个大麻烦](/img/kmeans_sensitive_to_outlier.png)\n- 每个点只能确定地属于或不属于某一个cluster。可以参考高斯混合模型和Fuzzy K-Means的soft assignment。\n- 在round shape情况下性能比较好（想想圆形和方形各点的欧几里得距离）。而且每个cluster最好数据点数目和分布的密度相近。\n- 如果数据点有非凸形状分布，算法表现糟糕。可以参考谱聚类（Spectral Clustering）或者kernel K-Means。\n\n针对K-Means，也有不少相关改进工作，参考下面这幅图吧。\n![K-Means Scaling Up](/img/kmeans_scaling_up.png)\n\n## MATLAB实验\n下面是我自己写的简单的K-Means demo。首先，在数据产生环节，确定$K = 3$个cluster的中心位置，并随机产生$N$个数据点，并使用`scatter`函数做出散点图。\n\n代码中的主要部分为`my_kmeans`函数的实现（为了不与内建的kmeans函数重名，故加上了`my`前缀）。在此函数内部，首先随机产生$K$个聚类中心，并对数据点进行初始的指定。接着，在迭代过程中，不断计算均值来更新聚类中心和assignment，当达到最大迭代次数或者相邻两次迭代中assignment的值不再变化为止，并计算对应的目标函数$J$的值。\n\n注意到该函数初始确定聚类中心时有一段注释的代码，该段代码用于debug时，指定聚类中心，去除随机性，以验证后续K-Means迭代的正确性。\n\n``` matlab\n%% generate data\nK = 3;   % number of clusters\npos = [-5, 5; 0, 1; 3, 6];  % position of cluster centers\nN = 20;    % number of data points\nR = 3;     % radius of clusters\ndata = zeros(N, 2);    % data\nclass = zeros(N, 1);   % index of cluster\n\nfor i = 1:N\n    idx = randi(3, 1);\n    dr = R*rand();\n    data(i, :) = pos(idx, :) + [dr*cos(rand()*2*pi), dr*sin(rand()*2*pi)];\n    class(i) = idx;\nend\n\n%% visualization data points\nfigure\nhold on\ncolor = [1,0,0; 0,1,0; 0,0,1];\nfor i = 1:K\n    x = data(class == i, 1);\n    y = data(class == i, 2);\n    scatter(x, y, 150, repmat(color(i,:), [length(x), 1]), 'filled');\nend\n\n%% K-Means\nbest_J = 1E100;\nbest_idx = 0;\nfor times = 1:5  % 5 times experiments to choose the best result\n    [mu, assignment, J] = my_kmeans(data, K);\n    if best_J > J\n        best_idx = times;\n        best_J = J;\n    end\n    fprintf('%d experiment: J = %f\\n', times, J);\n    disp(mu);\nend\nfprintf('best: %d experiment: J = %f\\n', best_idx, best_J);\n\n%% basic functions\nfunction J = ssd(X, mu, assignment)\n% sum of square distance\n% X -- data, N*D matrix\n% mu -- centers of clusters, K*D matrix\n% assignment -- current assignment of data to clusters\nJ = 0;\nK = size(mu, 1);\nfor k = 1:K\n    x_k = X(assignment == k, :);\n    mu_k = mu(k, :);\n    err2 = bsxfun(@minus, x_k, mu_k).^2;\n    J = J + sum(err2(:));\nend\nJ = J / size(X, 1);\nend\n\nfunction mu = compute_mu(X, assignment, K)\nmu = zeros(K, size(X, 2));\nfor k = 1:K\n    x_k = X(assignment == k, :);\n    mu(k, :) = mean(x_k, 1);\nend\nend\n\nfunction assignment = assign(X, mu)\n% assign data points to clusters\nN = size(X, 1);\nassignment = zeros(N, 1);\nfor i = 1:N\n    x = X(i, :);\n    err2 = bsxfun(@minus, x, mu).^2;\n    dis = sum(err2, 2);\n    [~, idx] = min(dis);\n    assignment(i) = idx;\nend\nend\n\nfunction [mu, assignment, J] = my_kmeans(X, K)\nN = size(X, 1);\nassignment = zeros(N, 1);\nidx = randsample(N, K);\nmu = X(idx, :);\n\n% for i = 1:K\n%     for j = 1:N\n%         if assignment_gt(j) == i\n%             mu(i,:) = X(j,:);\n%             break;\n%         end\n%     end\n% end\nfigure\nhold on\ncolor = [1,0,0; 0,1,0; 0,0,1];\nscatter(mu(:,1), mu(:,2), 200, color, 'd');\nfor iter = 1:20\n    assignment_prev = assignment;\n    assignment = assign(X, mu);\n    if assignment == assignment_prev\n        break;\n    end\n    mu_prev = mu;\n    mu = compute_mu(X, assignment, K);\n    scatter(mu(:, 1), mu(:, 2), 200, color, 'd');\n    MU = zeros(2*K, 2);\n    MU(1:2:end, :) = mu_prev;\n    MU(2:2:end, :) = mu;\n    mu_x = reshape(MU(:, 1), [], K);\n    mu_y = reshape(MU(:, 2), [], K);\n    plot(mu_x, mu_y, 'k-.');\n\nend\nfor i = 1:K\n    x = X(assignment == i, 1);\n    y = X(assignment == i, 2);\n    scatter(x, y, 150, repmat(color(i,:), [length(x), 1]), 'filled');\nend\nJ = ssd(X, mu, assignment);\nend\n```\n\n在demo中，随机选取初始化的聚类中心，重复进行$5$次实验。下图是产生的原始数据的散点图。用不同的颜色标明了cluster。\n![K-Means聚类](/img/kmeans_data_demo.png)\n\n下图是某次的实验结果，其中菱形和它们之间的连线指示了聚类中心的变化。注意，颜色只是用来区别不同的cluster，和上图并没有对应关系。可以看到，初始化时绿色标注的cluster的中心（也即是上图中的红色cluster）虽然偏离了很远，但是在迭代过程中也能实现纠正。\n\n![K-Means聚类](/img/kmeans_success.png)\n\n再换个大点的数据集来做，效果貌似还不错~\n![大一些](/img/kmeans_bigger_demo.png)\n## PS\n这里插一句与本文内容完全无关的一个tip：在Markdown中使用LaTex时，如果在内联形式（也就是行内使用两个美元符号插入公式）时，如果有多个下划线，那么Markdown有时会将下划线之间的部分认为是自己的斜体标记，如下所示：\n```\n$c_{i}^\\ast$ XXX $\\delta_{ij}^\\ast$\n```\n它的显示效果为$c_{i}^\\ast$ XXX $\\delta_{ij}^\\ast$。\n\n这时候，只需在下划线前加入反斜线进行转义即可（虽然略显麻烦）。如下所示：\n```\n$c\\_{i}^\\ast$ XXX $\\delta\\_{ij}^\\ast$\n```\n它的显示效果为$c\\_{i}^\\ast$ XXX $\\delta\\_{ij}^\\ast$。\n\n具体分析可以参见[博客](http://lukang.me/2014/mathjax-for-hexo.html)。\n","slug":"cs131-kmeans","published":1,"updated":"2018-01-12T06:22:20.460Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjcu6vcrz000cqu46zs9i694l","content":"<p><a href=\"https://zh.wikipedia.org/wiki/K-平均算法\" target=\"_blank\" rel=\"external\">K-Means聚类</a>是把n个点（可以是样本的一次观察或一个实例）划分到$k$个聚类中，使得每个点都属于离他最近的均值（此即聚类中心）对应的聚类，以之作为聚类的标准，满足最小化聚类中心和属于该中心的数据点之间的平方距离和（Sum of Square Distance，SSD）。</p>\n<script type=\"math/tex; mode=display\">\\text{SSD} = \\sum_{i=1}^{k}\\sum_{x\\in c_i}(x-c_i)^2</script><p><img src=\"/img/kmeans_demo.png\" alt=\"K-Means Demo\"></p>\n<a id=\"more\"></a>\n<h2 id=\"目标函数\"><a href=\"#目标函数\" class=\"headerlink\" title=\"目标函数\"></a>目标函数</h2><p>K-Means方法实际上需要确定两个参数，$c^\\ast$和$\\delta^\\ast$。其中$c_{i}^\\ast$代表各个聚类中心的位置，$\\delta_{ij}^\\ast$的取值为$\\lbrace 0,1\\rbrace$，代表点$x_j$是否被分到了第$i$个聚类中心。</p>\n<p>那么，目标函数可以写成如下的形式。</p>\n<script type=\"math/tex; mode=display\">c^\\ast, \\delta^\\ast = \\arg\\min_{c,\\delta} \\frac{1}{N}\\sum_{j=1}^{N}\\sum_{i=1}^{k}\\delta_{i,j}(c_i-x_j)^2</script><p>然而这样一来，造成了一种很尴尬的局面。一方面，要想优化$c_i$，需要我们给定每个点所属的类；另一方面，优化$\\delta_{i,j}$，需要我们给定聚类中心。由于这两个优化变量之间是纠缠在一起的，K-Means算法如何能够达到收敛就变成了一个讨论“先有鸡还是先有蛋”的问题了。</p>\n<p>实际使用中，K-Means的迭代过程，实际是EM算法的一个特例。</p>\n<h2 id=\"算法流程\"><a href=\"#算法流程\" class=\"headerlink\" title=\"算法流程\"></a>算法流程</h2><p>K-Means算法的流程如下所示。<br><img src=\"/img/kmeans_algorithm.png\" alt=\"K-Means算法流程\"></p>\n<p>假设我们有$N$个样本点，$\\lbrace x_1, \\dots, x_N\\rbrace, x_i\\in\\mathbb{R}^D$，并给出聚类数目$k$。</p>\n<p>首先，随机选取一系列的聚类中心点$\\mu_i, i = 1,\\dots, k$。接着，我们按照距离最近的原则为每个数据点指定相应的聚类中心并计算新的数据点均值更新聚类中心。如此这般，直到收敛。</p>\n<h2 id=\"算法细节\"><a href=\"#算法细节\" class=\"headerlink\" title=\"算法细节\"></a>算法细节</h2><h3 id=\"初始化\"><a href=\"#初始化\" class=\"headerlink\" title=\"初始化\"></a>初始化</h3><p>上面的K-Means方法对初始值很敏感。朴素的初始化方法是直接在数据点中随机抽取$k$个作为聚类初始中心点。不够好的初始值可能造成收敛速度很慢或者聚类失败。下面介绍两种改进方法。</p>\n<ul>\n<li><p>kmeans++方法，尽可能地使初始聚类中心分散开（spread-out）。<br>这种方法首先随机产生一个初始聚类中心点，然后按照概率$P = \\omega(x-c_i)^2$选取其他的聚类中心点。其中$\\omega$是归一化系数。</p>\n</li>\n<li><p>多次初始化，保留最好的结果。</p>\n</li>\n</ul>\n<h3 id=\"K值的选取\"><a href=\"#K值的选取\" class=\"headerlink\" title=\"K值的选取\"></a>K值的选取</h3><p>在上面的讨论中，我们一直把$k$当做已经给定的参数。但是在实际操作中，聚类类别数通常都是未知的。如何确定超参数$k$呢？</p>\n<p>我们可以使用不同的$k$值进行试验，并作出不同试验收敛后目标函数随$k$的变化。如下图所示，分别使用$1$到$4$之间的几个数字作为$k$值，曲线在$k=2$处有一个较为明显的弯曲点（elbow point）。故确定$k$取值为2。<br><img src=\"/img/kmeans_object_fun_vs_k.png\" alt=\"参数K的确定\"></p>\n<h3 id=\"距离的度量\"><a href=\"#距离的度量\" class=\"headerlink\" title=\"距离的度量\"></a>距离的度量</h3><p>目标函数是各个cluster中的点与其中心点的距离均值。其中就涉及到了“距离”的量度。下面是几种较为常用的距离度量方法。</p>\n<ul>\n<li>欧几里得距离（最为常用）</li>\n<li>余弦距离（向量的夹角）</li>\n<li>核函数（<a href=\"http://www.public.asu.edu/~jye02/CLASSES/Fall-2005/PAPERS/kdd_spectral_kernelkmeans.pdf\" target=\"_blank\" rel=\"external\">Kernel K-Means</a>）</li>\n</ul>\n<h3 id=\"迭代终止条件\"><a href=\"#迭代终止条件\" class=\"headerlink\" title=\"迭代终止条件\"></a>迭代终止条件</h3><p>当判断算法已经收敛时，退出迭代。常用的迭代终止条件如下：</p>\n<ul>\n<li>达到了预先给定的最大迭代次数</li>\n<li>在将数据点assign到不同聚类中心时，和上轮结果没有变化（说明已经收敛）</li>\n<li>目标函数（平均的距离）下降小于阈值</li>\n</ul>\n<h2 id=\"基于K-Means的图像分割\"><a href=\"#基于K-Means的图像分割\" class=\"headerlink\" title=\"基于K-Means的图像分割\"></a>基于K-Means的图像分割</h2><p>图像分割中可以使用K-Means算法。我们首先将一副图像转换为特征空间（Feature Space）。例如使用灰度或者RGB的值作为特征向量，就得到了1D或者3D的特征空间。之后，可以基于Feature Space上各点的相似度，将它们聚类。这样，就完成了对原始图像的分割操作。如下所示。<br><img src=\"/img/kmeans_image_seg_via_intensity.png\" alt=\"图像分割结果1\"></p>\n<p>然而，上述方法有一个缺点。那就是在真实图像中，属于同一个物体的像素点常常在空间上是相关的（Spatial Coherent）。而上述方法没有考虑到这一点。我们可以根据颜色亮度和空间上的距离构建Feature Space，获得更加合理的分割效果。</p>\n<p>在2012年PAMI上有一篇文章<a href=\"https://infoscience.epfl.ch/record/177415/files/Superpixel_PAMI2011-2.pdf\" target=\"_blank\" rel=\"external\">SLIC Superpixels Compared to State-of-the-art Superpixel Methods</a>介绍了使用K-Means进行超像素分割的相关内容，可以参考。这里不再赘述了。</p>\n<h2 id=\"优点和不足\"><a href=\"#优点和不足\" class=\"headerlink\" title=\"优点和不足\"></a>优点和不足</h2><p>作为非监督学习中的经典算法，K-Means的一大优点便是实现简单，速度较快，最小化条件方差（conditional variance，good represention of data，这里没有具体解释，不太懂）。</p>\n<p>它的缺点主要有：</p>\n<ul>\n<li>对outlier比较敏感。如下图所示。由于outlier的存在，右边的cluster硬是被拉长了。这方面，K-Medians更有鲁棒性。<br><img src=\"/img/kmeans_sensitive_to_outlier.png\" alt=\"outlier是个大麻烦\"></li>\n<li>每个点只能确定地属于或不属于某一个cluster。可以参考高斯混合模型和Fuzzy K-Means的soft assignment。</li>\n<li>在round shape情况下性能比较好（想想圆形和方形各点的欧几里得距离）。而且每个cluster最好数据点数目和分布的密度相近。</li>\n<li>如果数据点有非凸形状分布，算法表现糟糕。可以参考谱聚类（Spectral Clustering）或者kernel K-Means。</li>\n</ul>\n<p>针对K-Means，也有不少相关改进工作，参考下面这幅图吧。<br><img src=\"/img/kmeans_scaling_up.png\" alt=\"K-Means Scaling Up\"></p>\n<h2 id=\"MATLAB实验\"><a href=\"#MATLAB实验\" class=\"headerlink\" title=\"MATLAB实验\"></a>MATLAB实验</h2><p>下面是我自己写的简单的K-Means demo。首先，在数据产生环节，确定$K = 3$个cluster的中心位置，并随机产生$N$个数据点，并使用<code>scatter</code>函数做出散点图。</p>\n<p>代码中的主要部分为<code>my_kmeans</code>函数的实现（为了不与内建的kmeans函数重名，故加上了<code>my</code>前缀）。在此函数内部，首先随机产生$K$个聚类中心，并对数据点进行初始的指定。接着，在迭代过程中，不断计算均值来更新聚类中心和assignment，当达到最大迭代次数或者相邻两次迭代中assignment的值不再变化为止，并计算对应的目标函数$J$的值。</p>\n<p>注意到该函数初始确定聚类中心时有一段注释的代码，该段代码用于debug时，指定聚类中心，去除随机性，以验证后续K-Means迭代的正确性。</p>\n<figure class=\"highlight matlab\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div><div class=\"line\">47</div><div class=\"line\">48</div><div class=\"line\">49</div><div class=\"line\">50</div><div class=\"line\">51</div><div class=\"line\">52</div><div class=\"line\">53</div><div class=\"line\">54</div><div class=\"line\">55</div><div class=\"line\">56</div><div class=\"line\">57</div><div class=\"line\">58</div><div class=\"line\">59</div><div class=\"line\">60</div><div class=\"line\">61</div><div class=\"line\">62</div><div class=\"line\">63</div><div class=\"line\">64</div><div class=\"line\">65</div><div class=\"line\">66</div><div class=\"line\">67</div><div class=\"line\">68</div><div class=\"line\">69</div><div class=\"line\">70</div><div class=\"line\">71</div><div class=\"line\">72</div><div class=\"line\">73</div><div class=\"line\">74</div><div class=\"line\">75</div><div class=\"line\">76</div><div class=\"line\">77</div><div class=\"line\">78</div><div class=\"line\">79</div><div class=\"line\">80</div><div class=\"line\">81</div><div class=\"line\">82</div><div class=\"line\">83</div><div class=\"line\">84</div><div class=\"line\">85</div><div class=\"line\">86</div><div class=\"line\">87</div><div class=\"line\">88</div><div class=\"line\">89</div><div class=\"line\">90</div><div class=\"line\">91</div><div class=\"line\">92</div><div class=\"line\">93</div><div class=\"line\">94</div><div class=\"line\">95</div><div class=\"line\">96</div><div class=\"line\">97</div><div class=\"line\">98</div><div class=\"line\">99</div><div class=\"line\">100</div><div class=\"line\">101</div><div class=\"line\">102</div><div class=\"line\">103</div><div class=\"line\">104</div><div class=\"line\">105</div><div class=\"line\">106</div><div class=\"line\">107</div><div class=\"line\">108</div><div class=\"line\">109</div><div class=\"line\">110</div><div class=\"line\">111</div><div class=\"line\">112</div><div class=\"line\">113</div><div class=\"line\">114</div><div class=\"line\">115</div><div class=\"line\">116</div><div class=\"line\">117</div><div class=\"line\">118</div><div class=\"line\">119</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\">%% generate data</span></div><div class=\"line\">K = <span class=\"number\">3</span>;   <span class=\"comment\">% number of clusters</span></div><div class=\"line\">pos = [<span class=\"number\">-5</span>, <span class=\"number\">5</span>; <span class=\"number\">0</span>, <span class=\"number\">1</span>; <span class=\"number\">3</span>, <span class=\"number\">6</span>];  <span class=\"comment\">% position of cluster centers</span></div><div class=\"line\">N = <span class=\"number\">20</span>;    <span class=\"comment\">% number of data points</span></div><div class=\"line\">R = <span class=\"number\">3</span>;     <span class=\"comment\">% radius of clusters</span></div><div class=\"line\">data = <span class=\"built_in\">zeros</span>(N, <span class=\"number\">2</span>);    <span class=\"comment\">% data</span></div><div class=\"line\">class = <span class=\"built_in\">zeros</span>(N, <span class=\"number\">1</span>);   <span class=\"comment\">% index of cluster</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"keyword\">for</span> <span class=\"built_in\">i</span> = <span class=\"number\">1</span>:N</div><div class=\"line\">    idx = randi(<span class=\"number\">3</span>, <span class=\"number\">1</span>);</div><div class=\"line\">    dr = R*<span class=\"built_in\">rand</span>();</div><div class=\"line\">    data(<span class=\"built_in\">i</span>, :) = pos(idx, :) + [dr*cos(rand()*<span class=\"number\">2</span>*pi), dr*sin(rand()*<span class=\"number\">2</span>*pi)];</div><div class=\"line\">    class(<span class=\"built_in\">i</span>) = idx;</div><div class=\"line\"><span class=\"keyword\">end</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">%% visualization data points</span></div><div class=\"line\">figure</div><div class=\"line\">hold on</div><div class=\"line\">color = [<span class=\"number\">1</span>,<span class=\"number\">0</span>,<span class=\"number\">0</span>; <span class=\"number\">0</span>,<span class=\"number\">1</span>,<span class=\"number\">0</span>; <span class=\"number\">0</span>,<span class=\"number\">0</span>,<span class=\"number\">1</span>];</div><div class=\"line\"><span class=\"keyword\">for</span> <span class=\"built_in\">i</span> = <span class=\"number\">1</span>:K</div><div class=\"line\">    x = data(class == <span class=\"built_in\">i</span>, <span class=\"number\">1</span>);</div><div class=\"line\">    y = data(class == <span class=\"built_in\">i</span>, <span class=\"number\">2</span>);</div><div class=\"line\">    scatter(x, y, <span class=\"number\">150</span>, <span class=\"built_in\">repmat</span>(color(<span class=\"built_in\">i</span>,:), [length(x), <span class=\"number\">1</span>]), <span class=\"string\">'filled'</span>);</div><div class=\"line\"><span class=\"keyword\">end</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">%% K-Means</span></div><div class=\"line\">best_J = <span class=\"number\">1E100</span>;</div><div class=\"line\">best_idx = <span class=\"number\">0</span>;</div><div class=\"line\"><span class=\"keyword\">for</span> times = <span class=\"number\">1</span>:<span class=\"number\">5</span>  <span class=\"comment\">% 5 times experiments to choose the best result</span></div><div class=\"line\">    [mu, assignment, J] = my_kmeans(data, K);</div><div class=\"line\">    <span class=\"keyword\">if</span> best_J &gt; J</div><div class=\"line\">        best_idx = times;</div><div class=\"line\">        best_J = J;</div><div class=\"line\">    <span class=\"keyword\">end</span></div><div class=\"line\">    fprintf(<span class=\"string\">'%d experiment: J = %f\\n'</span>, times, J);</div><div class=\"line\">    <span class=\"built_in\">disp</span>(mu);</div><div class=\"line\"><span class=\"keyword\">end</span></div><div class=\"line\">fprintf(<span class=\"string\">'best: %d experiment: J = %f\\n'</span>, best_idx, best_J);</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">%% basic functions</span></div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">function</span> <span class=\"title\">J</span> = <span class=\"title\">ssd</span><span class=\"params\">(X, mu, assignment)</span></span></div><div class=\"line\"><span class=\"comment\">% sum of square distance</span></div><div class=\"line\"><span class=\"comment\">% X -- data, N*D matrix</span></div><div class=\"line\"><span class=\"comment\">% mu -- centers of clusters, K*D matrix</span></div><div class=\"line\"><span class=\"comment\">% assignment -- current assignment of data to clusters</span></div><div class=\"line\">J = <span class=\"number\">0</span>;</div><div class=\"line\">K = <span class=\"built_in\">size</span>(mu, <span class=\"number\">1</span>);</div><div class=\"line\"><span class=\"keyword\">for</span> k = <span class=\"number\">1</span>:K</div><div class=\"line\">    x_k = X(assignment == k, :);</div><div class=\"line\">    mu_k = mu(k, :);</div><div class=\"line\">    err2 = <span class=\"built_in\">bsxfun</span>(@minus, x_k, mu_k).^<span class=\"number\">2</span>;</div><div class=\"line\">    J = J + sum(err2(:));</div><div class=\"line\"><span class=\"keyword\">end</span></div><div class=\"line\">J = J / <span class=\"built_in\">size</span>(X, <span class=\"number\">1</span>);</div><div class=\"line\"><span class=\"keyword\">end</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">function</span> <span class=\"title\">mu</span> = <span class=\"title\">compute_mu</span><span class=\"params\">(X, assignment, K)</span></span></div><div class=\"line\">mu = <span class=\"built_in\">zeros</span>(K, <span class=\"built_in\">size</span>(X, <span class=\"number\">2</span>));</div><div class=\"line\"><span class=\"keyword\">for</span> k = <span class=\"number\">1</span>:K</div><div class=\"line\">    x_k = X(assignment == k, :);</div><div class=\"line\">    mu(k, :) = mean(x_k, <span class=\"number\">1</span>);</div><div class=\"line\"><span class=\"keyword\">end</span></div><div class=\"line\"><span class=\"keyword\">end</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">function</span> <span class=\"title\">assignment</span> = <span class=\"title\">assign</span><span class=\"params\">(X, mu)</span></span></div><div class=\"line\"><span class=\"comment\">% assign data points to clusters</span></div><div class=\"line\">N = <span class=\"built_in\">size</span>(X, <span class=\"number\">1</span>);</div><div class=\"line\">assignment = <span class=\"built_in\">zeros</span>(N, <span class=\"number\">1</span>);</div><div class=\"line\"><span class=\"keyword\">for</span> <span class=\"built_in\">i</span> = <span class=\"number\">1</span>:N</div><div class=\"line\">    x = X(<span class=\"built_in\">i</span>, :);</div><div class=\"line\">    err2 = <span class=\"built_in\">bsxfun</span>(@minus, x, mu).^<span class=\"number\">2</span>;</div><div class=\"line\">    dis = sum(err2, <span class=\"number\">2</span>);</div><div class=\"line\">    [~, idx] = min(dis);</div><div class=\"line\">    assignment(<span class=\"built_in\">i</span>) = idx;</div><div class=\"line\"><span class=\"keyword\">end</span></div><div class=\"line\"><span class=\"keyword\">end</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">function</span> <span class=\"params\">[mu, assignment, J]</span> = <span class=\"title\">my_kmeans</span><span class=\"params\">(X, K)</span></span></div><div class=\"line\">N = <span class=\"built_in\">size</span>(X, <span class=\"number\">1</span>);</div><div class=\"line\">assignment = <span class=\"built_in\">zeros</span>(N, <span class=\"number\">1</span>);</div><div class=\"line\">idx = randsample(N, K);</div><div class=\"line\">mu = X(idx, :);</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">% for i = 1:K</span></div><div class=\"line\"><span class=\"comment\">%     for j = 1:N</span></div><div class=\"line\"><span class=\"comment\">%         if assignment_gt(j) == i</span></div><div class=\"line\"><span class=\"comment\">%             mu(i,:) = X(j,:);</span></div><div class=\"line\"><span class=\"comment\">%             break;</span></div><div class=\"line\"><span class=\"comment\">%         end</span></div><div class=\"line\"><span class=\"comment\">%     end</span></div><div class=\"line\"><span class=\"comment\">% end</span></div><div class=\"line\">figure</div><div class=\"line\">hold on</div><div class=\"line\">color = [<span class=\"number\">1</span>,<span class=\"number\">0</span>,<span class=\"number\">0</span>; <span class=\"number\">0</span>,<span class=\"number\">1</span>,<span class=\"number\">0</span>; <span class=\"number\">0</span>,<span class=\"number\">0</span>,<span class=\"number\">1</span>];</div><div class=\"line\">scatter(mu(:,<span class=\"number\">1</span>), mu(:,<span class=\"number\">2</span>), <span class=\"number\">200</span>, color, <span class=\"string\">'d'</span>);</div><div class=\"line\"><span class=\"keyword\">for</span> iter = <span class=\"number\">1</span>:<span class=\"number\">20</span></div><div class=\"line\">    assignment_prev = assignment;</div><div class=\"line\">    assignment = assign(X, mu);</div><div class=\"line\">    <span class=\"keyword\">if</span> assignment == assignment_prev</div><div class=\"line\">        <span class=\"keyword\">break</span>;</div><div class=\"line\">    <span class=\"keyword\">end</span></div><div class=\"line\">    mu_prev = mu;</div><div class=\"line\">    mu = compute_mu(X, assignment, K);</div><div class=\"line\">    scatter(mu(:, <span class=\"number\">1</span>), mu(:, <span class=\"number\">2</span>), <span class=\"number\">200</span>, color, <span class=\"string\">'d'</span>);</div><div class=\"line\">    MU = <span class=\"built_in\">zeros</span>(<span class=\"number\">2</span>*K, <span class=\"number\">2</span>);</div><div class=\"line\">    MU(<span class=\"number\">1</span>:<span class=\"number\">2</span>:<span class=\"keyword\">end</span>, :) = mu_prev;</div><div class=\"line\">    MU(<span class=\"number\">2</span>:<span class=\"number\">2</span>:<span class=\"keyword\">end</span>, :) = mu;</div><div class=\"line\">    mu_x = <span class=\"built_in\">reshape</span>(MU(:, <span class=\"number\">1</span>), [], K);</div><div class=\"line\">    mu_y = <span class=\"built_in\">reshape</span>(MU(:, <span class=\"number\">2</span>), [], K);</div><div class=\"line\">    plot(mu_x, mu_y, <span class=\"string\">'k-.'</span>);</div><div class=\"line\"></div><div class=\"line\"><span class=\"keyword\">end</span></div><div class=\"line\"><span class=\"keyword\">for</span> <span class=\"built_in\">i</span> = <span class=\"number\">1</span>:K</div><div class=\"line\">    x = X(assignment == <span class=\"built_in\">i</span>, <span class=\"number\">1</span>);</div><div class=\"line\">    y = X(assignment == <span class=\"built_in\">i</span>, <span class=\"number\">2</span>);</div><div class=\"line\">    scatter(x, y, <span class=\"number\">150</span>, <span class=\"built_in\">repmat</span>(color(<span class=\"built_in\">i</span>,:), [length(x), <span class=\"number\">1</span>]), <span class=\"string\">'filled'</span>);</div><div class=\"line\"><span class=\"keyword\">end</span></div><div class=\"line\">J = ssd(X, mu, assignment);</div><div class=\"line\"><span class=\"keyword\">end</span></div></pre></td></tr></table></figure>\n<p>在demo中，随机选取初始化的聚类中心，重复进行$5$次实验。下图是产生的原始数据的散点图。用不同的颜色标明了cluster。<br><img src=\"/img/kmeans_data_demo.png\" alt=\"K-Means聚类\"></p>\n<p>下图是某次的实验结果，其中菱形和它们之间的连线指示了聚类中心的变化。注意，颜色只是用来区别不同的cluster，和上图并没有对应关系。可以看到，初始化时绿色标注的cluster的中心（也即是上图中的红色cluster）虽然偏离了很远，但是在迭代过程中也能实现纠正。</p>\n<p><img src=\"/img/kmeans_success.png\" alt=\"K-Means聚类\"></p>\n<p>再换个大点的数据集来做，效果貌似还不错~<br><img src=\"/img/kmeans_bigger_demo.png\" alt=\"大一些\"></p>\n<h2 id=\"PS\"><a href=\"#PS\" class=\"headerlink\" title=\"PS\"></a>PS</h2><p>这里插一句与本文内容完全无关的一个tip：在Markdown中使用LaTex时，如果在内联形式（也就是行内使用两个美元符号插入公式）时，如果有多个下划线，那么Markdown有时会将下划线之间的部分认为是自己的斜体标记，如下所示：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">$c_&#123;i&#125;^\\ast$ XXX $\\delta_&#123;ij&#125;^\\ast$</div></pre></td></tr></table></figure></p>\n<p>它的显示效果为$c<em>{i}^\\ast$ XXX $\\delta</em>{ij}^\\ast$。</p>\n<p>这时候，只需在下划线前加入反斜线进行转义即可（虽然略显麻烦）。如下所示：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">$c\\_&#123;i&#125;^\\ast$ XXX $\\delta\\_&#123;ij&#125;^\\ast$</div></pre></td></tr></table></figure></p>\n<p>它的显示效果为$c_{i}^\\ast$ XXX $\\delta_{ij}^\\ast$。</p>\n<p>具体分析可以参见<a href=\"http://lukang.me/2014/mathjax-for-hexo.html\" target=\"_blank\" rel=\"external\">博客</a>。</p>\n","excerpt":"<p><a href=\"https://zh.wikipedia.org/wiki/K-平均算法\">K-Means聚类</a>是把n个点（可以是样本的一次观察或一个实例）划分到$k$个聚类中，使得每个点都属于离他最近的均值（此即聚类中心）对应的聚类，以之作为聚类的标准，满足最小化聚类中心和属于该中心的数据点之间的平方距离和（Sum of Square Distance，SSD）。</p>\n<script type=\"math/tex; mode=display\">\\text{SSD} = \\sum_{i=1}^{k}\\sum_{x\\in c_i}(x-c_i)^2</script><p><img src=\"/img/kmeans_demo.png\" alt=\"K-Means Demo\"></p>","more":"<h2 id=\"目标函数\"><a href=\"#目标函数\" class=\"headerlink\" title=\"目标函数\"></a>目标函数</h2><p>K-Means方法实际上需要确定两个参数，$c^\\ast$和$\\delta^\\ast$。其中$c_{i}^\\ast$代表各个聚类中心的位置，$\\delta_{ij}^\\ast$的取值为$\\lbrace 0,1\\rbrace$，代表点$x_j$是否被分到了第$i$个聚类中心。</p>\n<p>那么，目标函数可以写成如下的形式。</p>\n<script type=\"math/tex; mode=display\">c^\\ast, \\delta^\\ast = \\arg\\min_{c,\\delta} \\frac{1}{N}\\sum_{j=1}^{N}\\sum_{i=1}^{k}\\delta_{i,j}(c_i-x_j)^2</script><p>然而这样一来，造成了一种很尴尬的局面。一方面，要想优化$c_i$，需要我们给定每个点所属的类；另一方面，优化$\\delta_{i,j}$，需要我们给定聚类中心。由于这两个优化变量之间是纠缠在一起的，K-Means算法如何能够达到收敛就变成了一个讨论“先有鸡还是先有蛋”的问题了。</p>\n<p>实际使用中，K-Means的迭代过程，实际是EM算法的一个特例。</p>\n<h2 id=\"算法流程\"><a href=\"#算法流程\" class=\"headerlink\" title=\"算法流程\"></a>算法流程</h2><p>K-Means算法的流程如下所示。<br><img src=\"/img/kmeans_algorithm.png\" alt=\"K-Means算法流程\"></p>\n<p>假设我们有$N$个样本点，$\\lbrace x_1, \\dots, x_N\\rbrace, x_i\\in\\mathbb{R}^D$，并给出聚类数目$k$。</p>\n<p>首先，随机选取一系列的聚类中心点$\\mu_i, i = 1,\\dots, k$。接着，我们按照距离最近的原则为每个数据点指定相应的聚类中心并计算新的数据点均值更新聚类中心。如此这般，直到收敛。</p>\n<h2 id=\"算法细节\"><a href=\"#算法细节\" class=\"headerlink\" title=\"算法细节\"></a>算法细节</h2><h3 id=\"初始化\"><a href=\"#初始化\" class=\"headerlink\" title=\"初始化\"></a>初始化</h3><p>上面的K-Means方法对初始值很敏感。朴素的初始化方法是直接在数据点中随机抽取$k$个作为聚类初始中心点。不够好的初始值可能造成收敛速度很慢或者聚类失败。下面介绍两种改进方法。</p>\n<ul>\n<li><p>kmeans++方法，尽可能地使初始聚类中心分散开（spread-out）。<br>这种方法首先随机产生一个初始聚类中心点，然后按照概率$P = \\omega(x-c_i)^2$选取其他的聚类中心点。其中$\\omega$是归一化系数。</p>\n</li>\n<li><p>多次初始化，保留最好的结果。</p>\n</li>\n</ul>\n<h3 id=\"K值的选取\"><a href=\"#K值的选取\" class=\"headerlink\" title=\"K值的选取\"></a>K值的选取</h3><p>在上面的讨论中，我们一直把$k$当做已经给定的参数。但是在实际操作中，聚类类别数通常都是未知的。如何确定超参数$k$呢？</p>\n<p>我们可以使用不同的$k$值进行试验，并作出不同试验收敛后目标函数随$k$的变化。如下图所示，分别使用$1$到$4$之间的几个数字作为$k$值，曲线在$k=2$处有一个较为明显的弯曲点（elbow point）。故确定$k$取值为2。<br><img src=\"/img/kmeans_object_fun_vs_k.png\" alt=\"参数K的确定\"></p>\n<h3 id=\"距离的度量\"><a href=\"#距离的度量\" class=\"headerlink\" title=\"距离的度量\"></a>距离的度量</h3><p>目标函数是各个cluster中的点与其中心点的距离均值。其中就涉及到了“距离”的量度。下面是几种较为常用的距离度量方法。</p>\n<ul>\n<li>欧几里得距离（最为常用）</li>\n<li>余弦距离（向量的夹角）</li>\n<li>核函数（<a href=\"http://www.public.asu.edu/~jye02/CLASSES/Fall-2005/PAPERS/kdd_spectral_kernelkmeans.pdf\">Kernel K-Means</a>）</li>\n</ul>\n<h3 id=\"迭代终止条件\"><a href=\"#迭代终止条件\" class=\"headerlink\" title=\"迭代终止条件\"></a>迭代终止条件</h3><p>当判断算法已经收敛时，退出迭代。常用的迭代终止条件如下：</p>\n<ul>\n<li>达到了预先给定的最大迭代次数</li>\n<li>在将数据点assign到不同聚类中心时，和上轮结果没有变化（说明已经收敛）</li>\n<li>目标函数（平均的距离）下降小于阈值</li>\n</ul>\n<h2 id=\"基于K-Means的图像分割\"><a href=\"#基于K-Means的图像分割\" class=\"headerlink\" title=\"基于K-Means的图像分割\"></a>基于K-Means的图像分割</h2><p>图像分割中可以使用K-Means算法。我们首先将一副图像转换为特征空间（Feature Space）。例如使用灰度或者RGB的值作为特征向量，就得到了1D或者3D的特征空间。之后，可以基于Feature Space上各点的相似度，将它们聚类。这样，就完成了对原始图像的分割操作。如下所示。<br><img src=\"/img/kmeans_image_seg_via_intensity.png\" alt=\"图像分割结果1\"></p>\n<p>然而，上述方法有一个缺点。那就是在真实图像中，属于同一个物体的像素点常常在空间上是相关的（Spatial Coherent）。而上述方法没有考虑到这一点。我们可以根据颜色亮度和空间上的距离构建Feature Space，获得更加合理的分割效果。</p>\n<p>在2012年PAMI上有一篇文章<a href=\"https://infoscience.epfl.ch/record/177415/files/Superpixel_PAMI2011-2.pdf\">SLIC Superpixels Compared to State-of-the-art Superpixel Methods</a>介绍了使用K-Means进行超像素分割的相关内容，可以参考。这里不再赘述了。</p>\n<h2 id=\"优点和不足\"><a href=\"#优点和不足\" class=\"headerlink\" title=\"优点和不足\"></a>优点和不足</h2><p>作为非监督学习中的经典算法，K-Means的一大优点便是实现简单，速度较快，最小化条件方差（conditional variance，good represention of data，这里没有具体解释，不太懂）。</p>\n<p>它的缺点主要有：</p>\n<ul>\n<li>对outlier比较敏感。如下图所示。由于outlier的存在，右边的cluster硬是被拉长了。这方面，K-Medians更有鲁棒性。<br><img src=\"/img/kmeans_sensitive_to_outlier.png\" alt=\"outlier是个大麻烦\"></li>\n<li>每个点只能确定地属于或不属于某一个cluster。可以参考高斯混合模型和Fuzzy K-Means的soft assignment。</li>\n<li>在round shape情况下性能比较好（想想圆形和方形各点的欧几里得距离）。而且每个cluster最好数据点数目和分布的密度相近。</li>\n<li>如果数据点有非凸形状分布，算法表现糟糕。可以参考谱聚类（Spectral Clustering）或者kernel K-Means。</li>\n</ul>\n<p>针对K-Means，也有不少相关改进工作，参考下面这幅图吧。<br><img src=\"/img/kmeans_scaling_up.png\" alt=\"K-Means Scaling Up\"></p>\n<h2 id=\"MATLAB实验\"><a href=\"#MATLAB实验\" class=\"headerlink\" title=\"MATLAB实验\"></a>MATLAB实验</h2><p>下面是我自己写的简单的K-Means demo。首先，在数据产生环节，确定$K = 3$个cluster的中心位置，并随机产生$N$个数据点，并使用<code>scatter</code>函数做出散点图。</p>\n<p>代码中的主要部分为<code>my_kmeans</code>函数的实现（为了不与内建的kmeans函数重名，故加上了<code>my</code>前缀）。在此函数内部，首先随机产生$K$个聚类中心，并对数据点进行初始的指定。接着，在迭代过程中，不断计算均值来更新聚类中心和assignment，当达到最大迭代次数或者相邻两次迭代中assignment的值不再变化为止，并计算对应的目标函数$J$的值。</p>\n<p>注意到该函数初始确定聚类中心时有一段注释的代码，该段代码用于debug时，指定聚类中心，去除随机性，以验证后续K-Means迭代的正确性。</p>\n<figure class=\"highlight matlab\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div><div class=\"line\">47</div><div class=\"line\">48</div><div class=\"line\">49</div><div class=\"line\">50</div><div class=\"line\">51</div><div class=\"line\">52</div><div class=\"line\">53</div><div class=\"line\">54</div><div class=\"line\">55</div><div class=\"line\">56</div><div class=\"line\">57</div><div class=\"line\">58</div><div class=\"line\">59</div><div class=\"line\">60</div><div class=\"line\">61</div><div class=\"line\">62</div><div class=\"line\">63</div><div class=\"line\">64</div><div class=\"line\">65</div><div class=\"line\">66</div><div class=\"line\">67</div><div class=\"line\">68</div><div class=\"line\">69</div><div class=\"line\">70</div><div class=\"line\">71</div><div class=\"line\">72</div><div class=\"line\">73</div><div class=\"line\">74</div><div class=\"line\">75</div><div class=\"line\">76</div><div class=\"line\">77</div><div class=\"line\">78</div><div class=\"line\">79</div><div class=\"line\">80</div><div class=\"line\">81</div><div class=\"line\">82</div><div class=\"line\">83</div><div class=\"line\">84</div><div class=\"line\">85</div><div class=\"line\">86</div><div class=\"line\">87</div><div class=\"line\">88</div><div class=\"line\">89</div><div class=\"line\">90</div><div class=\"line\">91</div><div class=\"line\">92</div><div class=\"line\">93</div><div class=\"line\">94</div><div class=\"line\">95</div><div class=\"line\">96</div><div class=\"line\">97</div><div class=\"line\">98</div><div class=\"line\">99</div><div class=\"line\">100</div><div class=\"line\">101</div><div class=\"line\">102</div><div class=\"line\">103</div><div class=\"line\">104</div><div class=\"line\">105</div><div class=\"line\">106</div><div class=\"line\">107</div><div class=\"line\">108</div><div class=\"line\">109</div><div class=\"line\">110</div><div class=\"line\">111</div><div class=\"line\">112</div><div class=\"line\">113</div><div class=\"line\">114</div><div class=\"line\">115</div><div class=\"line\">116</div><div class=\"line\">117</div><div class=\"line\">118</div><div class=\"line\">119</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\">%% generate data</span></div><div class=\"line\">K = <span class=\"number\">3</span>;   <span class=\"comment\">% number of clusters</span></div><div class=\"line\">pos = [<span class=\"number\">-5</span>, <span class=\"number\">5</span>; <span class=\"number\">0</span>, <span class=\"number\">1</span>; <span class=\"number\">3</span>, <span class=\"number\">6</span>];  <span class=\"comment\">% position of cluster centers</span></div><div class=\"line\">N = <span class=\"number\">20</span>;    <span class=\"comment\">% number of data points</span></div><div class=\"line\">R = <span class=\"number\">3</span>;     <span class=\"comment\">% radius of clusters</span></div><div class=\"line\">data = <span class=\"built_in\">zeros</span>(N, <span class=\"number\">2</span>);    <span class=\"comment\">% data</span></div><div class=\"line\">class = <span class=\"built_in\">zeros</span>(N, <span class=\"number\">1</span>);   <span class=\"comment\">% index of cluster</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"keyword\">for</span> <span class=\"built_in\">i</span> = <span class=\"number\">1</span>:N</div><div class=\"line\">    idx = randi(<span class=\"number\">3</span>, <span class=\"number\">1</span>);</div><div class=\"line\">    dr = R*<span class=\"built_in\">rand</span>();</div><div class=\"line\">    data(<span class=\"built_in\">i</span>, :) = pos(idx, :) + [dr*cos(rand()*<span class=\"number\">2</span>*pi), dr*sin(rand()*<span class=\"number\">2</span>*pi)];</div><div class=\"line\">    class(<span class=\"built_in\">i</span>) = idx;</div><div class=\"line\"><span class=\"keyword\">end</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">%% visualization data points</span></div><div class=\"line\">figure</div><div class=\"line\">hold on</div><div class=\"line\">color = [<span class=\"number\">1</span>,<span class=\"number\">0</span>,<span class=\"number\">0</span>; <span class=\"number\">0</span>,<span class=\"number\">1</span>,<span class=\"number\">0</span>; <span class=\"number\">0</span>,<span class=\"number\">0</span>,<span class=\"number\">1</span>];</div><div class=\"line\"><span class=\"keyword\">for</span> <span class=\"built_in\">i</span> = <span class=\"number\">1</span>:K</div><div class=\"line\">    x = data(class == <span class=\"built_in\">i</span>, <span class=\"number\">1</span>);</div><div class=\"line\">    y = data(class == <span class=\"built_in\">i</span>, <span class=\"number\">2</span>);</div><div class=\"line\">    scatter(x, y, <span class=\"number\">150</span>, <span class=\"built_in\">repmat</span>(color(<span class=\"built_in\">i</span>,:), [length(x), <span class=\"number\">1</span>]), <span class=\"string\">'filled'</span>);</div><div class=\"line\"><span class=\"keyword\">end</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">%% K-Means</span></div><div class=\"line\">best_J = <span class=\"number\">1E100</span>;</div><div class=\"line\">best_idx = <span class=\"number\">0</span>;</div><div class=\"line\"><span class=\"keyword\">for</span> times = <span class=\"number\">1</span>:<span class=\"number\">5</span>  <span class=\"comment\">% 5 times experiments to choose the best result</span></div><div class=\"line\">    [mu, assignment, J] = my_kmeans(data, K);</div><div class=\"line\">    <span class=\"keyword\">if</span> best_J &gt; J</div><div class=\"line\">        best_idx = times;</div><div class=\"line\">        best_J = J;</div><div class=\"line\">    <span class=\"keyword\">end</span></div><div class=\"line\">    fprintf(<span class=\"string\">'%d experiment: J = %f\\n'</span>, times, J);</div><div class=\"line\">    <span class=\"built_in\">disp</span>(mu);</div><div class=\"line\"><span class=\"keyword\">end</span></div><div class=\"line\">fprintf(<span class=\"string\">'best: %d experiment: J = %f\\n'</span>, best_idx, best_J);</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">%% basic functions</span></div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">function</span> <span class=\"title\">J</span> = <span class=\"title\">ssd</span><span class=\"params\">(X, mu, assignment)</span></span></div><div class=\"line\"><span class=\"comment\">% sum of square distance</span></div><div class=\"line\"><span class=\"comment\">% X -- data, N*D matrix</span></div><div class=\"line\"><span class=\"comment\">% mu -- centers of clusters, K*D matrix</span></div><div class=\"line\"><span class=\"comment\">% assignment -- current assignment of data to clusters</span></div><div class=\"line\">J = <span class=\"number\">0</span>;</div><div class=\"line\">K = <span class=\"built_in\">size</span>(mu, <span class=\"number\">1</span>);</div><div class=\"line\"><span class=\"keyword\">for</span> k = <span class=\"number\">1</span>:K</div><div class=\"line\">    x_k = X(assignment == k, :);</div><div class=\"line\">    mu_k = mu(k, :);</div><div class=\"line\">    err2 = <span class=\"built_in\">bsxfun</span>(@minus, x_k, mu_k).^<span class=\"number\">2</span>;</div><div class=\"line\">    J = J + sum(err2(:));</div><div class=\"line\"><span class=\"keyword\">end</span></div><div class=\"line\">J = J / <span class=\"built_in\">size</span>(X, <span class=\"number\">1</span>);</div><div class=\"line\"><span class=\"keyword\">end</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">function</span> <span class=\"title\">mu</span> = <span class=\"title\">compute_mu</span><span class=\"params\">(X, assignment, K)</span></span></div><div class=\"line\">mu = <span class=\"built_in\">zeros</span>(K, <span class=\"built_in\">size</span>(X, <span class=\"number\">2</span>));</div><div class=\"line\"><span class=\"keyword\">for</span> k = <span class=\"number\">1</span>:K</div><div class=\"line\">    x_k = X(assignment == k, :);</div><div class=\"line\">    mu(k, :) = mean(x_k, <span class=\"number\">1</span>);</div><div class=\"line\"><span class=\"keyword\">end</span></div><div class=\"line\"><span class=\"keyword\">end</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">function</span> <span class=\"title\">assignment</span> = <span class=\"title\">assign</span><span class=\"params\">(X, mu)</span></span></div><div class=\"line\"><span class=\"comment\">% assign data points to clusters</span></div><div class=\"line\">N = <span class=\"built_in\">size</span>(X, <span class=\"number\">1</span>);</div><div class=\"line\">assignment = <span class=\"built_in\">zeros</span>(N, <span class=\"number\">1</span>);</div><div class=\"line\"><span class=\"keyword\">for</span> <span class=\"built_in\">i</span> = <span class=\"number\">1</span>:N</div><div class=\"line\">    x = X(<span class=\"built_in\">i</span>, :);</div><div class=\"line\">    err2 = <span class=\"built_in\">bsxfun</span>(@minus, x, mu).^<span class=\"number\">2</span>;</div><div class=\"line\">    dis = sum(err2, <span class=\"number\">2</span>);</div><div class=\"line\">    [~, idx] = min(dis);</div><div class=\"line\">    assignment(<span class=\"built_in\">i</span>) = idx;</div><div class=\"line\"><span class=\"keyword\">end</span></div><div class=\"line\"><span class=\"keyword\">end</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">function</span> <span class=\"params\">[mu, assignment, J]</span> = <span class=\"title\">my_kmeans</span><span class=\"params\">(X, K)</span></span></div><div class=\"line\">N = <span class=\"built_in\">size</span>(X, <span class=\"number\">1</span>);</div><div class=\"line\">assignment = <span class=\"built_in\">zeros</span>(N, <span class=\"number\">1</span>);</div><div class=\"line\">idx = randsample(N, K);</div><div class=\"line\">mu = X(idx, :);</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">% for i = 1:K</span></div><div class=\"line\"><span class=\"comment\">%     for j = 1:N</span></div><div class=\"line\"><span class=\"comment\">%         if assignment_gt(j) == i</span></div><div class=\"line\"><span class=\"comment\">%             mu(i,:) = X(j,:);</span></div><div class=\"line\"><span class=\"comment\">%             break;</span></div><div class=\"line\"><span class=\"comment\">%         end</span></div><div class=\"line\"><span class=\"comment\">%     end</span></div><div class=\"line\"><span class=\"comment\">% end</span></div><div class=\"line\">figure</div><div class=\"line\">hold on</div><div class=\"line\">color = [<span class=\"number\">1</span>,<span class=\"number\">0</span>,<span class=\"number\">0</span>; <span class=\"number\">0</span>,<span class=\"number\">1</span>,<span class=\"number\">0</span>; <span class=\"number\">0</span>,<span class=\"number\">0</span>,<span class=\"number\">1</span>];</div><div class=\"line\">scatter(mu(:,<span class=\"number\">1</span>), mu(:,<span class=\"number\">2</span>), <span class=\"number\">200</span>, color, <span class=\"string\">'d'</span>);</div><div class=\"line\"><span class=\"keyword\">for</span> iter = <span class=\"number\">1</span>:<span class=\"number\">20</span></div><div class=\"line\">    assignment_prev = assignment;</div><div class=\"line\">    assignment = assign(X, mu);</div><div class=\"line\">    <span class=\"keyword\">if</span> assignment == assignment_prev</div><div class=\"line\">        <span class=\"keyword\">break</span>;</div><div class=\"line\">    <span class=\"keyword\">end</span></div><div class=\"line\">    mu_prev = mu;</div><div class=\"line\">    mu = compute_mu(X, assignment, K);</div><div class=\"line\">    scatter(mu(:, <span class=\"number\">1</span>), mu(:, <span class=\"number\">2</span>), <span class=\"number\">200</span>, color, <span class=\"string\">'d'</span>);</div><div class=\"line\">    MU = <span class=\"built_in\">zeros</span>(<span class=\"number\">2</span>*K, <span class=\"number\">2</span>);</div><div class=\"line\">    MU(<span class=\"number\">1</span>:<span class=\"number\">2</span>:<span class=\"keyword\">end</span>, :) = mu_prev;</div><div class=\"line\">    MU(<span class=\"number\">2</span>:<span class=\"number\">2</span>:<span class=\"keyword\">end</span>, :) = mu;</div><div class=\"line\">    mu_x = <span class=\"built_in\">reshape</span>(MU(:, <span class=\"number\">1</span>), [], K);</div><div class=\"line\">    mu_y = <span class=\"built_in\">reshape</span>(MU(:, <span class=\"number\">2</span>), [], K);</div><div class=\"line\">    plot(mu_x, mu_y, <span class=\"string\">'k-.'</span>);</div><div class=\"line\"></div><div class=\"line\"><span class=\"keyword\">end</span></div><div class=\"line\"><span class=\"keyword\">for</span> <span class=\"built_in\">i</span> = <span class=\"number\">1</span>:K</div><div class=\"line\">    x = X(assignment == <span class=\"built_in\">i</span>, <span class=\"number\">1</span>);</div><div class=\"line\">    y = X(assignment == <span class=\"built_in\">i</span>, <span class=\"number\">2</span>);</div><div class=\"line\">    scatter(x, y, <span class=\"number\">150</span>, <span class=\"built_in\">repmat</span>(color(<span class=\"built_in\">i</span>,:), [length(x), <span class=\"number\">1</span>]), <span class=\"string\">'filled'</span>);</div><div class=\"line\"><span class=\"keyword\">end</span></div><div class=\"line\">J = ssd(X, mu, assignment);</div><div class=\"line\"><span class=\"keyword\">end</span></div></pre></td></tr></table></figure>\n<p>在demo中，随机选取初始化的聚类中心，重复进行$5$次实验。下图是产生的原始数据的散点图。用不同的颜色标明了cluster。<br><img src=\"/img/kmeans_data_demo.png\" alt=\"K-Means聚类\"></p>\n<p>下图是某次的实验结果，其中菱形和它们之间的连线指示了聚类中心的变化。注意，颜色只是用来区别不同的cluster，和上图并没有对应关系。可以看到，初始化时绿色标注的cluster的中心（也即是上图中的红色cluster）虽然偏离了很远，但是在迭代过程中也能实现纠正。</p>\n<p><img src=\"/img/kmeans_success.png\" alt=\"K-Means聚类\"></p>\n<p>再换个大点的数据集来做，效果貌似还不错~<br><img src=\"/img/kmeans_bigger_demo.png\" alt=\"大一些\"></p>\n<h2 id=\"PS\"><a href=\"#PS\" class=\"headerlink\" title=\"PS\"></a>PS</h2><p>这里插一句与本文内容完全无关的一个tip：在Markdown中使用LaTex时，如果在内联形式（也就是行内使用两个美元符号插入公式）时，如果有多个下划线，那么Markdown有时会将下划线之间的部分认为是自己的斜体标记，如下所示：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">$c_&#123;i&#125;^\\ast$ XXX $\\delta_&#123;ij&#125;^\\ast$</div></pre></td></tr></table></figure></p>\n<p>它的显示效果为$c<em>{i}^\\ast$ XXX $\\delta</em>{ij}^\\ast$。</p>\n<p>这时候，只需在下划线前加入反斜线进行转义即可（虽然略显麻烦）。如下所示：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">$c\\_&#123;i&#125;^\\ast$ XXX $\\delta\\_&#123;ij&#125;^\\ast$</div></pre></td></tr></table></figure></p>\n<p>它的显示效果为$c_{i}^\\ast$ XXX $\\delta_{ij}^\\ast$。</p>\n<p>具体分析可以参见<a href=\"http://lukang.me/2014/mathjax-for-hexo.html\">博客</a>。</p>"},{"title":"CS131-MeanShift","date":"2017-02-12T14:28:15.000Z","_content":"[MeanShift](https://en.wikipedia.org/wiki/Mean_shift)最初由Fukunaga和Hostetler在1975年提出，但是一直到2000左右这篇PAMI的论文[Mean Shift: A Robust Approach Toward Feature Space Analysis](http://courses.csail.mit.edu/6.869/handouts/PAMIMeanshift.pdf)，将它的原理和收敛性等重新整理阐述，并应用于计算机视觉和图像处理领域之后，才逐渐为人熟知。\n\nMeanShift是一种用来寻找特征空间内[模态](https://en.wikipedia.org/wiki/Mode_(statistics)的方法。所谓模态（Mode），就是指数据集中最经常出现的数据。例如，连续随机变量概率密度函数的模态就是指函数的极大值。从概率的角度看，我们可以认为数据集（或者特征空间）内的数据点都是从某个概率分布中随机抽取出来的。这样，数据点越密集的地方就说明这里越有可能是密度函数的极大值。MeanShift就是一种能够从离散的抽样点中估计密度函数局部极大值的方法。\n![MeanShift](/img/meanshift_basics.jpg)\n\n<!-- more -->\n## 核密度估计\n上面提到的PAMI论文篇幅较长，且数学名词较多，我不是很理解。下面的说明过程主要参考了[这篇博客](https://saravananthirumuruganathan.wordpress.com/2010/04/01/introduction-to-mean-shift-algorithm/)和[这篇讲义](https://saravananthirumuruganathan.wordpress.com/2010/04/01/introduction-to-mean-shift-algorithm/)。\n\n注意下面的推导中$x$是一个$d$维的向量。为了书写简单，没有写成向量的加粗形式。首先，先介绍核函数（Kernel Function）的概念。如果某个$\\mathbb{R}^n\\rightarrow \\mathbb{R}$的函数满足以下条件，就能将其作为核函数。\n![kernel](/img/meanshift_kernel_function.png)\n\n比如高斯核函数：\n$$K(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp(-\\frac{x^2}{2\\sigma^2})$$\n\n核密度估计是一种用来估计随机变量密度函数的非参数化方法。给定核函数$K$，带宽（bandwidth）参数$h$（指观察窗口的大小）。那么密度函数可以使用核函数进行估计，如下面的形式所示。其中，$n$是窗口内的数据点的数量。\n$$f(x) = \\frac{1}{nh^d}\\sum_{i=1}^{n}K(\\frac{x-x_i}{h})$$\n\n如果核函数是放射形状且对称的（例如高斯核函数），那么$K(x)$可以写成如下的形式，其中$c_{k,d}$是为了使得核函数满足积分为$1$的正则系数。\n$$K(x) = c_{k,d}k(\\Arrowvert x\\Arrowvert ^2)$$\n\n## mean shift向量\n那么，原来密度函数的极值点就是使得$f(x)$梯度为$0$的点。求取$f(x)$的梯度，可以得到下式。其中$g(s) = -k^\\prime(s)$。\n![密度函数的梯度](/img/meanshift_gradient_of_density.png)\n\n观察后可以发现，乘积第一项连带前面的系数，相当于是使用核函数$G(x) = c_{k,d}g(\\Arrowvert x\\Arrowvert^2)$得到的点$x$处的密度函数估计值（差了常数倍因子）（所以对于给定的某一点$x = x_0$，这一项是定值），后面一项拿出来，定义为$m_h(x)$，称为mean shift向量。\n$$m_h(x) = \\frac{\\sum_{i=1}^{n}x_i g(\\Arrowvert \\frac{x-x_i}{h} \\Arrowvert^2)}{\\sum_{i=1}^{n}g(\\Arrowvert \\frac{x-x_i}{h} \\Arrowvert^2)}-x$$\n\n所以说，$m_h(x)$的指向和$f(x)$的梯度是相同的。另外，从感性的角度出发。$m_h(x)$中的第一项（分式的那一大坨），相当于是在计算$x$周围窗口大小为$h$的这个领域的均值（$g(s)$是加权的系数）。所以$m_h(x)$实际上指示了局部均值和当前点$x$之间的位移。我们想要找到密度函数的局部极大值，不就应该让局部均值向着点密集的方向移动吗？\n\n## 算法流程\n所以，在给定初始位置$x_0$后，我们首先计算此点处的$m_h$，之后，将$x$沿着$m_h$移动即可。下面是一个简单的例子。数据点服从二维高斯分布，均值为$[1, 2]$。其中，红色菱形指示了迭代过程中mean shift的移动。\n\n``` matlab\n%% generate data\nmu = [1 2];\nSigma = [1 0; 0 2]; R = chol(Sigma);\nN = 250;\ndata = repmat(mu, N, 1) + randn(N, 2)*R;\nfigure\nhold on\nscatter(data(:, 1), data(:, 2), 50, 'filled');\n%% meanshift\nmu0 = rand(1,2) * 5;\nmu = mean_shift(mu0, 10, data);\n\nfunction out = gaussian_kernel(x, sigma)\n% gauss kernel, g(x) = \\exp(-x^2/2\\sigma^2)\nout = exp(-x.*x/(2*sigma*sigma));\nend\n\nfunction mu = mean_shift(mu0, h, data)\n% implementation of meanshift algorithm\n% mu_{k+1} = meanshift(mu_{k}) + mu_{k} = \\frac{\\sum_i=1^n xg}{\\sum_i=1^n g}\nmu = mu0;\nsigma = 1;    % parameter for gaussian kernel function\nfor iter = 1:20    \n    fprintf('iter = %d, mu = [%f, %f]\\n', iter, mu(1), mu(2));\n    scatter(mu(1), mu(2), 50, [1,0,0], 'd', 'filled');\n    offset = bsxfun(@minus, mu, data);    % offset = x-x_i\n    dis = sum(offset.^2, 2);              % dis = ||x-x_i||^2\n    x = data(dis < h, :);                 % neighborhood with bandwidth = h\n    g = gaussian_kernel(offset(dis < h), sigma);\n    xg = x.*g;\n    mu_prev = mu;\n    mu = sum(xg, 1) / sum(g, 1);\n    if norm(mu_prev - mu, 2) < 1E-2\n        break;\n    end\n    plot([mu_prev(1) mu(1)], [mu_prev(2), mu(2)], 'b-.', 'linewidth', 2);\nend\nscatter(mu(1), mu(2), 50, [1,0,0], 'd', 'filled');\nend\n```\n![](/img/meanshift_simple_demo.png)\n\n同时，我也试验了其他的kernel函数，如神似logistaic形式，效果也是相似的。\n$$K(x) = \\frac{1}{e^x+e^{-x}+2}$$\n\n``` matlab\nfunction out = logistic_kernel(x)\nout = 1./(exp(x) + exp(-x) + 2);\nend\n```\n","source":"_posts/cs131-mean-shift.md","raw":"---\ntitle: CS131-MeanShift\ndate: 2017-02-12 22:28:15\ntags:\n    - cs131\n    - 公开课\n---\n[MeanShift](https://en.wikipedia.org/wiki/Mean_shift)最初由Fukunaga和Hostetler在1975年提出，但是一直到2000左右这篇PAMI的论文[Mean Shift: A Robust Approach Toward Feature Space Analysis](http://courses.csail.mit.edu/6.869/handouts/PAMIMeanshift.pdf)，将它的原理和收敛性等重新整理阐述，并应用于计算机视觉和图像处理领域之后，才逐渐为人熟知。\n\nMeanShift是一种用来寻找特征空间内[模态](https://en.wikipedia.org/wiki/Mode_(statistics)的方法。所谓模态（Mode），就是指数据集中最经常出现的数据。例如，连续随机变量概率密度函数的模态就是指函数的极大值。从概率的角度看，我们可以认为数据集（或者特征空间）内的数据点都是从某个概率分布中随机抽取出来的。这样，数据点越密集的地方就说明这里越有可能是密度函数的极大值。MeanShift就是一种能够从离散的抽样点中估计密度函数局部极大值的方法。\n![MeanShift](/img/meanshift_basics.jpg)\n\n<!-- more -->\n## 核密度估计\n上面提到的PAMI论文篇幅较长，且数学名词较多，我不是很理解。下面的说明过程主要参考了[这篇博客](https://saravananthirumuruganathan.wordpress.com/2010/04/01/introduction-to-mean-shift-algorithm/)和[这篇讲义](https://saravananthirumuruganathan.wordpress.com/2010/04/01/introduction-to-mean-shift-algorithm/)。\n\n注意下面的推导中$x$是一个$d$维的向量。为了书写简单，没有写成向量的加粗形式。首先，先介绍核函数（Kernel Function）的概念。如果某个$\\mathbb{R}^n\\rightarrow \\mathbb{R}$的函数满足以下条件，就能将其作为核函数。\n![kernel](/img/meanshift_kernel_function.png)\n\n比如高斯核函数：\n$$K(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp(-\\frac{x^2}{2\\sigma^2})$$\n\n核密度估计是一种用来估计随机变量密度函数的非参数化方法。给定核函数$K$，带宽（bandwidth）参数$h$（指观察窗口的大小）。那么密度函数可以使用核函数进行估计，如下面的形式所示。其中，$n$是窗口内的数据点的数量。\n$$f(x) = \\frac{1}{nh^d}\\sum_{i=1}^{n}K(\\frac{x-x_i}{h})$$\n\n如果核函数是放射形状且对称的（例如高斯核函数），那么$K(x)$可以写成如下的形式，其中$c_{k,d}$是为了使得核函数满足积分为$1$的正则系数。\n$$K(x) = c_{k,d}k(\\Arrowvert x\\Arrowvert ^2)$$\n\n## mean shift向量\n那么，原来密度函数的极值点就是使得$f(x)$梯度为$0$的点。求取$f(x)$的梯度，可以得到下式。其中$g(s) = -k^\\prime(s)$。\n![密度函数的梯度](/img/meanshift_gradient_of_density.png)\n\n观察后可以发现，乘积第一项连带前面的系数，相当于是使用核函数$G(x) = c_{k,d}g(\\Arrowvert x\\Arrowvert^2)$得到的点$x$处的密度函数估计值（差了常数倍因子）（所以对于给定的某一点$x = x_0$，这一项是定值），后面一项拿出来，定义为$m_h(x)$，称为mean shift向量。\n$$m_h(x) = \\frac{\\sum_{i=1}^{n}x_i g(\\Arrowvert \\frac{x-x_i}{h} \\Arrowvert^2)}{\\sum_{i=1}^{n}g(\\Arrowvert \\frac{x-x_i}{h} \\Arrowvert^2)}-x$$\n\n所以说，$m_h(x)$的指向和$f(x)$的梯度是相同的。另外，从感性的角度出发。$m_h(x)$中的第一项（分式的那一大坨），相当于是在计算$x$周围窗口大小为$h$的这个领域的均值（$g(s)$是加权的系数）。所以$m_h(x)$实际上指示了局部均值和当前点$x$之间的位移。我们想要找到密度函数的局部极大值，不就应该让局部均值向着点密集的方向移动吗？\n\n## 算法流程\n所以，在给定初始位置$x_0$后，我们首先计算此点处的$m_h$，之后，将$x$沿着$m_h$移动即可。下面是一个简单的例子。数据点服从二维高斯分布，均值为$[1, 2]$。其中，红色菱形指示了迭代过程中mean shift的移动。\n\n``` matlab\n%% generate data\nmu = [1 2];\nSigma = [1 0; 0 2]; R = chol(Sigma);\nN = 250;\ndata = repmat(mu, N, 1) + randn(N, 2)*R;\nfigure\nhold on\nscatter(data(:, 1), data(:, 2), 50, 'filled');\n%% meanshift\nmu0 = rand(1,2) * 5;\nmu = mean_shift(mu0, 10, data);\n\nfunction out = gaussian_kernel(x, sigma)\n% gauss kernel, g(x) = \\exp(-x^2/2\\sigma^2)\nout = exp(-x.*x/(2*sigma*sigma));\nend\n\nfunction mu = mean_shift(mu0, h, data)\n% implementation of meanshift algorithm\n% mu_{k+1} = meanshift(mu_{k}) + mu_{k} = \\frac{\\sum_i=1^n xg}{\\sum_i=1^n g}\nmu = mu0;\nsigma = 1;    % parameter for gaussian kernel function\nfor iter = 1:20    \n    fprintf('iter = %d, mu = [%f, %f]\\n', iter, mu(1), mu(2));\n    scatter(mu(1), mu(2), 50, [1,0,0], 'd', 'filled');\n    offset = bsxfun(@minus, mu, data);    % offset = x-x_i\n    dis = sum(offset.^2, 2);              % dis = ||x-x_i||^2\n    x = data(dis < h, :);                 % neighborhood with bandwidth = h\n    g = gaussian_kernel(offset(dis < h), sigma);\n    xg = x.*g;\n    mu_prev = mu;\n    mu = sum(xg, 1) / sum(g, 1);\n    if norm(mu_prev - mu, 2) < 1E-2\n        break;\n    end\n    plot([mu_prev(1) mu(1)], [mu_prev(2), mu(2)], 'b-.', 'linewidth', 2);\nend\nscatter(mu(1), mu(2), 50, [1,0,0], 'd', 'filled');\nend\n```\n![](/img/meanshift_simple_demo.png)\n\n同时，我也试验了其他的kernel函数，如神似logistaic形式，效果也是相似的。\n$$K(x) = \\frac{1}{e^x+e^{-x}+2}$$\n\n``` matlab\nfunction out = logistic_kernel(x)\nout = 1./(exp(x) + exp(-x) + 2);\nend\n```\n","slug":"cs131-mean-shift","published":1,"updated":"2018-01-12T06:22:20.461Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjcu6vcs0000equ46zflv0x6r","content":"<p><a href=\"https://en.wikipedia.org/wiki/Mean_shift\" target=\"_blank\" rel=\"external\">MeanShift</a>最初由Fukunaga和Hostetler在1975年提出，但是一直到2000左右这篇PAMI的论文<a href=\"http://courses.csail.mit.edu/6.869/handouts/PAMIMeanshift.pdf\" target=\"_blank\" rel=\"external\">Mean Shift: A Robust Approach Toward Feature Space Analysis</a>，将它的原理和收敛性等重新整理阐述，并应用于计算机视觉和图像处理领域之后，才逐渐为人熟知。</p>\n<p>MeanShift是一种用来寻找特征空间内<a href=\"https://en.wikipedia.org/wiki/Mode_(statistics\" target=\"_blank\" rel=\"external\">模态</a>的方法。所谓模态（Mode），就是指数据集中最经常出现的数据。例如，连续随机变量概率密度函数的模态就是指函数的极大值。从概率的角度看，我们可以认为数据集（或者特征空间）内的数据点都是从某个概率分布中随机抽取出来的。这样，数据点越密集的地方就说明这里越有可能是密度函数的极大值。MeanShift就是一种能够从离散的抽样点中估计密度函数局部极大值的方法。<br><img src=\"/img/meanshift_basics.jpg\" alt=\"MeanShift\"></p>\n<a id=\"more\"></a>\n<h2 id=\"核密度估计\"><a href=\"#核密度估计\" class=\"headerlink\" title=\"核密度估计\"></a>核密度估计</h2><p>上面提到的PAMI论文篇幅较长，且数学名词较多，我不是很理解。下面的说明过程主要参考了<a href=\"https://saravananthirumuruganathan.wordpress.com/2010/04/01/introduction-to-mean-shift-algorithm/\" target=\"_blank\" rel=\"external\">这篇博客</a>和<a href=\"https://saravananthirumuruganathan.wordpress.com/2010/04/01/introduction-to-mean-shift-algorithm/\" target=\"_blank\" rel=\"external\">这篇讲义</a>。</p>\n<p>注意下面的推导中$x$是一个$d$维的向量。为了书写简单，没有写成向量的加粗形式。首先，先介绍核函数（Kernel Function）的概念。如果某个$\\mathbb{R}^n\\rightarrow \\mathbb{R}$的函数满足以下条件，就能将其作为核函数。<br><img src=\"/img/meanshift_kernel_function.png\" alt=\"kernel\"></p>\n<p>比如高斯核函数：</p>\n<script type=\"math/tex; mode=display\">K(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp(-\\frac{x^2}{2\\sigma^2})</script><p>核密度估计是一种用来估计随机变量密度函数的非参数化方法。给定核函数$K$，带宽（bandwidth）参数$h$（指观察窗口的大小）。那么密度函数可以使用核函数进行估计，如下面的形式所示。其中，$n$是窗口内的数据点的数量。</p>\n<script type=\"math/tex; mode=display\">f(x) = \\frac{1}{nh^d}\\sum_{i=1}^{n}K(\\frac{x-x_i}{h})</script><p>如果核函数是放射形状且对称的（例如高斯核函数），那么$K(x)$可以写成如下的形式，其中$c_{k,d}$是为了使得核函数满足积分为$1$的正则系数。</p>\n<script type=\"math/tex; mode=display\">K(x) = c_{k,d}k(\\Arrowvert x\\Arrowvert ^2)</script><h2 id=\"mean-shift向量\"><a href=\"#mean-shift向量\" class=\"headerlink\" title=\"mean shift向量\"></a>mean shift向量</h2><p>那么，原来密度函数的极值点就是使得$f(x)$梯度为$0$的点。求取$f(x)$的梯度，可以得到下式。其中$g(s) = -k^\\prime(s)$。<br><img src=\"/img/meanshift_gradient_of_density.png\" alt=\"密度函数的梯度\"></p>\n<p>观察后可以发现，乘积第一项连带前面的系数，相当于是使用核函数$G(x) = c_{k,d}g(\\Arrowvert x\\Arrowvert^2)$得到的点$x$处的密度函数估计值（差了常数倍因子）（所以对于给定的某一点$x = x_0$，这一项是定值），后面一项拿出来，定义为$m_h(x)$，称为mean shift向量。</p>\n<script type=\"math/tex; mode=display\">m_h(x) = \\frac{\\sum_{i=1}^{n}x_i g(\\Arrowvert \\frac{x-x_i}{h} \\Arrowvert^2)}{\\sum_{i=1}^{n}g(\\Arrowvert \\frac{x-x_i}{h} \\Arrowvert^2)}-x</script><p>所以说，$m_h(x)$的指向和$f(x)$的梯度是相同的。另外，从感性的角度出发。$m_h(x)$中的第一项（分式的那一大坨），相当于是在计算$x$周围窗口大小为$h$的这个领域的均值（$g(s)$是加权的系数）。所以$m_h(x)$实际上指示了局部均值和当前点$x$之间的位移。我们想要找到密度函数的局部极大值，不就应该让局部均值向着点密集的方向移动吗？</p>\n<h2 id=\"算法流程\"><a href=\"#算法流程\" class=\"headerlink\" title=\"算法流程\"></a>算法流程</h2><p>所以，在给定初始位置$x_0$后，我们首先计算此点处的$m_h$，之后，将$x$沿着$m_h$移动即可。下面是一个简单的例子。数据点服从二维高斯分布，均值为$[1, 2]$。其中，红色菱形指示了迭代过程中mean shift的移动。</p>\n<figure class=\"highlight matlab\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\">%% generate data</span></div><div class=\"line\">mu = [<span class=\"number\">1</span> <span class=\"number\">2</span>];</div><div class=\"line\">Sigma = [<span class=\"number\">1</span> <span class=\"number\">0</span>; <span class=\"number\">0</span> <span class=\"number\">2</span>]; R = chol(Sigma);</div><div class=\"line\">N = <span class=\"number\">250</span>;</div><div class=\"line\">data = <span class=\"built_in\">repmat</span>(mu, N, <span class=\"number\">1</span>) + <span class=\"built_in\">randn</span>(N, <span class=\"number\">2</span>)*R;</div><div class=\"line\">figure</div><div class=\"line\">hold on</div><div class=\"line\">scatter(data(:, <span class=\"number\">1</span>), data(:, <span class=\"number\">2</span>), <span class=\"number\">50</span>, <span class=\"string\">'filled'</span>);</div><div class=\"line\"><span class=\"comment\">%% meanshift</span></div><div class=\"line\">mu0 = <span class=\"built_in\">rand</span>(<span class=\"number\">1</span>,<span class=\"number\">2</span>) * <span class=\"number\">5</span>;</div><div class=\"line\">mu = mean_shift(mu0, <span class=\"number\">10</span>, data);</div><div class=\"line\"></div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">function</span> <span class=\"title\">out</span> = <span class=\"title\">gaussian_kernel</span><span class=\"params\">(x, sigma)</span></span></div><div class=\"line\"><span class=\"comment\">% gauss kernel, g(x) = \\exp(-x^2/2\\sigma^2)</span></div><div class=\"line\">out = <span class=\"built_in\">exp</span>(-x.*x/(<span class=\"number\">2</span>*sigma*sigma));</div><div class=\"line\"><span class=\"keyword\">end</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">function</span> <span class=\"title\">mu</span> = <span class=\"title\">mean_shift</span><span class=\"params\">(mu0, h, data)</span></span></div><div class=\"line\"><span class=\"comment\">% implementation of meanshift algorithm</span></div><div class=\"line\"><span class=\"comment\">% mu_&#123;k+1&#125; = meanshift(mu_&#123;k&#125;) + mu_&#123;k&#125; = \\frac&#123;\\sum_i=1^n xg&#125;&#123;\\sum_i=1^n g&#125;</span></div><div class=\"line\">mu = mu0;</div><div class=\"line\">sigma = <span class=\"number\">1</span>;    <span class=\"comment\">% parameter for gaussian kernel function</span></div><div class=\"line\"><span class=\"keyword\">for</span> iter = <span class=\"number\">1</span>:<span class=\"number\">20</span>    </div><div class=\"line\">    fprintf(<span class=\"string\">'iter = %d, mu = [%f, %f]\\n'</span>, iter, mu(<span class=\"number\">1</span>), mu(<span class=\"number\">2</span>));</div><div class=\"line\">    scatter(mu(<span class=\"number\">1</span>), mu(<span class=\"number\">2</span>), <span class=\"number\">50</span>, [<span class=\"number\">1</span>,<span class=\"number\">0</span>,<span class=\"number\">0</span>], <span class=\"string\">'d'</span>, <span class=\"string\">'filled'</span>);</div><div class=\"line\">    offset = <span class=\"built_in\">bsxfun</span>(@minus, mu, data);    <span class=\"comment\">% offset = x-x_i</span></div><div class=\"line\">    dis = sum(offset.^<span class=\"number\">2</span>, <span class=\"number\">2</span>);              <span class=\"comment\">% dis = ||x-x_i||^2</span></div><div class=\"line\">    x = data(dis &lt; h, :);                 <span class=\"comment\">% neighborhood with bandwidth = h</span></div><div class=\"line\">    g = gaussian_kernel(offset(dis &lt; h), sigma);</div><div class=\"line\">    xg = x.*g;</div><div class=\"line\">    mu_prev = mu;</div><div class=\"line\">    mu = sum(xg, <span class=\"number\">1</span>) / sum(g, <span class=\"number\">1</span>);</div><div class=\"line\">    <span class=\"keyword\">if</span> norm(mu_prev - mu, <span class=\"number\">2</span>) &lt; <span class=\"number\">1E-2</span></div><div class=\"line\">        <span class=\"keyword\">break</span>;</div><div class=\"line\">    <span class=\"keyword\">end</span></div><div class=\"line\">    plot([mu_prev(<span class=\"number\">1</span>) mu(<span class=\"number\">1</span>)], [mu_prev(<span class=\"number\">2</span>), mu(<span class=\"number\">2</span>)], <span class=\"string\">'b-.'</span>, <span class=\"string\">'linewidth'</span>, <span class=\"number\">2</span>);</div><div class=\"line\"><span class=\"keyword\">end</span></div><div class=\"line\">scatter(mu(<span class=\"number\">1</span>), mu(<span class=\"number\">2</span>), <span class=\"number\">50</span>, [<span class=\"number\">1</span>,<span class=\"number\">0</span>,<span class=\"number\">0</span>], <span class=\"string\">'d'</span>, <span class=\"string\">'filled'</span>);</div><div class=\"line\"><span class=\"keyword\">end</span></div></pre></td></tr></table></figure>\n<p><img src=\"/img/meanshift_simple_demo.png\" alt=\"\"></p>\n<p>同时，我也试验了其他的kernel函数，如神似logistaic形式，效果也是相似的。</p>\n<script type=\"math/tex; mode=display\">K(x) = \\frac{1}{e^x+e^{-x}+2}</script><figure class=\"highlight matlab\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"function\"><span class=\"keyword\">function</span> <span class=\"title\">out</span> = <span class=\"title\">logistic_kernel</span><span class=\"params\">(x)</span></span></div><div class=\"line\">out = <span class=\"number\">1.</span>/(<span class=\"built_in\">exp</span>(x) + <span class=\"built_in\">exp</span>(-x) + <span class=\"number\">2</span>);</div><div class=\"line\"><span class=\"keyword\">end</span></div></pre></td></tr></table></figure>\n","excerpt":"<p><a href=\"https://en.wikipedia.org/wiki/Mean_shift\">MeanShift</a>最初由Fukunaga和Hostetler在1975年提出，但是一直到2000左右这篇PAMI的论文<a href=\"http://courses.csail.mit.edu/6.869/handouts/PAMIMeanshift.pdf\">Mean Shift: A Robust Approach Toward Feature Space Analysis</a>，将它的原理和收敛性等重新整理阐述，并应用于计算机视觉和图像处理领域之后，才逐渐为人熟知。</p>\n<p>MeanShift是一种用来寻找特征空间内<a href=\"https://en.wikipedia.org/wiki/Mode_(statistics\">模态</a>的方法。所谓模态（Mode），就是指数据集中最经常出现的数据。例如，连续随机变量概率密度函数的模态就是指函数的极大值。从概率的角度看，我们可以认为数据集（或者特征空间）内的数据点都是从某个概率分布中随机抽取出来的。这样，数据点越密集的地方就说明这里越有可能是密度函数的极大值。MeanShift就是一种能够从离散的抽样点中估计密度函数局部极大值的方法。<br><img src=\"/img/meanshift_basics.jpg\" alt=\"MeanShift\"></p>","more":"<h2 id=\"核密度估计\"><a href=\"#核密度估计\" class=\"headerlink\" title=\"核密度估计\"></a>核密度估计</h2><p>上面提到的PAMI论文篇幅较长，且数学名词较多，我不是很理解。下面的说明过程主要参考了<a href=\"https://saravananthirumuruganathan.wordpress.com/2010/04/01/introduction-to-mean-shift-algorithm/\">这篇博客</a>和<a href=\"https://saravananthirumuruganathan.wordpress.com/2010/04/01/introduction-to-mean-shift-algorithm/\">这篇讲义</a>。</p>\n<p>注意下面的推导中$x$是一个$d$维的向量。为了书写简单，没有写成向量的加粗形式。首先，先介绍核函数（Kernel Function）的概念。如果某个$\\mathbb{R}^n\\rightarrow \\mathbb{R}$的函数满足以下条件，就能将其作为核函数。<br><img src=\"/img/meanshift_kernel_function.png\" alt=\"kernel\"></p>\n<p>比如高斯核函数：</p>\n<script type=\"math/tex; mode=display\">K(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp(-\\frac{x^2}{2\\sigma^2})</script><p>核密度估计是一种用来估计随机变量密度函数的非参数化方法。给定核函数$K$，带宽（bandwidth）参数$h$（指观察窗口的大小）。那么密度函数可以使用核函数进行估计，如下面的形式所示。其中，$n$是窗口内的数据点的数量。</p>\n<script type=\"math/tex; mode=display\">f(x) = \\frac{1}{nh^d}\\sum_{i=1}^{n}K(\\frac{x-x_i}{h})</script><p>如果核函数是放射形状且对称的（例如高斯核函数），那么$K(x)$可以写成如下的形式，其中$c_{k,d}$是为了使得核函数满足积分为$1$的正则系数。</p>\n<script type=\"math/tex; mode=display\">K(x) = c_{k,d}k(\\Arrowvert x\\Arrowvert ^2)</script><h2 id=\"mean-shift向量\"><a href=\"#mean-shift向量\" class=\"headerlink\" title=\"mean shift向量\"></a>mean shift向量</h2><p>那么，原来密度函数的极值点就是使得$f(x)$梯度为$0$的点。求取$f(x)$的梯度，可以得到下式。其中$g(s) = -k^\\prime(s)$。<br><img src=\"/img/meanshift_gradient_of_density.png\" alt=\"密度函数的梯度\"></p>\n<p>观察后可以发现，乘积第一项连带前面的系数，相当于是使用核函数$G(x) = c_{k,d}g(\\Arrowvert x\\Arrowvert^2)$得到的点$x$处的密度函数估计值（差了常数倍因子）（所以对于给定的某一点$x = x_0$，这一项是定值），后面一项拿出来，定义为$m_h(x)$，称为mean shift向量。</p>\n<script type=\"math/tex; mode=display\">m_h(x) = \\frac{\\sum_{i=1}^{n}x_i g(\\Arrowvert \\frac{x-x_i}{h} \\Arrowvert^2)}{\\sum_{i=1}^{n}g(\\Arrowvert \\frac{x-x_i}{h} \\Arrowvert^2)}-x</script><p>所以说，$m_h(x)$的指向和$f(x)$的梯度是相同的。另外，从感性的角度出发。$m_h(x)$中的第一项（分式的那一大坨），相当于是在计算$x$周围窗口大小为$h$的这个领域的均值（$g(s)$是加权的系数）。所以$m_h(x)$实际上指示了局部均值和当前点$x$之间的位移。我们想要找到密度函数的局部极大值，不就应该让局部均值向着点密集的方向移动吗？</p>\n<h2 id=\"算法流程\"><a href=\"#算法流程\" class=\"headerlink\" title=\"算法流程\"></a>算法流程</h2><p>所以，在给定初始位置$x_0$后，我们首先计算此点处的$m_h$，之后，将$x$沿着$m_h$移动即可。下面是一个简单的例子。数据点服从二维高斯分布，均值为$[1, 2]$。其中，红色菱形指示了迭代过程中mean shift的移动。</p>\n<figure class=\"highlight matlab\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\">%% generate data</span></div><div class=\"line\">mu = [<span class=\"number\">1</span> <span class=\"number\">2</span>];</div><div class=\"line\">Sigma = [<span class=\"number\">1</span> <span class=\"number\">0</span>; <span class=\"number\">0</span> <span class=\"number\">2</span>]; R = chol(Sigma);</div><div class=\"line\">N = <span class=\"number\">250</span>;</div><div class=\"line\">data = <span class=\"built_in\">repmat</span>(mu, N, <span class=\"number\">1</span>) + <span class=\"built_in\">randn</span>(N, <span class=\"number\">2</span>)*R;</div><div class=\"line\">figure</div><div class=\"line\">hold on</div><div class=\"line\">scatter(data(:, <span class=\"number\">1</span>), data(:, <span class=\"number\">2</span>), <span class=\"number\">50</span>, <span class=\"string\">'filled'</span>);</div><div class=\"line\"><span class=\"comment\">%% meanshift</span></div><div class=\"line\">mu0 = <span class=\"built_in\">rand</span>(<span class=\"number\">1</span>,<span class=\"number\">2</span>) * <span class=\"number\">5</span>;</div><div class=\"line\">mu = mean_shift(mu0, <span class=\"number\">10</span>, data);</div><div class=\"line\"></div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">function</span> <span class=\"title\">out</span> = <span class=\"title\">gaussian_kernel</span><span class=\"params\">(x, sigma)</span></span></div><div class=\"line\"><span class=\"comment\">% gauss kernel, g(x) = \\exp(-x^2/2\\sigma^2)</span></div><div class=\"line\">out = <span class=\"built_in\">exp</span>(-x.*x/(<span class=\"number\">2</span>*sigma*sigma));</div><div class=\"line\"><span class=\"keyword\">end</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">function</span> <span class=\"title\">mu</span> = <span class=\"title\">mean_shift</span><span class=\"params\">(mu0, h, data)</span></span></div><div class=\"line\"><span class=\"comment\">% implementation of meanshift algorithm</span></div><div class=\"line\"><span class=\"comment\">% mu_&#123;k+1&#125; = meanshift(mu_&#123;k&#125;) + mu_&#123;k&#125; = \\frac&#123;\\sum_i=1^n xg&#125;&#123;\\sum_i=1^n g&#125;</span></div><div class=\"line\">mu = mu0;</div><div class=\"line\">sigma = <span class=\"number\">1</span>;    <span class=\"comment\">% parameter for gaussian kernel function</span></div><div class=\"line\"><span class=\"keyword\">for</span> iter = <span class=\"number\">1</span>:<span class=\"number\">20</span>    </div><div class=\"line\">    fprintf(<span class=\"string\">'iter = %d, mu = [%f, %f]\\n'</span>, iter, mu(<span class=\"number\">1</span>), mu(<span class=\"number\">2</span>));</div><div class=\"line\">    scatter(mu(<span class=\"number\">1</span>), mu(<span class=\"number\">2</span>), <span class=\"number\">50</span>, [<span class=\"number\">1</span>,<span class=\"number\">0</span>,<span class=\"number\">0</span>], <span class=\"string\">'d'</span>, <span class=\"string\">'filled'</span>);</div><div class=\"line\">    offset = <span class=\"built_in\">bsxfun</span>(@minus, mu, data);    <span class=\"comment\">% offset = x-x_i</span></div><div class=\"line\">    dis = sum(offset.^<span class=\"number\">2</span>, <span class=\"number\">2</span>);              <span class=\"comment\">% dis = ||x-x_i||^2</span></div><div class=\"line\">    x = data(dis &lt; h, :);                 <span class=\"comment\">% neighborhood with bandwidth = h</span></div><div class=\"line\">    g = gaussian_kernel(offset(dis &lt; h), sigma);</div><div class=\"line\">    xg = x.*g;</div><div class=\"line\">    mu_prev = mu;</div><div class=\"line\">    mu = sum(xg, <span class=\"number\">1</span>) / sum(g, <span class=\"number\">1</span>);</div><div class=\"line\">    <span class=\"keyword\">if</span> norm(mu_prev - mu, <span class=\"number\">2</span>) &lt; <span class=\"number\">1E-2</span></div><div class=\"line\">        <span class=\"keyword\">break</span>;</div><div class=\"line\">    <span class=\"keyword\">end</span></div><div class=\"line\">    plot([mu_prev(<span class=\"number\">1</span>) mu(<span class=\"number\">1</span>)], [mu_prev(<span class=\"number\">2</span>), mu(<span class=\"number\">2</span>)], <span class=\"string\">'b-.'</span>, <span class=\"string\">'linewidth'</span>, <span class=\"number\">2</span>);</div><div class=\"line\"><span class=\"keyword\">end</span></div><div class=\"line\">scatter(mu(<span class=\"number\">1</span>), mu(<span class=\"number\">2</span>), <span class=\"number\">50</span>, [<span class=\"number\">1</span>,<span class=\"number\">0</span>,<span class=\"number\">0</span>], <span class=\"string\">'d'</span>, <span class=\"string\">'filled'</span>);</div><div class=\"line\"><span class=\"keyword\">end</span></div></pre></td></tr></table></figure>\n<p><img src=\"/img/meanshift_simple_demo.png\" alt=\"\"></p>\n<p>同时，我也试验了其他的kernel函数，如神似logistaic形式，效果也是相似的。</p>\n<script type=\"math/tex; mode=display\">K(x) = \\frac{1}{e^x+e^{-x}+2}</script><figure class=\"highlight matlab\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"function\"><span class=\"keyword\">function</span> <span class=\"title\">out</span> = <span class=\"title\">logistic_kernel</span><span class=\"params\">(x)</span></span></div><div class=\"line\">out = <span class=\"number\">1.</span>/(<span class=\"built_in\">exp</span>(x) + <span class=\"built_in\">exp</span>(-x) + <span class=\"number\">2</span>);</div><div class=\"line\"><span class=\"keyword\">end</span></div></pre></td></tr></table></figure>"},{"title":"CS131-线代基础","date":"2017-01-22T07:38:01.000Z","_content":"\nCS131课程(Computer Vision: Foundations and Applications)，是斯坦福大学Li Feifei实验室开设的一门计算机视觉入门基础课程，[该课程](http://vision.stanford.edu/teaching/cs131_fall1617/index.html)目的在于为刚接触计算机视觉领域的学生提供基本原理和应用介绍。目前2016年冬季课程刚刚结束。CS131博客系列主要是关于本课的slide知识点总结与作业重点问题归纳，作为个人学习本门课程的心得体会和复习材料。\n\n由于是个人项目，所以会比较随意，只对个人感兴趣的内容做一总结。这篇文章是对课前[线代基础](http://vision.stanford.edu/teaching/cs131_fall1617/lectures/lecture2_linalg_review_cs131_2016.pdf)的复习与整理。\n![线性代数词云](/img/cs131_linear_algebra.jpg)\n\n<!-- more -->\n## 向量与矩阵\n数字图像可以看做二维矩阵，向量是特殊的矩阵，本课程默认的向量都是列向量。\nslide中给出了一些矩阵行列式和迹的性质，都比较简单，这里不再多说。\n## 矩阵作为线性变换\n通过线代知识，我们知道，在线性空间中，如果给定一组基，线性变换可以通过对应的矩阵来进行描述。\n\n### scale变换\n对角阵可以用来表示放缩变换。\n$$\n\\begin{bmatrix}\ns_x & 0\\\\\\\\\n0 & s_y\n\\end{bmatrix}\\begin{bmatrix}\nx\\\\\\\\\ny\n\\end{bmatrix} = \\begin{bmatrix}\ns_xx\\\\\\\\\ns_yy\n\\end{bmatrix}\n$$\n\n### 旋转变换\n如图所示，逆时针旋转$theta$角度，对应的旋转矩阵为：\n![旋转变换](/img/rotation.png)\n$$\n\\mathbf{R} = \\begin{bmatrix}\n\\cos\\theta &-\\sin\\theta \\\\\\\\\n\\sin\\theta &\\cos\\theta\n\\end{bmatrix}\n$$\n旋转矩阵是[酉矩阵](https://zh.wikipedia.org/wiki/酉矩阵)，矩阵内的各列（或者各行）相互正交。满足如下的关系式：\n$$\n\\mathbf{R}\\mathbf{R^{\\dagger}} = \\mathbf{I}\n$$\n由于$\\det{\\mathbf{R}} = \\det{\\mathbf{R^{\\dagger}}}$，所以，对于酉矩阵，$\\det{\\mathbf{R}} = \\pm 1$\n旋转矩阵是[酉矩阵](https://zh.wikipedia.org/wiki/酉矩阵)，矩阵内的各列（或者各行）相互正交。满足如下的关系式：\n$$\n\\mathbf{R}\\mathbf{R^{\\dagger}} = \\mathbf{I}\n$$\n由于$\\det{\\mathbf{R}} = \\det{\\mathbf{R^{\\dagger}}}$，所以，对于酉矩阵，$\\det{\\mathbf{R}} = \\pm 1$.\n\n### 齐次变换(Homogeneous Transform)\n只用上面的二维矩阵不能表达平移，使用齐次矩阵可以表达放缩，旋转和平移操作。\n$$\n\\mathbf{H} =\\begin{bmatrix}\na & b & t_x\\\\\\\\\nc & d & t_y\\\\\\\\\n0 & 0 & 1\n\\end{bmatrix},\\mathbf{H}\\begin{bmatrix}\nx\\\\\\\\\ny\\\\\\\\\n1\\\\\\\\\n\\end{bmatrix}=\\begin{bmatrix}\nax+by+t_x\\\\\\\\\ncx+dy+t_y\\\\\\\\\n1\n\\end{bmatrix}\n$$\n\n### SVD分解\n可以将矩阵分成若干个矩阵的乘积，叫做矩阵分解，比如QR分解，满秩分解等。SVD分解，即奇异值分解，也是一种特殊的矩阵分解方法。如下式所示，是将矩阵分解成为三个矩阵的乘积：\n$$\\mathbf{U}\\mathbf{\\Sigma}\\mathbf{V^\\dagger} = \\mathbf{A}$$\n其中矩阵$\\mathbf{A}$大小为$m\\times n$，矩阵$\\mathbf{U}$是大小为$m\\times m$的酉矩阵，$\\mathbf{V}$是大小为$n \\times n$的酉矩阵，$\\mathbf{\\Sigma}$是大小为$m \\times n$的旋转矩阵，即只有主对角元素不为0.\n\nSVD分解在主成分分析中年很有用。由于矩阵$\\mathbf{\\Sigma}$一般情况下是将奇异值按照从大到小的顺序摆放，所以矩阵$\\mathbf{U}$中，前面的若干列被视作主成分，后面的列显得相对不这么重要。可以抛弃后面的列，进行图像压缩。\n\n如下图，是使用前10个分量对原图片进行压缩的效果。\n\n``` matlab\nim = imread('./superman.png');\nim_gray = rbg2gray(im);\n[u, s, v] = svd(double(im_gray));\nk = 10;\nuk = u(:, 1:k);\nsigma = diag(s);\nsk = diag(sigma(1:k));\nvk = v(:, 1:k);\nim_k = uk*sk*vk';\nimshow(uint8(im_k))\n```\n\n![原始图像](/img/original_superman.png)\n![压缩图像](/img/svd_superman.png)\n","source":"_posts/cs131-linear-alg.md","raw":"---\ntitle: CS131-线代基础\ndate: 2017-01-22 15:38:01\ntags:\n    - cs131\n    - 公开课\n---\n\nCS131课程(Computer Vision: Foundations and Applications)，是斯坦福大学Li Feifei实验室开设的一门计算机视觉入门基础课程，[该课程](http://vision.stanford.edu/teaching/cs131_fall1617/index.html)目的在于为刚接触计算机视觉领域的学生提供基本原理和应用介绍。目前2016年冬季课程刚刚结束。CS131博客系列主要是关于本课的slide知识点总结与作业重点问题归纳，作为个人学习本门课程的心得体会和复习材料。\n\n由于是个人项目，所以会比较随意，只对个人感兴趣的内容做一总结。这篇文章是对课前[线代基础](http://vision.stanford.edu/teaching/cs131_fall1617/lectures/lecture2_linalg_review_cs131_2016.pdf)的复习与整理。\n![线性代数词云](/img/cs131_linear_algebra.jpg)\n\n<!-- more -->\n## 向量与矩阵\n数字图像可以看做二维矩阵，向量是特殊的矩阵，本课程默认的向量都是列向量。\nslide中给出了一些矩阵行列式和迹的性质，都比较简单，这里不再多说。\n## 矩阵作为线性变换\n通过线代知识，我们知道，在线性空间中，如果给定一组基，线性变换可以通过对应的矩阵来进行描述。\n\n### scale变换\n对角阵可以用来表示放缩变换。\n$$\n\\begin{bmatrix}\ns_x & 0\\\\\\\\\n0 & s_y\n\\end{bmatrix}\\begin{bmatrix}\nx\\\\\\\\\ny\n\\end{bmatrix} = \\begin{bmatrix}\ns_xx\\\\\\\\\ns_yy\n\\end{bmatrix}\n$$\n\n### 旋转变换\n如图所示，逆时针旋转$theta$角度，对应的旋转矩阵为：\n![旋转变换](/img/rotation.png)\n$$\n\\mathbf{R} = \\begin{bmatrix}\n\\cos\\theta &-\\sin\\theta \\\\\\\\\n\\sin\\theta &\\cos\\theta\n\\end{bmatrix}\n$$\n旋转矩阵是[酉矩阵](https://zh.wikipedia.org/wiki/酉矩阵)，矩阵内的各列（或者各行）相互正交。满足如下的关系式：\n$$\n\\mathbf{R}\\mathbf{R^{\\dagger}} = \\mathbf{I}\n$$\n由于$\\det{\\mathbf{R}} = \\det{\\mathbf{R^{\\dagger}}}$，所以，对于酉矩阵，$\\det{\\mathbf{R}} = \\pm 1$\n旋转矩阵是[酉矩阵](https://zh.wikipedia.org/wiki/酉矩阵)，矩阵内的各列（或者各行）相互正交。满足如下的关系式：\n$$\n\\mathbf{R}\\mathbf{R^{\\dagger}} = \\mathbf{I}\n$$\n由于$\\det{\\mathbf{R}} = \\det{\\mathbf{R^{\\dagger}}}$，所以，对于酉矩阵，$\\det{\\mathbf{R}} = \\pm 1$.\n\n### 齐次变换(Homogeneous Transform)\n只用上面的二维矩阵不能表达平移，使用齐次矩阵可以表达放缩，旋转和平移操作。\n$$\n\\mathbf{H} =\\begin{bmatrix}\na & b & t_x\\\\\\\\\nc & d & t_y\\\\\\\\\n0 & 0 & 1\n\\end{bmatrix},\\mathbf{H}\\begin{bmatrix}\nx\\\\\\\\\ny\\\\\\\\\n1\\\\\\\\\n\\end{bmatrix}=\\begin{bmatrix}\nax+by+t_x\\\\\\\\\ncx+dy+t_y\\\\\\\\\n1\n\\end{bmatrix}\n$$\n\n### SVD分解\n可以将矩阵分成若干个矩阵的乘积，叫做矩阵分解，比如QR分解，满秩分解等。SVD分解，即奇异值分解，也是一种特殊的矩阵分解方法。如下式所示，是将矩阵分解成为三个矩阵的乘积：\n$$\\mathbf{U}\\mathbf{\\Sigma}\\mathbf{V^\\dagger} = \\mathbf{A}$$\n其中矩阵$\\mathbf{A}$大小为$m\\times n$，矩阵$\\mathbf{U}$是大小为$m\\times m$的酉矩阵，$\\mathbf{V}$是大小为$n \\times n$的酉矩阵，$\\mathbf{\\Sigma}$是大小为$m \\times n$的旋转矩阵，即只有主对角元素不为0.\n\nSVD分解在主成分分析中年很有用。由于矩阵$\\mathbf{\\Sigma}$一般情况下是将奇异值按照从大到小的顺序摆放，所以矩阵$\\mathbf{U}$中，前面的若干列被视作主成分，后面的列显得相对不这么重要。可以抛弃后面的列，进行图像压缩。\n\n如下图，是使用前10个分量对原图片进行压缩的效果。\n\n``` matlab\nim = imread('./superman.png');\nim_gray = rbg2gray(im);\n[u, s, v] = svd(double(im_gray));\nk = 10;\nuk = u(:, 1:k);\nsigma = diag(s);\nsk = diag(sigma(1:k));\nvk = v(:, 1:k);\nim_k = uk*sk*vk';\nimshow(uint8(im_k))\n```\n\n![原始图像](/img/original_superman.png)\n![压缩图像](/img/svd_superman.png)\n","slug":"cs131-linear-alg","published":1,"updated":"2018-01-12T06:22:20.460Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjcu6vcs6000hqu46ix5rbdus","content":"<p>CS131课程(Computer Vision: Foundations and Applications)，是斯坦福大学Li Feifei实验室开设的一门计算机视觉入门基础课程，<a href=\"http://vision.stanford.edu/teaching/cs131_fall1617/index.html\" target=\"_blank\" rel=\"external\">该课程</a>目的在于为刚接触计算机视觉领域的学生提供基本原理和应用介绍。目前2016年冬季课程刚刚结束。CS131博客系列主要是关于本课的slide知识点总结与作业重点问题归纳，作为个人学习本门课程的心得体会和复习材料。</p>\n<p>由于是个人项目，所以会比较随意，只对个人感兴趣的内容做一总结。这篇文章是对课前<a href=\"http://vision.stanford.edu/teaching/cs131_fall1617/lectures/lecture2_linalg_review_cs131_2016.pdf\" target=\"_blank\" rel=\"external\">线代基础</a>的复习与整理。<br><img src=\"/img/cs131_linear_algebra.jpg\" alt=\"线性代数词云\"></p>\n<a id=\"more\"></a>\n<h2 id=\"向量与矩阵\"><a href=\"#向量与矩阵\" class=\"headerlink\" title=\"向量与矩阵\"></a>向量与矩阵</h2><p>数字图像可以看做二维矩阵，向量是特殊的矩阵，本课程默认的向量都是列向量。<br>slide中给出了一些矩阵行列式和迹的性质，都比较简单，这里不再多说。</p>\n<h2 id=\"矩阵作为线性变换\"><a href=\"#矩阵作为线性变换\" class=\"headerlink\" title=\"矩阵作为线性变换\"></a>矩阵作为线性变换</h2><p>通过线代知识，我们知道，在线性空间中，如果给定一组基，线性变换可以通过对应的矩阵来进行描述。</p>\n<h3 id=\"scale变换\"><a href=\"#scale变换\" class=\"headerlink\" title=\"scale变换\"></a>scale变换</h3><p>对角阵可以用来表示放缩变换。</p>\n<script type=\"math/tex; mode=display\">\n\\begin{bmatrix}\ns_x & 0\\\\\\\\\n0 & s_y\n\\end{bmatrix}\\begin{bmatrix}\nx\\\\\\\\\ny\n\\end{bmatrix} = \\begin{bmatrix}\ns_xx\\\\\\\\\ns_yy\n\\end{bmatrix}</script><h3 id=\"旋转变换\"><a href=\"#旋转变换\" class=\"headerlink\" title=\"旋转变换\"></a>旋转变换</h3><p>如图所示，逆时针旋转$theta$角度，对应的旋转矩阵为：<br><img src=\"/img/rotation.png\" alt=\"旋转变换\"></p>\n<script type=\"math/tex; mode=display\">\n\\mathbf{R} = \\begin{bmatrix}\n\\cos\\theta &-\\sin\\theta \\\\\\\\\n\\sin\\theta &\\cos\\theta\n\\end{bmatrix}</script><p>旋转矩阵是<a href=\"https://zh.wikipedia.org/wiki/酉矩阵\" target=\"_blank\" rel=\"external\">酉矩阵</a>，矩阵内的各列（或者各行）相互正交。满足如下的关系式：</p>\n<script type=\"math/tex; mode=display\">\n\\mathbf{R}\\mathbf{R^{\\dagger}} = \\mathbf{I}</script><p>由于$\\det{\\mathbf{R}} = \\det{\\mathbf{R^{\\dagger}}}$，所以，对于酉矩阵，$\\det{\\mathbf{R}} = \\pm 1$<br>旋转矩阵是<a href=\"https://zh.wikipedia.org/wiki/酉矩阵\" target=\"_blank\" rel=\"external\">酉矩阵</a>，矩阵内的各列（或者各行）相互正交。满足如下的关系式：</p>\n<script type=\"math/tex; mode=display\">\n\\mathbf{R}\\mathbf{R^{\\dagger}} = \\mathbf{I}</script><p>由于$\\det{\\mathbf{R}} = \\det{\\mathbf{R^{\\dagger}}}$，所以，对于酉矩阵，$\\det{\\mathbf{R}} = \\pm 1$.</p>\n<h3 id=\"齐次变换-Homogeneous-Transform\"><a href=\"#齐次变换-Homogeneous-Transform\" class=\"headerlink\" title=\"齐次变换(Homogeneous Transform)\"></a>齐次变换(Homogeneous Transform)</h3><p>只用上面的二维矩阵不能表达平移，使用齐次矩阵可以表达放缩，旋转和平移操作。</p>\n<script type=\"math/tex; mode=display\">\n\\mathbf{H} =\\begin{bmatrix}\na & b & t_x\\\\\\\\\nc & d & t_y\\\\\\\\\n0 & 0 & 1\n\\end{bmatrix},\\mathbf{H}\\begin{bmatrix}\nx\\\\\\\\\ny\\\\\\\\\n1\\\\\\\\\n\\end{bmatrix}=\\begin{bmatrix}\nax+by+t_x\\\\\\\\\ncx+dy+t_y\\\\\\\\\n1\n\\end{bmatrix}</script><h3 id=\"SVD分解\"><a href=\"#SVD分解\" class=\"headerlink\" title=\"SVD分解\"></a>SVD分解</h3><p>可以将矩阵分成若干个矩阵的乘积，叫做矩阵分解，比如QR分解，满秩分解等。SVD分解，即奇异值分解，也是一种特殊的矩阵分解方法。如下式所示，是将矩阵分解成为三个矩阵的乘积：</p>\n<script type=\"math/tex; mode=display\">\\mathbf{U}\\mathbf{\\Sigma}\\mathbf{V^\\dagger} = \\mathbf{A}</script><p>其中矩阵$\\mathbf{A}$大小为$m\\times n$，矩阵$\\mathbf{U}$是大小为$m\\times m$的酉矩阵，$\\mathbf{V}$是大小为$n \\times n$的酉矩阵，$\\mathbf{\\Sigma}$是大小为$m \\times n$的旋转矩阵，即只有主对角元素不为0.</p>\n<p>SVD分解在主成分分析中年很有用。由于矩阵$\\mathbf{\\Sigma}$一般情况下是将奇异值按照从大到小的顺序摆放，所以矩阵$\\mathbf{U}$中，前面的若干列被视作主成分，后面的列显得相对不这么重要。可以抛弃后面的列，进行图像压缩。</p>\n<p>如下图，是使用前10个分量对原图片进行压缩的效果。</p>\n<figure class=\"highlight matlab\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div></pre></td><td class=\"code\"><pre><div class=\"line\">im = imread(<span class=\"string\">'./superman.png'</span>);</div><div class=\"line\">im_gray = rbg2gray(im);</div><div class=\"line\">[u, s, v] = svd(double(im_gray));</div><div class=\"line\">k = <span class=\"number\">10</span>;</div><div class=\"line\">uk = u(:, <span class=\"number\">1</span>:k);</div><div class=\"line\">sigma = <span class=\"built_in\">diag</span>(s);</div><div class=\"line\">sk = <span class=\"built_in\">diag</span>(sigma(<span class=\"number\">1</span>:k));</div><div class=\"line\">vk = v(:, <span class=\"number\">1</span>:k);</div><div class=\"line\">im_k = uk*sk*vk';</div><div class=\"line\">imshow(uint8(im_k))</div></pre></td></tr></table></figure>\n<p><img src=\"/img/original_superman.png\" alt=\"原始图像\"><br><img src=\"/img/svd_superman.png\" alt=\"压缩图像\"></p>\n","excerpt":"<p>CS131课程(Computer Vision: Foundations and Applications)，是斯坦福大学Li Feifei实验室开设的一门计算机视觉入门基础课程，<a href=\"http://vision.stanford.edu/teaching/cs131_fall1617/index.html\">该课程</a>目的在于为刚接触计算机视觉领域的学生提供基本原理和应用介绍。目前2016年冬季课程刚刚结束。CS131博客系列主要是关于本课的slide知识点总结与作业重点问题归纳，作为个人学习本门课程的心得体会和复习材料。</p>\n<p>由于是个人项目，所以会比较随意，只对个人感兴趣的内容做一总结。这篇文章是对课前<a href=\"http://vision.stanford.edu/teaching/cs131_fall1617/lectures/lecture2_linalg_review_cs131_2016.pdf\">线代基础</a>的复习与整理。<br><img src=\"/img/cs131_linear_algebra.jpg\" alt=\"线性代数词云\"></p>","more":"<h2 id=\"向量与矩阵\"><a href=\"#向量与矩阵\" class=\"headerlink\" title=\"向量与矩阵\"></a>向量与矩阵</h2><p>数字图像可以看做二维矩阵，向量是特殊的矩阵，本课程默认的向量都是列向量。<br>slide中给出了一些矩阵行列式和迹的性质，都比较简单，这里不再多说。</p>\n<h2 id=\"矩阵作为线性变换\"><a href=\"#矩阵作为线性变换\" class=\"headerlink\" title=\"矩阵作为线性变换\"></a>矩阵作为线性变换</h2><p>通过线代知识，我们知道，在线性空间中，如果给定一组基，线性变换可以通过对应的矩阵来进行描述。</p>\n<h3 id=\"scale变换\"><a href=\"#scale变换\" class=\"headerlink\" title=\"scale变换\"></a>scale变换</h3><p>对角阵可以用来表示放缩变换。</p>\n<script type=\"math/tex; mode=display\">\n\\begin{bmatrix}\ns_x & 0\\\\\\\\\n0 & s_y\n\\end{bmatrix}\\begin{bmatrix}\nx\\\\\\\\\ny\n\\end{bmatrix} = \\begin{bmatrix}\ns_xx\\\\\\\\\ns_yy\n\\end{bmatrix}</script><h3 id=\"旋转变换\"><a href=\"#旋转变换\" class=\"headerlink\" title=\"旋转变换\"></a>旋转变换</h3><p>如图所示，逆时针旋转$theta$角度，对应的旋转矩阵为：<br><img src=\"/img/rotation.png\" alt=\"旋转变换\"></p>\n<script type=\"math/tex; mode=display\">\n\\mathbf{R} = \\begin{bmatrix}\n\\cos\\theta &-\\sin\\theta \\\\\\\\\n\\sin\\theta &\\cos\\theta\n\\end{bmatrix}</script><p>旋转矩阵是<a href=\"https://zh.wikipedia.org/wiki/酉矩阵\">酉矩阵</a>，矩阵内的各列（或者各行）相互正交。满足如下的关系式：</p>\n<script type=\"math/tex; mode=display\">\n\\mathbf{R}\\mathbf{R^{\\dagger}} = \\mathbf{I}</script><p>由于$\\det{\\mathbf{R}} = \\det{\\mathbf{R^{\\dagger}}}$，所以，对于酉矩阵，$\\det{\\mathbf{R}} = \\pm 1$<br>旋转矩阵是<a href=\"https://zh.wikipedia.org/wiki/酉矩阵\">酉矩阵</a>，矩阵内的各列（或者各行）相互正交。满足如下的关系式：</p>\n<script type=\"math/tex; mode=display\">\n\\mathbf{R}\\mathbf{R^{\\dagger}} = \\mathbf{I}</script><p>由于$\\det{\\mathbf{R}} = \\det{\\mathbf{R^{\\dagger}}}$，所以，对于酉矩阵，$\\det{\\mathbf{R}} = \\pm 1$.</p>\n<h3 id=\"齐次变换-Homogeneous-Transform\"><a href=\"#齐次变换-Homogeneous-Transform\" class=\"headerlink\" title=\"齐次变换(Homogeneous Transform)\"></a>齐次变换(Homogeneous Transform)</h3><p>只用上面的二维矩阵不能表达平移，使用齐次矩阵可以表达放缩，旋转和平移操作。</p>\n<script type=\"math/tex; mode=display\">\n\\mathbf{H} =\\begin{bmatrix}\na & b & t_x\\\\\\\\\nc & d & t_y\\\\\\\\\n0 & 0 & 1\n\\end{bmatrix},\\mathbf{H}\\begin{bmatrix}\nx\\\\\\\\\ny\\\\\\\\\n1\\\\\\\\\n\\end{bmatrix}=\\begin{bmatrix}\nax+by+t_x\\\\\\\\\ncx+dy+t_y\\\\\\\\\n1\n\\end{bmatrix}</script><h3 id=\"SVD分解\"><a href=\"#SVD分解\" class=\"headerlink\" title=\"SVD分解\"></a>SVD分解</h3><p>可以将矩阵分成若干个矩阵的乘积，叫做矩阵分解，比如QR分解，满秩分解等。SVD分解，即奇异值分解，也是一种特殊的矩阵分解方法。如下式所示，是将矩阵分解成为三个矩阵的乘积：</p>\n<script type=\"math/tex; mode=display\">\\mathbf{U}\\mathbf{\\Sigma}\\mathbf{V^\\dagger} = \\mathbf{A}</script><p>其中矩阵$\\mathbf{A}$大小为$m\\times n$，矩阵$\\mathbf{U}$是大小为$m\\times m$的酉矩阵，$\\mathbf{V}$是大小为$n \\times n$的酉矩阵，$\\mathbf{\\Sigma}$是大小为$m \\times n$的旋转矩阵，即只有主对角元素不为0.</p>\n<p>SVD分解在主成分分析中年很有用。由于矩阵$\\mathbf{\\Sigma}$一般情况下是将奇异值按照从大到小的顺序摆放，所以矩阵$\\mathbf{U}$中，前面的若干列被视作主成分，后面的列显得相对不这么重要。可以抛弃后面的列，进行图像压缩。</p>\n<p>如下图，是使用前10个分量对原图片进行压缩的效果。</p>\n<figure class=\"highlight matlab\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div></pre></td><td class=\"code\"><pre><div class=\"line\">im = imread(<span class=\"string\">'./superman.png'</span>);</div><div class=\"line\">im_gray = rbg2gray(im);</div><div class=\"line\">[u, s, v] = svd(double(im_gray));</div><div class=\"line\">k = <span class=\"number\">10</span>;</div><div class=\"line\">uk = u(:, <span class=\"number\">1</span>:k);</div><div class=\"line\">sigma = <span class=\"built_in\">diag</span>(s);</div><div class=\"line\">sk = <span class=\"built_in\">diag</span>(sigma(<span class=\"number\">1</span>:k));</div><div class=\"line\">vk = v(:, <span class=\"number\">1</span>:k);</div><div class=\"line\">im_k = uk*sk*vk';</div><div class=\"line\">imshow(uint8(im_k))</div></pre></td></tr></table></figure>\n<p><img src=\"/img/original_superman.png\" alt=\"原始图像\"><br><img src=\"/img/svd_superman.png\" alt=\"压缩图像\"></p>"},{"title":"CS131-光流估计","date":"2017-05-03T08:39:18.000Z","_content":"光流法是通过检测图像像素点的强度随时间的变化进而推断出物体移动速度及方向的方法。由于成像物体与相机之间存在相对运动，导致其成像后的像素强度值不同。通过连续观测运动物体图像序列帧间的像素强度变化，就可以估计物体的运动信息。\n\n~~是你让我的世界从那刻变成粉红色~~  划掉。。。\n![OpticalFlow可视化](/img/cs131_opticalflow_demo.jpg)\n<!-- more -->\n\n## 光流的计算\n光流（Optical Flow），是指图像中像素强度的“表象”运动。这里的表象运动，是指图像中的像素变化并不一定是由于运动造成的，还有可能是由于外界光照的变化引起的。\n\n光流估计就是指利用时间上相邻的两帧图像，得到点的运动。满足以下几点假设：\n\n- 前后两帧点的位移不大（泰勒展开）\n- 外界光强保持恒定。\n- 空间相关性，每个点的运动和他们的邻居相似（连续函数，泰勒展开）\n\n其中第二条外界光强保持恒定，可以从下面的等式来理解。\n![光照强度保持恒定图解](/img/cs131_opticalflow_brightnessconstancy_assumption.png)\n\n在相邻的两帧图像中，点$(x,y)$发生了位移$(u,v)$，那么移动前后两点的亮度应该是相等的。如下：\n$$I(x,y,t-1) = I(x+u, y+v, t)$$\n\n从这个式子出发，我们将其利用Taylor展开做一阶线性近似。其中$I_x$, $I_y$, $I_t$分别是Image对这几个变量的偏导数。\n$$I(x+u,y+v,t) = I(x,y,t-1)+I_xu+I_yv+I_t$$\n\n上面两式联立，可以得到，\n$$I_xu+I_yv+I_t=0$$\n\n上式中，$I_x$, $I_y$可以通过图像沿$x$方向和$y$方向的导数计算，$I_t$可以通过$I(x,y,t)-I(x,y,t-1)$计算。未知数是$(u,v)$， 正是我们想要求解的每个像素在前后相邻两帧的位移。\n\n这里只有一个方程，却有两个未知数（实际是$N$个方程，$2N$个未知数，$N$是图像中待估计的像素点的个数，但是我们通过矩阵表示，将它们写成了如上式所述的紧凑形式），所以是一个不定方程。我们需要找出其它的约束求解方程。\n\n上面就是光流估计的基本思想。下面一节介绍估计光流的一种具体方法：Lucas-Kanade方法\n\n## L-K方法\n上述式子虽然给出了光流估计的思路，但是还是没有办法解出位移量。L-K方法依据相邻像素之间的位移相似的假设，通过一个观察窗口，将窗口内的像素点的位移看做是相同的，建立了一个超定方程，使用最小二乘法进行求解。下面是观察窗口为$5\\times 5$的时候，建立的方程。\n![L-K方程](/img/cs131_opticalflow_lkequation.png)\n\n使用最小二乘法求解，可以得到如下的式子，求和号代表是对窗口内的每一个像素点求和。\n![最小二乘法后的式子](/img/cs131_opticalflow_lkleastsquare.png)\n\n上式即是L-K方法求解光流估计问题的方程。通过求解这个方程，就可以得到光流的估计$(u,v)$。但是上式什么时候有解呢？\n\n- $\\mathbf{A}^\\dagger \\mathbf{A}$是可逆的。\n- $\\mathbf{A}^\\dagger \\mathbf{A}$不应该太小（噪声）。这意味着它的特征值$\\lambda_1$, $\\lambda_2$不应该太小。\n- $\\mathbf{A}^\\dagger \\mathbf{A}$不应该是病态的（稳定性）。这意味着它的特征值$\\lambda_1/\\lambda_2$不应该太大。\n\n而我们在Harris角点检测的时候已经讨论过$\\mathbf{A}^\\dagger \\mathbf{A}$这个矩阵的特征值情况了！也许，写成下面的形式更好看出来。\n![是不是和Harris角点更像了](/img/cs131_opticalflow_lkrelationshipwithharris.png)\n\n下面这张图就是当时的讨论结果。\n![不同点的分类](/img/cs131_opticalflow_lkharris.png)\n\n上面就是使用L-K方法估计光流的一般思路。\n\n## 金字塔方法\n在最开始的假设中，第一条指出点的位移应该是较小的。从上面的分析可以看出，当位移较大时，Taylor展开式一阶近似误差较大。其修正方法就是这里要介绍的金字塔方法。我们通过将图像降采样，就能够使得较大的位移在高层金字塔图像中变小，满足假设条件1.如下所示。\n\n![图像金字塔方法](/img/cs131_opticalflow_pyramid.png)\n\n## 作业：基于光流法的帧间插值\n### 问题描述\n假设视频流中的相邻两帧$I_0$和$I_1$，分别标记其时刻为$t=0$和$t=1$。我们希望能够在这两帧之间生成新的插值帧$I_t, 0<t<1$。比如说你手头的视频是24帧的帧率，想在一台刷新频率为60Hz的显示器上播放，那么这项技术可以带来更流畅的观看体验。\n\n### 简单粗暴法\n我们可以简单粗暴地使用线性插值方法，简单的认为插值帧是第一帧和最后一帧的线性组合，也就是说：\n$$I_t = (1-t)I_0+tI_1$$\n\n这种方法称为\"cross-fading\"。效果如下。可以看到有较多的模糊抖动。\n![简单粗暴法效果](/img/cs131_opticalflow_assignment_crossfade.png)\n\n### 基于光流法\n使用光流可以知道像素点在图像平面的运动信息，从而在帧间建立点的对应关系。我们记像素点在水平方向和竖直方向的速度分别为$u_t(x,y)$和$v_t(x,y)$。我们可以根据$t=0$和$t=1$的两帧图像解出光流信息，即$u_0(x,y)$和$v_0(x,y)$。那么我们认为光流保持不变，就可以计算插值帧的某一点在$t=0$时候的对应点坐标。接下来，赋值就可以了。如下式所示：\n$$I_t(x+tu_0(x,y), y+tv_0(x,y)) = I_0(x,y)$$\n\n用MATLAB实现如下：\n``` matlab\nfor y =1:height\n    for x = 1:width\n        dy = min(max(round(y+v0(y,x)*t), 1), height);\n        dx = min(max(round(x+u0(y,x)*t), 1), width);\n        img(dy,dx,:) = img0(y,x,:);\n    end\nend\n```\n\n### 改进\n上面的式子假定光流不变，但是实际上光流很可能也是改变的。考虑到光流的变化，下式的计算应该更加接近：\n$$I_t(x,y) = I_0(x-tu_t(x,y), y-tv_t(x,y))$$\n\n然而，我们并不能得到$u_t$和$v_t$的准确值。我们使用下面的方法近似求解。\n","source":"_posts/cs131-opticalflow.md","raw":"---\ntitle: CS131-光流估计\ndate: 2017-05-03 16:39:18\ntags:\n    - cs131\n    - 公开课\n---\n光流法是通过检测图像像素点的强度随时间的变化进而推断出物体移动速度及方向的方法。由于成像物体与相机之间存在相对运动，导致其成像后的像素强度值不同。通过连续观测运动物体图像序列帧间的像素强度变化，就可以估计物体的运动信息。\n\n~~是你让我的世界从那刻变成粉红色~~  划掉。。。\n![OpticalFlow可视化](/img/cs131_opticalflow_demo.jpg)\n<!-- more -->\n\n## 光流的计算\n光流（Optical Flow），是指图像中像素强度的“表象”运动。这里的表象运动，是指图像中的像素变化并不一定是由于运动造成的，还有可能是由于外界光照的变化引起的。\n\n光流估计就是指利用时间上相邻的两帧图像，得到点的运动。满足以下几点假设：\n\n- 前后两帧点的位移不大（泰勒展开）\n- 外界光强保持恒定。\n- 空间相关性，每个点的运动和他们的邻居相似（连续函数，泰勒展开）\n\n其中第二条外界光强保持恒定，可以从下面的等式来理解。\n![光照强度保持恒定图解](/img/cs131_opticalflow_brightnessconstancy_assumption.png)\n\n在相邻的两帧图像中，点$(x,y)$发生了位移$(u,v)$，那么移动前后两点的亮度应该是相等的。如下：\n$$I(x,y,t-1) = I(x+u, y+v, t)$$\n\n从这个式子出发，我们将其利用Taylor展开做一阶线性近似。其中$I_x$, $I_y$, $I_t$分别是Image对这几个变量的偏导数。\n$$I(x+u,y+v,t) = I(x,y,t-1)+I_xu+I_yv+I_t$$\n\n上面两式联立，可以得到，\n$$I_xu+I_yv+I_t=0$$\n\n上式中，$I_x$, $I_y$可以通过图像沿$x$方向和$y$方向的导数计算，$I_t$可以通过$I(x,y,t)-I(x,y,t-1)$计算。未知数是$(u,v)$， 正是我们想要求解的每个像素在前后相邻两帧的位移。\n\n这里只有一个方程，却有两个未知数（实际是$N$个方程，$2N$个未知数，$N$是图像中待估计的像素点的个数，但是我们通过矩阵表示，将它们写成了如上式所述的紧凑形式），所以是一个不定方程。我们需要找出其它的约束求解方程。\n\n上面就是光流估计的基本思想。下面一节介绍估计光流的一种具体方法：Lucas-Kanade方法\n\n## L-K方法\n上述式子虽然给出了光流估计的思路，但是还是没有办法解出位移量。L-K方法依据相邻像素之间的位移相似的假设，通过一个观察窗口，将窗口内的像素点的位移看做是相同的，建立了一个超定方程，使用最小二乘法进行求解。下面是观察窗口为$5\\times 5$的时候，建立的方程。\n![L-K方程](/img/cs131_opticalflow_lkequation.png)\n\n使用最小二乘法求解，可以得到如下的式子，求和号代表是对窗口内的每一个像素点求和。\n![最小二乘法后的式子](/img/cs131_opticalflow_lkleastsquare.png)\n\n上式即是L-K方法求解光流估计问题的方程。通过求解这个方程，就可以得到光流的估计$(u,v)$。但是上式什么时候有解呢？\n\n- $\\mathbf{A}^\\dagger \\mathbf{A}$是可逆的。\n- $\\mathbf{A}^\\dagger \\mathbf{A}$不应该太小（噪声）。这意味着它的特征值$\\lambda_1$, $\\lambda_2$不应该太小。\n- $\\mathbf{A}^\\dagger \\mathbf{A}$不应该是病态的（稳定性）。这意味着它的特征值$\\lambda_1/\\lambda_2$不应该太大。\n\n而我们在Harris角点检测的时候已经讨论过$\\mathbf{A}^\\dagger \\mathbf{A}$这个矩阵的特征值情况了！也许，写成下面的形式更好看出来。\n![是不是和Harris角点更像了](/img/cs131_opticalflow_lkrelationshipwithharris.png)\n\n下面这张图就是当时的讨论结果。\n![不同点的分类](/img/cs131_opticalflow_lkharris.png)\n\n上面就是使用L-K方法估计光流的一般思路。\n\n## 金字塔方法\n在最开始的假设中，第一条指出点的位移应该是较小的。从上面的分析可以看出，当位移较大时，Taylor展开式一阶近似误差较大。其修正方法就是这里要介绍的金字塔方法。我们通过将图像降采样，就能够使得较大的位移在高层金字塔图像中变小，满足假设条件1.如下所示。\n\n![图像金字塔方法](/img/cs131_opticalflow_pyramid.png)\n\n## 作业：基于光流法的帧间插值\n### 问题描述\n假设视频流中的相邻两帧$I_0$和$I_1$，分别标记其时刻为$t=0$和$t=1$。我们希望能够在这两帧之间生成新的插值帧$I_t, 0<t<1$。比如说你手头的视频是24帧的帧率，想在一台刷新频率为60Hz的显示器上播放，那么这项技术可以带来更流畅的观看体验。\n\n### 简单粗暴法\n我们可以简单粗暴地使用线性插值方法，简单的认为插值帧是第一帧和最后一帧的线性组合，也就是说：\n$$I_t = (1-t)I_0+tI_1$$\n\n这种方法称为\"cross-fading\"。效果如下。可以看到有较多的模糊抖动。\n![简单粗暴法效果](/img/cs131_opticalflow_assignment_crossfade.png)\n\n### 基于光流法\n使用光流可以知道像素点在图像平面的运动信息，从而在帧间建立点的对应关系。我们记像素点在水平方向和竖直方向的速度分别为$u_t(x,y)$和$v_t(x,y)$。我们可以根据$t=0$和$t=1$的两帧图像解出光流信息，即$u_0(x,y)$和$v_0(x,y)$。那么我们认为光流保持不变，就可以计算插值帧的某一点在$t=0$时候的对应点坐标。接下来，赋值就可以了。如下式所示：\n$$I_t(x+tu_0(x,y), y+tv_0(x,y)) = I_0(x,y)$$\n\n用MATLAB实现如下：\n``` matlab\nfor y =1:height\n    for x = 1:width\n        dy = min(max(round(y+v0(y,x)*t), 1), height);\n        dx = min(max(round(x+u0(y,x)*t), 1), width);\n        img(dy,dx,:) = img0(y,x,:);\n    end\nend\n```\n\n### 改进\n上面的式子假定光流不变，但是实际上光流很可能也是改变的。考虑到光流的变化，下式的计算应该更加接近：\n$$I_t(x,y) = I_0(x-tu_t(x,y), y-tv_t(x,y))$$\n\n然而，我们并不能得到$u_t$和$v_t$的准确值。我们使用下面的方法近似求解。\n","slug":"cs131-opticalflow","published":1,"updated":"2018-01-12T06:22:20.461Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjcu6vcs8000jqu46k5shk9as","content":"<p>光流法是通过检测图像像素点的强度随时间的变化进而推断出物体移动速度及方向的方法。由于成像物体与相机之间存在相对运动，导致其成像后的像素强度值不同。通过连续观测运动物体图像序列帧间的像素强度变化，就可以估计物体的运动信息。</p>\n<p><del>是你让我的世界从那刻变成粉红色</del>  划掉。。。<br><img src=\"/img/cs131_opticalflow_demo.jpg\" alt=\"OpticalFlow可视化\"><br><a id=\"more\"></a></p>\n<h2 id=\"光流的计算\"><a href=\"#光流的计算\" class=\"headerlink\" title=\"光流的计算\"></a>光流的计算</h2><p>光流（Optical Flow），是指图像中像素强度的“表象”运动。这里的表象运动，是指图像中的像素变化并不一定是由于运动造成的，还有可能是由于外界光照的变化引起的。</p>\n<p>光流估计就是指利用时间上相邻的两帧图像，得到点的运动。满足以下几点假设：</p>\n<ul>\n<li>前后两帧点的位移不大（泰勒展开）</li>\n<li>外界光强保持恒定。</li>\n<li>空间相关性，每个点的运动和他们的邻居相似（连续函数，泰勒展开）</li>\n</ul>\n<p>其中第二条外界光强保持恒定，可以从下面的等式来理解。<br><img src=\"/img/cs131_opticalflow_brightnessconstancy_assumption.png\" alt=\"光照强度保持恒定图解\"></p>\n<p>在相邻的两帧图像中，点$(x,y)$发生了位移$(u,v)$，那么移动前后两点的亮度应该是相等的。如下：</p>\n<script type=\"math/tex; mode=display\">I(x,y,t-1) = I(x+u, y+v, t)</script><p>从这个式子出发，我们将其利用Taylor展开做一阶线性近似。其中$I_x$, $I_y$, $I_t$分别是Image对这几个变量的偏导数。</p>\n<script type=\"math/tex; mode=display\">I(x+u,y+v,t) = I(x,y,t-1)+I_xu+I_yv+I_t</script><p>上面两式联立，可以得到，</p>\n<script type=\"math/tex; mode=display\">I_xu+I_yv+I_t=0</script><p>上式中，$I_x$, $I_y$可以通过图像沿$x$方向和$y$方向的导数计算，$I_t$可以通过$I(x,y,t)-I(x,y,t-1)$计算。未知数是$(u,v)$， 正是我们想要求解的每个像素在前后相邻两帧的位移。</p>\n<p>这里只有一个方程，却有两个未知数（实际是$N$个方程，$2N$个未知数，$N$是图像中待估计的像素点的个数，但是我们通过矩阵表示，将它们写成了如上式所述的紧凑形式），所以是一个不定方程。我们需要找出其它的约束求解方程。</p>\n<p>上面就是光流估计的基本思想。下面一节介绍估计光流的一种具体方法：Lucas-Kanade方法</p>\n<h2 id=\"L-K方法\"><a href=\"#L-K方法\" class=\"headerlink\" title=\"L-K方法\"></a>L-K方法</h2><p>上述式子虽然给出了光流估计的思路，但是还是没有办法解出位移量。L-K方法依据相邻像素之间的位移相似的假设，通过一个观察窗口，将窗口内的像素点的位移看做是相同的，建立了一个超定方程，使用最小二乘法进行求解。下面是观察窗口为$5\\times 5$的时候，建立的方程。<br><img src=\"/img/cs131_opticalflow_lkequation.png\" alt=\"L-K方程\"></p>\n<p>使用最小二乘法求解，可以得到如下的式子，求和号代表是对窗口内的每一个像素点求和。<br><img src=\"/img/cs131_opticalflow_lkleastsquare.png\" alt=\"最小二乘法后的式子\"></p>\n<p>上式即是L-K方法求解光流估计问题的方程。通过求解这个方程，就可以得到光流的估计$(u,v)$。但是上式什么时候有解呢？</p>\n<ul>\n<li>$\\mathbf{A}^\\dagger \\mathbf{A}$是可逆的。</li>\n<li>$\\mathbf{A}^\\dagger \\mathbf{A}$不应该太小（噪声）。这意味着它的特征值$\\lambda_1$, $\\lambda_2$不应该太小。</li>\n<li>$\\mathbf{A}^\\dagger \\mathbf{A}$不应该是病态的（稳定性）。这意味着它的特征值$\\lambda_1/\\lambda_2$不应该太大。</li>\n</ul>\n<p>而我们在Harris角点检测的时候已经讨论过$\\mathbf{A}^\\dagger \\mathbf{A}$这个矩阵的特征值情况了！也许，写成下面的形式更好看出来。<br><img src=\"/img/cs131_opticalflow_lkrelationshipwithharris.png\" alt=\"是不是和Harris角点更像了\"></p>\n<p>下面这张图就是当时的讨论结果。<br><img src=\"/img/cs131_opticalflow_lkharris.png\" alt=\"不同点的分类\"></p>\n<p>上面就是使用L-K方法估计光流的一般思路。</p>\n<h2 id=\"金字塔方法\"><a href=\"#金字塔方法\" class=\"headerlink\" title=\"金字塔方法\"></a>金字塔方法</h2><p>在最开始的假设中，第一条指出点的位移应该是较小的。从上面的分析可以看出，当位移较大时，Taylor展开式一阶近似误差较大。其修正方法就是这里要介绍的金字塔方法。我们通过将图像降采样，就能够使得较大的位移在高层金字塔图像中变小，满足假设条件1.如下所示。</p>\n<p><img src=\"/img/cs131_opticalflow_pyramid.png\" alt=\"图像金字塔方法\"></p>\n<h2 id=\"作业：基于光流法的帧间插值\"><a href=\"#作业：基于光流法的帧间插值\" class=\"headerlink\" title=\"作业：基于光流法的帧间插值\"></a>作业：基于光流法的帧间插值</h2><h3 id=\"问题描述\"><a href=\"#问题描述\" class=\"headerlink\" title=\"问题描述\"></a>问题描述</h3><p>假设视频流中的相邻两帧$I_0$和$I_1$，分别标记其时刻为$t=0$和$t=1$。我们希望能够在这两帧之间生成新的插值帧$I_t, 0&lt;t&lt;1$。比如说你手头的视频是24帧的帧率，想在一台刷新频率为60Hz的显示器上播放，那么这项技术可以带来更流畅的观看体验。</p>\n<h3 id=\"简单粗暴法\"><a href=\"#简单粗暴法\" class=\"headerlink\" title=\"简单粗暴法\"></a>简单粗暴法</h3><p>我们可以简单粗暴地使用线性插值方法，简单的认为插值帧是第一帧和最后一帧的线性组合，也就是说：</p>\n<script type=\"math/tex; mode=display\">I_t = (1-t)I_0+tI_1</script><p>这种方法称为”cross-fading”。效果如下。可以看到有较多的模糊抖动。<br><img src=\"/img/cs131_opticalflow_assignment_crossfade.png\" alt=\"简单粗暴法效果\"></p>\n<h3 id=\"基于光流法\"><a href=\"#基于光流法\" class=\"headerlink\" title=\"基于光流法\"></a>基于光流法</h3><p>使用光流可以知道像素点在图像平面的运动信息，从而在帧间建立点的对应关系。我们记像素点在水平方向和竖直方向的速度分别为$u_t(x,y)$和$v_t(x,y)$。我们可以根据$t=0$和$t=1$的两帧图像解出光流信息，即$u_0(x,y)$和$v_0(x,y)$。那么我们认为光流保持不变，就可以计算插值帧的某一点在$t=0$时候的对应点坐标。接下来，赋值就可以了。如下式所示：</p>\n<script type=\"math/tex; mode=display\">I_t(x+tu_0(x,y), y+tv_0(x,y)) = I_0(x,y)</script><p>用MATLAB实现如下：<br><figure class=\"highlight matlab\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">for</span> y =<span class=\"number\">1</span>:height</div><div class=\"line\">    <span class=\"keyword\">for</span> x = <span class=\"number\">1</span>:width</div><div class=\"line\">        dy = min(max(<span class=\"built_in\">round</span>(y+v0(y,x)*t), <span class=\"number\">1</span>), height);</div><div class=\"line\">        dx = min(max(<span class=\"built_in\">round</span>(x+u0(y,x)*t), <span class=\"number\">1</span>), width);</div><div class=\"line\">        img(dy,dx,:) = img0(y,x,:);</div><div class=\"line\">    <span class=\"keyword\">end</span></div><div class=\"line\"><span class=\"keyword\">end</span></div></pre></td></tr></table></figure></p>\n<h3 id=\"改进\"><a href=\"#改进\" class=\"headerlink\" title=\"改进\"></a>改进</h3><p>上面的式子假定光流不变，但是实际上光流很可能也是改变的。考虑到光流的变化，下式的计算应该更加接近：</p>\n<script type=\"math/tex; mode=display\">I_t(x,y) = I_0(x-tu_t(x,y), y-tv_t(x,y))</script><p>然而，我们并不能得到$u_t$和$v_t$的准确值。我们使用下面的方法近似求解。</p>\n","excerpt":"<p>光流法是通过检测图像像素点的强度随时间的变化进而推断出物体移动速度及方向的方法。由于成像物体与相机之间存在相对运动，导致其成像后的像素强度值不同。通过连续观测运动物体图像序列帧间的像素强度变化，就可以估计物体的运动信息。</p>\n<p><del>是你让我的世界从那刻变成粉红色</del>  划掉。。。<br><img src=\"/img/cs131_opticalflow_demo.jpg\" alt=\"OpticalFlow可视化\"><br>","more":"</p>\n<h2 id=\"光流的计算\"><a href=\"#光流的计算\" class=\"headerlink\" title=\"光流的计算\"></a>光流的计算</h2><p>光流（Optical Flow），是指图像中像素强度的“表象”运动。这里的表象运动，是指图像中的像素变化并不一定是由于运动造成的，还有可能是由于外界光照的变化引起的。</p>\n<p>光流估计就是指利用时间上相邻的两帧图像，得到点的运动。满足以下几点假设：</p>\n<ul>\n<li>前后两帧点的位移不大（泰勒展开）</li>\n<li>外界光强保持恒定。</li>\n<li>空间相关性，每个点的运动和他们的邻居相似（连续函数，泰勒展开）</li>\n</ul>\n<p>其中第二条外界光强保持恒定，可以从下面的等式来理解。<br><img src=\"/img/cs131_opticalflow_brightnessconstancy_assumption.png\" alt=\"光照强度保持恒定图解\"></p>\n<p>在相邻的两帧图像中，点$(x,y)$发生了位移$(u,v)$，那么移动前后两点的亮度应该是相等的。如下：</p>\n<script type=\"math/tex; mode=display\">I(x,y,t-1) = I(x+u, y+v, t)</script><p>从这个式子出发，我们将其利用Taylor展开做一阶线性近似。其中$I_x$, $I_y$, $I_t$分别是Image对这几个变量的偏导数。</p>\n<script type=\"math/tex; mode=display\">I(x+u,y+v,t) = I(x,y,t-1)+I_xu+I_yv+I_t</script><p>上面两式联立，可以得到，</p>\n<script type=\"math/tex; mode=display\">I_xu+I_yv+I_t=0</script><p>上式中，$I_x$, $I_y$可以通过图像沿$x$方向和$y$方向的导数计算，$I_t$可以通过$I(x,y,t)-I(x,y,t-1)$计算。未知数是$(u,v)$， 正是我们想要求解的每个像素在前后相邻两帧的位移。</p>\n<p>这里只有一个方程，却有两个未知数（实际是$N$个方程，$2N$个未知数，$N$是图像中待估计的像素点的个数，但是我们通过矩阵表示，将它们写成了如上式所述的紧凑形式），所以是一个不定方程。我们需要找出其它的约束求解方程。</p>\n<p>上面就是光流估计的基本思想。下面一节介绍估计光流的一种具体方法：Lucas-Kanade方法</p>\n<h2 id=\"L-K方法\"><a href=\"#L-K方法\" class=\"headerlink\" title=\"L-K方法\"></a>L-K方法</h2><p>上述式子虽然给出了光流估计的思路，但是还是没有办法解出位移量。L-K方法依据相邻像素之间的位移相似的假设，通过一个观察窗口，将窗口内的像素点的位移看做是相同的，建立了一个超定方程，使用最小二乘法进行求解。下面是观察窗口为$5\\times 5$的时候，建立的方程。<br><img src=\"/img/cs131_opticalflow_lkequation.png\" alt=\"L-K方程\"></p>\n<p>使用最小二乘法求解，可以得到如下的式子，求和号代表是对窗口内的每一个像素点求和。<br><img src=\"/img/cs131_opticalflow_lkleastsquare.png\" alt=\"最小二乘法后的式子\"></p>\n<p>上式即是L-K方法求解光流估计问题的方程。通过求解这个方程，就可以得到光流的估计$(u,v)$。但是上式什么时候有解呢？</p>\n<ul>\n<li>$\\mathbf{A}^\\dagger \\mathbf{A}$是可逆的。</li>\n<li>$\\mathbf{A}^\\dagger \\mathbf{A}$不应该太小（噪声）。这意味着它的特征值$\\lambda_1$, $\\lambda_2$不应该太小。</li>\n<li>$\\mathbf{A}^\\dagger \\mathbf{A}$不应该是病态的（稳定性）。这意味着它的特征值$\\lambda_1/\\lambda_2$不应该太大。</li>\n</ul>\n<p>而我们在Harris角点检测的时候已经讨论过$\\mathbf{A}^\\dagger \\mathbf{A}$这个矩阵的特征值情况了！也许，写成下面的形式更好看出来。<br><img src=\"/img/cs131_opticalflow_lkrelationshipwithharris.png\" alt=\"是不是和Harris角点更像了\"></p>\n<p>下面这张图就是当时的讨论结果。<br><img src=\"/img/cs131_opticalflow_lkharris.png\" alt=\"不同点的分类\"></p>\n<p>上面就是使用L-K方法估计光流的一般思路。</p>\n<h2 id=\"金字塔方法\"><a href=\"#金字塔方法\" class=\"headerlink\" title=\"金字塔方法\"></a>金字塔方法</h2><p>在最开始的假设中，第一条指出点的位移应该是较小的。从上面的分析可以看出，当位移较大时，Taylor展开式一阶近似误差较大。其修正方法就是这里要介绍的金字塔方法。我们通过将图像降采样，就能够使得较大的位移在高层金字塔图像中变小，满足假设条件1.如下所示。</p>\n<p><img src=\"/img/cs131_opticalflow_pyramid.png\" alt=\"图像金字塔方法\"></p>\n<h2 id=\"作业：基于光流法的帧间插值\"><a href=\"#作业：基于光流法的帧间插值\" class=\"headerlink\" title=\"作业：基于光流法的帧间插值\"></a>作业：基于光流法的帧间插值</h2><h3 id=\"问题描述\"><a href=\"#问题描述\" class=\"headerlink\" title=\"问题描述\"></a>问题描述</h3><p>假设视频流中的相邻两帧$I_0$和$I_1$，分别标记其时刻为$t=0$和$t=1$。我们希望能够在这两帧之间生成新的插值帧$I_t, 0&lt;t&lt;1$。比如说你手头的视频是24帧的帧率，想在一台刷新频率为60Hz的显示器上播放，那么这项技术可以带来更流畅的观看体验。</p>\n<h3 id=\"简单粗暴法\"><a href=\"#简单粗暴法\" class=\"headerlink\" title=\"简单粗暴法\"></a>简单粗暴法</h3><p>我们可以简单粗暴地使用线性插值方法，简单的认为插值帧是第一帧和最后一帧的线性组合，也就是说：</p>\n<script type=\"math/tex; mode=display\">I_t = (1-t)I_0+tI_1</script><p>这种方法称为”cross-fading”。效果如下。可以看到有较多的模糊抖动。<br><img src=\"/img/cs131_opticalflow_assignment_crossfade.png\" alt=\"简单粗暴法效果\"></p>\n<h3 id=\"基于光流法\"><a href=\"#基于光流法\" class=\"headerlink\" title=\"基于光流法\"></a>基于光流法</h3><p>使用光流可以知道像素点在图像平面的运动信息，从而在帧间建立点的对应关系。我们记像素点在水平方向和竖直方向的速度分别为$u_t(x,y)$和$v_t(x,y)$。我们可以根据$t=0$和$t=1$的两帧图像解出光流信息，即$u_0(x,y)$和$v_0(x,y)$。那么我们认为光流保持不变，就可以计算插值帧的某一点在$t=0$时候的对应点坐标。接下来，赋值就可以了。如下式所示：</p>\n<script type=\"math/tex; mode=display\">I_t(x+tu_0(x,y), y+tv_0(x,y)) = I_0(x,y)</script><p>用MATLAB实现如下：<br><figure class=\"highlight matlab\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">for</span> y =<span class=\"number\">1</span>:height</div><div class=\"line\">    <span class=\"keyword\">for</span> x = <span class=\"number\">1</span>:width</div><div class=\"line\">        dy = min(max(<span class=\"built_in\">round</span>(y+v0(y,x)*t), <span class=\"number\">1</span>), height);</div><div class=\"line\">        dx = min(max(<span class=\"built_in\">round</span>(x+u0(y,x)*t), <span class=\"number\">1</span>), width);</div><div class=\"line\">        img(dy,dx,:) = img0(y,x,:);</div><div class=\"line\">    <span class=\"keyword\">end</span></div><div class=\"line\"><span class=\"keyword\">end</span></div></pre></td></tr></table></figure></p>\n<h3 id=\"改进\"><a href=\"#改进\" class=\"headerlink\" title=\"改进\"></a>改进</h3><p>上面的式子假定光流不变，但是实际上光流很可能也是改变的。考虑到光流的变化，下式的计算应该更加接近：</p>\n<script type=\"math/tex; mode=display\">I_t(x,y) = I_0(x-tu_t(x,y), y-tv_t(x,y))</script><p>然而，我们并不能得到$u_t$和$v_t$的准确值。我们使用下面的方法近似求解。</p>"},{"title":"CS131-描述图像的特征(SIFT)","date":"2017-01-30T14:16:18.000Z","_content":"\n[SIFT(尺度不变特征变换，Scale Invariant Feature Transform)](https://en.wikipedia.org/wiki/Scale-invariant_feature_transform),最早由Lowe提出，目的是为了解决目标检测（Object Detection）问题中提取图像特征的问题。从名字可以看出，SIFT的优势就在于对尺度变换的不变性，同时SIFT还满足平移变换和旋转变换下的不变性，并对光照变化和3D视角变换有一定的不变性。它的主要步骤如下：\n- scale space上的极值检测。使用DoG在不同的scaleast和image position下找到interest point。\n- interest point的localization。对上面的interest point进行稳定度检测，并确定其所在的scale和position。\n- 确定方向。通过计算图像的梯度图，确定key point的方向，下一步的feature operation就是在这个方向，scale和position上进行的。\n- 确定key point的描述子。使用图像的局部梯度作为key point的描述子，最终构成SIFT特征向量。\n\n![SIFT图示](/img/sift_picture.jpg)\n\n<!-- more -->\n## SIFT介绍\n上讲中介绍的Harris角点方法计算简便，并具有平移不变性和旋转不变性。特征$f$对某种变换$\\mathcal{T}$具有不变性，是指在经过变换后原特征保持不变，也就是$f(I) = f(\\mathcal{T}(I))$。但是Harris角点不具有尺度变换不变性，如下图所示。当图像被放大后，原图的角点被判定为了边缘点。\n![harris的尺度变换不满足尺度不变性](/img/harris_non_scale_constant.png)\n\n而我们想要得到一种对尺度变换保持不变性的特征计算方法。例如图像patch的像素平均亮度，如下图所示。region size缩小为原始的一半后，亮度直方图的形状不变，即平均亮度不变。\n![平均亮度满足尺度变化呢不变性](/img/patch_average_intensity_scale_constant.png)\n\n而Lowe想到了使用局部极值来作为feature来保证对scale变换的不变性。在算法的具体实现中，他使用DoG来获得局部极值。\n\n[Lowe的论文](http://www.cs.ubc.ca/~lowe/papers/ijcv04.pdf)中也提到了SIFT特征的应用。SIFT特征可以产生稠密（dense）的key point，例如一张500x500像素大小的图像，一般可以产生~2000个稳定的SIFT特征。在进行image matching和recognition时，可以将ref image的SIFT特征提前计算出来保存在数据库中，并计算待处理图像的SIFT特征，根据特征向量的欧氏距离进行匹配。\n\n这篇博客主要是[Lowe上述论文](http://www.cs.ubc.ca/~lowe/papers/ijcv04.pdf)的读书笔记，按照SIFT特征的计算步骤进行组织。\n\n## 尺度空间极值的检测方法\n前人研究已经指出，在一系列合理假设下，高斯核是唯一满足尺度不变性的。所谓的尺度空间，就是指原始图像$I(x,y)$和可变尺度的高斯核$G(x,y,\\sigma)$的卷积结果。如下式所示：\n$$L(x,y,\\sigma) = G(x,y,\\sigma)\\ast I(x,y)$$\n\n其中，$G(x,y, \\sigma) = \\frac{1}{2\\pi\\sigma^2}\\exp(-(x^2+y^2)/2\\sigma^2)$。不同的$\\sigma$代表不同的尺度。\n\nDoG(difference of Gaussian)函数定义为不同尺度的高斯核与图像卷积结果之差，即，\n$$D(x,y,\\sigma) = L(x,y,k\\sigma) - L(x,y,\\sigma)$$\n\n如下图所示，输入的图像重复地与高斯核进行卷积得到结果图像$L$（左侧），这样构成了一个octave。相邻的$L$之间作差得到DoG图像（右侧）。当一个octave处理完之后，将当前octave的最后一张高斯卷积结果图像降采样两倍，按照前述方法构造下一个octave。在图中，我们将一个octave划分为$s$个间隔（即$s+1$个图像）时，设$\\sigma$最终变成了2倍（即$\\sigma$加倍）。那么，显然有相邻两层之间的$k = 2^{1/s}$。不过为了保证首尾两张图像也能够计算差值，我们实际上需要再补上两张（做一个padding），也就是说一个octave内的总图像为$s+3$（下图中的$s=2$）。\n![DoG的计算](/img/sift_dog.png)\n\n为何我们要费力气得到DoG呢？论文中作者给出的解释是：DoG对scale-normalized后的Guassian Laplace函数$\\sigma^2\\Delta G$提供了足够的近似。其中前面的$\\sigma^2$系数正是为了保证尺度不变性。而前人的研究则指出，$\\sigma \\Delta G$函数的极值，和其他一大票其他function比较时，能够提供最稳定的feature。\n\n对于高斯核函数，有以下性质：\n$$\\frac{\\partial G}{\\partial \\sigma} = \\sigma \\Delta G$$\n\n我们将式子左侧的微分变成差分，得到了下式：\n$$\\sigma\\Delta G \\approx \\frac{G(x,y,k\\sigma)-G(x,y,\\sigma)}{k\\sigma - \\sigma}$$\n\n也就是：\n$$G(x,y,k\\sigma)-G(x,y,\\sigma) \\approx (k-1)\\sigma^2 \\Delta G$$\n当$k=1$时，上式的近似误差为0（即上面的$s=\\infty$，也就是说octave的划分是连续的）。但是当$k$较大时，如$\\sqrt{2}$（这时$s=2$，octave划分十分粗糙了），仍然保证了稳定性。同时，由于相邻两层的比例$k$是定值，所以$k-1$这个因子对于检测极值没有影响。我们在计算时，无需除以$k-1$。\n\n构造完了DoG图像金字塔，下面我们的任务就是检测其中的极值。如下图，对于每个点，我们将它与金字塔中相邻的26个点作比较，找到局部极值。\n![检测极值](/img/sift_detection_maximum.png)\n\n另外，我们将输入图像行列预先使用线性插值的方法resize为原来的2倍，获取更多的key point。\n\n此外，在Lowe的论文中还提到了使用DoG的泰勒展开和海森矩阵进行key point的精细确定方法，并对key point进行过滤。这里也不再赘述了。\n\n## 128维feature的获取\n我们需要在每个key point处提取128维的特征向量。这里结合CS131课程作业2的相关练习作出说明。对于每个key point，我们关注它位于图像金字塔中的具体层数以及像素点位置，也就是说通过索引`pyramid{scale}(y, x)`就可以获取key point。我们遍历key point，为它们赋予一个128维的特征向量。\n\n我们选取point周围16x16大小的区域，称为一个patch。将其分为4x4共计16个cell。这样，每个cell内共有像素点16个。对于每个cell，我们计算它的局部梯度方向直方图，直方图共有8个bin。也就是说每个cell可以得到一个8维的特征向量。将16个cell的特征向量首尾相接，就得到了128维的SIFT特征向量。在下面的代码中，使用变量`patch_mag`和`patch_theta`分别代表patch的梯度幅值和角度，它们可以很简单地使用卷积和数学运算得到。\n\n``` matlab\npatch_mag = sqrt(patch_dx.^2 + patch_dy.^2);\npatch_theta = atan2(patch_dy, patch_dx);  % atan2的返回结果在区间[-pi, pi]上。\npatch_theta = mod(patch_theta, 2*pi);   % 这里我们要将其转换为[0, 2pi]\n```\n\n之后，我们需要获取key point的主方向。其定义可见slide，即为key point扩展出来的patch的梯度方向直方图的峰值对应的角度。\n![何为主方向](/img/sift_dominant_orientation.png)\n\n所以我们首先应该设计如何构建局部梯度方向直方图。我们只要将`[0, 2pi]`区间划分为若干个`bin`，并将patch内的每个点使用其梯度大小向对应的`bin`内投票即可。如下所示：\n\n``` matlab\nfunction [histogram, angles] = ComputeGradientHistogram(num_bins, gradient_magnitudes, gradient_angles)\n% Compute a gradient histogram using gradient magnitudes and directions.\n% Each point is assigned to one of num_bins depending on its gradient\n% direction; the gradient magnitude of that point is added to its bin.\n%\n% INPUT\n% num_bins: The number of bins to which points should be assigned.\n% gradient_magnitudes, gradient angles:\n%       Two arrays of the same shape where gradient_magnitudes(i) and\n%       gradient_angles(i) give the magnitude and direction of the gradient\n%       for the ith point. gradient_angles ranges from 0 to 2*pi\n%                                      \n% OUTPUT\n% histogram: A 1 x num_bins array containing the gradient histogram. Entry 1 is\n%       the sum of entries in gradient_magnitudes whose corresponding\n%       gradient_angles lie between 0 and angle_step. Similarly, entry 2 is for\n%       angles between angle_step and 2*angle_step. Angle_step is calculated as\n%       2*pi/num_bins.\n\n% angles: A 1 x num_bins array which holds the histogram bin lower bounds.\n%       In other words, histogram(i) contains the sum of the\n%       gradient magnitudes of all points whose gradient directions fall\n%       in the range [angles(i), angles(i + 1))\n\n    angle_step = 2 * pi / num_bins;\n    angles = 0 : angle_step : (2*pi-angle_step);\n\n    histogram = zeros(1, num_bins);\n    num = numel(gradient_angles);\n    for n = 1:num\n        index = floor(gradient_angles(n) / angle_step) + 1;\n        histogram(index) = histogram(index) + gradient_magnitudes(n);\n    end    \nend\n\n```\n\nLowe论文中推荐的`bin`数目为36个，计算主方向的函数如下：\n\n``` matlab\nfunction direction = ComputeDominantDirection(gradient_magnitudes, gradient_angles)\n% Computes the dominant gradient direction for the region around a keypoint\n% given the scale of the keypoint and the gradient magnitudes and gradient\n% angles of the pixels in the region surrounding the keypoint.\n%\n% INPUT\n% gradient_magnitudes, gradient_angles:\n%   Two arrays of the same shape where gradient_magnitudes(i) and\n%   gradient_angles(i) give the magnitude and direction of the gradient for\n%   the ith point.\n\n    % Compute a gradient histogram using the weighted gradient magnitudes.\n    % In David Lowe's paper he suggests using 36 bins for this histogram.\n    num_bins = 36;\n    % Step 1:\n    % compute the 36-bin histogram of angles using ComputeGradientHistogram()\n    [histogram, angle_bound] = ComputeGradientHistogram(num_bins, gradient_magnitudes, gradient_angles);\n    % Step 2:\n    % Find the maximum value of the gradient histogram, and set \"direction\"\n    % to the angle corresponding to the maximum. (To match our solutions,\n    % just use the lower-bound angle of the max histogram bin. (E.g. return\n    % 0 radians if it's bin 1.)\n    [~, max_index] = max(histogram);\n    direction = angle_bound(max_index);\nend\n```\n\n之后，我们更新patch内各点的梯度方向，计算其与主方向的夹角，作为新的方向。并将梯度进行高斯平滑。\n\n``` matlab\npatch_theta = patch_theta - ComputeDominantDirection(patch_mag, patch_theta);;\npatch_theta = mod(patch_theta, 2*pi);\npatch_mag = patch_mag .* fspecial('gaussian', patch_size, patch_size / 2); % patch_size = 16\n```\n\n遍历cell，计算feature如下：\n\n``` matlab\nfeature = [];\nrow_iter = 1;\nfor y = 1:num_histograms\n    col_iter = 1;\n    for x = 1:num_histograms\n        cell_mag = patch_mag(row_iter: row_iter + pixelsPerHistogram - 1, ...\n                             col_iter: col_iter + pixelsPerHistogram - 1);\n        cell_theta = patch_theta(row_iter: row_iter + pixelsPerHistogram - 1, ...\n                             col_iter: col_iter + pixelsPerHistogram - 1);\n        [histogram, ~] = ComputeGradientHistogram(num_angles, cell_mag, cell_theta);\n        feature = [feature, histogram];\n        col_iter = col_iter + pixelsPerHistogram;\n    end\n    row_iter = row_iter + pixelsPerHistogram;\nend\n```\n\n最后，对feature做normalization。首先将feature化为单位长度，并将其中太大的分量进行限幅（如0.2的阈值），之后再重新将其转换为单位长度。\n\n这样，就完成了SIFT特征的计算。在Lowe的论文中，有更多对实现细节的讨论，这里只是跟随课程slide和作业走完了算法流程，不再赘述。\n\n## 应用：图像特征点匹配\n和Harris角点一样，SIFT特征可以用作两幅图像的特征点匹配，并且具有多种变换不变性的优点。对于两幅图像分别计算得到的特征点SIFT特征向量，可以使用下面简单的方法搜寻匹配点。计算每组点对之间的欧式距离，如果最近的距离比第二近的距离小得多，那么可以认为有一对成功匹配的特征点。其MATLAB代码如下，其中`descriptor`是两幅图像的SIFT特征向量。阈值默认为取做0.7。\n\n``` matlab\nfunction match = SIFTSimpleMatcher(descriptor1, descriptor2, thresh)\n% SIFTSimpleMatcher\n%   Match one set of SIFT descriptors (descriptor1) to another set of\n%   descriptors (decriptor2). Each descriptor from descriptor1 can at\n%   most be matched to one member of descriptor2, but descriptors from\n%   descriptor2 can be matched more than once.\n%   \n%   Matches are determined as follows:\n%   For each descriptor vector in descriptor1, find the Euclidean distance\n%   between it and each descriptor vector in descriptor2. If the smallest\n%   distance is less than thresh*(the next smallest distance), we say that\n%   the two vectors are a match, and we add the row [d1 index, d2 index] to\n%   the \"match\" array.\n%   \n% INPUT:\n%   descriptor1: N1 * 128 matrix, each row is a SIFT descriptor.\n%   descriptor2: N2 * 128 matrix, each row is a SIFT descriptor.\n%   thresh: a given threshold of ratio. Typically 0.7\n%\n% OUTPUT:\n%   Match: N * 2 matrix, each row is a match.\n%          For example, Match(k, :) = [i, j] means i-th descriptor in\n%          descriptor1 is matched to j-th descriptor in descriptor2.\n    if ~exist('thresh', 'var'),\n        thresh = 0.7;\n    end\n\n    match = [];\n    [N1, ~] = size(descriptor1);\n    for i = 1:N1\n        fea = descriptor1(i, :);\n        err = bsxfun(@minus, fea, descriptor2);\n        dis = sqrt(sum(err.^2, 2));\n        [sorted_dis, ind] = sort(dis, 1);\n        if sorted_dis(1) < thresh * sorted_dis(2)\n            match = [match; [i, ind(1)]];\n        end\n    end\nend\n\n```\n\n接下来，我们可以使用最小二乘法计算两幅图像之间的仿射变换矩阵（齐次变换矩阵）$H$。其中$H$满足：\n$$Hp_{\\text{before}} = p_{\\text{after}}$$\n\n其中\n$$p = \\begin{bmatrix}x \\\\\\\\ y \\\\\\\\ 1\\end{bmatrix}$$\n\n对上式稍作变形，有\n$$p_{\\text{before}}^\\dagger H^\\dagger = p_{\\text{after}}\\dagger$$\n\n就可以使用标准的最小二乘正则方程进行求解了。代码如下：\n\n``` matlab\nfunction H = ComputeAffineMatrix( Pt1, Pt2 )\n%ComputeAffineMatrix\n%   Computes the transformation matrix that transforms a point from\n%   coordinate frame 1 to coordinate frame 2\n%Input:\n%   Pt1: N * 2 matrix, each row is a point in image 1\n%       (N must be at least 3)\n%   Pt2: N * 2 matrix, each row is the point in image 2 that\n%       matches the same point in image 1 (N should be more than 3)\n%Output:\n%   H: 3 * 3 affine transformation matrix,\n%       such that H*pt1(i,:) = pt2(i,:)\n\n    N = size(Pt1,1);\n    if size(Pt1, 1) ~= size(Pt2, 1),\n        error('Dimensions unmatched.');\n    elseif N<3\n        error('At least 3 points are required.');\n    end\n\n    % Convert the input points to homogeneous coordintes.\n    P1 = [Pt1';ones(1,N)];\n    P2 = [Pt2';ones(1,N)];\n\n    H = P1*P1'\\P1*P2';\n    H = H';\n\n    % Sometimes numerical issues cause least-squares to produce a bottom\n    % row which is not exactly [0 0 1], which confuses some of the later\n    % code. So we'll ensure the bottom row is exactly [0 0 1].\n    H(3,:) = [0 0 1];\nend\n```\n\n作业中的其他例子也很有趣，这里不再多说了。贴上两张图像拼接的实验结果吧~\n![result 1](/img/sift_experiment_1.png)\n![result 2](/img/sift_experiment_2.png)\n","source":"_posts/cs131-sift.md","raw":"---\ntitle: CS131-描述图像的特征(SIFT)\ndate: 2017-01-30 22:16:18\ntags:\n     - cs131\n     - 公开课\n---\n\n[SIFT(尺度不变特征变换，Scale Invariant Feature Transform)](https://en.wikipedia.org/wiki/Scale-invariant_feature_transform),最早由Lowe提出，目的是为了解决目标检测（Object Detection）问题中提取图像特征的问题。从名字可以看出，SIFT的优势就在于对尺度变换的不变性，同时SIFT还满足平移变换和旋转变换下的不变性，并对光照变化和3D视角变换有一定的不变性。它的主要步骤如下：\n- scale space上的极值检测。使用DoG在不同的scaleast和image position下找到interest point。\n- interest point的localization。对上面的interest point进行稳定度检测，并确定其所在的scale和position。\n- 确定方向。通过计算图像的梯度图，确定key point的方向，下一步的feature operation就是在这个方向，scale和position上进行的。\n- 确定key point的描述子。使用图像的局部梯度作为key point的描述子，最终构成SIFT特征向量。\n\n![SIFT图示](/img/sift_picture.jpg)\n\n<!-- more -->\n## SIFT介绍\n上讲中介绍的Harris角点方法计算简便，并具有平移不变性和旋转不变性。特征$f$对某种变换$\\mathcal{T}$具有不变性，是指在经过变换后原特征保持不变，也就是$f(I) = f(\\mathcal{T}(I))$。但是Harris角点不具有尺度变换不变性，如下图所示。当图像被放大后，原图的角点被判定为了边缘点。\n![harris的尺度变换不满足尺度不变性](/img/harris_non_scale_constant.png)\n\n而我们想要得到一种对尺度变换保持不变性的特征计算方法。例如图像patch的像素平均亮度，如下图所示。region size缩小为原始的一半后，亮度直方图的形状不变，即平均亮度不变。\n![平均亮度满足尺度变化呢不变性](/img/patch_average_intensity_scale_constant.png)\n\n而Lowe想到了使用局部极值来作为feature来保证对scale变换的不变性。在算法的具体实现中，他使用DoG来获得局部极值。\n\n[Lowe的论文](http://www.cs.ubc.ca/~lowe/papers/ijcv04.pdf)中也提到了SIFT特征的应用。SIFT特征可以产生稠密（dense）的key point，例如一张500x500像素大小的图像，一般可以产生~2000个稳定的SIFT特征。在进行image matching和recognition时，可以将ref image的SIFT特征提前计算出来保存在数据库中，并计算待处理图像的SIFT特征，根据特征向量的欧氏距离进行匹配。\n\n这篇博客主要是[Lowe上述论文](http://www.cs.ubc.ca/~lowe/papers/ijcv04.pdf)的读书笔记，按照SIFT特征的计算步骤进行组织。\n\n## 尺度空间极值的检测方法\n前人研究已经指出，在一系列合理假设下，高斯核是唯一满足尺度不变性的。所谓的尺度空间，就是指原始图像$I(x,y)$和可变尺度的高斯核$G(x,y,\\sigma)$的卷积结果。如下式所示：\n$$L(x,y,\\sigma) = G(x,y,\\sigma)\\ast I(x,y)$$\n\n其中，$G(x,y, \\sigma) = \\frac{1}{2\\pi\\sigma^2}\\exp(-(x^2+y^2)/2\\sigma^2)$。不同的$\\sigma$代表不同的尺度。\n\nDoG(difference of Gaussian)函数定义为不同尺度的高斯核与图像卷积结果之差，即，\n$$D(x,y,\\sigma) = L(x,y,k\\sigma) - L(x,y,\\sigma)$$\n\n如下图所示，输入的图像重复地与高斯核进行卷积得到结果图像$L$（左侧），这样构成了一个octave。相邻的$L$之间作差得到DoG图像（右侧）。当一个octave处理完之后，将当前octave的最后一张高斯卷积结果图像降采样两倍，按照前述方法构造下一个octave。在图中，我们将一个octave划分为$s$个间隔（即$s+1$个图像）时，设$\\sigma$最终变成了2倍（即$\\sigma$加倍）。那么，显然有相邻两层之间的$k = 2^{1/s}$。不过为了保证首尾两张图像也能够计算差值，我们实际上需要再补上两张（做一个padding），也就是说一个octave内的总图像为$s+3$（下图中的$s=2$）。\n![DoG的计算](/img/sift_dog.png)\n\n为何我们要费力气得到DoG呢？论文中作者给出的解释是：DoG对scale-normalized后的Guassian Laplace函数$\\sigma^2\\Delta G$提供了足够的近似。其中前面的$\\sigma^2$系数正是为了保证尺度不变性。而前人的研究则指出，$\\sigma \\Delta G$函数的极值，和其他一大票其他function比较时，能够提供最稳定的feature。\n\n对于高斯核函数，有以下性质：\n$$\\frac{\\partial G}{\\partial \\sigma} = \\sigma \\Delta G$$\n\n我们将式子左侧的微分变成差分，得到了下式：\n$$\\sigma\\Delta G \\approx \\frac{G(x,y,k\\sigma)-G(x,y,\\sigma)}{k\\sigma - \\sigma}$$\n\n也就是：\n$$G(x,y,k\\sigma)-G(x,y,\\sigma) \\approx (k-1)\\sigma^2 \\Delta G$$\n当$k=1$时，上式的近似误差为0（即上面的$s=\\infty$，也就是说octave的划分是连续的）。但是当$k$较大时，如$\\sqrt{2}$（这时$s=2$，octave划分十分粗糙了），仍然保证了稳定性。同时，由于相邻两层的比例$k$是定值，所以$k-1$这个因子对于检测极值没有影响。我们在计算时，无需除以$k-1$。\n\n构造完了DoG图像金字塔，下面我们的任务就是检测其中的极值。如下图，对于每个点，我们将它与金字塔中相邻的26个点作比较，找到局部极值。\n![检测极值](/img/sift_detection_maximum.png)\n\n另外，我们将输入图像行列预先使用线性插值的方法resize为原来的2倍，获取更多的key point。\n\n此外，在Lowe的论文中还提到了使用DoG的泰勒展开和海森矩阵进行key point的精细确定方法，并对key point进行过滤。这里也不再赘述了。\n\n## 128维feature的获取\n我们需要在每个key point处提取128维的特征向量。这里结合CS131课程作业2的相关练习作出说明。对于每个key point，我们关注它位于图像金字塔中的具体层数以及像素点位置，也就是说通过索引`pyramid{scale}(y, x)`就可以获取key point。我们遍历key point，为它们赋予一个128维的特征向量。\n\n我们选取point周围16x16大小的区域，称为一个patch。将其分为4x4共计16个cell。这样，每个cell内共有像素点16个。对于每个cell，我们计算它的局部梯度方向直方图，直方图共有8个bin。也就是说每个cell可以得到一个8维的特征向量。将16个cell的特征向量首尾相接，就得到了128维的SIFT特征向量。在下面的代码中，使用变量`patch_mag`和`patch_theta`分别代表patch的梯度幅值和角度，它们可以很简单地使用卷积和数学运算得到。\n\n``` matlab\npatch_mag = sqrt(patch_dx.^2 + patch_dy.^2);\npatch_theta = atan2(patch_dy, patch_dx);  % atan2的返回结果在区间[-pi, pi]上。\npatch_theta = mod(patch_theta, 2*pi);   % 这里我们要将其转换为[0, 2pi]\n```\n\n之后，我们需要获取key point的主方向。其定义可见slide，即为key point扩展出来的patch的梯度方向直方图的峰值对应的角度。\n![何为主方向](/img/sift_dominant_orientation.png)\n\n所以我们首先应该设计如何构建局部梯度方向直方图。我们只要将`[0, 2pi]`区间划分为若干个`bin`，并将patch内的每个点使用其梯度大小向对应的`bin`内投票即可。如下所示：\n\n``` matlab\nfunction [histogram, angles] = ComputeGradientHistogram(num_bins, gradient_magnitudes, gradient_angles)\n% Compute a gradient histogram using gradient magnitudes and directions.\n% Each point is assigned to one of num_bins depending on its gradient\n% direction; the gradient magnitude of that point is added to its bin.\n%\n% INPUT\n% num_bins: The number of bins to which points should be assigned.\n% gradient_magnitudes, gradient angles:\n%       Two arrays of the same shape where gradient_magnitudes(i) and\n%       gradient_angles(i) give the magnitude and direction of the gradient\n%       for the ith point. gradient_angles ranges from 0 to 2*pi\n%                                      \n% OUTPUT\n% histogram: A 1 x num_bins array containing the gradient histogram. Entry 1 is\n%       the sum of entries in gradient_magnitudes whose corresponding\n%       gradient_angles lie between 0 and angle_step. Similarly, entry 2 is for\n%       angles between angle_step and 2*angle_step. Angle_step is calculated as\n%       2*pi/num_bins.\n\n% angles: A 1 x num_bins array which holds the histogram bin lower bounds.\n%       In other words, histogram(i) contains the sum of the\n%       gradient magnitudes of all points whose gradient directions fall\n%       in the range [angles(i), angles(i + 1))\n\n    angle_step = 2 * pi / num_bins;\n    angles = 0 : angle_step : (2*pi-angle_step);\n\n    histogram = zeros(1, num_bins);\n    num = numel(gradient_angles);\n    for n = 1:num\n        index = floor(gradient_angles(n) / angle_step) + 1;\n        histogram(index) = histogram(index) + gradient_magnitudes(n);\n    end    \nend\n\n```\n\nLowe论文中推荐的`bin`数目为36个，计算主方向的函数如下：\n\n``` matlab\nfunction direction = ComputeDominantDirection(gradient_magnitudes, gradient_angles)\n% Computes the dominant gradient direction for the region around a keypoint\n% given the scale of the keypoint and the gradient magnitudes and gradient\n% angles of the pixels in the region surrounding the keypoint.\n%\n% INPUT\n% gradient_magnitudes, gradient_angles:\n%   Two arrays of the same shape where gradient_magnitudes(i) and\n%   gradient_angles(i) give the magnitude and direction of the gradient for\n%   the ith point.\n\n    % Compute a gradient histogram using the weighted gradient magnitudes.\n    % In David Lowe's paper he suggests using 36 bins for this histogram.\n    num_bins = 36;\n    % Step 1:\n    % compute the 36-bin histogram of angles using ComputeGradientHistogram()\n    [histogram, angle_bound] = ComputeGradientHistogram(num_bins, gradient_magnitudes, gradient_angles);\n    % Step 2:\n    % Find the maximum value of the gradient histogram, and set \"direction\"\n    % to the angle corresponding to the maximum. (To match our solutions,\n    % just use the lower-bound angle of the max histogram bin. (E.g. return\n    % 0 radians if it's bin 1.)\n    [~, max_index] = max(histogram);\n    direction = angle_bound(max_index);\nend\n```\n\n之后，我们更新patch内各点的梯度方向，计算其与主方向的夹角，作为新的方向。并将梯度进行高斯平滑。\n\n``` matlab\npatch_theta = patch_theta - ComputeDominantDirection(patch_mag, patch_theta);;\npatch_theta = mod(patch_theta, 2*pi);\npatch_mag = patch_mag .* fspecial('gaussian', patch_size, patch_size / 2); % patch_size = 16\n```\n\n遍历cell，计算feature如下：\n\n``` matlab\nfeature = [];\nrow_iter = 1;\nfor y = 1:num_histograms\n    col_iter = 1;\n    for x = 1:num_histograms\n        cell_mag = patch_mag(row_iter: row_iter + pixelsPerHistogram - 1, ...\n                             col_iter: col_iter + pixelsPerHistogram - 1);\n        cell_theta = patch_theta(row_iter: row_iter + pixelsPerHistogram - 1, ...\n                             col_iter: col_iter + pixelsPerHistogram - 1);\n        [histogram, ~] = ComputeGradientHistogram(num_angles, cell_mag, cell_theta);\n        feature = [feature, histogram];\n        col_iter = col_iter + pixelsPerHistogram;\n    end\n    row_iter = row_iter + pixelsPerHistogram;\nend\n```\n\n最后，对feature做normalization。首先将feature化为单位长度，并将其中太大的分量进行限幅（如0.2的阈值），之后再重新将其转换为单位长度。\n\n这样，就完成了SIFT特征的计算。在Lowe的论文中，有更多对实现细节的讨论，这里只是跟随课程slide和作业走完了算法流程，不再赘述。\n\n## 应用：图像特征点匹配\n和Harris角点一样，SIFT特征可以用作两幅图像的特征点匹配，并且具有多种变换不变性的优点。对于两幅图像分别计算得到的特征点SIFT特征向量，可以使用下面简单的方法搜寻匹配点。计算每组点对之间的欧式距离，如果最近的距离比第二近的距离小得多，那么可以认为有一对成功匹配的特征点。其MATLAB代码如下，其中`descriptor`是两幅图像的SIFT特征向量。阈值默认为取做0.7。\n\n``` matlab\nfunction match = SIFTSimpleMatcher(descriptor1, descriptor2, thresh)\n% SIFTSimpleMatcher\n%   Match one set of SIFT descriptors (descriptor1) to another set of\n%   descriptors (decriptor2). Each descriptor from descriptor1 can at\n%   most be matched to one member of descriptor2, but descriptors from\n%   descriptor2 can be matched more than once.\n%   \n%   Matches are determined as follows:\n%   For each descriptor vector in descriptor1, find the Euclidean distance\n%   between it and each descriptor vector in descriptor2. If the smallest\n%   distance is less than thresh*(the next smallest distance), we say that\n%   the two vectors are a match, and we add the row [d1 index, d2 index] to\n%   the \"match\" array.\n%   \n% INPUT:\n%   descriptor1: N1 * 128 matrix, each row is a SIFT descriptor.\n%   descriptor2: N2 * 128 matrix, each row is a SIFT descriptor.\n%   thresh: a given threshold of ratio. Typically 0.7\n%\n% OUTPUT:\n%   Match: N * 2 matrix, each row is a match.\n%          For example, Match(k, :) = [i, j] means i-th descriptor in\n%          descriptor1 is matched to j-th descriptor in descriptor2.\n    if ~exist('thresh', 'var'),\n        thresh = 0.7;\n    end\n\n    match = [];\n    [N1, ~] = size(descriptor1);\n    for i = 1:N1\n        fea = descriptor1(i, :);\n        err = bsxfun(@minus, fea, descriptor2);\n        dis = sqrt(sum(err.^2, 2));\n        [sorted_dis, ind] = sort(dis, 1);\n        if sorted_dis(1) < thresh * sorted_dis(2)\n            match = [match; [i, ind(1)]];\n        end\n    end\nend\n\n```\n\n接下来，我们可以使用最小二乘法计算两幅图像之间的仿射变换矩阵（齐次变换矩阵）$H$。其中$H$满足：\n$$Hp_{\\text{before}} = p_{\\text{after}}$$\n\n其中\n$$p = \\begin{bmatrix}x \\\\\\\\ y \\\\\\\\ 1\\end{bmatrix}$$\n\n对上式稍作变形，有\n$$p_{\\text{before}}^\\dagger H^\\dagger = p_{\\text{after}}\\dagger$$\n\n就可以使用标准的最小二乘正则方程进行求解了。代码如下：\n\n``` matlab\nfunction H = ComputeAffineMatrix( Pt1, Pt2 )\n%ComputeAffineMatrix\n%   Computes the transformation matrix that transforms a point from\n%   coordinate frame 1 to coordinate frame 2\n%Input:\n%   Pt1: N * 2 matrix, each row is a point in image 1\n%       (N must be at least 3)\n%   Pt2: N * 2 matrix, each row is the point in image 2 that\n%       matches the same point in image 1 (N should be more than 3)\n%Output:\n%   H: 3 * 3 affine transformation matrix,\n%       such that H*pt1(i,:) = pt2(i,:)\n\n    N = size(Pt1,1);\n    if size(Pt1, 1) ~= size(Pt2, 1),\n        error('Dimensions unmatched.');\n    elseif N<3\n        error('At least 3 points are required.');\n    end\n\n    % Convert the input points to homogeneous coordintes.\n    P1 = [Pt1';ones(1,N)];\n    P2 = [Pt2';ones(1,N)];\n\n    H = P1*P1'\\P1*P2';\n    H = H';\n\n    % Sometimes numerical issues cause least-squares to produce a bottom\n    % row which is not exactly [0 0 1], which confuses some of the later\n    % code. So we'll ensure the bottom row is exactly [0 0 1].\n    H(3,:) = [0 0 1];\nend\n```\n\n作业中的其他例子也很有趣，这里不再多说了。贴上两张图像拼接的实验结果吧~\n![result 1](/img/sift_experiment_1.png)\n![result 2](/img/sift_experiment_2.png)\n","slug":"cs131-sift","published":1,"updated":"2018-01-12T06:22:20.462Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjcu6vcsb000lqu46mg8mu8jz","content":"<p><a href=\"https://en.wikipedia.org/wiki/Scale-invariant_feature_transform\" target=\"_blank\" rel=\"external\">SIFT(尺度不变特征变换，Scale Invariant Feature Transform)</a>,最早由Lowe提出，目的是为了解决目标检测（Object Detection）问题中提取图像特征的问题。从名字可以看出，SIFT的优势就在于对尺度变换的不变性，同时SIFT还满足平移变换和旋转变换下的不变性，并对光照变化和3D视角变换有一定的不变性。它的主要步骤如下：</p>\n<ul>\n<li>scale space上的极值检测。使用DoG在不同的scaleast和image position下找到interest point。</li>\n<li>interest point的localization。对上面的interest point进行稳定度检测，并确定其所在的scale和position。</li>\n<li>确定方向。通过计算图像的梯度图，确定key point的方向，下一步的feature operation就是在这个方向，scale和position上进行的。</li>\n<li>确定key point的描述子。使用图像的局部梯度作为key point的描述子，最终构成SIFT特征向量。</li>\n</ul>\n<p><img src=\"/img/sift_picture.jpg\" alt=\"SIFT图示\"></p>\n<a id=\"more\"></a>\n<h2 id=\"SIFT介绍\"><a href=\"#SIFT介绍\" class=\"headerlink\" title=\"SIFT介绍\"></a>SIFT介绍</h2><p>上讲中介绍的Harris角点方法计算简便，并具有平移不变性和旋转不变性。特征$f$对某种变换$\\mathcal{T}$具有不变性，是指在经过变换后原特征保持不变，也就是$f(I) = f(\\mathcal{T}(I))$。但是Harris角点不具有尺度变换不变性，如下图所示。当图像被放大后，原图的角点被判定为了边缘点。<br><img src=\"/img/harris_non_scale_constant.png\" alt=\"harris的尺度变换不满足尺度不变性\"></p>\n<p>而我们想要得到一种对尺度变换保持不变性的特征计算方法。例如图像patch的像素平均亮度，如下图所示。region size缩小为原始的一半后，亮度直方图的形状不变，即平均亮度不变。<br><img src=\"/img/patch_average_intensity_scale_constant.png\" alt=\"平均亮度满足尺度变化呢不变性\"></p>\n<p>而Lowe想到了使用局部极值来作为feature来保证对scale变换的不变性。在算法的具体实现中，他使用DoG来获得局部极值。</p>\n<p><a href=\"http://www.cs.ubc.ca/~lowe/papers/ijcv04.pdf\" target=\"_blank\" rel=\"external\">Lowe的论文</a>中也提到了SIFT特征的应用。SIFT特征可以产生稠密（dense）的key point，例如一张500x500像素大小的图像，一般可以产生~2000个稳定的SIFT特征。在进行image matching和recognition时，可以将ref image的SIFT特征提前计算出来保存在数据库中，并计算待处理图像的SIFT特征，根据特征向量的欧氏距离进行匹配。</p>\n<p>这篇博客主要是<a href=\"http://www.cs.ubc.ca/~lowe/papers/ijcv04.pdf\" target=\"_blank\" rel=\"external\">Lowe上述论文</a>的读书笔记，按照SIFT特征的计算步骤进行组织。</p>\n<h2 id=\"尺度空间极值的检测方法\"><a href=\"#尺度空间极值的检测方法\" class=\"headerlink\" title=\"尺度空间极值的检测方法\"></a>尺度空间极值的检测方法</h2><p>前人研究已经指出，在一系列合理假设下，高斯核是唯一满足尺度不变性的。所谓的尺度空间，就是指原始图像$I(x,y)$和可变尺度的高斯核$G(x,y,\\sigma)$的卷积结果。如下式所示：</p>\n<script type=\"math/tex; mode=display\">L(x,y,\\sigma) = G(x,y,\\sigma)\\ast I(x,y)</script><p>其中，$G(x,y, \\sigma) = \\frac{1}{2\\pi\\sigma^2}\\exp(-(x^2+y^2)/2\\sigma^2)$。不同的$\\sigma$代表不同的尺度。</p>\n<p>DoG(difference of Gaussian)函数定义为不同尺度的高斯核与图像卷积结果之差，即，</p>\n<script type=\"math/tex; mode=display\">D(x,y,\\sigma) = L(x,y,k\\sigma) - L(x,y,\\sigma)</script><p>如下图所示，输入的图像重复地与高斯核进行卷积得到结果图像$L$（左侧），这样构成了一个octave。相邻的$L$之间作差得到DoG图像（右侧）。当一个octave处理完之后，将当前octave的最后一张高斯卷积结果图像降采样两倍，按照前述方法构造下一个octave。在图中，我们将一个octave划分为$s$个间隔（即$s+1$个图像）时，设$\\sigma$最终变成了2倍（即$\\sigma$加倍）。那么，显然有相邻两层之间的$k = 2^{1/s}$。不过为了保证首尾两张图像也能够计算差值，我们实际上需要再补上两张（做一个padding），也就是说一个octave内的总图像为$s+3$（下图中的$s=2$）。<br><img src=\"/img/sift_dog.png\" alt=\"DoG的计算\"></p>\n<p>为何我们要费力气得到DoG呢？论文中作者给出的解释是：DoG对scale-normalized后的Guassian Laplace函数$\\sigma^2\\Delta G$提供了足够的近似。其中前面的$\\sigma^2$系数正是为了保证尺度不变性。而前人的研究则指出，$\\sigma \\Delta G$函数的极值，和其他一大票其他function比较时，能够提供最稳定的feature。</p>\n<p>对于高斯核函数，有以下性质：</p>\n<script type=\"math/tex; mode=display\">\\frac{\\partial G}{\\partial \\sigma} = \\sigma \\Delta G</script><p>我们将式子左侧的微分变成差分，得到了下式：</p>\n<script type=\"math/tex; mode=display\">\\sigma\\Delta G \\approx \\frac{G(x,y,k\\sigma)-G(x,y,\\sigma)}{k\\sigma - \\sigma}</script><p>也就是：</p>\n<script type=\"math/tex; mode=display\">G(x,y,k\\sigma)-G(x,y,\\sigma) \\approx (k-1)\\sigma^2 \\Delta G</script><p>当$k=1$时，上式的近似误差为0（即上面的$s=\\infty$，也就是说octave的划分是连续的）。但是当$k$较大时，如$\\sqrt{2}$（这时$s=2$，octave划分十分粗糙了），仍然保证了稳定性。同时，由于相邻两层的比例$k$是定值，所以$k-1$这个因子对于检测极值没有影响。我们在计算时，无需除以$k-1$。</p>\n<p>构造完了DoG图像金字塔，下面我们的任务就是检测其中的极值。如下图，对于每个点，我们将它与金字塔中相邻的26个点作比较，找到局部极值。<br><img src=\"/img/sift_detection_maximum.png\" alt=\"检测极值\"></p>\n<p>另外，我们将输入图像行列预先使用线性插值的方法resize为原来的2倍，获取更多的key point。</p>\n<p>此外，在Lowe的论文中还提到了使用DoG的泰勒展开和海森矩阵进行key point的精细确定方法，并对key point进行过滤。这里也不再赘述了。</p>\n<h2 id=\"128维feature的获取\"><a href=\"#128维feature的获取\" class=\"headerlink\" title=\"128维feature的获取\"></a>128维feature的获取</h2><p>我们需要在每个key point处提取128维的特征向量。这里结合CS131课程作业2的相关练习作出说明。对于每个key point，我们关注它位于图像金字塔中的具体层数以及像素点位置，也就是说通过索引<code>pyramid{scale}(y, x)</code>就可以获取key point。我们遍历key point，为它们赋予一个128维的特征向量。</p>\n<p>我们选取point周围16x16大小的区域，称为一个patch。将其分为4x4共计16个cell。这样，每个cell内共有像素点16个。对于每个cell，我们计算它的局部梯度方向直方图，直方图共有8个bin。也就是说每个cell可以得到一个8维的特征向量。将16个cell的特征向量首尾相接，就得到了128维的SIFT特征向量。在下面的代码中，使用变量<code>patch_mag</code>和<code>patch_theta</code>分别代表patch的梯度幅值和角度，它们可以很简单地使用卷积和数学运算得到。</p>\n<figure class=\"highlight matlab\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\">patch_mag = <span class=\"built_in\">sqrt</span>(patch_dx.^<span class=\"number\">2</span> + patch_dy.^<span class=\"number\">2</span>);</div><div class=\"line\">patch_theta = <span class=\"built_in\">atan2</span>(patch_dy, patch_dx);  <span class=\"comment\">% atan2的返回结果在区间[-pi, pi]上。</span></div><div class=\"line\">patch_theta = <span class=\"built_in\">mod</span>(patch_theta, <span class=\"number\">2</span>*<span class=\"built_in\">pi</span>);   <span class=\"comment\">% 这里我们要将其转换为[0, 2pi]</span></div></pre></td></tr></table></figure>\n<p>之后，我们需要获取key point的主方向。其定义可见slide，即为key point扩展出来的patch的梯度方向直方图的峰值对应的角度。<br><img src=\"/img/sift_dominant_orientation.png\" alt=\"何为主方向\"></p>\n<p>所以我们首先应该设计如何构建局部梯度方向直方图。我们只要将<code>[0, 2pi]</code>区间划分为若干个<code>bin</code>，并将patch内的每个点使用其梯度大小向对应的<code>bin</code>内投票即可。如下所示：</p>\n<figure class=\"highlight matlab\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"function\"><span class=\"keyword\">function</span> <span class=\"params\">[histogram, angles]</span> = <span class=\"title\">ComputeGradientHistogram</span><span class=\"params\">(num_bins, gradient_magnitudes, gradient_angles)</span></span></div><div class=\"line\"><span class=\"comment\">% Compute a gradient histogram using gradient magnitudes and directions.</span></div><div class=\"line\"><span class=\"comment\">% Each point is assigned to one of num_bins depending on its gradient</span></div><div class=\"line\"><span class=\"comment\">% direction; the gradient magnitude of that point is added to its bin.</span></div><div class=\"line\"><span class=\"comment\">%</span></div><div class=\"line\"><span class=\"comment\">% INPUT</span></div><div class=\"line\"><span class=\"comment\">% num_bins: The number of bins to which points should be assigned.</span></div><div class=\"line\"><span class=\"comment\">% gradient_magnitudes, gradient angles:</span></div><div class=\"line\"><span class=\"comment\">%       Two arrays of the same shape where gradient_magnitudes(i) and</span></div><div class=\"line\"><span class=\"comment\">%       gradient_angles(i) give the magnitude and direction of the gradient</span></div><div class=\"line\"><span class=\"comment\">%       for the ith point. gradient_angles ranges from 0 to 2*pi</span></div><div class=\"line\"><span class=\"comment\">%                                      </span></div><div class=\"line\"><span class=\"comment\">% OUTPUT</span></div><div class=\"line\"><span class=\"comment\">% histogram: A 1 x num_bins array containing the gradient histogram. Entry 1 is</span></div><div class=\"line\"><span class=\"comment\">%       the sum of entries in gradient_magnitudes whose corresponding</span></div><div class=\"line\"><span class=\"comment\">%       gradient_angles lie between 0 and angle_step. Similarly, entry 2 is for</span></div><div class=\"line\"><span class=\"comment\">%       angles between angle_step and 2*angle_step. Angle_step is calculated as</span></div><div class=\"line\"><span class=\"comment\">%       2*pi/num_bins.</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">% angles: A 1 x num_bins array which holds the histogram bin lower bounds.</span></div><div class=\"line\"><span class=\"comment\">%       In other words, histogram(i) contains the sum of the</span></div><div class=\"line\"><span class=\"comment\">%       gradient magnitudes of all points whose gradient directions fall</span></div><div class=\"line\"><span class=\"comment\">%       in the range [angles(i), angles(i + 1))</span></div><div class=\"line\"></div><div class=\"line\">    angle_step = <span class=\"number\">2</span> * <span class=\"built_in\">pi</span> / num_bins;</div><div class=\"line\">    angles = <span class=\"number\">0</span> : angle_step : (<span class=\"number\">2</span>*<span class=\"built_in\">pi</span>-angle_step);</div><div class=\"line\"></div><div class=\"line\">    histogram = <span class=\"built_in\">zeros</span>(<span class=\"number\">1</span>, num_bins);</div><div class=\"line\">    num = <span class=\"built_in\">numel</span>(gradient_angles);</div><div class=\"line\">    <span class=\"keyword\">for</span> n = <span class=\"number\">1</span>:num</div><div class=\"line\">        index = <span class=\"built_in\">floor</span>(gradient_angles(n) / angle_step) + <span class=\"number\">1</span>;</div><div class=\"line\">        histogram(index) = histogram(index) + gradient_magnitudes(n);</div><div class=\"line\">    <span class=\"keyword\">end</span>    </div><div class=\"line\"><span class=\"keyword\">end</span></div></pre></td></tr></table></figure>\n<p>Lowe论文中推荐的<code>bin</code>数目为36个，计算主方向的函数如下：</p>\n<figure class=\"highlight matlab\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"function\"><span class=\"keyword\">function</span> <span class=\"title\">direction</span> = <span class=\"title\">ComputeDominantDirection</span><span class=\"params\">(gradient_magnitudes, gradient_angles)</span></span></div><div class=\"line\"><span class=\"comment\">% Computes the dominant gradient direction for the region around a keypoint</span></div><div class=\"line\"><span class=\"comment\">% given the scale of the keypoint and the gradient magnitudes and gradient</span></div><div class=\"line\"><span class=\"comment\">% angles of the pixels in the region surrounding the keypoint.</span></div><div class=\"line\"><span class=\"comment\">%</span></div><div class=\"line\"><span class=\"comment\">% INPUT</span></div><div class=\"line\"><span class=\"comment\">% gradient_magnitudes, gradient_angles:</span></div><div class=\"line\"><span class=\"comment\">%   Two arrays of the same shape where gradient_magnitudes(i) and</span></div><div class=\"line\"><span class=\"comment\">%   gradient_angles(i) give the magnitude and direction of the gradient for</span></div><div class=\"line\"><span class=\"comment\">%   the ith point.</span></div><div class=\"line\"></div><div class=\"line\">    <span class=\"comment\">% Compute a gradient histogram using the weighted gradient magnitudes.</span></div><div class=\"line\">    <span class=\"comment\">% In David Lowe's paper he suggests using 36 bins for this histogram.</span></div><div class=\"line\">    num_bins = <span class=\"number\">36</span>;</div><div class=\"line\">    <span class=\"comment\">% Step 1:</span></div><div class=\"line\">    <span class=\"comment\">% compute the 36-bin histogram of angles using ComputeGradientHistogram()</span></div><div class=\"line\">    [histogram, angle_bound] = ComputeGradientHistogram(num_bins, gradient_magnitudes, gradient_angles);</div><div class=\"line\">    <span class=\"comment\">% Step 2:</span></div><div class=\"line\">    <span class=\"comment\">% Find the maximum value of the gradient histogram, and set \"direction\"</span></div><div class=\"line\">    <span class=\"comment\">% to the angle corresponding to the maximum. (To match our solutions,</span></div><div class=\"line\">    <span class=\"comment\">% just use the lower-bound angle of the max histogram bin. (E.g. return</span></div><div class=\"line\">    <span class=\"comment\">% 0 radians if it's bin 1.)</span></div><div class=\"line\">    [~, max_index] = max(histogram);</div><div class=\"line\">    direction = angle_bound(max_index);</div><div class=\"line\"><span class=\"keyword\">end</span></div></pre></td></tr></table></figure>\n<p>之后，我们更新patch内各点的梯度方向，计算其与主方向的夹角，作为新的方向。并将梯度进行高斯平滑。</p>\n<figure class=\"highlight matlab\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\">patch_theta = patch_theta - ComputeDominantDirection(patch_mag, patch_theta);;</div><div class=\"line\">patch_theta = <span class=\"built_in\">mod</span>(patch_theta, <span class=\"number\">2</span>*<span class=\"built_in\">pi</span>);</div><div class=\"line\">patch_mag = patch_mag .* fspecial(<span class=\"string\">'gaussian'</span>, patch_size, patch_size / <span class=\"number\">2</span>); <span class=\"comment\">% patch_size = 16</span></div></pre></td></tr></table></figure>\n<p>遍历cell，计算feature如下：</p>\n<figure class=\"highlight matlab\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div></pre></td><td class=\"code\"><pre><div class=\"line\">feature = [];</div><div class=\"line\">row_iter = <span class=\"number\">1</span>;</div><div class=\"line\"><span class=\"keyword\">for</span> y = <span class=\"number\">1</span>:num_histograms</div><div class=\"line\">    col_iter = <span class=\"number\">1</span>;</div><div class=\"line\">    <span class=\"keyword\">for</span> x = <span class=\"number\">1</span>:num_histograms</div><div class=\"line\">        cell_mag = patch_mag(row_iter: row_iter + pixelsPerHistogram - <span class=\"number\">1</span>, ...</div><div class=\"line\">                             col_iter: col_iter + pixelsPerHistogram - <span class=\"number\">1</span>);</div><div class=\"line\">        cell_theta = patch_theta(row_iter: row_iter + pixelsPerHistogram - <span class=\"number\">1</span>, ...</div><div class=\"line\">                             col_iter: col_iter + pixelsPerHistogram - <span class=\"number\">1</span>);</div><div class=\"line\">        [histogram, ~] = ComputeGradientHistogram(num_angles, cell_mag, cell_theta);</div><div class=\"line\">        feature = [feature, histogram];</div><div class=\"line\">        col_iter = col_iter + pixelsPerHistogram;</div><div class=\"line\">    <span class=\"keyword\">end</span></div><div class=\"line\">    row_iter = row_iter + pixelsPerHistogram;</div><div class=\"line\"><span class=\"keyword\">end</span></div></pre></td></tr></table></figure>\n<p>最后，对feature做normalization。首先将feature化为单位长度，并将其中太大的分量进行限幅（如0.2的阈值），之后再重新将其转换为单位长度。</p>\n<p>这样，就完成了SIFT特征的计算。在Lowe的论文中，有更多对实现细节的讨论，这里只是跟随课程slide和作业走完了算法流程，不再赘述。</p>\n<h2 id=\"应用：图像特征点匹配\"><a href=\"#应用：图像特征点匹配\" class=\"headerlink\" title=\"应用：图像特征点匹配\"></a>应用：图像特征点匹配</h2><p>和Harris角点一样，SIFT特征可以用作两幅图像的特征点匹配，并且具有多种变换不变性的优点。对于两幅图像分别计算得到的特征点SIFT特征向量，可以使用下面简单的方法搜寻匹配点。计算每组点对之间的欧式距离，如果最近的距离比第二近的距离小得多，那么可以认为有一对成功匹配的特征点。其MATLAB代码如下，其中<code>descriptor</code>是两幅图像的SIFT特征向量。阈值默认为取做0.7。</p>\n<figure class=\"highlight matlab\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"function\"><span class=\"keyword\">function</span> <span class=\"title\">match</span> = <span class=\"title\">SIFTSimpleMatcher</span><span class=\"params\">(descriptor1, descriptor2, thresh)</span></span></div><div class=\"line\"><span class=\"comment\">% SIFTSimpleMatcher</span></div><div class=\"line\"><span class=\"comment\">%   Match one set of SIFT descriptors (descriptor1) to another set of</span></div><div class=\"line\"><span class=\"comment\">%   descriptors (decriptor2). Each descriptor from descriptor1 can at</span></div><div class=\"line\"><span class=\"comment\">%   most be matched to one member of descriptor2, but descriptors from</span></div><div class=\"line\"><span class=\"comment\">%   descriptor2 can be matched more than once.</span></div><div class=\"line\"><span class=\"comment\">%   </span></div><div class=\"line\"><span class=\"comment\">%   Matches are determined as follows:</span></div><div class=\"line\"><span class=\"comment\">%   For each descriptor vector in descriptor1, find the Euclidean distance</span></div><div class=\"line\"><span class=\"comment\">%   between it and each descriptor vector in descriptor2. If the smallest</span></div><div class=\"line\"><span class=\"comment\">%   distance is less than thresh*(the next smallest distance), we say that</span></div><div class=\"line\"><span class=\"comment\">%   the two vectors are a match, and we add the row [d1 index, d2 index] to</span></div><div class=\"line\"><span class=\"comment\">%   the \"match\" array.</span></div><div class=\"line\"><span class=\"comment\">%   </span></div><div class=\"line\"><span class=\"comment\">% INPUT:</span></div><div class=\"line\"><span class=\"comment\">%   descriptor1: N1 * 128 matrix, each row is a SIFT descriptor.</span></div><div class=\"line\"><span class=\"comment\">%   descriptor2: N2 * 128 matrix, each row is a SIFT descriptor.</span></div><div class=\"line\"><span class=\"comment\">%   thresh: a given threshold of ratio. Typically 0.7</span></div><div class=\"line\"><span class=\"comment\">%</span></div><div class=\"line\"><span class=\"comment\">% OUTPUT:</span></div><div class=\"line\"><span class=\"comment\">%   Match: N * 2 matrix, each row is a match.</span></div><div class=\"line\"><span class=\"comment\">%          For example, Match(k, :) = [i, j] means i-th descriptor in</span></div><div class=\"line\"><span class=\"comment\">%          descriptor1 is matched to j-th descriptor in descriptor2.</span></div><div class=\"line\">    <span class=\"keyword\">if</span> ~exist(<span class=\"string\">'thresh'</span>, <span class=\"string\">'var'</span>),</div><div class=\"line\">        thresh = <span class=\"number\">0.7</span>;</div><div class=\"line\">    <span class=\"keyword\">end</span></div><div class=\"line\"></div><div class=\"line\">    match = [];</div><div class=\"line\">    [N1, ~] = <span class=\"built_in\">size</span>(descriptor1);</div><div class=\"line\">    <span class=\"keyword\">for</span> <span class=\"built_in\">i</span> = <span class=\"number\">1</span>:N1</div><div class=\"line\">        fea = descriptor1(<span class=\"built_in\">i</span>, :);</div><div class=\"line\">        err = <span class=\"built_in\">bsxfun</span>(@minus, fea, descriptor2);</div><div class=\"line\">        dis = <span class=\"built_in\">sqrt</span>(sum(err.^<span class=\"number\">2</span>, <span class=\"number\">2</span>));</div><div class=\"line\">        [sorted_dis, ind] = sort(dis, <span class=\"number\">1</span>);</div><div class=\"line\">        <span class=\"keyword\">if</span> sorted_dis(<span class=\"number\">1</span>) &lt; thresh * sorted_dis(<span class=\"number\">2</span>)</div><div class=\"line\">            match = [match; [i, ind(<span class=\"number\">1</span>)]];</div><div class=\"line\">        <span class=\"keyword\">end</span></div><div class=\"line\">    <span class=\"keyword\">end</span></div><div class=\"line\"><span class=\"keyword\">end</span></div></pre></td></tr></table></figure>\n<p>接下来，我们可以使用最小二乘法计算两幅图像之间的仿射变换矩阵（齐次变换矩阵）$H$。其中$H$满足：</p>\n<script type=\"math/tex; mode=display\">Hp_{\\text{before}} = p_{\\text{after}}</script><p>其中</p>\n<script type=\"math/tex; mode=display\">p = \\begin{bmatrix}x \\\\\\\\ y \\\\\\\\ 1\\end{bmatrix}</script><p>对上式稍作变形，有</p>\n<script type=\"math/tex; mode=display\">p_{\\text{before}}^\\dagger H^\\dagger = p_{\\text{after}}\\dagger</script><p>就可以使用标准的最小二乘正则方程进行求解了。代码如下：</p>\n<figure class=\"highlight matlab\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"function\"><span class=\"keyword\">function</span> <span class=\"title\">H</span> = <span class=\"title\">ComputeAffineMatrix</span><span class=\"params\">( Pt1, Pt2 )</span></span></div><div class=\"line\"><span class=\"comment\">%ComputeAffineMatrix</span></div><div class=\"line\"><span class=\"comment\">%   Computes the transformation matrix that transforms a point from</span></div><div class=\"line\"><span class=\"comment\">%   coordinate frame 1 to coordinate frame 2</span></div><div class=\"line\"><span class=\"comment\">%Input:</span></div><div class=\"line\"><span class=\"comment\">%   Pt1: N * 2 matrix, each row is a point in image 1</span></div><div class=\"line\"><span class=\"comment\">%       (N must be at least 3)</span></div><div class=\"line\"><span class=\"comment\">%   Pt2: N * 2 matrix, each row is the point in image 2 that</span></div><div class=\"line\"><span class=\"comment\">%       matches the same point in image 1 (N should be more than 3)</span></div><div class=\"line\"><span class=\"comment\">%Output:</span></div><div class=\"line\"><span class=\"comment\">%   H: 3 * 3 affine transformation matrix,</span></div><div class=\"line\"><span class=\"comment\">%       such that H*pt1(i,:) = pt2(i,:)</span></div><div class=\"line\"></div><div class=\"line\">    N = <span class=\"built_in\">size</span>(Pt1,<span class=\"number\">1</span>);</div><div class=\"line\">    <span class=\"keyword\">if</span> <span class=\"built_in\">size</span>(Pt1, <span class=\"number\">1</span>) ~= <span class=\"built_in\">size</span>(Pt2, <span class=\"number\">1</span>),</div><div class=\"line\">        error(<span class=\"string\">'Dimensions unmatched.'</span>);</div><div class=\"line\">    <span class=\"keyword\">elseif</span> N&lt;<span class=\"number\">3</span></div><div class=\"line\">        error(<span class=\"string\">'At least 3 points are required.'</span>);</div><div class=\"line\">    <span class=\"keyword\">end</span></div><div class=\"line\"></div><div class=\"line\">    <span class=\"comment\">% Convert the input points to homogeneous coordintes.</span></div><div class=\"line\">    P1 = [Pt1<span class=\"string\">';ones(1,N)];</span></div><div class=\"line\">    P2 = [Pt2';ones(<span class=\"number\">1</span>,N)];</div><div class=\"line\"></div><div class=\"line\">    H = P1*P1'\\P1*P2';</div><div class=\"line\">    H = H';</div><div class=\"line\"></div><div class=\"line\">    <span class=\"comment\">% Sometimes numerical issues cause least-squares to produce a bottom</span></div><div class=\"line\">    <span class=\"comment\">% row which is not exactly [0 0 1], which confuses some of the later</span></div><div class=\"line\">    <span class=\"comment\">% code. So we'll ensure the bottom row is exactly [0 0 1].</span></div><div class=\"line\">    H(<span class=\"number\">3</span>,:) = [<span class=\"number\">0</span> <span class=\"number\">0</span> <span class=\"number\">1</span>];</div><div class=\"line\"><span class=\"keyword\">end</span></div></pre></td></tr></table></figure>\n<p>作业中的其他例子也很有趣，这里不再多说了。贴上两张图像拼接的实验结果吧~<br><img src=\"/img/sift_experiment_1.png\" alt=\"result 1\"><br><img src=\"/img/sift_experiment_2.png\" alt=\"result 2\"></p>\n","excerpt":"<p><a href=\"https://en.wikipedia.org/wiki/Scale-invariant_feature_transform\">SIFT(尺度不变特征变换，Scale Invariant Feature Transform)</a>,最早由Lowe提出，目的是为了解决目标检测（Object Detection）问题中提取图像特征的问题。从名字可以看出，SIFT的优势就在于对尺度变换的不变性，同时SIFT还满足平移变换和旋转变换下的不变性，并对光照变化和3D视角变换有一定的不变性。它的主要步骤如下：</p>\n<ul>\n<li>scale space上的极值检测。使用DoG在不同的scaleast和image position下找到interest point。</li>\n<li>interest point的localization。对上面的interest point进行稳定度检测，并确定其所在的scale和position。</li>\n<li>确定方向。通过计算图像的梯度图，确定key point的方向，下一步的feature operation就是在这个方向，scale和position上进行的。</li>\n<li>确定key point的描述子。使用图像的局部梯度作为key point的描述子，最终构成SIFT特征向量。</li>\n</ul>\n<p><img src=\"/img/sift_picture.jpg\" alt=\"SIFT图示\"></p>","more":"<h2 id=\"SIFT介绍\"><a href=\"#SIFT介绍\" class=\"headerlink\" title=\"SIFT介绍\"></a>SIFT介绍</h2><p>上讲中介绍的Harris角点方法计算简便，并具有平移不变性和旋转不变性。特征$f$对某种变换$\\mathcal{T}$具有不变性，是指在经过变换后原特征保持不变，也就是$f(I) = f(\\mathcal{T}(I))$。但是Harris角点不具有尺度变换不变性，如下图所示。当图像被放大后，原图的角点被判定为了边缘点。<br><img src=\"/img/harris_non_scale_constant.png\" alt=\"harris的尺度变换不满足尺度不变性\"></p>\n<p>而我们想要得到一种对尺度变换保持不变性的特征计算方法。例如图像patch的像素平均亮度，如下图所示。region size缩小为原始的一半后，亮度直方图的形状不变，即平均亮度不变。<br><img src=\"/img/patch_average_intensity_scale_constant.png\" alt=\"平均亮度满足尺度变化呢不变性\"></p>\n<p>而Lowe想到了使用局部极值来作为feature来保证对scale变换的不变性。在算法的具体实现中，他使用DoG来获得局部极值。</p>\n<p><a href=\"http://www.cs.ubc.ca/~lowe/papers/ijcv04.pdf\">Lowe的论文</a>中也提到了SIFT特征的应用。SIFT特征可以产生稠密（dense）的key point，例如一张500x500像素大小的图像，一般可以产生~2000个稳定的SIFT特征。在进行image matching和recognition时，可以将ref image的SIFT特征提前计算出来保存在数据库中，并计算待处理图像的SIFT特征，根据特征向量的欧氏距离进行匹配。</p>\n<p>这篇博客主要是<a href=\"http://www.cs.ubc.ca/~lowe/papers/ijcv04.pdf\">Lowe上述论文</a>的读书笔记，按照SIFT特征的计算步骤进行组织。</p>\n<h2 id=\"尺度空间极值的检测方法\"><a href=\"#尺度空间极值的检测方法\" class=\"headerlink\" title=\"尺度空间极值的检测方法\"></a>尺度空间极值的检测方法</h2><p>前人研究已经指出，在一系列合理假设下，高斯核是唯一满足尺度不变性的。所谓的尺度空间，就是指原始图像$I(x,y)$和可变尺度的高斯核$G(x,y,\\sigma)$的卷积结果。如下式所示：</p>\n<script type=\"math/tex; mode=display\">L(x,y,\\sigma) = G(x,y,\\sigma)\\ast I(x,y)</script><p>其中，$G(x,y, \\sigma) = \\frac{1}{2\\pi\\sigma^2}\\exp(-(x^2+y^2)/2\\sigma^2)$。不同的$\\sigma$代表不同的尺度。</p>\n<p>DoG(difference of Gaussian)函数定义为不同尺度的高斯核与图像卷积结果之差，即，</p>\n<script type=\"math/tex; mode=display\">D(x,y,\\sigma) = L(x,y,k\\sigma) - L(x,y,\\sigma)</script><p>如下图所示，输入的图像重复地与高斯核进行卷积得到结果图像$L$（左侧），这样构成了一个octave。相邻的$L$之间作差得到DoG图像（右侧）。当一个octave处理完之后，将当前octave的最后一张高斯卷积结果图像降采样两倍，按照前述方法构造下一个octave。在图中，我们将一个octave划分为$s$个间隔（即$s+1$个图像）时，设$\\sigma$最终变成了2倍（即$\\sigma$加倍）。那么，显然有相邻两层之间的$k = 2^{1/s}$。不过为了保证首尾两张图像也能够计算差值，我们实际上需要再补上两张（做一个padding），也就是说一个octave内的总图像为$s+3$（下图中的$s=2$）。<br><img src=\"/img/sift_dog.png\" alt=\"DoG的计算\"></p>\n<p>为何我们要费力气得到DoG呢？论文中作者给出的解释是：DoG对scale-normalized后的Guassian Laplace函数$\\sigma^2\\Delta G$提供了足够的近似。其中前面的$\\sigma^2$系数正是为了保证尺度不变性。而前人的研究则指出，$\\sigma \\Delta G$函数的极值，和其他一大票其他function比较时，能够提供最稳定的feature。</p>\n<p>对于高斯核函数，有以下性质：</p>\n<script type=\"math/tex; mode=display\">\\frac{\\partial G}{\\partial \\sigma} = \\sigma \\Delta G</script><p>我们将式子左侧的微分变成差分，得到了下式：</p>\n<script type=\"math/tex; mode=display\">\\sigma\\Delta G \\approx \\frac{G(x,y,k\\sigma)-G(x,y,\\sigma)}{k\\sigma - \\sigma}</script><p>也就是：</p>\n<script type=\"math/tex; mode=display\">G(x,y,k\\sigma)-G(x,y,\\sigma) \\approx (k-1)\\sigma^2 \\Delta G</script><p>当$k=1$时，上式的近似误差为0（即上面的$s=\\infty$，也就是说octave的划分是连续的）。但是当$k$较大时，如$\\sqrt{2}$（这时$s=2$，octave划分十分粗糙了），仍然保证了稳定性。同时，由于相邻两层的比例$k$是定值，所以$k-1$这个因子对于检测极值没有影响。我们在计算时，无需除以$k-1$。</p>\n<p>构造完了DoG图像金字塔，下面我们的任务就是检测其中的极值。如下图，对于每个点，我们将它与金字塔中相邻的26个点作比较，找到局部极值。<br><img src=\"/img/sift_detection_maximum.png\" alt=\"检测极值\"></p>\n<p>另外，我们将输入图像行列预先使用线性插值的方法resize为原来的2倍，获取更多的key point。</p>\n<p>此外，在Lowe的论文中还提到了使用DoG的泰勒展开和海森矩阵进行key point的精细确定方法，并对key point进行过滤。这里也不再赘述了。</p>\n<h2 id=\"128维feature的获取\"><a href=\"#128维feature的获取\" class=\"headerlink\" title=\"128维feature的获取\"></a>128维feature的获取</h2><p>我们需要在每个key point处提取128维的特征向量。这里结合CS131课程作业2的相关练习作出说明。对于每个key point，我们关注它位于图像金字塔中的具体层数以及像素点位置，也就是说通过索引<code>pyramid{scale}(y, x)</code>就可以获取key point。我们遍历key point，为它们赋予一个128维的特征向量。</p>\n<p>我们选取point周围16x16大小的区域，称为一个patch。将其分为4x4共计16个cell。这样，每个cell内共有像素点16个。对于每个cell，我们计算它的局部梯度方向直方图，直方图共有8个bin。也就是说每个cell可以得到一个8维的特征向量。将16个cell的特征向量首尾相接，就得到了128维的SIFT特征向量。在下面的代码中，使用变量<code>patch_mag</code>和<code>patch_theta</code>分别代表patch的梯度幅值和角度，它们可以很简单地使用卷积和数学运算得到。</p>\n<figure class=\"highlight matlab\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\">patch_mag = <span class=\"built_in\">sqrt</span>(patch_dx.^<span class=\"number\">2</span> + patch_dy.^<span class=\"number\">2</span>);</div><div class=\"line\">patch_theta = <span class=\"built_in\">atan2</span>(patch_dy, patch_dx);  <span class=\"comment\">% atan2的返回结果在区间[-pi, pi]上。</span></div><div class=\"line\">patch_theta = <span class=\"built_in\">mod</span>(patch_theta, <span class=\"number\">2</span>*<span class=\"built_in\">pi</span>);   <span class=\"comment\">% 这里我们要将其转换为[0, 2pi]</span></div></pre></td></tr></table></figure>\n<p>之后，我们需要获取key point的主方向。其定义可见slide，即为key point扩展出来的patch的梯度方向直方图的峰值对应的角度。<br><img src=\"/img/sift_dominant_orientation.png\" alt=\"何为主方向\"></p>\n<p>所以我们首先应该设计如何构建局部梯度方向直方图。我们只要将<code>[0, 2pi]</code>区间划分为若干个<code>bin</code>，并将patch内的每个点使用其梯度大小向对应的<code>bin</code>内投票即可。如下所示：</p>\n<figure class=\"highlight matlab\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"function\"><span class=\"keyword\">function</span> <span class=\"params\">[histogram, angles]</span> = <span class=\"title\">ComputeGradientHistogram</span><span class=\"params\">(num_bins, gradient_magnitudes, gradient_angles)</span></span></div><div class=\"line\"><span class=\"comment\">% Compute a gradient histogram using gradient magnitudes and directions.</span></div><div class=\"line\"><span class=\"comment\">% Each point is assigned to one of num_bins depending on its gradient</span></div><div class=\"line\"><span class=\"comment\">% direction; the gradient magnitude of that point is added to its bin.</span></div><div class=\"line\"><span class=\"comment\">%</span></div><div class=\"line\"><span class=\"comment\">% INPUT</span></div><div class=\"line\"><span class=\"comment\">% num_bins: The number of bins to which points should be assigned.</span></div><div class=\"line\"><span class=\"comment\">% gradient_magnitudes, gradient angles:</span></div><div class=\"line\"><span class=\"comment\">%       Two arrays of the same shape where gradient_magnitudes(i) and</span></div><div class=\"line\"><span class=\"comment\">%       gradient_angles(i) give the magnitude and direction of the gradient</span></div><div class=\"line\"><span class=\"comment\">%       for the ith point. gradient_angles ranges from 0 to 2*pi</span></div><div class=\"line\"><span class=\"comment\">%                                      </span></div><div class=\"line\"><span class=\"comment\">% OUTPUT</span></div><div class=\"line\"><span class=\"comment\">% histogram: A 1 x num_bins array containing the gradient histogram. Entry 1 is</span></div><div class=\"line\"><span class=\"comment\">%       the sum of entries in gradient_magnitudes whose corresponding</span></div><div class=\"line\"><span class=\"comment\">%       gradient_angles lie between 0 and angle_step. Similarly, entry 2 is for</span></div><div class=\"line\"><span class=\"comment\">%       angles between angle_step and 2*angle_step. Angle_step is calculated as</span></div><div class=\"line\"><span class=\"comment\">%       2*pi/num_bins.</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">% angles: A 1 x num_bins array which holds the histogram bin lower bounds.</span></div><div class=\"line\"><span class=\"comment\">%       In other words, histogram(i) contains the sum of the</span></div><div class=\"line\"><span class=\"comment\">%       gradient magnitudes of all points whose gradient directions fall</span></div><div class=\"line\"><span class=\"comment\">%       in the range [angles(i), angles(i + 1))</span></div><div class=\"line\"></div><div class=\"line\">    angle_step = <span class=\"number\">2</span> * <span class=\"built_in\">pi</span> / num_bins;</div><div class=\"line\">    angles = <span class=\"number\">0</span> : angle_step : (<span class=\"number\">2</span>*<span class=\"built_in\">pi</span>-angle_step);</div><div class=\"line\"></div><div class=\"line\">    histogram = <span class=\"built_in\">zeros</span>(<span class=\"number\">1</span>, num_bins);</div><div class=\"line\">    num = <span class=\"built_in\">numel</span>(gradient_angles);</div><div class=\"line\">    <span class=\"keyword\">for</span> n = <span class=\"number\">1</span>:num</div><div class=\"line\">        index = <span class=\"built_in\">floor</span>(gradient_angles(n) / angle_step) + <span class=\"number\">1</span>;</div><div class=\"line\">        histogram(index) = histogram(index) + gradient_magnitudes(n);</div><div class=\"line\">    <span class=\"keyword\">end</span>    </div><div class=\"line\"><span class=\"keyword\">end</span></div></pre></td></tr></table></figure>\n<p>Lowe论文中推荐的<code>bin</code>数目为36个，计算主方向的函数如下：</p>\n<figure class=\"highlight matlab\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"function\"><span class=\"keyword\">function</span> <span class=\"title\">direction</span> = <span class=\"title\">ComputeDominantDirection</span><span class=\"params\">(gradient_magnitudes, gradient_angles)</span></span></div><div class=\"line\"><span class=\"comment\">% Computes the dominant gradient direction for the region around a keypoint</span></div><div class=\"line\"><span class=\"comment\">% given the scale of the keypoint and the gradient magnitudes and gradient</span></div><div class=\"line\"><span class=\"comment\">% angles of the pixels in the region surrounding the keypoint.</span></div><div class=\"line\"><span class=\"comment\">%</span></div><div class=\"line\"><span class=\"comment\">% INPUT</span></div><div class=\"line\"><span class=\"comment\">% gradient_magnitudes, gradient_angles:</span></div><div class=\"line\"><span class=\"comment\">%   Two arrays of the same shape where gradient_magnitudes(i) and</span></div><div class=\"line\"><span class=\"comment\">%   gradient_angles(i) give the magnitude and direction of the gradient for</span></div><div class=\"line\"><span class=\"comment\">%   the ith point.</span></div><div class=\"line\"></div><div class=\"line\">    <span class=\"comment\">% Compute a gradient histogram using the weighted gradient magnitudes.</span></div><div class=\"line\">    <span class=\"comment\">% In David Lowe's paper he suggests using 36 bins for this histogram.</span></div><div class=\"line\">    num_bins = <span class=\"number\">36</span>;</div><div class=\"line\">    <span class=\"comment\">% Step 1:</span></div><div class=\"line\">    <span class=\"comment\">% compute the 36-bin histogram of angles using ComputeGradientHistogram()</span></div><div class=\"line\">    [histogram, angle_bound] = ComputeGradientHistogram(num_bins, gradient_magnitudes, gradient_angles);</div><div class=\"line\">    <span class=\"comment\">% Step 2:</span></div><div class=\"line\">    <span class=\"comment\">% Find the maximum value of the gradient histogram, and set \"direction\"</span></div><div class=\"line\">    <span class=\"comment\">% to the angle corresponding to the maximum. (To match our solutions,</span></div><div class=\"line\">    <span class=\"comment\">% just use the lower-bound angle of the max histogram bin. (E.g. return</span></div><div class=\"line\">    <span class=\"comment\">% 0 radians if it's bin 1.)</span></div><div class=\"line\">    [~, max_index] = max(histogram);</div><div class=\"line\">    direction = angle_bound(max_index);</div><div class=\"line\"><span class=\"keyword\">end</span></div></pre></td></tr></table></figure>\n<p>之后，我们更新patch内各点的梯度方向，计算其与主方向的夹角，作为新的方向。并将梯度进行高斯平滑。</p>\n<figure class=\"highlight matlab\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\">patch_theta = patch_theta - ComputeDominantDirection(patch_mag, patch_theta);;</div><div class=\"line\">patch_theta = <span class=\"built_in\">mod</span>(patch_theta, <span class=\"number\">2</span>*<span class=\"built_in\">pi</span>);</div><div class=\"line\">patch_mag = patch_mag .* fspecial(<span class=\"string\">'gaussian'</span>, patch_size, patch_size / <span class=\"number\">2</span>); <span class=\"comment\">% patch_size = 16</span></div></pre></td></tr></table></figure>\n<p>遍历cell，计算feature如下：</p>\n<figure class=\"highlight matlab\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div></pre></td><td class=\"code\"><pre><div class=\"line\">feature = [];</div><div class=\"line\">row_iter = <span class=\"number\">1</span>;</div><div class=\"line\"><span class=\"keyword\">for</span> y = <span class=\"number\">1</span>:num_histograms</div><div class=\"line\">    col_iter = <span class=\"number\">1</span>;</div><div class=\"line\">    <span class=\"keyword\">for</span> x = <span class=\"number\">1</span>:num_histograms</div><div class=\"line\">        cell_mag = patch_mag(row_iter: row_iter + pixelsPerHistogram - <span class=\"number\">1</span>, ...</div><div class=\"line\">                             col_iter: col_iter + pixelsPerHistogram - <span class=\"number\">1</span>);</div><div class=\"line\">        cell_theta = patch_theta(row_iter: row_iter + pixelsPerHistogram - <span class=\"number\">1</span>, ...</div><div class=\"line\">                             col_iter: col_iter + pixelsPerHistogram - <span class=\"number\">1</span>);</div><div class=\"line\">        [histogram, ~] = ComputeGradientHistogram(num_angles, cell_mag, cell_theta);</div><div class=\"line\">        feature = [feature, histogram];</div><div class=\"line\">        col_iter = col_iter + pixelsPerHistogram;</div><div class=\"line\">    <span class=\"keyword\">end</span></div><div class=\"line\">    row_iter = row_iter + pixelsPerHistogram;</div><div class=\"line\"><span class=\"keyword\">end</span></div></pre></td></tr></table></figure>\n<p>最后，对feature做normalization。首先将feature化为单位长度，并将其中太大的分量进行限幅（如0.2的阈值），之后再重新将其转换为单位长度。</p>\n<p>这样，就完成了SIFT特征的计算。在Lowe的论文中，有更多对实现细节的讨论，这里只是跟随课程slide和作业走完了算法流程，不再赘述。</p>\n<h2 id=\"应用：图像特征点匹配\"><a href=\"#应用：图像特征点匹配\" class=\"headerlink\" title=\"应用：图像特征点匹配\"></a>应用：图像特征点匹配</h2><p>和Harris角点一样，SIFT特征可以用作两幅图像的特征点匹配，并且具有多种变换不变性的优点。对于两幅图像分别计算得到的特征点SIFT特征向量，可以使用下面简单的方法搜寻匹配点。计算每组点对之间的欧式距离，如果最近的距离比第二近的距离小得多，那么可以认为有一对成功匹配的特征点。其MATLAB代码如下，其中<code>descriptor</code>是两幅图像的SIFT特征向量。阈值默认为取做0.7。</p>\n<figure class=\"highlight matlab\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"function\"><span class=\"keyword\">function</span> <span class=\"title\">match</span> = <span class=\"title\">SIFTSimpleMatcher</span><span class=\"params\">(descriptor1, descriptor2, thresh)</span></span></div><div class=\"line\"><span class=\"comment\">% SIFTSimpleMatcher</span></div><div class=\"line\"><span class=\"comment\">%   Match one set of SIFT descriptors (descriptor1) to another set of</span></div><div class=\"line\"><span class=\"comment\">%   descriptors (decriptor2). Each descriptor from descriptor1 can at</span></div><div class=\"line\"><span class=\"comment\">%   most be matched to one member of descriptor2, but descriptors from</span></div><div class=\"line\"><span class=\"comment\">%   descriptor2 can be matched more than once.</span></div><div class=\"line\"><span class=\"comment\">%   </span></div><div class=\"line\"><span class=\"comment\">%   Matches are determined as follows:</span></div><div class=\"line\"><span class=\"comment\">%   For each descriptor vector in descriptor1, find the Euclidean distance</span></div><div class=\"line\"><span class=\"comment\">%   between it and each descriptor vector in descriptor2. If the smallest</span></div><div class=\"line\"><span class=\"comment\">%   distance is less than thresh*(the next smallest distance), we say that</span></div><div class=\"line\"><span class=\"comment\">%   the two vectors are a match, and we add the row [d1 index, d2 index] to</span></div><div class=\"line\"><span class=\"comment\">%   the \"match\" array.</span></div><div class=\"line\"><span class=\"comment\">%   </span></div><div class=\"line\"><span class=\"comment\">% INPUT:</span></div><div class=\"line\"><span class=\"comment\">%   descriptor1: N1 * 128 matrix, each row is a SIFT descriptor.</span></div><div class=\"line\"><span class=\"comment\">%   descriptor2: N2 * 128 matrix, each row is a SIFT descriptor.</span></div><div class=\"line\"><span class=\"comment\">%   thresh: a given threshold of ratio. Typically 0.7</span></div><div class=\"line\"><span class=\"comment\">%</span></div><div class=\"line\"><span class=\"comment\">% OUTPUT:</span></div><div class=\"line\"><span class=\"comment\">%   Match: N * 2 matrix, each row is a match.</span></div><div class=\"line\"><span class=\"comment\">%          For example, Match(k, :) = [i, j] means i-th descriptor in</span></div><div class=\"line\"><span class=\"comment\">%          descriptor1 is matched to j-th descriptor in descriptor2.</span></div><div class=\"line\">    <span class=\"keyword\">if</span> ~exist(<span class=\"string\">'thresh'</span>, <span class=\"string\">'var'</span>),</div><div class=\"line\">        thresh = <span class=\"number\">0.7</span>;</div><div class=\"line\">    <span class=\"keyword\">end</span></div><div class=\"line\"></div><div class=\"line\">    match = [];</div><div class=\"line\">    [N1, ~] = <span class=\"built_in\">size</span>(descriptor1);</div><div class=\"line\">    <span class=\"keyword\">for</span> <span class=\"built_in\">i</span> = <span class=\"number\">1</span>:N1</div><div class=\"line\">        fea = descriptor1(<span class=\"built_in\">i</span>, :);</div><div class=\"line\">        err = <span class=\"built_in\">bsxfun</span>(@minus, fea, descriptor2);</div><div class=\"line\">        dis = <span class=\"built_in\">sqrt</span>(sum(err.^<span class=\"number\">2</span>, <span class=\"number\">2</span>));</div><div class=\"line\">        [sorted_dis, ind] = sort(dis, <span class=\"number\">1</span>);</div><div class=\"line\">        <span class=\"keyword\">if</span> sorted_dis(<span class=\"number\">1</span>) &lt; thresh * sorted_dis(<span class=\"number\">2</span>)</div><div class=\"line\">            match = [match; [i, ind(<span class=\"number\">1</span>)]];</div><div class=\"line\">        <span class=\"keyword\">end</span></div><div class=\"line\">    <span class=\"keyword\">end</span></div><div class=\"line\"><span class=\"keyword\">end</span></div></pre></td></tr></table></figure>\n<p>接下来，我们可以使用最小二乘法计算两幅图像之间的仿射变换矩阵（齐次变换矩阵）$H$。其中$H$满足：</p>\n<script type=\"math/tex; mode=display\">Hp_{\\text{before}} = p_{\\text{after}}</script><p>其中</p>\n<script type=\"math/tex; mode=display\">p = \\begin{bmatrix}x \\\\\\\\ y \\\\\\\\ 1\\end{bmatrix}</script><p>对上式稍作变形，有</p>\n<script type=\"math/tex; mode=display\">p_{\\text{before}}^\\dagger H^\\dagger = p_{\\text{after}}\\dagger</script><p>就可以使用标准的最小二乘正则方程进行求解了。代码如下：</p>\n<figure class=\"highlight matlab\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"function\"><span class=\"keyword\">function</span> <span class=\"title\">H</span> = <span class=\"title\">ComputeAffineMatrix</span><span class=\"params\">( Pt1, Pt2 )</span></span></div><div class=\"line\"><span class=\"comment\">%ComputeAffineMatrix</span></div><div class=\"line\"><span class=\"comment\">%   Computes the transformation matrix that transforms a point from</span></div><div class=\"line\"><span class=\"comment\">%   coordinate frame 1 to coordinate frame 2</span></div><div class=\"line\"><span class=\"comment\">%Input:</span></div><div class=\"line\"><span class=\"comment\">%   Pt1: N * 2 matrix, each row is a point in image 1</span></div><div class=\"line\"><span class=\"comment\">%       (N must be at least 3)</span></div><div class=\"line\"><span class=\"comment\">%   Pt2: N * 2 matrix, each row is the point in image 2 that</span></div><div class=\"line\"><span class=\"comment\">%       matches the same point in image 1 (N should be more than 3)</span></div><div class=\"line\"><span class=\"comment\">%Output:</span></div><div class=\"line\"><span class=\"comment\">%   H: 3 * 3 affine transformation matrix,</span></div><div class=\"line\"><span class=\"comment\">%       such that H*pt1(i,:) = pt2(i,:)</span></div><div class=\"line\"></div><div class=\"line\">    N = <span class=\"built_in\">size</span>(Pt1,<span class=\"number\">1</span>);</div><div class=\"line\">    <span class=\"keyword\">if</span> <span class=\"built_in\">size</span>(Pt1, <span class=\"number\">1</span>) ~= <span class=\"built_in\">size</span>(Pt2, <span class=\"number\">1</span>),</div><div class=\"line\">        error(<span class=\"string\">'Dimensions unmatched.'</span>);</div><div class=\"line\">    <span class=\"keyword\">elseif</span> N&lt;<span class=\"number\">3</span></div><div class=\"line\">        error(<span class=\"string\">'At least 3 points are required.'</span>);</div><div class=\"line\">    <span class=\"keyword\">end</span></div><div class=\"line\"></div><div class=\"line\">    <span class=\"comment\">% Convert the input points to homogeneous coordintes.</span></div><div class=\"line\">    P1 = [Pt1<span class=\"string\">';ones(1,N)];</div><div class=\"line\">    P2 = [Pt2'</span>;ones(<span class=\"number\">1</span>,N)];</div><div class=\"line\"></div><div class=\"line\">    H = P1*P1'\\P1*P2';</div><div class=\"line\">    H = H';</div><div class=\"line\"></div><div class=\"line\">    <span class=\"comment\">% Sometimes numerical issues cause least-squares to produce a bottom</span></div><div class=\"line\">    <span class=\"comment\">% row which is not exactly [0 0 1], which confuses some of the later</span></div><div class=\"line\">    <span class=\"comment\">% code. So we'll ensure the bottom row is exactly [0 0 1].</span></div><div class=\"line\">    H(<span class=\"number\">3</span>,:) = [<span class=\"number\">0</span> <span class=\"number\">0</span> <span class=\"number\">1</span>];</div><div class=\"line\"><span class=\"keyword\">end</span></div></pre></td></tr></table></figure>\n<p>作业中的其他例子也很有趣，这里不再多说了。贴上两张图像拼接的实验结果吧~<br><img src=\"/img/sift_experiment_1.png\" alt=\"result 1\"><br><img src=\"/img/sift_experiment_2.png\" alt=\"result 2\"></p>"},{"title":"使用IPDB调试Python代码","date":"2017-08-21T07:53:16.000Z","_content":"IPDB是什么？IPDB（Ipython Debugger），和GDB类似，是一款集成了Ipython的Python代码命令行调试工具，可以看做PDB的升级版。这篇文章总结IPDB的使用方法，主要是若干命令的使用。更多详细的教程或文档还请参考Google。\n<!-- more -->\n## 安装与使用\nIPDB以Python第三方库的形式给出，使用`pip install ipdb`即可轻松安装。\n\n在使用时，有两种常见方式。\n\n### 集成到源代码中\n通过在代码开头导入包，可以直接在代码指定位置插入断点。如下所示：\n``` py\nimport ipdb\n# some code\nx = 10\nipdb.set_trace()\ny = 20\n# other code\n```\n则程序会在执行完`x = 10`这条语句之后停止，展开Ipython环境，就可以自由地调试了。\n\n### 命令式\n上面的方法很方便，但是也有不灵活的缺点。对于一段比较棘手的代码，我们可能需要按步执行，边运行边跟踪代码流并进行调试，这时候使用交互式的命令式调试方法更加有效。启动IPDB调试环境的方法也很简单：\n``` sh\npython -m ipdb your_code.py\n```\n\n## 常用命令\nIPDB调试环境提供的常见命令有：\n### 帮助\n帮助文档就是这样一个东西：当你写的时候觉得这TM也要写？当你看别人的东西的时候觉得这TM都没写？\n\n使用`h`即可调出IPDB的帮助。可以使用`help command`的方法查询特定命令的具体用法。\n\n### 下一条语句\n使用`n`(next)执行下一条语句。注意一个函数调用也是一个语句。如何能够实现类似“进入函数内部”的功能呢？\n### 进入函数内部\n使用`s`(step into)进入函数调用的内部。\n### 打断点\n使用`b line_number`(break)的方式给指定的行号位置加上断点。使用`b file_name:line_number`的方法给指定的文件（还没执行到的代码可能在外部文件中）中指定行号位置打上断点。\n\n另外，打断点还支持指定条件下进入，可以查询帮助文档。\n### 一直执行直到遇到下一个断点\n使用`c`(continue)执行代码直到遇到某个断点或程序执行完毕。\n### 一直执行直到返回\n使用`r`(return)执行代码直到当前所在的这个函数返回。\n### 跳过某段代码\n使用`j line_number`(jump)可以跳过某段代码，直接执行指定行号所在的代码。\n### 更多上下文\n在IPDB调试环境中，默认只显示当前执行的代码行，以及其上下各一行的代码。如果想要看到更多的上下文代码，可以使用`l first[, second]`(list)命令。\n\n其中`first`指示向上最多显示的行号，`second`指示向下最多显示的行号（可以省略）。当`second`小于`first`时，`second`指的是从`first`开始的向下的行数（相对值vs绝对值）。\n\n根据[SO上的这个问题](https://stackoverflow.com/questions/6240887/how-can-i-make-ipdb-show-more-lines-of-context-while-debugging)，你还可以修改IPDB的源码，一劳永逸地改变上下文的行数。\n### 我在哪里\n调试兴起，可能你会忘了自己目前所在的行号。例如在打印了若干变量值后，屏幕完全被这些值占据。使用`w`或者`where`可以打印出目前所在的行号位置以及上下文信息。\n\n### 这是啥\n我们可以使用`whatis variable_name`的方法，查看变量的类别（感觉有点鸡肋，用`type`也可以办到）。\n### 列出当前函数的全部参数\n当你身处一个函数内部的时候，可以使用`a`(argument)打印出传入函数的所有参数的值。\n### 打印\n使用`p`(print)和`pp`(pretty print)可以打印表达式的值。\n\n### 清除断点\n使用`cl`或者`clear file:line_number`清除断点。如果没有参数，则清除所有断点。\n### 再来一次\n使用`restart`重新启动调试器，断点等信息都会保留。`restart`实际是`run`的别名，使用`run args`的方式传入参数。\n### 退出\n使用`q`退出调试，并清除所有信息。\n\n当然，这并不是IPDB的全部。其他的命令还请参照帮助文档。文档在手，天下我有！\n","source":"_posts/debugging-with-ipdb.md","raw":"---\ntitle: 使用IPDB调试Python代码\ndate: 2017-08-21 15:53:16\ntags:\n    - python\n---\nIPDB是什么？IPDB（Ipython Debugger），和GDB类似，是一款集成了Ipython的Python代码命令行调试工具，可以看做PDB的升级版。这篇文章总结IPDB的使用方法，主要是若干命令的使用。更多详细的教程或文档还请参考Google。\n<!-- more -->\n## 安装与使用\nIPDB以Python第三方库的形式给出，使用`pip install ipdb`即可轻松安装。\n\n在使用时，有两种常见方式。\n\n### 集成到源代码中\n通过在代码开头导入包，可以直接在代码指定位置插入断点。如下所示：\n``` py\nimport ipdb\n# some code\nx = 10\nipdb.set_trace()\ny = 20\n# other code\n```\n则程序会在执行完`x = 10`这条语句之后停止，展开Ipython环境，就可以自由地调试了。\n\n### 命令式\n上面的方法很方便，但是也有不灵活的缺点。对于一段比较棘手的代码，我们可能需要按步执行，边运行边跟踪代码流并进行调试，这时候使用交互式的命令式调试方法更加有效。启动IPDB调试环境的方法也很简单：\n``` sh\npython -m ipdb your_code.py\n```\n\n## 常用命令\nIPDB调试环境提供的常见命令有：\n### 帮助\n帮助文档就是这样一个东西：当你写的时候觉得这TM也要写？当你看别人的东西的时候觉得这TM都没写？\n\n使用`h`即可调出IPDB的帮助。可以使用`help command`的方法查询特定命令的具体用法。\n\n### 下一条语句\n使用`n`(next)执行下一条语句。注意一个函数调用也是一个语句。如何能够实现类似“进入函数内部”的功能呢？\n### 进入函数内部\n使用`s`(step into)进入函数调用的内部。\n### 打断点\n使用`b line_number`(break)的方式给指定的行号位置加上断点。使用`b file_name:line_number`的方法给指定的文件（还没执行到的代码可能在外部文件中）中指定行号位置打上断点。\n\n另外，打断点还支持指定条件下进入，可以查询帮助文档。\n### 一直执行直到遇到下一个断点\n使用`c`(continue)执行代码直到遇到某个断点或程序执行完毕。\n### 一直执行直到返回\n使用`r`(return)执行代码直到当前所在的这个函数返回。\n### 跳过某段代码\n使用`j line_number`(jump)可以跳过某段代码，直接执行指定行号所在的代码。\n### 更多上下文\n在IPDB调试环境中，默认只显示当前执行的代码行，以及其上下各一行的代码。如果想要看到更多的上下文代码，可以使用`l first[, second]`(list)命令。\n\n其中`first`指示向上最多显示的行号，`second`指示向下最多显示的行号（可以省略）。当`second`小于`first`时，`second`指的是从`first`开始的向下的行数（相对值vs绝对值）。\n\n根据[SO上的这个问题](https://stackoverflow.com/questions/6240887/how-can-i-make-ipdb-show-more-lines-of-context-while-debugging)，你还可以修改IPDB的源码，一劳永逸地改变上下文的行数。\n### 我在哪里\n调试兴起，可能你会忘了自己目前所在的行号。例如在打印了若干变量值后，屏幕完全被这些值占据。使用`w`或者`where`可以打印出目前所在的行号位置以及上下文信息。\n\n### 这是啥\n我们可以使用`whatis variable_name`的方法，查看变量的类别（感觉有点鸡肋，用`type`也可以办到）。\n### 列出当前函数的全部参数\n当你身处一个函数内部的时候，可以使用`a`(argument)打印出传入函数的所有参数的值。\n### 打印\n使用`p`(print)和`pp`(pretty print)可以打印表达式的值。\n\n### 清除断点\n使用`cl`或者`clear file:line_number`清除断点。如果没有参数，则清除所有断点。\n### 再来一次\n使用`restart`重新启动调试器，断点等信息都会保留。`restart`实际是`run`的别名，使用`run args`的方式传入参数。\n### 退出\n使用`q`退出调试，并清除所有信息。\n\n当然，这并不是IPDB的全部。其他的命令还请参照帮助文档。文档在手，天下我有！\n","slug":"debugging-with-ipdb","published":1,"updated":"2018-01-12T06:22:20.462Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjcu6vcsc000nqu46df3r9kvl","content":"<p>IPDB是什么？IPDB（Ipython Debugger），和GDB类似，是一款集成了Ipython的Python代码命令行调试工具，可以看做PDB的升级版。这篇文章总结IPDB的使用方法，主要是若干命令的使用。更多详细的教程或文档还请参考Google。<br><a id=\"more\"></a></p>\n<h2 id=\"安装与使用\"><a href=\"#安装与使用\" class=\"headerlink\" title=\"安装与使用\"></a>安装与使用</h2><p>IPDB以Python第三方库的形式给出，使用<code>pip install ipdb</code>即可轻松安装。</p>\n<p>在使用时，有两种常见方式。</p>\n<h3 id=\"集成到源代码中\"><a href=\"#集成到源代码中\" class=\"headerlink\" title=\"集成到源代码中\"></a>集成到源代码中</h3><p>通过在代码开头导入包，可以直接在代码指定位置插入断点。如下所示：<br><figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">import</span> ipdb</div><div class=\"line\"><span class=\"comment\"># some code</span></div><div class=\"line\">x = <span class=\"number\">10</span></div><div class=\"line\">ipdb.set_trace()</div><div class=\"line\">y = <span class=\"number\">20</span></div><div class=\"line\"><span class=\"comment\"># other code</span></div></pre></td></tr></table></figure></p>\n<p>则程序会在执行完<code>x = 10</code>这条语句之后停止，展开Ipython环境，就可以自由地调试了。</p>\n<h3 id=\"命令式\"><a href=\"#命令式\" class=\"headerlink\" title=\"命令式\"></a>命令式</h3><p>上面的方法很方便，但是也有不灵活的缺点。对于一段比较棘手的代码，我们可能需要按步执行，边运行边跟踪代码流并进行调试，这时候使用交互式的命令式调试方法更加有效。启动IPDB调试环境的方法也很简单：<br><figure class=\"highlight sh\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">python -m ipdb your_code.py</div></pre></td></tr></table></figure></p>\n<h2 id=\"常用命令\"><a href=\"#常用命令\" class=\"headerlink\" title=\"常用命令\"></a>常用命令</h2><p>IPDB调试环境提供的常见命令有：</p>\n<h3 id=\"帮助\"><a href=\"#帮助\" class=\"headerlink\" title=\"帮助\"></a>帮助</h3><p>帮助文档就是这样一个东西：当你写的时候觉得这TM也要写？当你看别人的东西的时候觉得这TM都没写？</p>\n<p>使用<code>h</code>即可调出IPDB的帮助。可以使用<code>help command</code>的方法查询特定命令的具体用法。</p>\n<h3 id=\"下一条语句\"><a href=\"#下一条语句\" class=\"headerlink\" title=\"下一条语句\"></a>下一条语句</h3><p>使用<code>n</code>(next)执行下一条语句。注意一个函数调用也是一个语句。如何能够实现类似“进入函数内部”的功能呢？</p>\n<h3 id=\"进入函数内部\"><a href=\"#进入函数内部\" class=\"headerlink\" title=\"进入函数内部\"></a>进入函数内部</h3><p>使用<code>s</code>(step into)进入函数调用的内部。</p>\n<h3 id=\"打断点\"><a href=\"#打断点\" class=\"headerlink\" title=\"打断点\"></a>打断点</h3><p>使用<code>b line_number</code>(break)的方式给指定的行号位置加上断点。使用<code>b file_name:line_number</code>的方法给指定的文件（还没执行到的代码可能在外部文件中）中指定行号位置打上断点。</p>\n<p>另外，打断点还支持指定条件下进入，可以查询帮助文档。</p>\n<h3 id=\"一直执行直到遇到下一个断点\"><a href=\"#一直执行直到遇到下一个断点\" class=\"headerlink\" title=\"一直执行直到遇到下一个断点\"></a>一直执行直到遇到下一个断点</h3><p>使用<code>c</code>(continue)执行代码直到遇到某个断点或程序执行完毕。</p>\n<h3 id=\"一直执行直到返回\"><a href=\"#一直执行直到返回\" class=\"headerlink\" title=\"一直执行直到返回\"></a>一直执行直到返回</h3><p>使用<code>r</code>(return)执行代码直到当前所在的这个函数返回。</p>\n<h3 id=\"跳过某段代码\"><a href=\"#跳过某段代码\" class=\"headerlink\" title=\"跳过某段代码\"></a>跳过某段代码</h3><p>使用<code>j line_number</code>(jump)可以跳过某段代码，直接执行指定行号所在的代码。</p>\n<h3 id=\"更多上下文\"><a href=\"#更多上下文\" class=\"headerlink\" title=\"更多上下文\"></a>更多上下文</h3><p>在IPDB调试环境中，默认只显示当前执行的代码行，以及其上下各一行的代码。如果想要看到更多的上下文代码，可以使用<code>l first[, second]</code>(list)命令。</p>\n<p>其中<code>first</code>指示向上最多显示的行号，<code>second</code>指示向下最多显示的行号（可以省略）。当<code>second</code>小于<code>first</code>时，<code>second</code>指的是从<code>first</code>开始的向下的行数（相对值vs绝对值）。</p>\n<p>根据<a href=\"https://stackoverflow.com/questions/6240887/how-can-i-make-ipdb-show-more-lines-of-context-while-debugging\" target=\"_blank\" rel=\"external\">SO上的这个问题</a>，你还可以修改IPDB的源码，一劳永逸地改变上下文的行数。</p>\n<h3 id=\"我在哪里\"><a href=\"#我在哪里\" class=\"headerlink\" title=\"我在哪里\"></a>我在哪里</h3><p>调试兴起，可能你会忘了自己目前所在的行号。例如在打印了若干变量值后，屏幕完全被这些值占据。使用<code>w</code>或者<code>where</code>可以打印出目前所在的行号位置以及上下文信息。</p>\n<h3 id=\"这是啥\"><a href=\"#这是啥\" class=\"headerlink\" title=\"这是啥\"></a>这是啥</h3><p>我们可以使用<code>whatis variable_name</code>的方法，查看变量的类别（感觉有点鸡肋，用<code>type</code>也可以办到）。</p>\n<h3 id=\"列出当前函数的全部参数\"><a href=\"#列出当前函数的全部参数\" class=\"headerlink\" title=\"列出当前函数的全部参数\"></a>列出当前函数的全部参数</h3><p>当你身处一个函数内部的时候，可以使用<code>a</code>(argument)打印出传入函数的所有参数的值。</p>\n<h3 id=\"打印\"><a href=\"#打印\" class=\"headerlink\" title=\"打印\"></a>打印</h3><p>使用<code>p</code>(print)和<code>pp</code>(pretty print)可以打印表达式的值。</p>\n<h3 id=\"清除断点\"><a href=\"#清除断点\" class=\"headerlink\" title=\"清除断点\"></a>清除断点</h3><p>使用<code>cl</code>或者<code>clear file:line_number</code>清除断点。如果没有参数，则清除所有断点。</p>\n<h3 id=\"再来一次\"><a href=\"#再来一次\" class=\"headerlink\" title=\"再来一次\"></a>再来一次</h3><p>使用<code>restart</code>重新启动调试器，断点等信息都会保留。<code>restart</code>实际是<code>run</code>的别名，使用<code>run args</code>的方式传入参数。</p>\n<h3 id=\"退出\"><a href=\"#退出\" class=\"headerlink\" title=\"退出\"></a>退出</h3><p>使用<code>q</code>退出调试，并清除所有信息。</p>\n<p>当然，这并不是IPDB的全部。其他的命令还请参照帮助文档。文档在手，天下我有！</p>\n","excerpt":"<p>IPDB是什么？IPDB（Ipython Debugger），和GDB类似，是一款集成了Ipython的Python代码命令行调试工具，可以看做PDB的升级版。这篇文章总结IPDB的使用方法，主要是若干命令的使用。更多详细的教程或文档还请参考Google。<br>","more":"</p>\n<h2 id=\"安装与使用\"><a href=\"#安装与使用\" class=\"headerlink\" title=\"安装与使用\"></a>安装与使用</h2><p>IPDB以Python第三方库的形式给出，使用<code>pip install ipdb</code>即可轻松安装。</p>\n<p>在使用时，有两种常见方式。</p>\n<h3 id=\"集成到源代码中\"><a href=\"#集成到源代码中\" class=\"headerlink\" title=\"集成到源代码中\"></a>集成到源代码中</h3><p>通过在代码开头导入包，可以直接在代码指定位置插入断点。如下所示：<br><figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">import</span> ipdb</div><div class=\"line\"><span class=\"comment\"># some code</span></div><div class=\"line\">x = <span class=\"number\">10</span></div><div class=\"line\">ipdb.set_trace()</div><div class=\"line\">y = <span class=\"number\">20</span></div><div class=\"line\"><span class=\"comment\"># other code</span></div></pre></td></tr></table></figure></p>\n<p>则程序会在执行完<code>x = 10</code>这条语句之后停止，展开Ipython环境，就可以自由地调试了。</p>\n<h3 id=\"命令式\"><a href=\"#命令式\" class=\"headerlink\" title=\"命令式\"></a>命令式</h3><p>上面的方法很方便，但是也有不灵活的缺点。对于一段比较棘手的代码，我们可能需要按步执行，边运行边跟踪代码流并进行调试，这时候使用交互式的命令式调试方法更加有效。启动IPDB调试环境的方法也很简单：<br><figure class=\"highlight sh\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">python -m ipdb your_code.py</div></pre></td></tr></table></figure></p>\n<h2 id=\"常用命令\"><a href=\"#常用命令\" class=\"headerlink\" title=\"常用命令\"></a>常用命令</h2><p>IPDB调试环境提供的常见命令有：</p>\n<h3 id=\"帮助\"><a href=\"#帮助\" class=\"headerlink\" title=\"帮助\"></a>帮助</h3><p>帮助文档就是这样一个东西：当你写的时候觉得这TM也要写？当你看别人的东西的时候觉得这TM都没写？</p>\n<p>使用<code>h</code>即可调出IPDB的帮助。可以使用<code>help command</code>的方法查询特定命令的具体用法。</p>\n<h3 id=\"下一条语句\"><a href=\"#下一条语句\" class=\"headerlink\" title=\"下一条语句\"></a>下一条语句</h3><p>使用<code>n</code>(next)执行下一条语句。注意一个函数调用也是一个语句。如何能够实现类似“进入函数内部”的功能呢？</p>\n<h3 id=\"进入函数内部\"><a href=\"#进入函数内部\" class=\"headerlink\" title=\"进入函数内部\"></a>进入函数内部</h3><p>使用<code>s</code>(step into)进入函数调用的内部。</p>\n<h3 id=\"打断点\"><a href=\"#打断点\" class=\"headerlink\" title=\"打断点\"></a>打断点</h3><p>使用<code>b line_number</code>(break)的方式给指定的行号位置加上断点。使用<code>b file_name:line_number</code>的方法给指定的文件（还没执行到的代码可能在外部文件中）中指定行号位置打上断点。</p>\n<p>另外，打断点还支持指定条件下进入，可以查询帮助文档。</p>\n<h3 id=\"一直执行直到遇到下一个断点\"><a href=\"#一直执行直到遇到下一个断点\" class=\"headerlink\" title=\"一直执行直到遇到下一个断点\"></a>一直执行直到遇到下一个断点</h3><p>使用<code>c</code>(continue)执行代码直到遇到某个断点或程序执行完毕。</p>\n<h3 id=\"一直执行直到返回\"><a href=\"#一直执行直到返回\" class=\"headerlink\" title=\"一直执行直到返回\"></a>一直执行直到返回</h3><p>使用<code>r</code>(return)执行代码直到当前所在的这个函数返回。</p>\n<h3 id=\"跳过某段代码\"><a href=\"#跳过某段代码\" class=\"headerlink\" title=\"跳过某段代码\"></a>跳过某段代码</h3><p>使用<code>j line_number</code>(jump)可以跳过某段代码，直接执行指定行号所在的代码。</p>\n<h3 id=\"更多上下文\"><a href=\"#更多上下文\" class=\"headerlink\" title=\"更多上下文\"></a>更多上下文</h3><p>在IPDB调试环境中，默认只显示当前执行的代码行，以及其上下各一行的代码。如果想要看到更多的上下文代码，可以使用<code>l first[, second]</code>(list)命令。</p>\n<p>其中<code>first</code>指示向上最多显示的行号，<code>second</code>指示向下最多显示的行号（可以省略）。当<code>second</code>小于<code>first</code>时，<code>second</code>指的是从<code>first</code>开始的向下的行数（相对值vs绝对值）。</p>\n<p>根据<a href=\"https://stackoverflow.com/questions/6240887/how-can-i-make-ipdb-show-more-lines-of-context-while-debugging\">SO上的这个问题</a>，你还可以修改IPDB的源码，一劳永逸地改变上下文的行数。</p>\n<h3 id=\"我在哪里\"><a href=\"#我在哪里\" class=\"headerlink\" title=\"我在哪里\"></a>我在哪里</h3><p>调试兴起，可能你会忘了自己目前所在的行号。例如在打印了若干变量值后，屏幕完全被这些值占据。使用<code>w</code>或者<code>where</code>可以打印出目前所在的行号位置以及上下文信息。</p>\n<h3 id=\"这是啥\"><a href=\"#这是啥\" class=\"headerlink\" title=\"这是啥\"></a>这是啥</h3><p>我们可以使用<code>whatis variable_name</code>的方法，查看变量的类别（感觉有点鸡肋，用<code>type</code>也可以办到）。</p>\n<h3 id=\"列出当前函数的全部参数\"><a href=\"#列出当前函数的全部参数\" class=\"headerlink\" title=\"列出当前函数的全部参数\"></a>列出当前函数的全部参数</h3><p>当你身处一个函数内部的时候，可以使用<code>a</code>(argument)打印出传入函数的所有参数的值。</p>\n<h3 id=\"打印\"><a href=\"#打印\" class=\"headerlink\" title=\"打印\"></a>打印</h3><p>使用<code>p</code>(print)和<code>pp</code>(pretty print)可以打印表达式的值。</p>\n<h3 id=\"清除断点\"><a href=\"#清除断点\" class=\"headerlink\" title=\"清除断点\"></a>清除断点</h3><p>使用<code>cl</code>或者<code>clear file:line_number</code>清除断点。如果没有参数，则清除所有断点。</p>\n<h3 id=\"再来一次\"><a href=\"#再来一次\" class=\"headerlink\" title=\"再来一次\"></a>再来一次</h3><p>使用<code>restart</code>重新启动调试器，断点等信息都会保留。<code>restart</code>实际是<code>run</code>的别名，使用<code>run args</code>的方式传入参数。</p>\n<h3 id=\"退出\"><a href=\"#退出\" class=\"headerlink\" title=\"退出\"></a>退出</h3><p>使用<code>q</code>退出调试，并清除所有信息。</p>\n<p>当然，这并不是IPDB的全部。其他的命令还请参照帮助文档。文档在手，天下我有！</p>"},{"title":"在DigitalOcean上配置Shadowsocks实现IPV4/IPV6翻墙","date":"2017-02-08T09:31:35.000Z","_content":"~~身在天朝，GFW是一个很蛋疼的东西。有人说GFW挡住了天朝与世界其他地方的交流，尤其给科研造成了很多不便；也有人认为GFW挡住了美利坚的“坚船利炮”（Google，Facebook，Tweet等），让中国的互联网企业高速发展。这两种思路都很有道理，然而GFW对计算机相关行业人员造成不便倒也是千真万确，让人在无数次404之后，脱口而出F***。也有人打趣，会不会FQ，已经成为了CS入行的第一道关卡，正好用来刷掉一批人。~~ （2017/10/27）以前这段话真他妈Naive！滑稽。。。\n\n\n之前我使用GoAgent来FQ，后来又使用了一段时间的免费SS服务，后来机缘巧合从一个印度佬那里挣了一些美元，所以搬到了DigitalOcean上。DO的机器，对我一个学生来说，说实话并不算便宜了，不过好在手里还有一些美元，也在GitHub那里进行了学生认证，算是也可以应付了。\n\n之前我已经在DO的机器上配置过shadowsocks，还顺手给iPad解决了FQ的问题。然而当时没有记录，这次我换了一台机器，机房位于NY，把ss重新配置了一遍，再不做些记录，下次恐怕又要东翻西找。\n![佛跳墙](/img/god_use_vpn.png)\n\n<!-- more -->\n## 申请机器\n在进行GitHub学生认证后，可以使用它发给的一个优惠码注册DO，不过仍然要绑定自己的PayPal账户，先交上五美金。之后，DO会赠送50刀。\n\n申请机器时，直接选择Ubuntu 16.04操作系统，并勾选IPV6 Enable，省去后面的麻烦。\n\n## 安装ss\n远程登录后，我们需要安装ss。安装命令很简单。\n``` bash\napt-get install python-pip\npip install shadowsocks\n```\n然而，在安装时，我遇到了一个奇怪的问题，提示我`unsupported locale setting`，后来搜索得知，是语言配置的问题，见[这篇博文](http://www.linfuyan.com/locale_error_unsupported_locale_setting/)，解决办法如下：\n``` bash\nexport LC_ALL=C\n```\n\n## 编辑配置文件\n之后，进入`/etc`目录，建立一个名叫`shadowsocks.json`的文件（文件名任意，一会对应即可），文件配置内容如下：\n```\n{\n\"server\":\"::\",  \n\"server_port\":8388,\n\"local_address\": \"127.0.0.1\",\n\"local_port\": 1080,\n\"password\":\"your_password（任写）\",\n\"timeout\":600,\n\"method\":\"aes-256-cfb\"\n}\n```\n其中第一行写成`::`即是为了IPV6连接。\n\n## 编辑启动项，设置自动启动\n之后，我们编辑启动项配置文件，使得ss服务能够开机自动启动。\n\n编辑`/etc/rc.local`文件，在`exit 0`之前添加如下命令。\n```\nssserver -c /etc/shadowsocks.json -d start  # 这里的json文件名要相对应\n```\n\n之后，使用`reboot`命令重启即可。\n\n## 客户端配置\n客户端使用ss，编辑服务器，按照json文件中的内容填写即可。注意密码相对应。\n\n## IOS平台设置\n终于把Pad上的翻墙搞定了。。。参考资料为GitHub的相关页面，基本为傻瓜式操作。\n\n- [IPsec VPN 服务器一键安装脚本](https://github.com/hwdsl2/setup-ipsec-vpn/blob/master/README-zh.md)\n- [配置 IPsec/L2TP VPN 客户端](https://github.com/hwdsl2/setup-ipsec-vpn/blob/master/docs/clients-zh.md)\n\n首先，使用如下命令在VPN服务器上搭建IPsec服务：\n\n```\nwget https://git.io/vpnsetup -O vpnsetup.sh && sudo sh vpnsetup.sh\n```\n\n然后按照下面的步骤在IOS平台上进行设置。\n![](/img/ipsec_ios_vpn_setting.png)\n","source":"_posts/digitalocean-shadowsocks.md","raw":"---\ntitle: 在DigitalOcean上配置Shadowsocks实现IPV4/IPV6翻墙\ndate: 2017-02-08 17:31:35\ntags:\n    - tool\n---\n~~身在天朝，GFW是一个很蛋疼的东西。有人说GFW挡住了天朝与世界其他地方的交流，尤其给科研造成了很多不便；也有人认为GFW挡住了美利坚的“坚船利炮”（Google，Facebook，Tweet等），让中国的互联网企业高速发展。这两种思路都很有道理，然而GFW对计算机相关行业人员造成不便倒也是千真万确，让人在无数次404之后，脱口而出F***。也有人打趣，会不会FQ，已经成为了CS入行的第一道关卡，正好用来刷掉一批人。~~ （2017/10/27）以前这段话真他妈Naive！滑稽。。。\n\n\n之前我使用GoAgent来FQ，后来又使用了一段时间的免费SS服务，后来机缘巧合从一个印度佬那里挣了一些美元，所以搬到了DigitalOcean上。DO的机器，对我一个学生来说，说实话并不算便宜了，不过好在手里还有一些美元，也在GitHub那里进行了学生认证，算是也可以应付了。\n\n之前我已经在DO的机器上配置过shadowsocks，还顺手给iPad解决了FQ的问题。然而当时没有记录，这次我换了一台机器，机房位于NY，把ss重新配置了一遍，再不做些记录，下次恐怕又要东翻西找。\n![佛跳墙](/img/god_use_vpn.png)\n\n<!-- more -->\n## 申请机器\n在进行GitHub学生认证后，可以使用它发给的一个优惠码注册DO，不过仍然要绑定自己的PayPal账户，先交上五美金。之后，DO会赠送50刀。\n\n申请机器时，直接选择Ubuntu 16.04操作系统，并勾选IPV6 Enable，省去后面的麻烦。\n\n## 安装ss\n远程登录后，我们需要安装ss。安装命令很简单。\n``` bash\napt-get install python-pip\npip install shadowsocks\n```\n然而，在安装时，我遇到了一个奇怪的问题，提示我`unsupported locale setting`，后来搜索得知，是语言配置的问题，见[这篇博文](http://www.linfuyan.com/locale_error_unsupported_locale_setting/)，解决办法如下：\n``` bash\nexport LC_ALL=C\n```\n\n## 编辑配置文件\n之后，进入`/etc`目录，建立一个名叫`shadowsocks.json`的文件（文件名任意，一会对应即可），文件配置内容如下：\n```\n{\n\"server\":\"::\",  \n\"server_port\":8388,\n\"local_address\": \"127.0.0.1\",\n\"local_port\": 1080,\n\"password\":\"your_password（任写）\",\n\"timeout\":600,\n\"method\":\"aes-256-cfb\"\n}\n```\n其中第一行写成`::`即是为了IPV6连接。\n\n## 编辑启动项，设置自动启动\n之后，我们编辑启动项配置文件，使得ss服务能够开机自动启动。\n\n编辑`/etc/rc.local`文件，在`exit 0`之前添加如下命令。\n```\nssserver -c /etc/shadowsocks.json -d start  # 这里的json文件名要相对应\n```\n\n之后，使用`reboot`命令重启即可。\n\n## 客户端配置\n客户端使用ss，编辑服务器，按照json文件中的内容填写即可。注意密码相对应。\n\n## IOS平台设置\n终于把Pad上的翻墙搞定了。。。参考资料为GitHub的相关页面，基本为傻瓜式操作。\n\n- [IPsec VPN 服务器一键安装脚本](https://github.com/hwdsl2/setup-ipsec-vpn/blob/master/README-zh.md)\n- [配置 IPsec/L2TP VPN 客户端](https://github.com/hwdsl2/setup-ipsec-vpn/blob/master/docs/clients-zh.md)\n\n首先，使用如下命令在VPN服务器上搭建IPsec服务：\n\n```\nwget https://git.io/vpnsetup -O vpnsetup.sh && sudo sh vpnsetup.sh\n```\n\n然后按照下面的步骤在IOS平台上进行设置。\n![](/img/ipsec_ios_vpn_setting.png)\n","slug":"digitalocean-shadowsocks","published":1,"updated":"2018-01-12T06:22:20.463Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjcu6vcsl000pqu46aq2isouk","content":"<p><del>身在天朝，GFW是一个很蛋疼的东西。有人说GFW挡住了天朝与世界其他地方的交流，尤其给科研造成了很多不便；也有人认为GFW挡住了美利坚的“坚船利炮”（Google，Facebook，Tweet等），让中国的互联网企业高速发展。这两种思路都很有道理，然而GFW对计算机相关行业人员造成不便倒也是千真万确，让人在无数次404之后，脱口而出F<em>*</em>。也有人打趣，会不会FQ，已经成为了CS入行的第一道关卡，正好用来刷掉一批人。</del> （2017/10/27）以前这段话真他妈Naive！滑稽。。。</p>\n<p>之前我使用GoAgent来FQ，后来又使用了一段时间的免费SS服务，后来机缘巧合从一个印度佬那里挣了一些美元，所以搬到了DigitalOcean上。DO的机器，对我一个学生来说，说实话并不算便宜了，不过好在手里还有一些美元，也在GitHub那里进行了学生认证，算是也可以应付了。</p>\n<p>之前我已经在DO的机器上配置过shadowsocks，还顺手给iPad解决了FQ的问题。然而当时没有记录，这次我换了一台机器，机房位于NY，把ss重新配置了一遍，再不做些记录，下次恐怕又要东翻西找。<br><img src=\"/img/god_use_vpn.png\" alt=\"佛跳墙\"></p>\n<a id=\"more\"></a>\n<h2 id=\"申请机器\"><a href=\"#申请机器\" class=\"headerlink\" title=\"申请机器\"></a>申请机器</h2><p>在进行GitHub学生认证后，可以使用它发给的一个优惠码注册DO，不过仍然要绑定自己的PayPal账户，先交上五美金。之后，DO会赠送50刀。</p>\n<p>申请机器时，直接选择Ubuntu 16.04操作系统，并勾选IPV6 Enable，省去后面的麻烦。</p>\n<h2 id=\"安装ss\"><a href=\"#安装ss\" class=\"headerlink\" title=\"安装ss\"></a>安装ss</h2><p>远程登录后，我们需要安装ss。安装命令很简单。<br><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">apt-get install python-pip</div><div class=\"line\">pip install shadowsocks</div></pre></td></tr></table></figure></p>\n<p>然而，在安装时，我遇到了一个奇怪的问题，提示我<code>unsupported locale setting</code>，后来搜索得知，是语言配置的问题，见<a href=\"http://www.linfuyan.com/locale_error_unsupported_locale_setting/\" target=\"_blank\" rel=\"external\">这篇博文</a>，解决办法如下：<br><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"built_in\">export</span> LC_ALL=C</div></pre></td></tr></table></figure></p>\n<h2 id=\"编辑配置文件\"><a href=\"#编辑配置文件\" class=\"headerlink\" title=\"编辑配置文件\"></a>编辑配置文件</h2><p>之后，进入<code>/etc</code>目录，建立一个名叫<code>shadowsocks.json</code>的文件（文件名任意，一会对应即可），文件配置内容如下：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div></pre></td><td class=\"code\"><pre><div class=\"line\">&#123;</div><div class=\"line\">&quot;server&quot;:&quot;::&quot;,  </div><div class=\"line\">&quot;server_port&quot;:8388,</div><div class=\"line\">&quot;local_address&quot;: &quot;127.0.0.1&quot;,</div><div class=\"line\">&quot;local_port&quot;: 1080,</div><div class=\"line\">&quot;password&quot;:&quot;your_password（任写）&quot;,</div><div class=\"line\">&quot;timeout&quot;:600,</div><div class=\"line\">&quot;method&quot;:&quot;aes-256-cfb&quot;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure></p>\n<p>其中第一行写成<code>::</code>即是为了IPV6连接。</p>\n<h2 id=\"编辑启动项，设置自动启动\"><a href=\"#编辑启动项，设置自动启动\" class=\"headerlink\" title=\"编辑启动项，设置自动启动\"></a>编辑启动项，设置自动启动</h2><p>之后，我们编辑启动项配置文件，使得ss服务能够开机自动启动。</p>\n<p>编辑<code>/etc/rc.local</code>文件，在<code>exit 0</code>之前添加如下命令。<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">ssserver -c /etc/shadowsocks.json -d start  # 这里的json文件名要相对应</div></pre></td></tr></table></figure></p>\n<p>之后，使用<code>reboot</code>命令重启即可。</p>\n<h2 id=\"客户端配置\"><a href=\"#客户端配置\" class=\"headerlink\" title=\"客户端配置\"></a>客户端配置</h2><p>客户端使用ss，编辑服务器，按照json文件中的内容填写即可。注意密码相对应。</p>\n<h2 id=\"IOS平台设置\"><a href=\"#IOS平台设置\" class=\"headerlink\" title=\"IOS平台设置\"></a>IOS平台设置</h2><p>终于把Pad上的翻墙搞定了。。。参考资料为GitHub的相关页面，基本为傻瓜式操作。</p>\n<ul>\n<li><a href=\"https://github.com/hwdsl2/setup-ipsec-vpn/blob/master/README-zh.md\" target=\"_blank\" rel=\"external\">IPsec VPN 服务器一键安装脚本</a></li>\n<li><a href=\"https://github.com/hwdsl2/setup-ipsec-vpn/blob/master/docs/clients-zh.md\" target=\"_blank\" rel=\"external\">配置 IPsec/L2TP VPN 客户端</a></li>\n</ul>\n<p>首先，使用如下命令在VPN服务器上搭建IPsec服务：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">wget https://git.io/vpnsetup -O vpnsetup.sh &amp;&amp; sudo sh vpnsetup.sh</div></pre></td></tr></table></figure>\n<p>然后按照下面的步骤在IOS平台上进行设置。<br><img src=\"/img/ipsec_ios_vpn_setting.png\" alt=\"\"></p>\n","excerpt":"<p><del>身在天朝，GFW是一个很蛋疼的东西。有人说GFW挡住了天朝与世界其他地方的交流，尤其给科研造成了很多不便；也有人认为GFW挡住了美利坚的“坚船利炮”（Google，Facebook，Tweet等），让中国的互联网企业高速发展。这两种思路都很有道理，然而GFW对计算机相关行业人员造成不便倒也是千真万确，让人在无数次404之后，脱口而出F<em>*</em>。也有人打趣，会不会FQ，已经成为了CS入行的第一道关卡，正好用来刷掉一批人。</del> （2017/10/27）以前这段话真他妈Naive！滑稽。。。</p>\n<p>之前我使用GoAgent来FQ，后来又使用了一段时间的免费SS服务，后来机缘巧合从一个印度佬那里挣了一些美元，所以搬到了DigitalOcean上。DO的机器，对我一个学生来说，说实话并不算便宜了，不过好在手里还有一些美元，也在GitHub那里进行了学生认证，算是也可以应付了。</p>\n<p>之前我已经在DO的机器上配置过shadowsocks，还顺手给iPad解决了FQ的问题。然而当时没有记录，这次我换了一台机器，机房位于NY，把ss重新配置了一遍，再不做些记录，下次恐怕又要东翻西找。<br><img src=\"/img/god_use_vpn.png\" alt=\"佛跳墙\"></p>","more":"<h2 id=\"申请机器\"><a href=\"#申请机器\" class=\"headerlink\" title=\"申请机器\"></a>申请机器</h2><p>在进行GitHub学生认证后，可以使用它发给的一个优惠码注册DO，不过仍然要绑定自己的PayPal账户，先交上五美金。之后，DO会赠送50刀。</p>\n<p>申请机器时，直接选择Ubuntu 16.04操作系统，并勾选IPV6 Enable，省去后面的麻烦。</p>\n<h2 id=\"安装ss\"><a href=\"#安装ss\" class=\"headerlink\" title=\"安装ss\"></a>安装ss</h2><p>远程登录后，我们需要安装ss。安装命令很简单。<br><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">apt-get install python-pip</div><div class=\"line\">pip install shadowsocks</div></pre></td></tr></table></figure></p>\n<p>然而，在安装时，我遇到了一个奇怪的问题，提示我<code>unsupported locale setting</code>，后来搜索得知，是语言配置的问题，见<a href=\"http://www.linfuyan.com/locale_error_unsupported_locale_setting/\">这篇博文</a>，解决办法如下：<br><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"built_in\">export</span> LC_ALL=C</div></pre></td></tr></table></figure></p>\n<h2 id=\"编辑配置文件\"><a href=\"#编辑配置文件\" class=\"headerlink\" title=\"编辑配置文件\"></a>编辑配置文件</h2><p>之后，进入<code>/etc</code>目录，建立一个名叫<code>shadowsocks.json</code>的文件（文件名任意，一会对应即可），文件配置内容如下：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div></pre></td><td class=\"code\"><pre><div class=\"line\">&#123;</div><div class=\"line\">&quot;server&quot;:&quot;::&quot;,  </div><div class=\"line\">&quot;server_port&quot;:8388,</div><div class=\"line\">&quot;local_address&quot;: &quot;127.0.0.1&quot;,</div><div class=\"line\">&quot;local_port&quot;: 1080,</div><div class=\"line\">&quot;password&quot;:&quot;your_password（任写）&quot;,</div><div class=\"line\">&quot;timeout&quot;:600,</div><div class=\"line\">&quot;method&quot;:&quot;aes-256-cfb&quot;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure></p>\n<p>其中第一行写成<code>::</code>即是为了IPV6连接。</p>\n<h2 id=\"编辑启动项，设置自动启动\"><a href=\"#编辑启动项，设置自动启动\" class=\"headerlink\" title=\"编辑启动项，设置自动启动\"></a>编辑启动项，设置自动启动</h2><p>之后，我们编辑启动项配置文件，使得ss服务能够开机自动启动。</p>\n<p>编辑<code>/etc/rc.local</code>文件，在<code>exit 0</code>之前添加如下命令。<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">ssserver -c /etc/shadowsocks.json -d start  # 这里的json文件名要相对应</div></pre></td></tr></table></figure></p>\n<p>之后，使用<code>reboot</code>命令重启即可。</p>\n<h2 id=\"客户端配置\"><a href=\"#客户端配置\" class=\"headerlink\" title=\"客户端配置\"></a>客户端配置</h2><p>客户端使用ss，编辑服务器，按照json文件中的内容填写即可。注意密码相对应。</p>\n<h2 id=\"IOS平台设置\"><a href=\"#IOS平台设置\" class=\"headerlink\" title=\"IOS平台设置\"></a>IOS平台设置</h2><p>终于把Pad上的翻墙搞定了。。。参考资料为GitHub的相关页面，基本为傻瓜式操作。</p>\n<ul>\n<li><a href=\"https://github.com/hwdsl2/setup-ipsec-vpn/blob/master/README-zh.md\">IPsec VPN 服务器一键安装脚本</a></li>\n<li><a href=\"https://github.com/hwdsl2/setup-ipsec-vpn/blob/master/docs/clients-zh.md\">配置 IPsec/L2TP VPN 客户端</a></li>\n</ul>\n<p>首先，使用如下命令在VPN服务器上搭建IPsec服务：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">wget https://git.io/vpnsetup -O vpnsetup.sh &amp;&amp; sudo sh vpnsetup.sh</div></pre></td></tr></table></figure>\n<p>然后按照下面的步骤在IOS平台上进行设置。<br><img src=\"/img/ipsec_ios_vpn_setting.png\" alt=\"\"></p>"},{"title":"Effective CPP 阅读 - Chapter 1 让自己习惯C++","date":"2017-04-20T05:49:42.000Z","_content":"本系列是《Effective C++》一书的阅读摘记，分章整理各个条款。\n![写C++需要人品](/img/effectivecpp_01_cpp_rely_on_renpin.jpg)\n<!-- more-->\n\n### 01 将C++视作语言联邦\nC++在发明之初，只是一个带类的C。现在的C++已经变成了同时支持面向过程，面向对象，支持泛型和模板元编程的巨兽。这部分内容可以参见“C++的设计与演化”一书。\n\n本条款中，将C++概括为四个次语言组成的联邦：\n- 传统C：面向过程，也规定了C++基本的语法。\n- OOP：面向对象，带类的C，加入了继承，虚函数等概念。\n- Template：很多针对模板需要特殊注意的条款，甚至催生了模板元编程。\n- STL：标准模板库。使用STL要遵守它的约定。\n\n想要高效地使用C++，必须根据不同的情况遵守不同的编程规范。\n\n### 02 尽量使用`const`, `enum`, `inline`替换 `#define`（以编译器替换预处理器）\n\n`#define`是C时代遗留下来的预编译指令。\n\n当`#define`用来定义某个常量时，通常`const`是一个更好的选择。\n\n``` cpp\n#define PI 3.14\nconst double PI = 3.14;\n```\n当此常量为整数类型（`int`, `char`, `bool`）等时，也可以使用`enum`定义常量。这种做法常常用在模板元编程中。\n\n``` cpp\nenum {K = 1};\n```\n对于`const`常量，你可以获取变量的地址，但是对于`enum`来说，无法获取变量的地址。对于这一点来说，`enum`和`#define`相类似。\n\n另一种可能使用`#define`的场景是宏定义。这种情形可以使用`inline`声明内联函数解决。\n\n总之，尽可能相信编译器的力量。使用`#define`将遮蔽编译器的视野，带来奇怪的问题。\n\n### 03 尽可能使用`const`\n`const`不止是给程序员看的，而且为编译器指定了一个语义约束，即这个对象是不该被改变的。所以任何试图修改这个对象的操作，都会被编译器检查出来，并给出error。\n\n所以，如果某一变量满足`const`的要求，那么请加上`const`，和编译器签订一份契约，保护你的行为。\n\n这里不再讨论`const`的寻常用法。提示一下：当修饰指针变量时，`const`在星号左边，是指指针所指物是常量；当`const`在星号右边，是指指针本身是常量。如下所示：\n\n``` cpp\nconst int* p = &a;\n*p = 5;   // 非法\np = &b;   // 合法\nint* const p = &a;\np = &b;    // 非法\n*p = 5;    // 合法\n```\n\nSTL中，如果声明某个迭代器为`const`，是指该迭代器本身是常量；如果你的意思是迭代器指向的元素为常量，那么使用`const_iterator`。\n\n`const`更丰富的用法是用于函数声明中，\n\n- 当修饰返回值时，意思是返回值不能修改。这可以让你避免无意义的赋值，尤其是以下的错误：\n\n``` cpp\nif (fun(a, b) = c)  // 这里错把 == 打成了 =\n```\n\n- 当修饰参数时，常常用做 pass-by-const-reference 的形式，不再多说了。\n- 当修饰函数本身时，常常用在类中的成员函数上，意思是这个函数将不改变对象的成员。\n\n这种情况下，可能会有`const`重载现象。\n\n``` cpp\nclass my_string{\n  const char& operator[](size_t pos) {\n\t  return this->ptr[pos];\n  }\n  char& operator[](size_t pos) {\n\t  return this->ptr[pos];\n  }\n};\n```\n\n实际调用时，根据调用该函数的对象是否是`const`的来决定究竟调用哪个版本。\n\n上面的实现未免过于复杂，我们还可以改成下面的形式：\n\n``` cpp\nclass my_string{\n  const char& operator[](size_t pos) {\n\t  return this->ptr[pos];\n  }\n  char& operator[](size_t pos) {\n\t  return const_cast<char&>(\n\t         static_cast<const my_string&>(*this)[pos]);\n  }\n};\n```\n\n注意上面的代码进行了两次类型转换。由`non-const reference`转为`const reference`是类型安全的，使用`static_cast`进行。最后我们要脱掉`const char&`的`const`属性，使用了`const_cast`。\n\n对于`const`成员函数，有时不得不修改类中的某些成员变量，可以将这些变量声明为`mutable`。\n\n### 04 确保对象在使用前已经被初始化\n\n使用未被初始化的变量有可能导致未定义的行为，导致奇怪的bug。所以推荐为所有变量进行初始化。\n\n对于内建类型，需要手动初始化。\n\n对于用户自定义类型，一般需要调用构造函数初始化。推荐在构造函数中使用初始化列表进行初始化，这样可以避免不必要的性能损失。原因见下：\n\n``` cpp\npublic A(name, age) {\n  this->name = name; // 这是赋值，不是初始化！\n  this->age = age;\n}\n```\n\n如果在类`A`的构造函数中使用初始化列表，就可以避免上面的赋值，而是使用`copy-construct`实现。\n\n需要注意，成员初始化的顺序与其在类中声明的顺序相同，与初始化列表中的顺序无关。所以推荐将两者统一。\n\n讨论完上述情况，再来看一种特殊变量：不同编译单元`non-local static`变量，是指不在某个函数scope下的`static`变量。这种变量的初始化顺序是未定义的，所以作者推荐使用单例模式，将它们移动到某个函数中去，明确初始化顺序。这里不再多说了。\n","source":"_posts/effective-cpp-01.md","raw":"---\ntitle: Effective CPP 阅读 - Chapter 1 让自己习惯C++\ndate: 2017-04-20 13:49:42\ntags:\n     - cpp\n---\n本系列是《Effective C++》一书的阅读摘记，分章整理各个条款。\n![写C++需要人品](/img/effectivecpp_01_cpp_rely_on_renpin.jpg)\n<!-- more-->\n\n### 01 将C++视作语言联邦\nC++在发明之初，只是一个带类的C。现在的C++已经变成了同时支持面向过程，面向对象，支持泛型和模板元编程的巨兽。这部分内容可以参见“C++的设计与演化”一书。\n\n本条款中，将C++概括为四个次语言组成的联邦：\n- 传统C：面向过程，也规定了C++基本的语法。\n- OOP：面向对象，带类的C，加入了继承，虚函数等概念。\n- Template：很多针对模板需要特殊注意的条款，甚至催生了模板元编程。\n- STL：标准模板库。使用STL要遵守它的约定。\n\n想要高效地使用C++，必须根据不同的情况遵守不同的编程规范。\n\n### 02 尽量使用`const`, `enum`, `inline`替换 `#define`（以编译器替换预处理器）\n\n`#define`是C时代遗留下来的预编译指令。\n\n当`#define`用来定义某个常量时，通常`const`是一个更好的选择。\n\n``` cpp\n#define PI 3.14\nconst double PI = 3.14;\n```\n当此常量为整数类型（`int`, `char`, `bool`）等时，也可以使用`enum`定义常量。这种做法常常用在模板元编程中。\n\n``` cpp\nenum {K = 1};\n```\n对于`const`常量，你可以获取变量的地址，但是对于`enum`来说，无法获取变量的地址。对于这一点来说，`enum`和`#define`相类似。\n\n另一种可能使用`#define`的场景是宏定义。这种情形可以使用`inline`声明内联函数解决。\n\n总之，尽可能相信编译器的力量。使用`#define`将遮蔽编译器的视野，带来奇怪的问题。\n\n### 03 尽可能使用`const`\n`const`不止是给程序员看的，而且为编译器指定了一个语义约束，即这个对象是不该被改变的。所以任何试图修改这个对象的操作，都会被编译器检查出来，并给出error。\n\n所以，如果某一变量满足`const`的要求，那么请加上`const`，和编译器签订一份契约，保护你的行为。\n\n这里不再讨论`const`的寻常用法。提示一下：当修饰指针变量时，`const`在星号左边，是指指针所指物是常量；当`const`在星号右边，是指指针本身是常量。如下所示：\n\n``` cpp\nconst int* p = &a;\n*p = 5;   // 非法\np = &b;   // 合法\nint* const p = &a;\np = &b;    // 非法\n*p = 5;    // 合法\n```\n\nSTL中，如果声明某个迭代器为`const`，是指该迭代器本身是常量；如果你的意思是迭代器指向的元素为常量，那么使用`const_iterator`。\n\n`const`更丰富的用法是用于函数声明中，\n\n- 当修饰返回值时，意思是返回值不能修改。这可以让你避免无意义的赋值，尤其是以下的错误：\n\n``` cpp\nif (fun(a, b) = c)  // 这里错把 == 打成了 =\n```\n\n- 当修饰参数时，常常用做 pass-by-const-reference 的形式，不再多说了。\n- 当修饰函数本身时，常常用在类中的成员函数上，意思是这个函数将不改变对象的成员。\n\n这种情况下，可能会有`const`重载现象。\n\n``` cpp\nclass my_string{\n  const char& operator[](size_t pos) {\n\t  return this->ptr[pos];\n  }\n  char& operator[](size_t pos) {\n\t  return this->ptr[pos];\n  }\n};\n```\n\n实际调用时，根据调用该函数的对象是否是`const`的来决定究竟调用哪个版本。\n\n上面的实现未免过于复杂，我们还可以改成下面的形式：\n\n``` cpp\nclass my_string{\n  const char& operator[](size_t pos) {\n\t  return this->ptr[pos];\n  }\n  char& operator[](size_t pos) {\n\t  return const_cast<char&>(\n\t         static_cast<const my_string&>(*this)[pos]);\n  }\n};\n```\n\n注意上面的代码进行了两次类型转换。由`non-const reference`转为`const reference`是类型安全的，使用`static_cast`进行。最后我们要脱掉`const char&`的`const`属性，使用了`const_cast`。\n\n对于`const`成员函数，有时不得不修改类中的某些成员变量，可以将这些变量声明为`mutable`。\n\n### 04 确保对象在使用前已经被初始化\n\n使用未被初始化的变量有可能导致未定义的行为，导致奇怪的bug。所以推荐为所有变量进行初始化。\n\n对于内建类型，需要手动初始化。\n\n对于用户自定义类型，一般需要调用构造函数初始化。推荐在构造函数中使用初始化列表进行初始化，这样可以避免不必要的性能损失。原因见下：\n\n``` cpp\npublic A(name, age) {\n  this->name = name; // 这是赋值，不是初始化！\n  this->age = age;\n}\n```\n\n如果在类`A`的构造函数中使用初始化列表，就可以避免上面的赋值，而是使用`copy-construct`实现。\n\n需要注意，成员初始化的顺序与其在类中声明的顺序相同，与初始化列表中的顺序无关。所以推荐将两者统一。\n\n讨论完上述情况，再来看一种特殊变量：不同编译单元`non-local static`变量，是指不在某个函数scope下的`static`变量。这种变量的初始化顺序是未定义的，所以作者推荐使用单例模式，将它们移动到某个函数中去，明确初始化顺序。这里不再多说了。\n","slug":"effective-cpp-01","published":1,"updated":"2018-01-12T06:22:20.464Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjcu6vcsn000qqu46kxddw44r","content":"<p>本系列是《Effective C++》一书的阅读摘记，分章整理各个条款。<br><img src=\"/img/effectivecpp_01_cpp_rely_on_renpin.jpg\" alt=\"写C++需要人品\"><br><a id=\"more\"></a></p>\n<h3 id=\"01-将C-视作语言联邦\"><a href=\"#01-将C-视作语言联邦\" class=\"headerlink\" title=\"01 将C++视作语言联邦\"></a>01 将C++视作语言联邦</h3><p>C++在发明之初，只是一个带类的C。现在的C++已经变成了同时支持面向过程，面向对象，支持泛型和模板元编程的巨兽。这部分内容可以参见“C++的设计与演化”一书。</p>\n<p>本条款中，将C++概括为四个次语言组成的联邦：</p>\n<ul>\n<li>传统C：面向过程，也规定了C++基本的语法。</li>\n<li>OOP：面向对象，带类的C，加入了继承，虚函数等概念。</li>\n<li>Template：很多针对模板需要特殊注意的条款，甚至催生了模板元编程。</li>\n<li>STL：标准模板库。使用STL要遵守它的约定。</li>\n</ul>\n<p>想要高效地使用C++，必须根据不同的情况遵守不同的编程规范。</p>\n<h3 id=\"02-尽量使用const-enum-inline替换-define（以编译器替换预处理器）\"><a href=\"#02-尽量使用const-enum-inline替换-define（以编译器替换预处理器）\" class=\"headerlink\" title=\"02 尽量使用const, enum, inline替换 #define（以编译器替换预处理器）\"></a>02 尽量使用<code>const</code>, <code>enum</code>, <code>inline</code>替换 <code>#define</code>（以编译器替换预处理器）</h3><p><code>#define</code>是C时代遗留下来的预编译指令。</p>\n<p>当<code>#define</code>用来定义某个常量时，通常<code>const</code>是一个更好的选择。</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">define</span> PI 3.14</span></div><div class=\"line\"><span class=\"keyword\">const</span> <span class=\"keyword\">double</span> PI = <span class=\"number\">3.14</span>;</div></pre></td></tr></table></figure>\n<p>当此常量为整数类型（<code>int</code>, <code>char</code>, <code>bool</code>）等时，也可以使用<code>enum</code>定义常量。这种做法常常用在模板元编程中。</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">enum</span> &#123;K = <span class=\"number\">1</span>&#125;;</div></pre></td></tr></table></figure>\n<p>对于<code>const</code>常量，你可以获取变量的地址，但是对于<code>enum</code>来说，无法获取变量的地址。对于这一点来说，<code>enum</code>和<code>#define</code>相类似。</p>\n<p>另一种可能使用<code>#define</code>的场景是宏定义。这种情形可以使用<code>inline</code>声明内联函数解决。</p>\n<p>总之，尽可能相信编译器的力量。使用<code>#define</code>将遮蔽编译器的视野，带来奇怪的问题。</p>\n<h3 id=\"03-尽可能使用const\"><a href=\"#03-尽可能使用const\" class=\"headerlink\" title=\"03 尽可能使用const\"></a>03 尽可能使用<code>const</code></h3><p><code>const</code>不止是给程序员看的，而且为编译器指定了一个语义约束，即这个对象是不该被改变的。所以任何试图修改这个对象的操作，都会被编译器检查出来，并给出error。</p>\n<p>所以，如果某一变量满足<code>const</code>的要求，那么请加上<code>const</code>，和编译器签订一份契约，保护你的行为。</p>\n<p>这里不再讨论<code>const</code>的寻常用法。提示一下：当修饰指针变量时，<code>const</code>在星号左边，是指指针所指物是常量；当<code>const</code>在星号右边，是指指针本身是常量。如下所示：</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">const</span> <span class=\"keyword\">int</span>* p = &amp;a;</div><div class=\"line\">*p = <span class=\"number\">5</span>;   <span class=\"comment\">// 非法</span></div><div class=\"line\">p = &amp;b;   <span class=\"comment\">// 合法</span></div><div class=\"line\"><span class=\"keyword\">int</span>* <span class=\"keyword\">const</span> p = &amp;a;</div><div class=\"line\">p = &amp;b;    <span class=\"comment\">// 非法</span></div><div class=\"line\">*p = <span class=\"number\">5</span>;    <span class=\"comment\">// 合法</span></div></pre></td></tr></table></figure>\n<p>STL中，如果声明某个迭代器为<code>const</code>，是指该迭代器本身是常量；如果你的意思是迭代器指向的元素为常量，那么使用<code>const_iterator</code>。</p>\n<p><code>const</code>更丰富的用法是用于函数声明中，</p>\n<ul>\n<li>当修饰返回值时，意思是返回值不能修改。这可以让你避免无意义的赋值，尤其是以下的错误：</li>\n</ul>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">if</span> (fun(a, b) = c)  <span class=\"comment\">// 这里错把 == 打成了 =</span></div></pre></td></tr></table></figure>\n<ul>\n<li>当修饰参数时，常常用做 pass-by-const-reference 的形式，不再多说了。</li>\n<li>当修饰函数本身时，常常用在类中的成员函数上，意思是这个函数将不改变对象的成员。</li>\n</ul>\n<p>这种情况下，可能会有<code>const</code>重载现象。</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">class</span> my_string&#123;</div><div class=\"line\">  <span class=\"keyword\">const</span> <span class=\"keyword\">char</span>&amp; <span class=\"keyword\">operator</span>[](<span class=\"keyword\">size_t</span> pos) &#123;</div><div class=\"line\">\t  <span class=\"keyword\">return</span> <span class=\"keyword\">this</span>-&gt;ptr[pos];</div><div class=\"line\">  &#125;</div><div class=\"line\">  <span class=\"keyword\">char</span>&amp; <span class=\"keyword\">operator</span>[](<span class=\"keyword\">size_t</span> pos) &#123;</div><div class=\"line\">\t  <span class=\"keyword\">return</span> <span class=\"keyword\">this</span>-&gt;ptr[pos];</div><div class=\"line\">  &#125;</div><div class=\"line\">&#125;;</div></pre></td></tr></table></figure>\n<p>实际调用时，根据调用该函数的对象是否是<code>const</code>的来决定究竟调用哪个版本。</p>\n<p>上面的实现未免过于复杂，我们还可以改成下面的形式：</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">class</span> my_string&#123;</div><div class=\"line\">  <span class=\"keyword\">const</span> <span class=\"keyword\">char</span>&amp; <span class=\"keyword\">operator</span>[](<span class=\"keyword\">size_t</span> pos) &#123;</div><div class=\"line\">\t  <span class=\"keyword\">return</span> <span class=\"keyword\">this</span>-&gt;ptr[pos];</div><div class=\"line\">  &#125;</div><div class=\"line\">  <span class=\"keyword\">char</span>&amp; <span class=\"keyword\">operator</span>[](<span class=\"keyword\">size_t</span> pos) &#123;</div><div class=\"line\">\t  <span class=\"keyword\">return</span> <span class=\"keyword\">const_cast</span>&lt;<span class=\"keyword\">char</span>&amp;&gt;(</div><div class=\"line\">\t         <span class=\"keyword\">static_cast</span>&lt;<span class=\"keyword\">const</span> my_string&amp;&gt;(*<span class=\"keyword\">this</span>)[pos]);</div><div class=\"line\">  &#125;</div><div class=\"line\">&#125;;</div></pre></td></tr></table></figure>\n<p>注意上面的代码进行了两次类型转换。由<code>non-const reference</code>转为<code>const reference</code>是类型安全的，使用<code>static_cast</code>进行。最后我们要脱掉<code>const char&amp;</code>的<code>const</code>属性，使用了<code>const_cast</code>。</p>\n<p>对于<code>const</code>成员函数，有时不得不修改类中的某些成员变量，可以将这些变量声明为<code>mutable</code>。</p>\n<h3 id=\"04-确保对象在使用前已经被初始化\"><a href=\"#04-确保对象在使用前已经被初始化\" class=\"headerlink\" title=\"04 确保对象在使用前已经被初始化\"></a>04 确保对象在使用前已经被初始化</h3><p>使用未被初始化的变量有可能导致未定义的行为，导致奇怪的bug。所以推荐为所有变量进行初始化。</p>\n<p>对于内建类型，需要手动初始化。</p>\n<p>对于用户自定义类型，一般需要调用构造函数初始化。推荐在构造函数中使用初始化列表进行初始化，这样可以避免不必要的性能损失。原因见下：</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> <span class=\"title\">A</span><span class=\"params\">(name, age)</span> </span>&#123;</div><div class=\"line\">  <span class=\"keyword\">this</span>-&gt;name = name; <span class=\"comment\">// 这是赋值，不是初始化！</span></div><div class=\"line\">  <span class=\"keyword\">this</span>-&gt;age = age;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<p>如果在类<code>A</code>的构造函数中使用初始化列表，就可以避免上面的赋值，而是使用<code>copy-construct</code>实现。</p>\n<p>需要注意，成员初始化的顺序与其在类中声明的顺序相同，与初始化列表中的顺序无关。所以推荐将两者统一。</p>\n<p>讨论完上述情况，再来看一种特殊变量：不同编译单元<code>non-local static</code>变量，是指不在某个函数scope下的<code>static</code>变量。这种变量的初始化顺序是未定义的，所以作者推荐使用单例模式，将它们移动到某个函数中去，明确初始化顺序。这里不再多说了。</p>\n","excerpt":"<p>本系列是《Effective C++》一书的阅读摘记，分章整理各个条款。<br><img src=\"/img/effectivecpp_01_cpp_rely_on_renpin.jpg\" alt=\"写C++需要人品\"><br>","more":"</p>\n<h3 id=\"01-将C-视作语言联邦\"><a href=\"#01-将C-视作语言联邦\" class=\"headerlink\" title=\"01 将C++视作语言联邦\"></a>01 将C++视作语言联邦</h3><p>C++在发明之初，只是一个带类的C。现在的C++已经变成了同时支持面向过程，面向对象，支持泛型和模板元编程的巨兽。这部分内容可以参见“C++的设计与演化”一书。</p>\n<p>本条款中，将C++概括为四个次语言组成的联邦：</p>\n<ul>\n<li>传统C：面向过程，也规定了C++基本的语法。</li>\n<li>OOP：面向对象，带类的C，加入了继承，虚函数等概念。</li>\n<li>Template：很多针对模板需要特殊注意的条款，甚至催生了模板元编程。</li>\n<li>STL：标准模板库。使用STL要遵守它的约定。</li>\n</ul>\n<p>想要高效地使用C++，必须根据不同的情况遵守不同的编程规范。</p>\n<h3 id=\"02-尽量使用const-enum-inline替换-define（以编译器替换预处理器）\"><a href=\"#02-尽量使用const-enum-inline替换-define（以编译器替换预处理器）\" class=\"headerlink\" title=\"02 尽量使用const, enum, inline替换 #define（以编译器替换预处理器）\"></a>02 尽量使用<code>const</code>, <code>enum</code>, <code>inline</code>替换 <code>#define</code>（以编译器替换预处理器）</h3><p><code>#define</code>是C时代遗留下来的预编译指令。</p>\n<p>当<code>#define</code>用来定义某个常量时，通常<code>const</code>是一个更好的选择。</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">define</span> PI 3.14</span></div><div class=\"line\"><span class=\"keyword\">const</span> <span class=\"keyword\">double</span> PI = <span class=\"number\">3.14</span>;</div></pre></td></tr></table></figure>\n<p>当此常量为整数类型（<code>int</code>, <code>char</code>, <code>bool</code>）等时，也可以使用<code>enum</code>定义常量。这种做法常常用在模板元编程中。</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">enum</span> &#123;K = <span class=\"number\">1</span>&#125;;</div></pre></td></tr></table></figure>\n<p>对于<code>const</code>常量，你可以获取变量的地址，但是对于<code>enum</code>来说，无法获取变量的地址。对于这一点来说，<code>enum</code>和<code>#define</code>相类似。</p>\n<p>另一种可能使用<code>#define</code>的场景是宏定义。这种情形可以使用<code>inline</code>声明内联函数解决。</p>\n<p>总之，尽可能相信编译器的力量。使用<code>#define</code>将遮蔽编译器的视野，带来奇怪的问题。</p>\n<h3 id=\"03-尽可能使用const\"><a href=\"#03-尽可能使用const\" class=\"headerlink\" title=\"03 尽可能使用const\"></a>03 尽可能使用<code>const</code></h3><p><code>const</code>不止是给程序员看的，而且为编译器指定了一个语义约束，即这个对象是不该被改变的。所以任何试图修改这个对象的操作，都会被编译器检查出来，并给出error。</p>\n<p>所以，如果某一变量满足<code>const</code>的要求，那么请加上<code>const</code>，和编译器签订一份契约，保护你的行为。</p>\n<p>这里不再讨论<code>const</code>的寻常用法。提示一下：当修饰指针变量时，<code>const</code>在星号左边，是指指针所指物是常量；当<code>const</code>在星号右边，是指指针本身是常量。如下所示：</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">const</span> <span class=\"keyword\">int</span>* p = &amp;a;</div><div class=\"line\">*p = <span class=\"number\">5</span>;   <span class=\"comment\">// 非法</span></div><div class=\"line\">p = &amp;b;   <span class=\"comment\">// 合法</span></div><div class=\"line\"><span class=\"keyword\">int</span>* <span class=\"keyword\">const</span> p = &amp;a;</div><div class=\"line\">p = &amp;b;    <span class=\"comment\">// 非法</span></div><div class=\"line\">*p = <span class=\"number\">5</span>;    <span class=\"comment\">// 合法</span></div></pre></td></tr></table></figure>\n<p>STL中，如果声明某个迭代器为<code>const</code>，是指该迭代器本身是常量；如果你的意思是迭代器指向的元素为常量，那么使用<code>const_iterator</code>。</p>\n<p><code>const</code>更丰富的用法是用于函数声明中，</p>\n<ul>\n<li>当修饰返回值时，意思是返回值不能修改。这可以让你避免无意义的赋值，尤其是以下的错误：</li>\n</ul>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">if</span> (fun(a, b) = c)  <span class=\"comment\">// 这里错把 == 打成了 =</span></div></pre></td></tr></table></figure>\n<ul>\n<li>当修饰参数时，常常用做 pass-by-const-reference 的形式，不再多说了。</li>\n<li>当修饰函数本身时，常常用在类中的成员函数上，意思是这个函数将不改变对象的成员。</li>\n</ul>\n<p>这种情况下，可能会有<code>const</code>重载现象。</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">class</span> my_string&#123;</div><div class=\"line\">  <span class=\"keyword\">const</span> <span class=\"keyword\">char</span>&amp; <span class=\"keyword\">operator</span>[](<span class=\"keyword\">size_t</span> pos) &#123;</div><div class=\"line\">\t  <span class=\"keyword\">return</span> <span class=\"keyword\">this</span>-&gt;ptr[pos];</div><div class=\"line\">  &#125;</div><div class=\"line\">  <span class=\"keyword\">char</span>&amp; <span class=\"keyword\">operator</span>[](<span class=\"keyword\">size_t</span> pos) &#123;</div><div class=\"line\">\t  <span class=\"keyword\">return</span> <span class=\"keyword\">this</span>-&gt;ptr[pos];</div><div class=\"line\">  &#125;</div><div class=\"line\">&#125;;</div></pre></td></tr></table></figure>\n<p>实际调用时，根据调用该函数的对象是否是<code>const</code>的来决定究竟调用哪个版本。</p>\n<p>上面的实现未免过于复杂，我们还可以改成下面的形式：</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">class</span> my_string&#123;</div><div class=\"line\">  <span class=\"keyword\">const</span> <span class=\"keyword\">char</span>&amp; <span class=\"keyword\">operator</span>[](<span class=\"keyword\">size_t</span> pos) &#123;</div><div class=\"line\">\t  <span class=\"keyword\">return</span> <span class=\"keyword\">this</span>-&gt;ptr[pos];</div><div class=\"line\">  &#125;</div><div class=\"line\">  <span class=\"keyword\">char</span>&amp; <span class=\"keyword\">operator</span>[](<span class=\"keyword\">size_t</span> pos) &#123;</div><div class=\"line\">\t  <span class=\"keyword\">return</span> <span class=\"keyword\">const_cast</span>&lt;<span class=\"keyword\">char</span>&amp;&gt;(</div><div class=\"line\">\t         <span class=\"keyword\">static_cast</span>&lt;<span class=\"keyword\">const</span> my_string&amp;&gt;(*<span class=\"keyword\">this</span>)[pos]);</div><div class=\"line\">  &#125;</div><div class=\"line\">&#125;;</div></pre></td></tr></table></figure>\n<p>注意上面的代码进行了两次类型转换。由<code>non-const reference</code>转为<code>const reference</code>是类型安全的，使用<code>static_cast</code>进行。最后我们要脱掉<code>const char&amp;</code>的<code>const</code>属性，使用了<code>const_cast</code>。</p>\n<p>对于<code>const</code>成员函数，有时不得不修改类中的某些成员变量，可以将这些变量声明为<code>mutable</code>。</p>\n<h3 id=\"04-确保对象在使用前已经被初始化\"><a href=\"#04-确保对象在使用前已经被初始化\" class=\"headerlink\" title=\"04 确保对象在使用前已经被初始化\"></a>04 确保对象在使用前已经被初始化</h3><p>使用未被初始化的变量有可能导致未定义的行为，导致奇怪的bug。所以推荐为所有变量进行初始化。</p>\n<p>对于内建类型，需要手动初始化。</p>\n<p>对于用户自定义类型，一般需要调用构造函数初始化。推荐在构造函数中使用初始化列表进行初始化，这样可以避免不必要的性能损失。原因见下：</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> <span class=\"title\">A</span><span class=\"params\">(name, age)</span> </span>&#123;</div><div class=\"line\">  <span class=\"keyword\">this</span>-&gt;name = name; <span class=\"comment\">// 这是赋值，不是初始化！</span></div><div class=\"line\">  <span class=\"keyword\">this</span>-&gt;age = age;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<p>如果在类<code>A</code>的构造函数中使用初始化列表，就可以避免上面的赋值，而是使用<code>copy-construct</code>实现。</p>\n<p>需要注意，成员初始化的顺序与其在类中声明的顺序相同，与初始化列表中的顺序无关。所以推荐将两者统一。</p>\n<p>讨论完上述情况，再来看一种特殊变量：不同编译单元<code>non-local static</code>变量，是指不在某个函数scope下的<code>static</code>变量。这种变量的初始化顺序是未定义的，所以作者推荐使用单例模式，将它们移动到某个函数中去，明确初始化顺序。这里不再多说了。</p>"},{"title":"Effective CPP阅读 - Chapter 3 资源管理","date":"2017-04-25T05:35:43.000Z","_content":"C++相信程序员，将内存等底层资源毫无保留地献给程序员使用。然而，做到正确处理资源，写出健壮的代码并不容易，内存泄漏的幽灵始终徘徊在C++程序员身边。遵守本章给出的建议能够使你尽可能地陷入资源泄漏的泥沼，避免奇怪而又毫无头绪的调试。\n![Pointer是什么？](/img/effectivecpp_02_pointers.png)\n<!-- more -->\n\n## 13 以对象管理资源\n书中对“资源”的解释为：一旦使用，将来必须还给系统。最常见的资源是动态分配的内存，此外还有文件描述器，互斥锁，图像界面的笔刷和字型，数据库连接和网络套接字等。\n\n本条款可以（较浅显地）归纳为：\n> 不要使用裸指针！要使用智能指针！\n\n我们不应该指望程序员有多么富有责任心。当获得资源时，不能寄希望于程序员会“良心发现”，在使用完后将其释放。解决这一问题的方法是使用对象管理资源。这样，当离开对象的作用域之后，对象自动析构，资源就会被返回给系统。\n\n许多资源动态分配在堆中。这种情况下，智能指针是一个很好的选择（在C++11中引入了`weak_ptr`和`shared_ptr`，请使用它们。如果没有C++11，请使用boost）。\n\n以对象管理资源的两个关键想法：\n- 获得资源后立即放进管理对象内。也就是所谓的RAII(Resource Acquisition Is Initialization)。暴露裸指针是危险的！\n- 管理对象利用析构机制确保资源被释放。不论控制流如何离开区块，一旦对象被销毁，其析构函数自然调用，资源被释放。\n\n## 14 在资源管理类中小心coping行为\n有时候，资源并非位于堆中（书中所给例子为互斥锁），这时可能需要我们自己建立资源管理类。\n\n如下面的例子，我们将对不同的底层资源使用情景给出不同的解决方案。\n\n``` cpp\n// Lock是互斥锁的资源管理类\nclass Lock {\npublic:\n    explicit Lock(Mutex* pm):mutexPtr(pm) { // 获得资源\n        lock(mutexPtr);\n    }\n    ~Lock() {unlock(mutexPtr); } //释放资源\nprivate:\n    Mutex* mutexPtr;\n};\n```\n这样，我们期望能够使用`Lock`对象实现对互斥锁的自动管理。\n\n``` cpp\nMutex m;   // 互斥锁\n// ...\n{\nLock ml(&m);\n// ...\n}    // 在区块外，ml自动析构，实现解锁\n```\n\n然而，如何处理`Lock`对象的拷贝？\n\n- 情景一，禁止复制。就像上例，很多时候对互斥锁的复制毫无道理。我们可以使用条款6中的trick禁止类的copying行为发生。\n- 情景二，对底层资源进行引用计数。也许我们可以利用`shared_ptr`，但是需要为其传入参数，指定其析构时并不是要返还资源，而是要解锁。具体请参看`shared_ptr`部分文档。\n- 情景三，复制底层资源。这时候要注意深度拷贝，例如字符串数组。\n- 情景四，移除底层资源所有权。将所有权移至新的对象。\n\n## 15 在资源管理类中提供对原始资源的访问\n许多API（尤其是和遗留下来的C代码API交互）时，需要获取底层资源的指涉。\n\n对于这种情况，智能指针提供了`get()`函数用来获取其原始指针的拷贝。同时，它们也重载了`->`和`*`操作符，允许隐式转换为原始指针。\n\n我们的自定义资源管理类也可以参考它们的实现。其中，隐式转换到类型`T`可以通过定义`operator T()`实现。隐式类型转换可能使得代码量更少，客户更方便。但是！请慎用隐式类型转换。\n\n## 16 成对使用`new`和`delete`时采取相同的形式\n这项条款是说如果动态分配内存时候使用了`new T()`得到了单个对象的内存空间，那么销毁时应该使用`delete`销毁；如果当初使用了`new T[]`得到了对象数组空间，那么销毁时应该使用`delete []`。两者不能混用，否则会导致未定义行为。\n\n另外，除非必要，不要使用原始数组。STL中的`vector`和`string`是替代数组的不错选择。\n\n## 17 以独立语句将newed对象置于智能指针\n以独立语句将newed对象存储于智能指针，否则一旦发生异常，有可能导致难以察觉的内存泄露。\n\n书中给出了一个例子，是由于逗号表达式的执行顺序不定造成的。\n\n如下面的函数声明：\n\n``` cpp\nint priority() { /*some code*/}\nvoid process(shared_ptr<Widget> pw, int priority) { /*some code*/}\n```\n在使用时，也许你会这样调用`process`函数。\n\n``` cpp\nprocess(new Widget(), priority());\n```\n首先，这样是不能通过编译器的。因为`shared_ptr`的构造函数是`explicit`的，不能够隐式将原始指针转换为`shared_ptr`对象。但是改为下面的代码就没问题了吗？\n\n``` cpp\nprocess(shared_ptr<Widget>(new Widget()), priority());\n```\n\n由于C++中函数参数的核算顺序是不确定的，所以可能发生：\n\n- new出来一个Widget资源\n- 调用`priority()`函数，注意此时可能引发异常，使得Widget资源无法回收\n- 构造`shared_ptr`对象\n\n问题已经很明确了。所以我们应该首先确保资源确实被智能指针获取到了，使用下面的独立语句更好。\n\n``` cpp\nauto pw = shared_ptr<Widget>(new Widget());\nprocess(pw, priority());\n```\n","source":"_posts/effective-cpp-03.md","raw":"---\ntitle: Effective CPP阅读 - Chapter 3 资源管理\ndate: 2017-04-25 13:35:43\ntags:\n    - cpp\n---\nC++相信程序员，将内存等底层资源毫无保留地献给程序员使用。然而，做到正确处理资源，写出健壮的代码并不容易，内存泄漏的幽灵始终徘徊在C++程序员身边。遵守本章给出的建议能够使你尽可能地陷入资源泄漏的泥沼，避免奇怪而又毫无头绪的调试。\n![Pointer是什么？](/img/effectivecpp_02_pointers.png)\n<!-- more -->\n\n## 13 以对象管理资源\n书中对“资源”的解释为：一旦使用，将来必须还给系统。最常见的资源是动态分配的内存，此外还有文件描述器，互斥锁，图像界面的笔刷和字型，数据库连接和网络套接字等。\n\n本条款可以（较浅显地）归纳为：\n> 不要使用裸指针！要使用智能指针！\n\n我们不应该指望程序员有多么富有责任心。当获得资源时，不能寄希望于程序员会“良心发现”，在使用完后将其释放。解决这一问题的方法是使用对象管理资源。这样，当离开对象的作用域之后，对象自动析构，资源就会被返回给系统。\n\n许多资源动态分配在堆中。这种情况下，智能指针是一个很好的选择（在C++11中引入了`weak_ptr`和`shared_ptr`，请使用它们。如果没有C++11，请使用boost）。\n\n以对象管理资源的两个关键想法：\n- 获得资源后立即放进管理对象内。也就是所谓的RAII(Resource Acquisition Is Initialization)。暴露裸指针是危险的！\n- 管理对象利用析构机制确保资源被释放。不论控制流如何离开区块，一旦对象被销毁，其析构函数自然调用，资源被释放。\n\n## 14 在资源管理类中小心coping行为\n有时候，资源并非位于堆中（书中所给例子为互斥锁），这时可能需要我们自己建立资源管理类。\n\n如下面的例子，我们将对不同的底层资源使用情景给出不同的解决方案。\n\n``` cpp\n// Lock是互斥锁的资源管理类\nclass Lock {\npublic:\n    explicit Lock(Mutex* pm):mutexPtr(pm) { // 获得资源\n        lock(mutexPtr);\n    }\n    ~Lock() {unlock(mutexPtr); } //释放资源\nprivate:\n    Mutex* mutexPtr;\n};\n```\n这样，我们期望能够使用`Lock`对象实现对互斥锁的自动管理。\n\n``` cpp\nMutex m;   // 互斥锁\n// ...\n{\nLock ml(&m);\n// ...\n}    // 在区块外，ml自动析构，实现解锁\n```\n\n然而，如何处理`Lock`对象的拷贝？\n\n- 情景一，禁止复制。就像上例，很多时候对互斥锁的复制毫无道理。我们可以使用条款6中的trick禁止类的copying行为发生。\n- 情景二，对底层资源进行引用计数。也许我们可以利用`shared_ptr`，但是需要为其传入参数，指定其析构时并不是要返还资源，而是要解锁。具体请参看`shared_ptr`部分文档。\n- 情景三，复制底层资源。这时候要注意深度拷贝，例如字符串数组。\n- 情景四，移除底层资源所有权。将所有权移至新的对象。\n\n## 15 在资源管理类中提供对原始资源的访问\n许多API（尤其是和遗留下来的C代码API交互）时，需要获取底层资源的指涉。\n\n对于这种情况，智能指针提供了`get()`函数用来获取其原始指针的拷贝。同时，它们也重载了`->`和`*`操作符，允许隐式转换为原始指针。\n\n我们的自定义资源管理类也可以参考它们的实现。其中，隐式转换到类型`T`可以通过定义`operator T()`实现。隐式类型转换可能使得代码量更少，客户更方便。但是！请慎用隐式类型转换。\n\n## 16 成对使用`new`和`delete`时采取相同的形式\n这项条款是说如果动态分配内存时候使用了`new T()`得到了单个对象的内存空间，那么销毁时应该使用`delete`销毁；如果当初使用了`new T[]`得到了对象数组空间，那么销毁时应该使用`delete []`。两者不能混用，否则会导致未定义行为。\n\n另外，除非必要，不要使用原始数组。STL中的`vector`和`string`是替代数组的不错选择。\n\n## 17 以独立语句将newed对象置于智能指针\n以独立语句将newed对象存储于智能指针，否则一旦发生异常，有可能导致难以察觉的内存泄露。\n\n书中给出了一个例子，是由于逗号表达式的执行顺序不定造成的。\n\n如下面的函数声明：\n\n``` cpp\nint priority() { /*some code*/}\nvoid process(shared_ptr<Widget> pw, int priority) { /*some code*/}\n```\n在使用时，也许你会这样调用`process`函数。\n\n``` cpp\nprocess(new Widget(), priority());\n```\n首先，这样是不能通过编译器的。因为`shared_ptr`的构造函数是`explicit`的，不能够隐式将原始指针转换为`shared_ptr`对象。但是改为下面的代码就没问题了吗？\n\n``` cpp\nprocess(shared_ptr<Widget>(new Widget()), priority());\n```\n\n由于C++中函数参数的核算顺序是不确定的，所以可能发生：\n\n- new出来一个Widget资源\n- 调用`priority()`函数，注意此时可能引发异常，使得Widget资源无法回收\n- 构造`shared_ptr`对象\n\n问题已经很明确了。所以我们应该首先确保资源确实被智能指针获取到了，使用下面的独立语句更好。\n\n``` cpp\nauto pw = shared_ptr<Widget>(new Widget());\nprocess(pw, priority());\n```\n","slug":"effective-cpp-03","published":1,"updated":"2018-01-12T06:22:20.464Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjcu6vcso000squ465w2o0909","content":"<p>C++相信程序员，将内存等底层资源毫无保留地献给程序员使用。然而，做到正确处理资源，写出健壮的代码并不容易，内存泄漏的幽灵始终徘徊在C++程序员身边。遵守本章给出的建议能够使你尽可能地陷入资源泄漏的泥沼，避免奇怪而又毫无头绪的调试。<br><img src=\"/img/effectivecpp_02_pointers.png\" alt=\"Pointer是什么？\"><br><a id=\"more\"></a></p>\n<h2 id=\"13-以对象管理资源\"><a href=\"#13-以对象管理资源\" class=\"headerlink\" title=\"13 以对象管理资源\"></a>13 以对象管理资源</h2><p>书中对“资源”的解释为：一旦使用，将来必须还给系统。最常见的资源是动态分配的内存，此外还有文件描述器，互斥锁，图像界面的笔刷和字型，数据库连接和网络套接字等。</p>\n<p>本条款可以（较浅显地）归纳为：</p>\n<blockquote>\n<p>不要使用裸指针！要使用智能指针！</p>\n</blockquote>\n<p>我们不应该指望程序员有多么富有责任心。当获得资源时，不能寄希望于程序员会“良心发现”，在使用完后将其释放。解决这一问题的方法是使用对象管理资源。这样，当离开对象的作用域之后，对象自动析构，资源就会被返回给系统。</p>\n<p>许多资源动态分配在堆中。这种情况下，智能指针是一个很好的选择（在C++11中引入了<code>weak_ptr</code>和<code>shared_ptr</code>，请使用它们。如果没有C++11，请使用boost）。</p>\n<p>以对象管理资源的两个关键想法：</p>\n<ul>\n<li>获得资源后立即放进管理对象内。也就是所谓的RAII(Resource Acquisition Is Initialization)。暴露裸指针是危险的！</li>\n<li>管理对象利用析构机制确保资源被释放。不论控制流如何离开区块，一旦对象被销毁，其析构函数自然调用，资源被释放。</li>\n</ul>\n<h2 id=\"14-在资源管理类中小心coping行为\"><a href=\"#14-在资源管理类中小心coping行为\" class=\"headerlink\" title=\"14 在资源管理类中小心coping行为\"></a>14 在资源管理类中小心coping行为</h2><p>有时候，资源并非位于堆中（书中所给例子为互斥锁），这时可能需要我们自己建立资源管理类。</p>\n<p>如下面的例子，我们将对不同的底层资源使用情景给出不同的解决方案。</p>\n<figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div></pre></td><td class=\"code\"><pre><div class=\"line\">// Lock是互斥锁的资源管理类</div><div class=\"line\">class Lock &#123;</div><div class=\"line\">public:</div><div class=\"line\">    explicit Lock(Mutex* pm):mutexPtr(pm) &#123; // 获得资源</div><div class=\"line\">        lock(mutexPtr);</div><div class=\"line\">    &#125;</div><div class=\"line\">    ~Lock() &#123;unlock(mutexPtr); &#125; //释放资源</div><div class=\"line\">private:</div><div class=\"line\">    Mutex* mutexPtr;</div><div class=\"line\">&#125;;</div></pre></td></tr></table></figure>\n<p>这样，我们期望能够使用<code>Lock</code>对象实现对互斥锁的自动管理。</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div></pre></td><td class=\"code\"><pre><div class=\"line\">Mutex m;   <span class=\"comment\">// 互斥锁</span></div><div class=\"line\"><span class=\"comment\">// ...</span></div><div class=\"line\">&#123;</div><div class=\"line\"><span class=\"function\">Lock <span class=\"title\">ml</span><span class=\"params\">(&amp;m)</span></span>;</div><div class=\"line\"><span class=\"comment\">// ...</span></div><div class=\"line\">&#125;    <span class=\"comment\">// 在区块外，ml自动析构，实现解锁</span></div></pre></td></tr></table></figure>\n<p>然而，如何处理<code>Lock</code>对象的拷贝？</p>\n<ul>\n<li>情景一，禁止复制。就像上例，很多时候对互斥锁的复制毫无道理。我们可以使用条款6中的trick禁止类的copying行为发生。</li>\n<li>情景二，对底层资源进行引用计数。也许我们可以利用<code>shared_ptr</code>，但是需要为其传入参数，指定其析构时并不是要返还资源，而是要解锁。具体请参看<code>shared_ptr</code>部分文档。</li>\n<li>情景三，复制底层资源。这时候要注意深度拷贝，例如字符串数组。</li>\n<li>情景四，移除底层资源所有权。将所有权移至新的对象。</li>\n</ul>\n<h2 id=\"15-在资源管理类中提供对原始资源的访问\"><a href=\"#15-在资源管理类中提供对原始资源的访问\" class=\"headerlink\" title=\"15 在资源管理类中提供对原始资源的访问\"></a>15 在资源管理类中提供对原始资源的访问</h2><p>许多API（尤其是和遗留下来的C代码API交互）时，需要获取底层资源的指涉。</p>\n<p>对于这种情况，智能指针提供了<code>get()</code>函数用来获取其原始指针的拷贝。同时，它们也重载了<code>-&gt;</code>和<code>*</code>操作符，允许隐式转换为原始指针。</p>\n<p>我们的自定义资源管理类也可以参考它们的实现。其中，隐式转换到类型<code>T</code>可以通过定义<code>operator T()</code>实现。隐式类型转换可能使得代码量更少，客户更方便。但是！请慎用隐式类型转换。</p>\n<h2 id=\"16-成对使用new和delete时采取相同的形式\"><a href=\"#16-成对使用new和delete时采取相同的形式\" class=\"headerlink\" title=\"16 成对使用new和delete时采取相同的形式\"></a>16 成对使用<code>new</code>和<code>delete</code>时采取相同的形式</h2><p>这项条款是说如果动态分配内存时候使用了<code>new T()</code>得到了单个对象的内存空间，那么销毁时应该使用<code>delete</code>销毁；如果当初使用了<code>new T[]</code>得到了对象数组空间，那么销毁时应该使用<code>delete []</code>。两者不能混用，否则会导致未定义行为。</p>\n<p>另外，除非必要，不要使用原始数组。STL中的<code>vector</code>和<code>string</code>是替代数组的不错选择。</p>\n<h2 id=\"17-以独立语句将newed对象置于智能指针\"><a href=\"#17-以独立语句将newed对象置于智能指针\" class=\"headerlink\" title=\"17 以独立语句将newed对象置于智能指针\"></a>17 以独立语句将newed对象置于智能指针</h2><p>以独立语句将newed对象存储于智能指针，否则一旦发生异常，有可能导致难以察觉的内存泄露。</p>\n<p>书中给出了一个例子，是由于逗号表达式的执行顺序不定造成的。</p>\n<p>如下面的函数声明：</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"function\"><span class=\"keyword\">int</span> <span class=\"title\">priority</span><span class=\"params\">()</span> </span>&#123; <span class=\"comment\">/*some code*/</span>&#125;</div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">process</span><span class=\"params\">(<span class=\"built_in\">shared_ptr</span>&lt;Widget&gt; pw, <span class=\"keyword\">int</span> priority)</span> </span>&#123; <span class=\"comment\">/*some code*/</span>&#125;</div></pre></td></tr></table></figure>\n<p>在使用时，也许你会这样调用<code>process</code>函数。</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">process(<span class=\"keyword\">new</span> Widget(), priority());</div></pre></td></tr></table></figure>\n<p>首先，这样是不能通过编译器的。因为<code>shared_ptr</code>的构造函数是<code>explicit</code>的，不能够隐式将原始指针转换为<code>shared_ptr</code>对象。但是改为下面的代码就没问题了吗？</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">process(<span class=\"built_in\">shared_ptr</span>&lt;Widget&gt;(<span class=\"keyword\">new</span> Widget()), priority());</div></pre></td></tr></table></figure>\n<p>由于C++中函数参数的核算顺序是不确定的，所以可能发生：</p>\n<ul>\n<li>new出来一个Widget资源</li>\n<li>调用<code>priority()</code>函数，注意此时可能引发异常，使得Widget资源无法回收</li>\n<li>构造<code>shared_ptr</code>对象</li>\n</ul>\n<p>问题已经很明确了。所以我们应该首先确保资源确实被智能指针获取到了，使用下面的独立语句更好。</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">auto</span> pw = <span class=\"built_in\">shared_ptr</span>&lt;Widget&gt;(<span class=\"keyword\">new</span> Widget());</div><div class=\"line\">process(pw, priority());</div></pre></td></tr></table></figure>\n","excerpt":"<p>C++相信程序员，将内存等底层资源毫无保留地献给程序员使用。然而，做到正确处理资源，写出健壮的代码并不容易，内存泄漏的幽灵始终徘徊在C++程序员身边。遵守本章给出的建议能够使你尽可能地陷入资源泄漏的泥沼，避免奇怪而又毫无头绪的调试。<br><img src=\"/img/effectivecpp_02_pointers.png\" alt=\"Pointer是什么？\"><br>","more":"</p>\n<h2 id=\"13-以对象管理资源\"><a href=\"#13-以对象管理资源\" class=\"headerlink\" title=\"13 以对象管理资源\"></a>13 以对象管理资源</h2><p>书中对“资源”的解释为：一旦使用，将来必须还给系统。最常见的资源是动态分配的内存，此外还有文件描述器，互斥锁，图像界面的笔刷和字型，数据库连接和网络套接字等。</p>\n<p>本条款可以（较浅显地）归纳为：</p>\n<blockquote>\n<p>不要使用裸指针！要使用智能指针！</p>\n</blockquote>\n<p>我们不应该指望程序员有多么富有责任心。当获得资源时，不能寄希望于程序员会“良心发现”，在使用完后将其释放。解决这一问题的方法是使用对象管理资源。这样，当离开对象的作用域之后，对象自动析构，资源就会被返回给系统。</p>\n<p>许多资源动态分配在堆中。这种情况下，智能指针是一个很好的选择（在C++11中引入了<code>weak_ptr</code>和<code>shared_ptr</code>，请使用它们。如果没有C++11，请使用boost）。</p>\n<p>以对象管理资源的两个关键想法：</p>\n<ul>\n<li>获得资源后立即放进管理对象内。也就是所谓的RAII(Resource Acquisition Is Initialization)。暴露裸指针是危险的！</li>\n<li>管理对象利用析构机制确保资源被释放。不论控制流如何离开区块，一旦对象被销毁，其析构函数自然调用，资源被释放。</li>\n</ul>\n<h2 id=\"14-在资源管理类中小心coping行为\"><a href=\"#14-在资源管理类中小心coping行为\" class=\"headerlink\" title=\"14 在资源管理类中小心coping行为\"></a>14 在资源管理类中小心coping行为</h2><p>有时候，资源并非位于堆中（书中所给例子为互斥锁），这时可能需要我们自己建立资源管理类。</p>\n<p>如下面的例子，我们将对不同的底层资源使用情景给出不同的解决方案。</p>\n<figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div></pre></td><td class=\"code\"><pre><div class=\"line\">// Lock是互斥锁的资源管理类</div><div class=\"line\">class Lock &#123;</div><div class=\"line\">public:</div><div class=\"line\">    explicit Lock(Mutex* pm):mutexPtr(pm) &#123; // 获得资源</div><div class=\"line\">        lock(mutexPtr);</div><div class=\"line\">    &#125;</div><div class=\"line\">    ~Lock() &#123;unlock(mutexPtr); &#125; //释放资源</div><div class=\"line\">private:</div><div class=\"line\">    Mutex* mutexPtr;</div><div class=\"line\">&#125;;</div></pre></td></tr></table></figure>\n<p>这样，我们期望能够使用<code>Lock</code>对象实现对互斥锁的自动管理。</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div></pre></td><td class=\"code\"><pre><div class=\"line\">Mutex m;   <span class=\"comment\">// 互斥锁</span></div><div class=\"line\"><span class=\"comment\">// ...</span></div><div class=\"line\">&#123;</div><div class=\"line\"><span class=\"function\">Lock <span class=\"title\">ml</span><span class=\"params\">(&amp;m)</span></span>;</div><div class=\"line\"><span class=\"comment\">// ...</span></div><div class=\"line\">&#125;    <span class=\"comment\">// 在区块外，ml自动析构，实现解锁</span></div></pre></td></tr></table></figure>\n<p>然而，如何处理<code>Lock</code>对象的拷贝？</p>\n<ul>\n<li>情景一，禁止复制。就像上例，很多时候对互斥锁的复制毫无道理。我们可以使用条款6中的trick禁止类的copying行为发生。</li>\n<li>情景二，对底层资源进行引用计数。也许我们可以利用<code>shared_ptr</code>，但是需要为其传入参数，指定其析构时并不是要返还资源，而是要解锁。具体请参看<code>shared_ptr</code>部分文档。</li>\n<li>情景三，复制底层资源。这时候要注意深度拷贝，例如字符串数组。</li>\n<li>情景四，移除底层资源所有权。将所有权移至新的对象。</li>\n</ul>\n<h2 id=\"15-在资源管理类中提供对原始资源的访问\"><a href=\"#15-在资源管理类中提供对原始资源的访问\" class=\"headerlink\" title=\"15 在资源管理类中提供对原始资源的访问\"></a>15 在资源管理类中提供对原始资源的访问</h2><p>许多API（尤其是和遗留下来的C代码API交互）时，需要获取底层资源的指涉。</p>\n<p>对于这种情况，智能指针提供了<code>get()</code>函数用来获取其原始指针的拷贝。同时，它们也重载了<code>-&gt;</code>和<code>*</code>操作符，允许隐式转换为原始指针。</p>\n<p>我们的自定义资源管理类也可以参考它们的实现。其中，隐式转换到类型<code>T</code>可以通过定义<code>operator T()</code>实现。隐式类型转换可能使得代码量更少，客户更方便。但是！请慎用隐式类型转换。</p>\n<h2 id=\"16-成对使用new和delete时采取相同的形式\"><a href=\"#16-成对使用new和delete时采取相同的形式\" class=\"headerlink\" title=\"16 成对使用new和delete时采取相同的形式\"></a>16 成对使用<code>new</code>和<code>delete</code>时采取相同的形式</h2><p>这项条款是说如果动态分配内存时候使用了<code>new T()</code>得到了单个对象的内存空间，那么销毁时应该使用<code>delete</code>销毁；如果当初使用了<code>new T[]</code>得到了对象数组空间，那么销毁时应该使用<code>delete []</code>。两者不能混用，否则会导致未定义行为。</p>\n<p>另外，除非必要，不要使用原始数组。STL中的<code>vector</code>和<code>string</code>是替代数组的不错选择。</p>\n<h2 id=\"17-以独立语句将newed对象置于智能指针\"><a href=\"#17-以独立语句将newed对象置于智能指针\" class=\"headerlink\" title=\"17 以独立语句将newed对象置于智能指针\"></a>17 以独立语句将newed对象置于智能指针</h2><p>以独立语句将newed对象存储于智能指针，否则一旦发生异常，有可能导致难以察觉的内存泄露。</p>\n<p>书中给出了一个例子，是由于逗号表达式的执行顺序不定造成的。</p>\n<p>如下面的函数声明：</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"function\"><span class=\"keyword\">int</span> <span class=\"title\">priority</span><span class=\"params\">()</span> </span>&#123; <span class=\"comment\">/*some code*/</span>&#125;</div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">process</span><span class=\"params\">(<span class=\"built_in\">shared_ptr</span>&lt;Widget&gt; pw, <span class=\"keyword\">int</span> priority)</span> </span>&#123; <span class=\"comment\">/*some code*/</span>&#125;</div></pre></td></tr></table></figure>\n<p>在使用时，也许你会这样调用<code>process</code>函数。</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">process(<span class=\"keyword\">new</span> Widget(), priority());</div></pre></td></tr></table></figure>\n<p>首先，这样是不能通过编译器的。因为<code>shared_ptr</code>的构造函数是<code>explicit</code>的，不能够隐式将原始指针转换为<code>shared_ptr</code>对象。但是改为下面的代码就没问题了吗？</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">process(<span class=\"built_in\">shared_ptr</span>&lt;Widget&gt;(<span class=\"keyword\">new</span> Widget()), priority());</div></pre></td></tr></table></figure>\n<p>由于C++中函数参数的核算顺序是不确定的，所以可能发生：</p>\n<ul>\n<li>new出来一个Widget资源</li>\n<li>调用<code>priority()</code>函数，注意此时可能引发异常，使得Widget资源无法回收</li>\n<li>构造<code>shared_ptr</code>对象</li>\n</ul>\n<p>问题已经很明确了。所以我们应该首先确保资源确实被智能指针获取到了，使用下面的独立语句更好。</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">auto</span> pw = <span class=\"built_in\">shared_ptr</span>&lt;Widget&gt;(<span class=\"keyword\">new</span> Widget());</div><div class=\"line\">process(pw, priority());</div></pre></td></tr></table></figure>"},{"title":"doc2dash——制作自己的dash文档","date":"2017-08-26T11:32:00.000Z","_content":"Dash是Mac上一款超棒的应用，提供了诸如C/C++/Python甚至是OpenCV/Vim等软件包或工具软件的参考文档。只要使用App的“Download Docsets”功能，就能轻松下载相应文档。使用的时候只需在Dash的搜索框内输入相应关键词，Dash会在所有文档中进行搜索，给出相关内容的列表。点击我们要寻找的条款，就能够直接在本地阅读文档。在Ubuntu/Windows平台上，Dash也有对应的替代品，例如[zeal](https://zealdocs.org)就是一款Windows/Linux平台通用的Dash替代软件。\n\n这样强大的软件，如果只能使用官方提供的文档岂不是有些大材小用？[doc2dash](https://doc2dash.readthedocs.io/en/stable/)就是一款能够自动生成Dash兼容docset文件的工具。例如，可以使用它为PyTorch生成本地化的docset文件，并导入Dash/zeal中，在本地进行搜索阅读。这不是美滋滋？\n\n本文章是基于doc2dash的官方介绍，对其使用进行的总结。\n![Demo](/img/doc2dash_pytorch_example.jpg)\n\n<!-- more -->\n## 安装doc2dash\ndoc2dash是基于Python开发的。按照官方网站介绍，为了避免Python包的冲突，最好使用虚拟环境进行安装。我的机器上安装有Anaconda环境，所以首先使用`conda create`命令新建用于doc2dash的虚拟环境。\n``` sh\nconda create -n doc2dash\n```\n接下来，激活虚拟环境，并使用`pip install`命令安装。\n``` sh\nsource activate doc2dash\npip install doc2dash\n```\n\ndoc2dash支持的输出格式可以通过sphinx或者pydoctor。其中前者更加常用。下面以PyTorch项目的文档生成为例，介绍doc2dash的具体用法。\n## 生成PyTorch文档\ndoc2dash使用sphinx生成相应的文档。在上述安装doc2dash的过程中，应该已经安装了sphinx包。不过我们还需要手动安装，以便处理rst文档。\n\n```\npip install sphinx_rtd_theme\n```\n进入PyTorch的文档目录`docs/`，PyTorch已经为我们提供了Makefile，调用sphinx包进行文档处理，可以选择`make html`命令生成相应的HTML文档，生成的位置为`build/html`。\n\n```\n# in directory $PYTORCH/docs, run\nmake html\n```\n\n接下来，就可以使用doc2dash来继续sphinx的工作，生成Dash可用的文档文件了~使用`-n`指定生成的文件名称，后面跟source文件夹路径即可。\n\n``` sh\n# $PYTORCH/docs/build/html即为生成的HTML目录\ndoc2dash -n pytorch $PYTORCH/docs/build/html\n```\n\n之后，把生成的`pytorch.docset`导入到Dash中即可。如下图所示，点击“+”找到文件添加即可。\n![添加docset](/img/doc2dash_how_to_add_docset.jpg)\n\n## 在Ubuntu上安装zeal\nzeal是Dash在非Mac平台上的替代软件。在Ubuntu上可以使用如下方式轻松安装（见[官方网站介绍](https://zealdocs.org/download.html#linux)）。\n\n``` sh\nsudo add-apt-repository ppa:zeal-developers/ppa\nsudo apt-get update\nsudo apt-get install zeal\n```\n\n安装后，可以使用`Tool/Docsets`下载相应的公开文档。如果想要添加自己生成的文档，只需要将生成的docset文件放到软件的文档库中即可，默认位置应在`$HOME/.local/share/Zeal/Zeal/docsets`。\n","source":"_posts/doc2dash-usage.md","raw":"---\ntitle: doc2dash——制作自己的dash文档\ndate: 2017-08-26 19:32:00\ntags:\n    - tool\n---\nDash是Mac上一款超棒的应用，提供了诸如C/C++/Python甚至是OpenCV/Vim等软件包或工具软件的参考文档。只要使用App的“Download Docsets”功能，就能轻松下载相应文档。使用的时候只需在Dash的搜索框内输入相应关键词，Dash会在所有文档中进行搜索，给出相关内容的列表。点击我们要寻找的条款，就能够直接在本地阅读文档。在Ubuntu/Windows平台上，Dash也有对应的替代品，例如[zeal](https://zealdocs.org)就是一款Windows/Linux平台通用的Dash替代软件。\n\n这样强大的软件，如果只能使用官方提供的文档岂不是有些大材小用？[doc2dash](https://doc2dash.readthedocs.io/en/stable/)就是一款能够自动生成Dash兼容docset文件的工具。例如，可以使用它为PyTorch生成本地化的docset文件，并导入Dash/zeal中，在本地进行搜索阅读。这不是美滋滋？\n\n本文章是基于doc2dash的官方介绍，对其使用进行的总结。\n![Demo](/img/doc2dash_pytorch_example.jpg)\n\n<!-- more -->\n## 安装doc2dash\ndoc2dash是基于Python开发的。按照官方网站介绍，为了避免Python包的冲突，最好使用虚拟环境进行安装。我的机器上安装有Anaconda环境，所以首先使用`conda create`命令新建用于doc2dash的虚拟环境。\n``` sh\nconda create -n doc2dash\n```\n接下来，激活虚拟环境，并使用`pip install`命令安装。\n``` sh\nsource activate doc2dash\npip install doc2dash\n```\n\ndoc2dash支持的输出格式可以通过sphinx或者pydoctor。其中前者更加常用。下面以PyTorch项目的文档生成为例，介绍doc2dash的具体用法。\n## 生成PyTorch文档\ndoc2dash使用sphinx生成相应的文档。在上述安装doc2dash的过程中，应该已经安装了sphinx包。不过我们还需要手动安装，以便处理rst文档。\n\n```\npip install sphinx_rtd_theme\n```\n进入PyTorch的文档目录`docs/`，PyTorch已经为我们提供了Makefile，调用sphinx包进行文档处理，可以选择`make html`命令生成相应的HTML文档，生成的位置为`build/html`。\n\n```\n# in directory $PYTORCH/docs, run\nmake html\n```\n\n接下来，就可以使用doc2dash来继续sphinx的工作，生成Dash可用的文档文件了~使用`-n`指定生成的文件名称，后面跟source文件夹路径即可。\n\n``` sh\n# $PYTORCH/docs/build/html即为生成的HTML目录\ndoc2dash -n pytorch $PYTORCH/docs/build/html\n```\n\n之后，把生成的`pytorch.docset`导入到Dash中即可。如下图所示，点击“+”找到文件添加即可。\n![添加docset](/img/doc2dash_how_to_add_docset.jpg)\n\n## 在Ubuntu上安装zeal\nzeal是Dash在非Mac平台上的替代软件。在Ubuntu上可以使用如下方式轻松安装（见[官方网站介绍](https://zealdocs.org/download.html#linux)）。\n\n``` sh\nsudo add-apt-repository ppa:zeal-developers/ppa\nsudo apt-get update\nsudo apt-get install zeal\n```\n\n安装后，可以使用`Tool/Docsets`下载相应的公开文档。如果想要添加自己生成的文档，只需要将生成的docset文件放到软件的文档库中即可，默认位置应在`$HOME/.local/share/Zeal/Zeal/docsets`。\n","slug":"doc2dash-usage","published":1,"updated":"2018-01-12T06:22:20.463Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjcu6vcsq000vqu467j0mbntx","content":"<p>Dash是Mac上一款超棒的应用，提供了诸如C/C++/Python甚至是OpenCV/Vim等软件包或工具软件的参考文档。只要使用App的“Download Docsets”功能，就能轻松下载相应文档。使用的时候只需在Dash的搜索框内输入相应关键词，Dash会在所有文档中进行搜索，给出相关内容的列表。点击我们要寻找的条款，就能够直接在本地阅读文档。在Ubuntu/Windows平台上，Dash也有对应的替代品，例如<a href=\"https://zealdocs.org\" target=\"_blank\" rel=\"external\">zeal</a>就是一款Windows/Linux平台通用的Dash替代软件。</p>\n<p>这样强大的软件，如果只能使用官方提供的文档岂不是有些大材小用？<a href=\"https://doc2dash.readthedocs.io/en/stable/\" target=\"_blank\" rel=\"external\">doc2dash</a>就是一款能够自动生成Dash兼容docset文件的工具。例如，可以使用它为PyTorch生成本地化的docset文件，并导入Dash/zeal中，在本地进行搜索阅读。这不是美滋滋？</p>\n<p>本文章是基于doc2dash的官方介绍，对其使用进行的总结。<br><img src=\"/img/doc2dash_pytorch_example.jpg\" alt=\"Demo\"></p>\n<a id=\"more\"></a>\n<h2 id=\"安装doc2dash\"><a href=\"#安装doc2dash\" class=\"headerlink\" title=\"安装doc2dash\"></a>安装doc2dash</h2><p>doc2dash是基于Python开发的。按照官方网站介绍，为了避免Python包的冲突，最好使用虚拟环境进行安装。我的机器上安装有Anaconda环境，所以首先使用<code>conda create</code>命令新建用于doc2dash的虚拟环境。<br><figure class=\"highlight sh\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">conda create -n doc2dash</div></pre></td></tr></table></figure></p>\n<p>接下来，激活虚拟环境，并使用<code>pip install</code>命令安装。<br><figure class=\"highlight sh\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"built_in\">source</span> activate doc2dash</div><div class=\"line\">pip install doc2dash</div></pre></td></tr></table></figure></p>\n<p>doc2dash支持的输出格式可以通过sphinx或者pydoctor。其中前者更加常用。下面以PyTorch项目的文档生成为例，介绍doc2dash的具体用法。</p>\n<h2 id=\"生成PyTorch文档\"><a href=\"#生成PyTorch文档\" class=\"headerlink\" title=\"生成PyTorch文档\"></a>生成PyTorch文档</h2><p>doc2dash使用sphinx生成相应的文档。在上述安装doc2dash的过程中，应该已经安装了sphinx包。不过我们还需要手动安装，以便处理rst文档。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">pip install sphinx_rtd_theme</div></pre></td></tr></table></figure>\n<p>进入PyTorch的文档目录<code>docs/</code>，PyTorch已经为我们提供了Makefile，调用sphinx包进行文档处理，可以选择<code>make html</code>命令生成相应的HTML文档，生成的位置为<code>build/html</code>。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\"># in directory $PYTORCH/docs, run</div><div class=\"line\">make html</div></pre></td></tr></table></figure>\n<p>接下来，就可以使用doc2dash来继续sphinx的工作，生成Dash可用的文档文件了~使用<code>-n</code>指定生成的文件名称，后面跟source文件夹路径即可。</p>\n<figure class=\"highlight sh\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\"># $PYTORCH/docs/build/html即为生成的HTML目录</span></div><div class=\"line\">doc2dash -n pytorch <span class=\"variable\">$PYTORCH</span>/docs/build/html</div></pre></td></tr></table></figure>\n<p>之后，把生成的<code>pytorch.docset</code>导入到Dash中即可。如下图所示，点击“+”找到文件添加即可。<br><img src=\"/img/doc2dash_how_to_add_docset.jpg\" alt=\"添加docset\"></p>\n<h2 id=\"在Ubuntu上安装zeal\"><a href=\"#在Ubuntu上安装zeal\" class=\"headerlink\" title=\"在Ubuntu上安装zeal\"></a>在Ubuntu上安装zeal</h2><p>zeal是Dash在非Mac平台上的替代软件。在Ubuntu上可以使用如下方式轻松安装（见<a href=\"https://zealdocs.org/download.html#linux\" target=\"_blank\" rel=\"external\">官方网站介绍</a>）。</p>\n<figure class=\"highlight sh\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\">sudo add-apt-repository ppa:zeal-developers/ppa</div><div class=\"line\">sudo apt-get update</div><div class=\"line\">sudo apt-get install zeal</div></pre></td></tr></table></figure>\n<p>安装后，可以使用<code>Tool/Docsets</code>下载相应的公开文档。如果想要添加自己生成的文档，只需要将生成的docset文件放到软件的文档库中即可，默认位置应在<code>$HOME/.local/share/Zeal/Zeal/docsets</code>。</p>\n","excerpt":"<p>Dash是Mac上一款超棒的应用，提供了诸如C/C++/Python甚至是OpenCV/Vim等软件包或工具软件的参考文档。只要使用App的“Download Docsets”功能，就能轻松下载相应文档。使用的时候只需在Dash的搜索框内输入相应关键词，Dash会在所有文档中进行搜索，给出相关内容的列表。点击我们要寻找的条款，就能够直接在本地阅读文档。在Ubuntu/Windows平台上，Dash也有对应的替代品，例如<a href=\"https://zealdocs.org\">zeal</a>就是一款Windows/Linux平台通用的Dash替代软件。</p>\n<p>这样强大的软件，如果只能使用官方提供的文档岂不是有些大材小用？<a href=\"https://doc2dash.readthedocs.io/en/stable/\">doc2dash</a>就是一款能够自动生成Dash兼容docset文件的工具。例如，可以使用它为PyTorch生成本地化的docset文件，并导入Dash/zeal中，在本地进行搜索阅读。这不是美滋滋？</p>\n<p>本文章是基于doc2dash的官方介绍，对其使用进行的总结。<br><img src=\"/img/doc2dash_pytorch_example.jpg\" alt=\"Demo\"></p>","more":"<h2 id=\"安装doc2dash\"><a href=\"#安装doc2dash\" class=\"headerlink\" title=\"安装doc2dash\"></a>安装doc2dash</h2><p>doc2dash是基于Python开发的。按照官方网站介绍，为了避免Python包的冲突，最好使用虚拟环境进行安装。我的机器上安装有Anaconda环境，所以首先使用<code>conda create</code>命令新建用于doc2dash的虚拟环境。<br><figure class=\"highlight sh\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">conda create -n doc2dash</div></pre></td></tr></table></figure></p>\n<p>接下来，激活虚拟环境，并使用<code>pip install</code>命令安装。<br><figure class=\"highlight sh\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"built_in\">source</span> activate doc2dash</div><div class=\"line\">pip install doc2dash</div></pre></td></tr></table></figure></p>\n<p>doc2dash支持的输出格式可以通过sphinx或者pydoctor。其中前者更加常用。下面以PyTorch项目的文档生成为例，介绍doc2dash的具体用法。</p>\n<h2 id=\"生成PyTorch文档\"><a href=\"#生成PyTorch文档\" class=\"headerlink\" title=\"生成PyTorch文档\"></a>生成PyTorch文档</h2><p>doc2dash使用sphinx生成相应的文档。在上述安装doc2dash的过程中，应该已经安装了sphinx包。不过我们还需要手动安装，以便处理rst文档。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">pip install sphinx_rtd_theme</div></pre></td></tr></table></figure>\n<p>进入PyTorch的文档目录<code>docs/</code>，PyTorch已经为我们提供了Makefile，调用sphinx包进行文档处理，可以选择<code>make html</code>命令生成相应的HTML文档，生成的位置为<code>build/html</code>。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\"># in directory $PYTORCH/docs, run</div><div class=\"line\">make html</div></pre></td></tr></table></figure>\n<p>接下来，就可以使用doc2dash来继续sphinx的工作，生成Dash可用的文档文件了~使用<code>-n</code>指定生成的文件名称，后面跟source文件夹路径即可。</p>\n<figure class=\"highlight sh\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\"># $PYTORCH/docs/build/html即为生成的HTML目录</span></div><div class=\"line\">doc2dash -n pytorch <span class=\"variable\">$PYTORCH</span>/docs/build/html</div></pre></td></tr></table></figure>\n<p>之后，把生成的<code>pytorch.docset</code>导入到Dash中即可。如下图所示，点击“+”找到文件添加即可。<br><img src=\"/img/doc2dash_how_to_add_docset.jpg\" alt=\"添加docset\"></p>\n<h2 id=\"在Ubuntu上安装zeal\"><a href=\"#在Ubuntu上安装zeal\" class=\"headerlink\" title=\"在Ubuntu上安装zeal\"></a>在Ubuntu上安装zeal</h2><p>zeal是Dash在非Mac平台上的替代软件。在Ubuntu上可以使用如下方式轻松安装（见<a href=\"https://zealdocs.org/download.html#linux\">官方网站介绍</a>）。</p>\n<figure class=\"highlight sh\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\">sudo add-apt-repository ppa:zeal-developers/ppa</div><div class=\"line\">sudo apt-get update</div><div class=\"line\">sudo apt-get install zeal</div></pre></td></tr></table></figure>\n<p>安装后，可以使用<code>Tool/Docsets</code>下载相应的公开文档。如果想要添加自己生成的文档，只需要将生成的docset文件放到软件的文档库中即可，默认位置应在<code>$HOME/.local/share/Zeal/Zeal/docsets</code>。</p>"},{"title":"Effective CPP 阅读 - Chapter 4 设计与声明","date":"2017-04-29T09:29:33.000Z","_content":"良好的代码架构能够使得后续编码工作变的简单。尤其在OOP的世界中，如何能够设计良好的C++接口？我们的目标是高效，易用，易拓展。\n![带类的C?](/img/effectivecpp_04_cwithclass.jpg)\n<!-- more -->\n\n## 18 让接口容易被使用，不易被误用\n首先，考虑客户可能犯什么错误。书中提到可以构建类型系统防范客户输入不合理的数据。同时限制什么可以做，什么不可以做。例如加入`const`限定修饰符。\n\n其次，尽量使接口与内建类型等保持一致。例如STL中统一使用`size()`方法获取容器的大小。\n\n任何接口如果强制客户记得某件事情，那么就会有犯错的危险。较佳的方法是先发制人，例如预定函数的返回值为智能指针，防止客户接触裸指针。\n\n## 19 设计`class`犹如设计`type`\n设计自定义的`class`要慎重，就好比语言设计者小心翼翼地设计语言的内置类型。一般有如下考虑：\n\n- 新的对象如何创建和销毁？这关系到构造和析构函数。\n- 对象初始化和赋值有何区别？这关系到构造函数和赋值运算符。\n- 新的对象如何以pass-by-value方式传递，意味着什么？这关系到copying函数的实现。\n- 什么是新类型的合法值？可能需要对`setter()`函数进行参数检查。\n- 新类型在继承图中的位置？这关系到虚函数，以及析构函数是否为虚函数。\n- 新类型需要什么样的转换？只能显式构造还是允许隐式转换（意味着你需要自己实现隐式转换函数）。\n- 什么样的操作符和函数对此类型是合理的？这涉及到访问权限，以及新类型与外界的交互。\n- 什么样的标准函数应该驳回？是否要禁止编译器生成默认函数。\n- 谁该取用成员？这决定了成员的访问权限，以及某些类或函数是否为`friend`。\n- 什么事新类型的未声明接口？\n- 新类型有多一般化？是否建立`template class`更好？\n- 新的类型真的必要吗？如果是要定义新的派生类来为已经存在的类添加功能，也许使用non-member函数或者模板技术是更好的选择。\n\n## 20 宁以pass-by-reference-to-const替换pass-by-value\n对于较大对象，pass-by-value有可能成为费时的操作。而且，如果以派生类对象实参传入一个以基类为形参的函数，会导致切片发生，也就是函数内部可见的仍然是基类对象，无法实现多态。\n\n总而言之，当按照值传递方法传入参数时，请再三考虑是否传入常值引用是更好的选择。但该条款不适用于内建类型和STL中的迭代器和函数对象。对它们而言，值传递通常更为恰当。\n\n## 21 必须返回对象时，不要妄想返回其reference\n函数返回时，存在局部对象析构和返回值的构造，不要妄图对此优化，返回局部non-static对象的引用几乎必然导致失败！\n\nC++11中引入的移动构造也许是解决这个问题的可行之道，以后总结。\n\n## 22 将成员变量声明为`private`\n封装，封装，还是封装！\n\n而且，请记住，其实只有两种访问权限：`private`（提供了封装）和其他（包括`protected`，不提供封装）。\n\n## 23 宁以non-member和non-friend函数替换member函数\n对于类中的数据进行操作时，常常可以使用成员函数的方法，也可以编写一个non-member函数，通过调用类的公开方法实现目的。作者认为应偏向后者。原因有三：\n\n- 封装性。我们以能够获取类私有成员变量的代码多少进行封装性的量度。如果引入类的成员函数，这个函数可以肆无忌惮地访问类内的所有成员，这使得封装被破坏。\n- 代码设计的弹性。使用成员函数需要对类进行修改，而使用后者，我们可以借助C++中的名字空间，将相似功能的函数组织在不同的hpp和cpp文件中。需要的时候可以随时添加（因为C++的名字空间支持跨文件，而类声明并不是）。\n- 编译开销。每次都要修改类的话，还要重新编译。而使用non-member函数，可以不断做加法，编译时完全可以只处理新文件。\n\n## 24 若所有参数均需要类型转换，请为此采用non-member函数\n作者举出自定义的有理数类与整型数做乘法的例子。首先，我们不将构造函数声明为`explicit`，可以完成整形到有理数类的隐式类型转换。\n\n重载乘法的运算符可以被声明为有理数类的成员函数，如下所示：\n``` cpp\nclass Rational {\n// ...\npublic:\n    Rational(int numerator=0, int denominator=1);\n    const Rational operator*(const Rational& rhs) const;\n};\n```\n\n然而，这样做的话，`auto res = 2*Rational(4,5)`就无法通过编译，因为`int`并没有实现`operator*(const Rational&)`操作。\n\n更好的方法是将其作为non-member函数，\n```cpp\nconst Rational operator*(const Rational& lhs, const Rational& rhs) {\n    //...\n}\n```\n\n## 25 考虑写出一个不抛出异常的`swap()`函数\n这一条款更像是模板特化规则的大杂烩。\n\nSTL中的`swap()`函数是交换两个对象内容的不错选择。它的实现大致如下（平淡无奇）：\n```cpp\nnamespace std {\ntemplate <typename T>\nvoid swap(T& a, T&b) {\n    T tmp(a);\n    a = b;\n    b = tmp;\n}\n}\n```\n但是对于某些pImpl（pointer to implementation）手法的类（指类的数据成员实际死一个指针，而不是数据成员的实在值），标准库的这一实现未免效率较低，因为我们实际上一般只需要交换两个对象的指针即可。\n\n如何对我们的对象`Widget`实现特化？\n\n如果`Widget`不是模板类，那么我们需要进行全特化。加入以下：\n``` cpp\nnamespace std {\ntemplate <>\nvoid swap<Widget>(Widget& a, Widget& b) {\n    swap(pImpl, b.pImpl);\n}\n}\n```\n更好的解决方法是先将`swap()`定义为`Widget`类的公共成员函数，然后再全特化标准库的`swap()`方法时调用。这样与STL的约定保持一致。STL中`vector`等容器即是这样的。一方面提供了公开方法进行交换，另一方面特化了`std`名字空间的`swap()`方法。\n\n当`Widget`是模板类时，需要进行偏特化。也许看上去是这样：\n``` cpp\nnamespace std {\ntemplate <typename T>\nvoid swap(Widget<T>& a, Widget<T>& b) {\n    a.swap(b);\n}\n}\n```\n\n但是程序员可以全特化`std`中的模板，却不能加入新的类或函数进入`std`中。在实际中，这样写出的程序一般仍然能够编译运行，但是这种行为确实是未定义的。所以最好不要这样做。\n\n所以，可以在`Widget`存在的名字空间内定义`swap()`（而不是加入`std`），这里涉及到C++中的模板实例化查找规则，不再多说了。作者在条款末尾总结了一般规则：\n\n- 一般使用标准库中的`swap()`即可。\n- 如果自己实现，首先提供一个`public`的`swap()`成员函数，注意这儿函数决不能抛出异常。\n- 在类或者模板在的名字空间中提供一个non-member的`swap()`函数，并令它调用上述的`swap()`成员函数。\n- 如果是类，而不是模板，那么特化`std::swap()`，并令它调用上述`swap()`成员函数。\n- 在客户端代码调用`swap()`时，确定包含一个`using`声明式，以便让`std::swap()`在你的函数内可见，然后不加任何名字空间修饰符，赤裸裸调用`swap()`。\n","source":"_posts/effective-cpp-04.md","raw":"---\ntitle: Effective CPP 阅读 - Chapter 4 设计与声明\ndate: 2017-04-29 17:29:33\ntags:\n     - cpp\n---\n良好的代码架构能够使得后续编码工作变的简单。尤其在OOP的世界中，如何能够设计良好的C++接口？我们的目标是高效，易用，易拓展。\n![带类的C?](/img/effectivecpp_04_cwithclass.jpg)\n<!-- more -->\n\n## 18 让接口容易被使用，不易被误用\n首先，考虑客户可能犯什么错误。书中提到可以构建类型系统防范客户输入不合理的数据。同时限制什么可以做，什么不可以做。例如加入`const`限定修饰符。\n\n其次，尽量使接口与内建类型等保持一致。例如STL中统一使用`size()`方法获取容器的大小。\n\n任何接口如果强制客户记得某件事情，那么就会有犯错的危险。较佳的方法是先发制人，例如预定函数的返回值为智能指针，防止客户接触裸指针。\n\n## 19 设计`class`犹如设计`type`\n设计自定义的`class`要慎重，就好比语言设计者小心翼翼地设计语言的内置类型。一般有如下考虑：\n\n- 新的对象如何创建和销毁？这关系到构造和析构函数。\n- 对象初始化和赋值有何区别？这关系到构造函数和赋值运算符。\n- 新的对象如何以pass-by-value方式传递，意味着什么？这关系到copying函数的实现。\n- 什么是新类型的合法值？可能需要对`setter()`函数进行参数检查。\n- 新类型在继承图中的位置？这关系到虚函数，以及析构函数是否为虚函数。\n- 新类型需要什么样的转换？只能显式构造还是允许隐式转换（意味着你需要自己实现隐式转换函数）。\n- 什么样的操作符和函数对此类型是合理的？这涉及到访问权限，以及新类型与外界的交互。\n- 什么样的标准函数应该驳回？是否要禁止编译器生成默认函数。\n- 谁该取用成员？这决定了成员的访问权限，以及某些类或函数是否为`friend`。\n- 什么事新类型的未声明接口？\n- 新类型有多一般化？是否建立`template class`更好？\n- 新的类型真的必要吗？如果是要定义新的派生类来为已经存在的类添加功能，也许使用non-member函数或者模板技术是更好的选择。\n\n## 20 宁以pass-by-reference-to-const替换pass-by-value\n对于较大对象，pass-by-value有可能成为费时的操作。而且，如果以派生类对象实参传入一个以基类为形参的函数，会导致切片发生，也就是函数内部可见的仍然是基类对象，无法实现多态。\n\n总而言之，当按照值传递方法传入参数时，请再三考虑是否传入常值引用是更好的选择。但该条款不适用于内建类型和STL中的迭代器和函数对象。对它们而言，值传递通常更为恰当。\n\n## 21 必须返回对象时，不要妄想返回其reference\n函数返回时，存在局部对象析构和返回值的构造，不要妄图对此优化，返回局部non-static对象的引用几乎必然导致失败！\n\nC++11中引入的移动构造也许是解决这个问题的可行之道，以后总结。\n\n## 22 将成员变量声明为`private`\n封装，封装，还是封装！\n\n而且，请记住，其实只有两种访问权限：`private`（提供了封装）和其他（包括`protected`，不提供封装）。\n\n## 23 宁以non-member和non-friend函数替换member函数\n对于类中的数据进行操作时，常常可以使用成员函数的方法，也可以编写一个non-member函数，通过调用类的公开方法实现目的。作者认为应偏向后者。原因有三：\n\n- 封装性。我们以能够获取类私有成员变量的代码多少进行封装性的量度。如果引入类的成员函数，这个函数可以肆无忌惮地访问类内的所有成员，这使得封装被破坏。\n- 代码设计的弹性。使用成员函数需要对类进行修改，而使用后者，我们可以借助C++中的名字空间，将相似功能的函数组织在不同的hpp和cpp文件中。需要的时候可以随时添加（因为C++的名字空间支持跨文件，而类声明并不是）。\n- 编译开销。每次都要修改类的话，还要重新编译。而使用non-member函数，可以不断做加法，编译时完全可以只处理新文件。\n\n## 24 若所有参数均需要类型转换，请为此采用non-member函数\n作者举出自定义的有理数类与整型数做乘法的例子。首先，我们不将构造函数声明为`explicit`，可以完成整形到有理数类的隐式类型转换。\n\n重载乘法的运算符可以被声明为有理数类的成员函数，如下所示：\n``` cpp\nclass Rational {\n// ...\npublic:\n    Rational(int numerator=0, int denominator=1);\n    const Rational operator*(const Rational& rhs) const;\n};\n```\n\n然而，这样做的话，`auto res = 2*Rational(4,5)`就无法通过编译，因为`int`并没有实现`operator*(const Rational&)`操作。\n\n更好的方法是将其作为non-member函数，\n```cpp\nconst Rational operator*(const Rational& lhs, const Rational& rhs) {\n    //...\n}\n```\n\n## 25 考虑写出一个不抛出异常的`swap()`函数\n这一条款更像是模板特化规则的大杂烩。\n\nSTL中的`swap()`函数是交换两个对象内容的不错选择。它的实现大致如下（平淡无奇）：\n```cpp\nnamespace std {\ntemplate <typename T>\nvoid swap(T& a, T&b) {\n    T tmp(a);\n    a = b;\n    b = tmp;\n}\n}\n```\n但是对于某些pImpl（pointer to implementation）手法的类（指类的数据成员实际死一个指针，而不是数据成员的实在值），标准库的这一实现未免效率较低，因为我们实际上一般只需要交换两个对象的指针即可。\n\n如何对我们的对象`Widget`实现特化？\n\n如果`Widget`不是模板类，那么我们需要进行全特化。加入以下：\n``` cpp\nnamespace std {\ntemplate <>\nvoid swap<Widget>(Widget& a, Widget& b) {\n    swap(pImpl, b.pImpl);\n}\n}\n```\n更好的解决方法是先将`swap()`定义为`Widget`类的公共成员函数，然后再全特化标准库的`swap()`方法时调用。这样与STL的约定保持一致。STL中`vector`等容器即是这样的。一方面提供了公开方法进行交换，另一方面特化了`std`名字空间的`swap()`方法。\n\n当`Widget`是模板类时，需要进行偏特化。也许看上去是这样：\n``` cpp\nnamespace std {\ntemplate <typename T>\nvoid swap(Widget<T>& a, Widget<T>& b) {\n    a.swap(b);\n}\n}\n```\n\n但是程序员可以全特化`std`中的模板，却不能加入新的类或函数进入`std`中。在实际中，这样写出的程序一般仍然能够编译运行，但是这种行为确实是未定义的。所以最好不要这样做。\n\n所以，可以在`Widget`存在的名字空间内定义`swap()`（而不是加入`std`），这里涉及到C++中的模板实例化查找规则，不再多说了。作者在条款末尾总结了一般规则：\n\n- 一般使用标准库中的`swap()`即可。\n- 如果自己实现，首先提供一个`public`的`swap()`成员函数，注意这儿函数决不能抛出异常。\n- 在类或者模板在的名字空间中提供一个non-member的`swap()`函数，并令它调用上述的`swap()`成员函数。\n- 如果是类，而不是模板，那么特化`std::swap()`，并令它调用上述`swap()`成员函数。\n- 在客户端代码调用`swap()`时，确定包含一个`using`声明式，以便让`std::swap()`在你的函数内可见，然后不加任何名字空间修饰符，赤裸裸调用`swap()`。\n","slug":"effective-cpp-04","published":1,"updated":"2018-01-12T06:22:20.465Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjcu6vcsr000xqu46edgi0ejw","content":"<p>良好的代码架构能够使得后续编码工作变的简单。尤其在OOP的世界中，如何能够设计良好的C++接口？我们的目标是高效，易用，易拓展。<br><img src=\"/img/effectivecpp_04_cwithclass.jpg\" alt=\"带类的C?\"><br><a id=\"more\"></a></p>\n<h2 id=\"18-让接口容易被使用，不易被误用\"><a href=\"#18-让接口容易被使用，不易被误用\" class=\"headerlink\" title=\"18 让接口容易被使用，不易被误用\"></a>18 让接口容易被使用，不易被误用</h2><p>首先，考虑客户可能犯什么错误。书中提到可以构建类型系统防范客户输入不合理的数据。同时限制什么可以做，什么不可以做。例如加入<code>const</code>限定修饰符。</p>\n<p>其次，尽量使接口与内建类型等保持一致。例如STL中统一使用<code>size()</code>方法获取容器的大小。</p>\n<p>任何接口如果强制客户记得某件事情，那么就会有犯错的危险。较佳的方法是先发制人，例如预定函数的返回值为智能指针，防止客户接触裸指针。</p>\n<h2 id=\"19-设计class犹如设计type\"><a href=\"#19-设计class犹如设计type\" class=\"headerlink\" title=\"19 设计class犹如设计type\"></a>19 设计<code>class</code>犹如设计<code>type</code></h2><p>设计自定义的<code>class</code>要慎重，就好比语言设计者小心翼翼地设计语言的内置类型。一般有如下考虑：</p>\n<ul>\n<li>新的对象如何创建和销毁？这关系到构造和析构函数。</li>\n<li>对象初始化和赋值有何区别？这关系到构造函数和赋值运算符。</li>\n<li>新的对象如何以pass-by-value方式传递，意味着什么？这关系到copying函数的实现。</li>\n<li>什么是新类型的合法值？可能需要对<code>setter()</code>函数进行参数检查。</li>\n<li>新类型在继承图中的位置？这关系到虚函数，以及析构函数是否为虚函数。</li>\n<li>新类型需要什么样的转换？只能显式构造还是允许隐式转换（意味着你需要自己实现隐式转换函数）。</li>\n<li>什么样的操作符和函数对此类型是合理的？这涉及到访问权限，以及新类型与外界的交互。</li>\n<li>什么样的标准函数应该驳回？是否要禁止编译器生成默认函数。</li>\n<li>谁该取用成员？这决定了成员的访问权限，以及某些类或函数是否为<code>friend</code>。</li>\n<li>什么事新类型的未声明接口？</li>\n<li>新类型有多一般化？是否建立<code>template class</code>更好？</li>\n<li>新的类型真的必要吗？如果是要定义新的派生类来为已经存在的类添加功能，也许使用non-member函数或者模板技术是更好的选择。</li>\n</ul>\n<h2 id=\"20-宁以pass-by-reference-to-const替换pass-by-value\"><a href=\"#20-宁以pass-by-reference-to-const替换pass-by-value\" class=\"headerlink\" title=\"20 宁以pass-by-reference-to-const替换pass-by-value\"></a>20 宁以pass-by-reference-to-const替换pass-by-value</h2><p>对于较大对象，pass-by-value有可能成为费时的操作。而且，如果以派生类对象实参传入一个以基类为形参的函数，会导致切片发生，也就是函数内部可见的仍然是基类对象，无法实现多态。</p>\n<p>总而言之，当按照值传递方法传入参数时，请再三考虑是否传入常值引用是更好的选择。但该条款不适用于内建类型和STL中的迭代器和函数对象。对它们而言，值传递通常更为恰当。</p>\n<h2 id=\"21-必须返回对象时，不要妄想返回其reference\"><a href=\"#21-必须返回对象时，不要妄想返回其reference\" class=\"headerlink\" title=\"21 必须返回对象时，不要妄想返回其reference\"></a>21 必须返回对象时，不要妄想返回其reference</h2><p>函数返回时，存在局部对象析构和返回值的构造，不要妄图对此优化，返回局部non-static对象的引用几乎必然导致失败！</p>\n<p>C++11中引入的移动构造也许是解决这个问题的可行之道，以后总结。</p>\n<h2 id=\"22-将成员变量声明为private\"><a href=\"#22-将成员变量声明为private\" class=\"headerlink\" title=\"22 将成员变量声明为private\"></a>22 将成员变量声明为<code>private</code></h2><p>封装，封装，还是封装！</p>\n<p>而且，请记住，其实只有两种访问权限：<code>private</code>（提供了封装）和其他（包括<code>protected</code>，不提供封装）。</p>\n<h2 id=\"23-宁以non-member和non-friend函数替换member函数\"><a href=\"#23-宁以non-member和non-friend函数替换member函数\" class=\"headerlink\" title=\"23 宁以non-member和non-friend函数替换member函数\"></a>23 宁以non-member和non-friend函数替换member函数</h2><p>对于类中的数据进行操作时，常常可以使用成员函数的方法，也可以编写一个non-member函数，通过调用类的公开方法实现目的。作者认为应偏向后者。原因有三：</p>\n<ul>\n<li>封装性。我们以能够获取类私有成员变量的代码多少进行封装性的量度。如果引入类的成员函数，这个函数可以肆无忌惮地访问类内的所有成员，这使得封装被破坏。</li>\n<li>代码设计的弹性。使用成员函数需要对类进行修改，而使用后者，我们可以借助C++中的名字空间，将相似功能的函数组织在不同的hpp和cpp文件中。需要的时候可以随时添加（因为C++的名字空间支持跨文件，而类声明并不是）。</li>\n<li>编译开销。每次都要修改类的话，还要重新编译。而使用non-member函数，可以不断做加法，编译时完全可以只处理新文件。</li>\n</ul>\n<h2 id=\"24-若所有参数均需要类型转换，请为此采用non-member函数\"><a href=\"#24-若所有参数均需要类型转换，请为此采用non-member函数\" class=\"headerlink\" title=\"24 若所有参数均需要类型转换，请为此采用non-member函数\"></a>24 若所有参数均需要类型转换，请为此采用non-member函数</h2><p>作者举出自定义的有理数类与整型数做乘法的例子。首先，我们不将构造函数声明为<code>explicit</code>，可以完成整形到有理数类的隐式类型转换。</p>\n<p>重载乘法的运算符可以被声明为有理数类的成员函数，如下所示：<br><figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">class</span> Rational &#123;</div><div class=\"line\"><span class=\"comment\">// ...</span></div><div class=\"line\"><span class=\"keyword\">public</span>:</div><div class=\"line\">    Rational(<span class=\"keyword\">int</span> numerator=<span class=\"number\">0</span>, <span class=\"keyword\">int</span> denominator=<span class=\"number\">1</span>);</div><div class=\"line\">    <span class=\"keyword\">const</span> Rational <span class=\"keyword\">operator</span>*(<span class=\"keyword\">const</span> Rational&amp; rhs) <span class=\"keyword\">const</span>;</div><div class=\"line\">&#125;;</div></pre></td></tr></table></figure></p>\n<p>然而，这样做的话，<code>auto res = 2*Rational(4,5)</code>就无法通过编译，因为<code>int</code>并没有实现<code>operator*(const Rational&amp;)</code>操作。</p>\n<p>更好的方法是将其作为non-member函数，<br><figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">const</span> Rational <span class=\"keyword\">operator</span>*(<span class=\"keyword\">const</span> Rational&amp; lhs, <span class=\"keyword\">const</span> Rational&amp; rhs) &#123;</div><div class=\"line\">    <span class=\"comment\">//...</span></div><div class=\"line\">&#125;</div></pre></td></tr></table></figure></p>\n<h2 id=\"25-考虑写出一个不抛出异常的swap-函数\"><a href=\"#25-考虑写出一个不抛出异常的swap-函数\" class=\"headerlink\" title=\"25 考虑写出一个不抛出异常的swap()函数\"></a>25 考虑写出一个不抛出异常的<code>swap()</code>函数</h2><p>这一条款更像是模板特化规则的大杂烩。</p>\n<p>STL中的<code>swap()</code>函数是交换两个对象内容的不错选择。它的实现大致如下（平淡无奇）：<br><figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">namespace</span> <span class=\"built_in\">std</span> &#123;</div><div class=\"line\"><span class=\"keyword\">template</span> &lt;<span class=\"keyword\">typename</span> T&gt;</div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">swap</span><span class=\"params\">(T&amp; a, T&amp;b)</span> </span>&#123;</div><div class=\"line\">    <span class=\"function\">T <span class=\"title\">tmp</span><span class=\"params\">(a)</span></span>;</div><div class=\"line\">    a = b;</div><div class=\"line\">    b = tmp;</div><div class=\"line\">&#125;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure></p>\n<p>但是对于某些pImpl（pointer to implementation）手法的类（指类的数据成员实际死一个指针，而不是数据成员的实在值），标准库的这一实现未免效率较低，因为我们实际上一般只需要交换两个对象的指针即可。</p>\n<p>如何对我们的对象<code>Widget</code>实现特化？</p>\n<p>如果<code>Widget</code>不是模板类，那么我们需要进行全特化。加入以下：<br><figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">namespace</span> <span class=\"built_in\">std</span> &#123;</div><div class=\"line\"><span class=\"keyword\">template</span> &lt;&gt;</div><div class=\"line\"><span class=\"keyword\">void</span> swap&lt;Widget&gt;(Widget&amp; a, Widget&amp; b) &#123;</div><div class=\"line\">    swap(pImpl, b.pImpl);</div><div class=\"line\">&#125;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure></p>\n<p>更好的解决方法是先将<code>swap()</code>定义为<code>Widget</code>类的公共成员函数，然后再全特化标准库的<code>swap()</code>方法时调用。这样与STL的约定保持一致。STL中<code>vector</code>等容器即是这样的。一方面提供了公开方法进行交换，另一方面特化了<code>std</code>名字空间的<code>swap()</code>方法。</p>\n<p>当<code>Widget</code>是模板类时，需要进行偏特化。也许看上去是这样：<br><figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">namespace</span> <span class=\"built_in\">std</span> &#123;</div><div class=\"line\"><span class=\"keyword\">template</span> &lt;<span class=\"keyword\">typename</span> T&gt;</div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">swap</span><span class=\"params\">(Widget&lt;T&gt;&amp; a, Widget&lt;T&gt;&amp; b)</span> </span>&#123;</div><div class=\"line\">    a.swap(b);</div><div class=\"line\">&#125;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure></p>\n<p>但是程序员可以全特化<code>std</code>中的模板，却不能加入新的类或函数进入<code>std</code>中。在实际中，这样写出的程序一般仍然能够编译运行，但是这种行为确实是未定义的。所以最好不要这样做。</p>\n<p>所以，可以在<code>Widget</code>存在的名字空间内定义<code>swap()</code>（而不是加入<code>std</code>），这里涉及到C++中的模板实例化查找规则，不再多说了。作者在条款末尾总结了一般规则：</p>\n<ul>\n<li>一般使用标准库中的<code>swap()</code>即可。</li>\n<li>如果自己实现，首先提供一个<code>public</code>的<code>swap()</code>成员函数，注意这儿函数决不能抛出异常。</li>\n<li>在类或者模板在的名字空间中提供一个non-member的<code>swap()</code>函数，并令它调用上述的<code>swap()</code>成员函数。</li>\n<li>如果是类，而不是模板，那么特化<code>std::swap()</code>，并令它调用上述<code>swap()</code>成员函数。</li>\n<li>在客户端代码调用<code>swap()</code>时，确定包含一个<code>using</code>声明式，以便让<code>std::swap()</code>在你的函数内可见，然后不加任何名字空间修饰符，赤裸裸调用<code>swap()</code>。</li>\n</ul>\n","excerpt":"<p>良好的代码架构能够使得后续编码工作变的简单。尤其在OOP的世界中，如何能够设计良好的C++接口？我们的目标是高效，易用，易拓展。<br><img src=\"/img/effectivecpp_04_cwithclass.jpg\" alt=\"带类的C?\"><br>","more":"</p>\n<h2 id=\"18-让接口容易被使用，不易被误用\"><a href=\"#18-让接口容易被使用，不易被误用\" class=\"headerlink\" title=\"18 让接口容易被使用，不易被误用\"></a>18 让接口容易被使用，不易被误用</h2><p>首先，考虑客户可能犯什么错误。书中提到可以构建类型系统防范客户输入不合理的数据。同时限制什么可以做，什么不可以做。例如加入<code>const</code>限定修饰符。</p>\n<p>其次，尽量使接口与内建类型等保持一致。例如STL中统一使用<code>size()</code>方法获取容器的大小。</p>\n<p>任何接口如果强制客户记得某件事情，那么就会有犯错的危险。较佳的方法是先发制人，例如预定函数的返回值为智能指针，防止客户接触裸指针。</p>\n<h2 id=\"19-设计class犹如设计type\"><a href=\"#19-设计class犹如设计type\" class=\"headerlink\" title=\"19 设计class犹如设计type\"></a>19 设计<code>class</code>犹如设计<code>type</code></h2><p>设计自定义的<code>class</code>要慎重，就好比语言设计者小心翼翼地设计语言的内置类型。一般有如下考虑：</p>\n<ul>\n<li>新的对象如何创建和销毁？这关系到构造和析构函数。</li>\n<li>对象初始化和赋值有何区别？这关系到构造函数和赋值运算符。</li>\n<li>新的对象如何以pass-by-value方式传递，意味着什么？这关系到copying函数的实现。</li>\n<li>什么是新类型的合法值？可能需要对<code>setter()</code>函数进行参数检查。</li>\n<li>新类型在继承图中的位置？这关系到虚函数，以及析构函数是否为虚函数。</li>\n<li>新类型需要什么样的转换？只能显式构造还是允许隐式转换（意味着你需要自己实现隐式转换函数）。</li>\n<li>什么样的操作符和函数对此类型是合理的？这涉及到访问权限，以及新类型与外界的交互。</li>\n<li>什么样的标准函数应该驳回？是否要禁止编译器生成默认函数。</li>\n<li>谁该取用成员？这决定了成员的访问权限，以及某些类或函数是否为<code>friend</code>。</li>\n<li>什么事新类型的未声明接口？</li>\n<li>新类型有多一般化？是否建立<code>template class</code>更好？</li>\n<li>新的类型真的必要吗？如果是要定义新的派生类来为已经存在的类添加功能，也许使用non-member函数或者模板技术是更好的选择。</li>\n</ul>\n<h2 id=\"20-宁以pass-by-reference-to-const替换pass-by-value\"><a href=\"#20-宁以pass-by-reference-to-const替换pass-by-value\" class=\"headerlink\" title=\"20 宁以pass-by-reference-to-const替换pass-by-value\"></a>20 宁以pass-by-reference-to-const替换pass-by-value</h2><p>对于较大对象，pass-by-value有可能成为费时的操作。而且，如果以派生类对象实参传入一个以基类为形参的函数，会导致切片发生，也就是函数内部可见的仍然是基类对象，无法实现多态。</p>\n<p>总而言之，当按照值传递方法传入参数时，请再三考虑是否传入常值引用是更好的选择。但该条款不适用于内建类型和STL中的迭代器和函数对象。对它们而言，值传递通常更为恰当。</p>\n<h2 id=\"21-必须返回对象时，不要妄想返回其reference\"><a href=\"#21-必须返回对象时，不要妄想返回其reference\" class=\"headerlink\" title=\"21 必须返回对象时，不要妄想返回其reference\"></a>21 必须返回对象时，不要妄想返回其reference</h2><p>函数返回时，存在局部对象析构和返回值的构造，不要妄图对此优化，返回局部non-static对象的引用几乎必然导致失败！</p>\n<p>C++11中引入的移动构造也许是解决这个问题的可行之道，以后总结。</p>\n<h2 id=\"22-将成员变量声明为private\"><a href=\"#22-将成员变量声明为private\" class=\"headerlink\" title=\"22 将成员变量声明为private\"></a>22 将成员变量声明为<code>private</code></h2><p>封装，封装，还是封装！</p>\n<p>而且，请记住，其实只有两种访问权限：<code>private</code>（提供了封装）和其他（包括<code>protected</code>，不提供封装）。</p>\n<h2 id=\"23-宁以non-member和non-friend函数替换member函数\"><a href=\"#23-宁以non-member和non-friend函数替换member函数\" class=\"headerlink\" title=\"23 宁以non-member和non-friend函数替换member函数\"></a>23 宁以non-member和non-friend函数替换member函数</h2><p>对于类中的数据进行操作时，常常可以使用成员函数的方法，也可以编写一个non-member函数，通过调用类的公开方法实现目的。作者认为应偏向后者。原因有三：</p>\n<ul>\n<li>封装性。我们以能够获取类私有成员变量的代码多少进行封装性的量度。如果引入类的成员函数，这个函数可以肆无忌惮地访问类内的所有成员，这使得封装被破坏。</li>\n<li>代码设计的弹性。使用成员函数需要对类进行修改，而使用后者，我们可以借助C++中的名字空间，将相似功能的函数组织在不同的hpp和cpp文件中。需要的时候可以随时添加（因为C++的名字空间支持跨文件，而类声明并不是）。</li>\n<li>编译开销。每次都要修改类的话，还要重新编译。而使用non-member函数，可以不断做加法，编译时完全可以只处理新文件。</li>\n</ul>\n<h2 id=\"24-若所有参数均需要类型转换，请为此采用non-member函数\"><a href=\"#24-若所有参数均需要类型转换，请为此采用non-member函数\" class=\"headerlink\" title=\"24 若所有参数均需要类型转换，请为此采用non-member函数\"></a>24 若所有参数均需要类型转换，请为此采用non-member函数</h2><p>作者举出自定义的有理数类与整型数做乘法的例子。首先，我们不将构造函数声明为<code>explicit</code>，可以完成整形到有理数类的隐式类型转换。</p>\n<p>重载乘法的运算符可以被声明为有理数类的成员函数，如下所示：<br><figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">class</span> Rational &#123;</div><div class=\"line\"><span class=\"comment\">// ...</span></div><div class=\"line\"><span class=\"keyword\">public</span>:</div><div class=\"line\">    Rational(<span class=\"keyword\">int</span> numerator=<span class=\"number\">0</span>, <span class=\"keyword\">int</span> denominator=<span class=\"number\">1</span>);</div><div class=\"line\">    <span class=\"keyword\">const</span> Rational <span class=\"keyword\">operator</span>*(<span class=\"keyword\">const</span> Rational&amp; rhs) <span class=\"keyword\">const</span>;</div><div class=\"line\">&#125;;</div></pre></td></tr></table></figure></p>\n<p>然而，这样做的话，<code>auto res = 2*Rational(4,5)</code>就无法通过编译，因为<code>int</code>并没有实现<code>operator*(const Rational&amp;)</code>操作。</p>\n<p>更好的方法是将其作为non-member函数，<br><figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">const</span> Rational <span class=\"keyword\">operator</span>*(<span class=\"keyword\">const</span> Rational&amp; lhs, <span class=\"keyword\">const</span> Rational&amp; rhs) &#123;</div><div class=\"line\">    <span class=\"comment\">//...</span></div><div class=\"line\">&#125;</div></pre></td></tr></table></figure></p>\n<h2 id=\"25-考虑写出一个不抛出异常的swap-函数\"><a href=\"#25-考虑写出一个不抛出异常的swap-函数\" class=\"headerlink\" title=\"25 考虑写出一个不抛出异常的swap()函数\"></a>25 考虑写出一个不抛出异常的<code>swap()</code>函数</h2><p>这一条款更像是模板特化规则的大杂烩。</p>\n<p>STL中的<code>swap()</code>函数是交换两个对象内容的不错选择。它的实现大致如下（平淡无奇）：<br><figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">namespace</span> <span class=\"built_in\">std</span> &#123;</div><div class=\"line\"><span class=\"keyword\">template</span> &lt;<span class=\"keyword\">typename</span> T&gt;</div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">swap</span><span class=\"params\">(T&amp; a, T&amp;b)</span> </span>&#123;</div><div class=\"line\">    <span class=\"function\">T <span class=\"title\">tmp</span><span class=\"params\">(a)</span></span>;</div><div class=\"line\">    a = b;</div><div class=\"line\">    b = tmp;</div><div class=\"line\">&#125;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure></p>\n<p>但是对于某些pImpl（pointer to implementation）手法的类（指类的数据成员实际死一个指针，而不是数据成员的实在值），标准库的这一实现未免效率较低，因为我们实际上一般只需要交换两个对象的指针即可。</p>\n<p>如何对我们的对象<code>Widget</code>实现特化？</p>\n<p>如果<code>Widget</code>不是模板类，那么我们需要进行全特化。加入以下：<br><figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">namespace</span> <span class=\"built_in\">std</span> &#123;</div><div class=\"line\"><span class=\"keyword\">template</span> &lt;&gt;</div><div class=\"line\"><span class=\"keyword\">void</span> swap&lt;Widget&gt;(Widget&amp; a, Widget&amp; b) &#123;</div><div class=\"line\">    swap(pImpl, b.pImpl);</div><div class=\"line\">&#125;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure></p>\n<p>更好的解决方法是先将<code>swap()</code>定义为<code>Widget</code>类的公共成员函数，然后再全特化标准库的<code>swap()</code>方法时调用。这样与STL的约定保持一致。STL中<code>vector</code>等容器即是这样的。一方面提供了公开方法进行交换，另一方面特化了<code>std</code>名字空间的<code>swap()</code>方法。</p>\n<p>当<code>Widget</code>是模板类时，需要进行偏特化。也许看上去是这样：<br><figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">namespace</span> <span class=\"built_in\">std</span> &#123;</div><div class=\"line\"><span class=\"keyword\">template</span> &lt;<span class=\"keyword\">typename</span> T&gt;</div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">swap</span><span class=\"params\">(Widget&lt;T&gt;&amp; a, Widget&lt;T&gt;&amp; b)</span> </span>&#123;</div><div class=\"line\">    a.swap(b);</div><div class=\"line\">&#125;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure></p>\n<p>但是程序员可以全特化<code>std</code>中的模板，却不能加入新的类或函数进入<code>std</code>中。在实际中，这样写出的程序一般仍然能够编译运行，但是这种行为确实是未定义的。所以最好不要这样做。</p>\n<p>所以，可以在<code>Widget</code>存在的名字空间内定义<code>swap()</code>（而不是加入<code>std</code>），这里涉及到C++中的模板实例化查找规则，不再多说了。作者在条款末尾总结了一般规则：</p>\n<ul>\n<li>一般使用标准库中的<code>swap()</code>即可。</li>\n<li>如果自己实现，首先提供一个<code>public</code>的<code>swap()</code>成员函数，注意这儿函数决不能抛出异常。</li>\n<li>在类或者模板在的名字空间中提供一个non-member的<code>swap()</code>函数，并令它调用上述的<code>swap()</code>成员函数。</li>\n<li>如果是类，而不是模板，那么特化<code>std::swap()</code>，并令它调用上述<code>swap()</code>成员函数。</li>\n<li>在客户端代码调用<code>swap()</code>时，确定包含一个<code>using</code>声明式，以便让<code>std::swap()</code>在你的函数内可见，然后不加任何名字空间修饰符，赤裸裸调用<code>swap()</code>。</li>\n</ul>"},{"title":"Effective CPP 阅读 - Chapter 6 继承与面向对象设计","date":"2017-06-17T02:40:19.000Z","_content":"C++中允许多重继承，并且可以指定继承是否是public or private等。成员函数也可以是虚函数或者非虚函数。如何在OOP这一C++联邦中的重要一员的规则下，写出易于拓展，易于维护且高效的代码？\n\n真•面向对象编程！\n![面向对象编程](/img/effectivecpp_06_joke.jpg)\n<!-- more -->\n\n## 32 确定`public`继承塑模出Is-a的关系\n请把这条规则记在心中：`public`继承意味着Is-a（XX是X的一种）的关系。适用于base class上的东西也一定能够用在derived class身上。因为每一个derived class对象也是一个base class的对象。\n\n不过，在实际使用时，可能并不是那么简单。举个例子，在鸟类这个基类中定义了`fly()`这一虚函数，而企鹅很显然是一种鸟，但是却没有飞翔的能力。类似的情况需要在编程实践中灵活处理。\n\n## 33 避免遮掩继承而来的名称\n这个题材实际和作用域有关。当C++遇到某个名称时，会首先在local域中寻找，如果找到，就不再继续寻找。这样，derived class中的名称可能会遮盖base class中的名称。\n\n一种解决办法是使用`using`声明。如下所示：\n``` cpp\nclass Base {\npublic:\n    virtual void f1() = 0;\n    void f3();\n    void f3(double);\n};\n\nclass Derived: public Base {\npublic:\n    using Base::f3;\n    virtual void f1();\n    void f3();\n};\n\nDerived d;\nd.f1();  // 没问题，调用了Derived中的f1\nd.f3();  // 没问题，调用了Derived中的f3\ndouble x;\nd.f3(x);  // 没问题，调用了Base中的f3。\n// 但是如果没有using声明的话，Base::f3会被冲掉。\n```\n\n## 34 区分接口继承和实现继承\n表面上直截了当的`public`继承，可以细分为函数接口继承和函数实现继承。以下面的这个例子来说明：\n\n``` cpp\nclass Shape {\npublic:\n    virtual void draw() const = 0;\n    virtual void error(const std::string& msg);\n    int getID() const;\n};\nclass Rect: public Shape {...};\nclass Circle: public Shape {...};\n```\n\n- 纯虚函数\n声明纯虚函数（如`draw()`函数）是为了让derived class只继承函数接口。乃是一种约定：“你一定要实现某某，但是我不管你如何实现”。\n不过，你仍然可以给纯虚函数提供函数定义。\n\n- 虚函数\n非纯虚函数（如`error()`函数）的目的是，让derived class继承该函数的接口和缺省实现。乃是约定“你必须支持XX，但是如果你不想自己实现，可以用我提供的这个”。\n然而可能会出现这样一种局面：derived class的表现与base class不同，但是又忘记了重写这个虚函数。为了避免这种情况，可以使用下面的技术来达到“除非你明确要求，否则我才不给你提供那个缺省定义”的目的。\n\n``` cpp\nclass Base {\npublic:\n    virtual void fun() = 0;   // 注意，我们改写成了纯虚函数\nprotected:\n    void default_fun() {...};   // 缺省实现\n};\n// 此时，若想使用缺省实现，就必须显式地调用\nclass Derived: public Base {\npublic:\n    virtual void fun() {\n        default_fun();\n    }\n};\n```\n不过这样导致一个多余的`default_fun()`函数。如果不想添加额外的函数，我们可以使用上述提到的拥有定义的纯虚函数来实现。\n\n``` cpp\nclass Base {\npublic:\n    virtual void fun() = 0;\n};\n// 为纯虚函数提供定义\nvoid Base::fun() {\n    // 缺省行为\n}\n\nclass Derived: public Base {\npublic:\n    // 显式调用基类的纯虚函数，实现缺省行为\n    virtual void fun() {Base::fun(); }\n};\nclass Derived2: public Base {\npublic:\n    // 实现自定义的行为\n    virtual void fun() {\n        // ...\n    }\n};\n```\n\n- 非虚函数\n这意味着你不应该在derived class中定义不同的行为（老老实实用我给你的！），使得其继承了一份接口和强制实现。\n\n## 35 考虑`virtual`函数之外的其他选择\n虚函数使得多态成为可能。不过在一些情况下，为了实现多态，不一定非要使用虚函数。本条款介绍了一些相关技术。\n\n在某游戏中，需要设计一个计算角色剩余血量的函数。下面是一种惯常的设计。\n``` cpp\nclass GameCharacter {\npublic:\n    virtual int healthValue() const;\n};\n```\n\n- 使用non-virtual interface实现template method模式\n这种流派主张`virtual`函数应该几乎总是私有的。较好的设计时将`healthValue()`函数设为非虚函数，并调用虚函数进行实现。这个调用函数中，可以做一些预先准备（互斥锁，日志等），后续可以做一些打扫工作。\n\n``` cpp\nclass GameCharacter {\npublic:\n    int healthValue() const {\n        // ... 前期准备\n        int ret_val = doHealthValue();\n        // ... 后续清理\n        return ret_val\n    }\nprivate:\n    virtual int doHealthValue() const {...}\n};\n```\n这样做的好处是基类明确定义了该如何实现求血量这个行为，同时又给了一定的自由，派生类可以重写`doHealthValue()`函数，针对自身的特点计算血量。\n\n- 使用函数指针实现策略模式\n上述方案实际上是对虚函数的调用进行了一次包装。我们还可以借由函数指针实现策略模式，为不同的派生类甚至不同的对象实例做出不同的实现。\n\n``` cpp\nclass GameCharacter;   // 前置声明\n// 计算血量的缺省方法\nint defaultHealthValue(const GameCharacter&);\n\nclass GameCharacter {\npublic:\n    typedef int (*HealthCalcFun) (const GameCharacter&);\n\n    explicit GameCharacter(HealthCalcFun f=defaultHealthValue\n        :healthFunc(f){\n        // ...\n    }\nprivate:\n    HealthCalcFun healthFunc;\n};\n```\n\n这样，我们通过在构造时候传入相应的函数指针，就可以实现计算血量的个性化设置。比如两个同样的boss，血量下降方式就可以不一样。\n或者我们可以在运行时候，通过设定`healthFunc`，来实现动态血量计算方法的变化。\n\n- 借由`std::function`实现策略模式\n作为上面的改进，我们可以使用`std::function`（C++11），这样，不止函数指针可以使用，函数对象等也都可以了。（关于`std::function`的大致介绍，可以看[这里](http://en.cppreference.com/w/cpp/utility/functional/function)）。\n\n我们只需将上面的`typedef`改掉即可。不再使用函数指针，而是更加高级更加通用的`std::function`。\n\n``` cpp\ntypedef std::function<int(const GameCharacter&)> HealthCalcFun;\n```\n\n- 使用古典的策略模式\n如下图所示。对于血量计算，我们单独抻出来一个基类，并有不同的实现。`GameCharacter`类中则含有一个指向`HealthCalcFun`类实例的指针。\n\n![使用UML表示的策略模式](/img/effectivecpp_strategy_pattern.png)\n\n``` cpp\n//我们首先定义HealthCalcFunc基类\nclass GameCharacter;    // 前向声明\nclass HealthCalcFunc {\npublic:\n    virtual int calc(const GameCharacter& gc) const {...}\n};\n\nHealthCalcFunc defaultCalcFunc;\n\nclass GameCharacter {\nprivate:\n    HealthCalcFunc* pfun;\npublic:\n    explicit GameCharacter(HealthCalcFunc* p=&defaultCalcFunc):\n        pfun(p) {}\n    int healthValue() const {\n        return pfun->calc(*this);\n    }\n};\n```\n\n该条款给出了虚函数的若干替代方案。\n\n## 36 绝不重新定义继承而来的非虚函数\n在条款34中已经指出，非虚函数是一种实现继承的约定。派生类不应该重新定义非虚函数。这破坏了约定。\n\n如下所示。\n``` cpp\nclass B {\npublic:\n    void mf() {...}\n};\nclass D: public B {\npublic:\n    void mf() {...}\n};\n\nD d;\nB* pb = &d;\nD* pd = &d;\n\npb->mf();   // 调用的是B::mf()\npd->mf();   // 调用的是D::mf()\n```\n\n这是因为非虚函数的绑定是编译期行为（和虚函数的动态绑定相对，其发生在运行时）。由于`pb`被声明为一个指向`B`的指针，所以其调用的是`B`的成员函数`mf()`。\n\n为了不至于让自己陷入精神分裂与背信弃义的境地，请不要重新定义继承而来的非虚函数。\n\n## 37 绝不重新定义继承而来的缺省参数值\n由于条款36的分析，所以我们只讨论继承而来的是带有缺省参数的虚函数。这样一来，本条款背后的逻辑就很清晰了：因为缺省参数同样是静态绑定的，而虚函数却是动态绑定。让我们再解释一下。\n\n静态类型是指在程序中被声明时的类型（不论其真实指向是什么）。\n``` cpp\n// Circle是Shape的派生类\nShape* ps;\nShape* pc = new Circle;   // 静态类型都是Shape\n```\n\n动态类型是指当前所指对象的类型。就上例来说，`pc`的动态类型是`Circle*`，而`ps`没有动态类型，因为它并没有指向任何对象实例。动态类型常常可以通过赋值改变。\n``` cpp\nps = new Circle;   // 现在ps的动态类型是Circle*\n```\n\n虚函数是运行时决定的，取决于发出调用的那个对象的动态类型。\n\n不过遵守此项条款，有时又会造成不便。看下例：\n\n``` cpp\nclass Shape {\npublic:\n    enum ShapeColor {RED, GREEN};\n    virtual void draw(ShapeColor c=RED) const=0;\n};\n\nclass Circle: public Shape {\npublic:\n    virtual void draw(ShapeColor c=RED) const;\n};\n```\n\n第一个问题，代码重复，我写了两遍缺省参数。第二造成了代码依存。比如我想换成`GREEN`为默认参数，需要在基类和派生类中同时修改。\n\n一种解决方法是采用条款35中的替代设计，如NVI方法。令基类中的一个public的非虚函数调用私有的虚函数，而后者可以被派生类重新定义。我们只需要在public的非虚函数中定义缺省参数即可。\n\n``` cpp\nclass Shape {\npublic:\n    void draw(ShapeColor c=RED) const {\n        doDraw(c);  // 调用私有的虚函数\n    }\nprivate:\n    //真正的工作在此完成\n    virtual void doDraw(ShapeColor c) const = 0;  \n};\n\nclass Circle: public Shape {\nprivate:\n    virtual void doDraw(ShapeColor c) const;  // 派生类重写这个真正的实现\n};\n```\n\n## 38 通过复合塑模has-a或“根据某物实现出”\n复合是指某种对象内含其他对象。复合实际有两层意义，一种较好理解，即has-a，如人有名字、性别等他类，一种是指根据某物实现（is-implemented-in-terms-of）。例如实现消息管理的某个类中含有队列作为实现。\n\n## 39 明智而审慎地使用`private`继承\n私有继承意味着条款38中的“根据某物实现出”。例如`D`私有继承自`B`，不是说`D`是某种`B`，私有继承完全是一种技术上的实现（和对现实的抽象没有半毛钱关系）。`B`的每样东西在`D`中都是不可见的，也就是成了黑箱，因为它们本身就是实现细节，你只是考虑用`B`来实现`D`的功能而已。\n\n但是复合也能达到相同的效果啊~我在`D`中加入一个`B`的对象实例不就好了？很多情况下的确是这样，如果没有必要，不建议使用私有继承。\n\n## 40 明智而审慎地使用多重继承\n使用多重继承有可能造成歧义。例如，`C`继承自`A`和`B`，而两个基类中都含有成员函数`mf()`。那么当`d.mf()`的时候，究竟是在调用哪个呢？你必须明确地指出,`d.A::mf()`。\n\n使用多重继承还可能会造成“钻石型”继承。任何时候继承体系中某个基类和派生类之间有一条以上的相通路线，就面临一个问题，是否要让基类中的每个成员变量经由每一条路线被复制？如果只想保留一份，那么需要将`File`定为虚基类，所有直接继承自它的类采用虚继承。\n![钻石型继承](/img/effectivecpp_diamond.png)\n\n``` cpp\nclass File {...};\nclass InputFile: virtual public File {...};\nclass OutputFile: virtual public File {...};\nclass IOFile: public InputFile, public OutputFile {...};\n```\n\n从正确的角度看，public的继承总应该是virtual的。不过这样会造成代码体积的膨胀和执行效率的下降。\n\n所以，如无必要，不要使用虚继承。即使使用，尽可能避免在其中放置数据（类似Java或C#中的接口Interface）\n\n## 附注 `std::function`的基本使用\n`std::function`的作用类似于函数指针，但是能力更加强大。我们可以将函数指针，函数对象，lambda表达式或者类中的成员函数作为`std::function`。\n如下所示：\n\n``` cpp\n#include <functional>\n#include <iostream>\n\nstruct Foo {\n    Foo(int num) : num_(num) {}\n    void print_add(int i) const { std::cout << num_+i << '\\n'; }\n    int num_;\n};\n\nvoid print_num(int i)\n{\n    std::cout << i << '\\n';\n}\n\nstruct PrintNum {\n    void operator()(int i) const\n    {\n        std::cout << i << '\\n';\n    }\n};\n\nint main()\n{\n    // store a free function\n    // 函数指针\n    std::function<void(int)> f_display = print_num;\n    f_display(-9);\n\n    // lambda表达式\n    // store a lambda\n    std::function<void()> f_display_42 = []() { print_num(42); };\n    f_display_42();\n\n    // store the result of a call to std::bind\n    // 绑定之后的函数对象\n    std::function<void()> f_display_31337 = std::bind(print_num, 31337);\n    f_display_31337();\n\n    // store a call to a member function\n    // 类中的成员函数，第一个参数为类实例的const reference\n    std::function<void(const Foo&, int)> f_add_display = &Foo::print_add;\n    const Foo foo(314159);\n    f_add_display(foo, 1);\n    f_add_display(314159, 1);\n\n    // store a call to a data member accessor\n    std::function<int(Foo const&)> f_num = &Foo::num_;\n    std::cout << \"num_: \" << f_num(foo) << '\\n';\n\n    // store a call to a member function and object\n    using std::placeholders::_1;\n    std::function<void(int)> f_add_display2 = std::bind( &Foo::print_add, foo, _1 );\n    f_add_display2(2);\n\n    // store a call to a member function and object ptr\n    std::function<void(int)> f_add_display3 = std::bind( &Foo::print_add, &foo, _1 );\n    f_add_display3(3);\n\n    // store a call to a function object\n    std::function<void(int)> f_display_obj = PrintNum();\n    f_display_obj(18);\n}\n```\n","source":"_posts/effective-cpp-06.md","raw":"---\ntitle: Effective CPP 阅读 - Chapter 6 继承与面向对象设计\ndate: 2017-06-17 10:40:19\ntags:\n    - cpp\n---\nC++中允许多重继承，并且可以指定继承是否是public or private等。成员函数也可以是虚函数或者非虚函数。如何在OOP这一C++联邦中的重要一员的规则下，写出易于拓展，易于维护且高效的代码？\n\n真•面向对象编程！\n![面向对象编程](/img/effectivecpp_06_joke.jpg)\n<!-- more -->\n\n## 32 确定`public`继承塑模出Is-a的关系\n请把这条规则记在心中：`public`继承意味着Is-a（XX是X的一种）的关系。适用于base class上的东西也一定能够用在derived class身上。因为每一个derived class对象也是一个base class的对象。\n\n不过，在实际使用时，可能并不是那么简单。举个例子，在鸟类这个基类中定义了`fly()`这一虚函数，而企鹅很显然是一种鸟，但是却没有飞翔的能力。类似的情况需要在编程实践中灵活处理。\n\n## 33 避免遮掩继承而来的名称\n这个题材实际和作用域有关。当C++遇到某个名称时，会首先在local域中寻找，如果找到，就不再继续寻找。这样，derived class中的名称可能会遮盖base class中的名称。\n\n一种解决办法是使用`using`声明。如下所示：\n``` cpp\nclass Base {\npublic:\n    virtual void f1() = 0;\n    void f3();\n    void f3(double);\n};\n\nclass Derived: public Base {\npublic:\n    using Base::f3;\n    virtual void f1();\n    void f3();\n};\n\nDerived d;\nd.f1();  // 没问题，调用了Derived中的f1\nd.f3();  // 没问题，调用了Derived中的f3\ndouble x;\nd.f3(x);  // 没问题，调用了Base中的f3。\n// 但是如果没有using声明的话，Base::f3会被冲掉。\n```\n\n## 34 区分接口继承和实现继承\n表面上直截了当的`public`继承，可以细分为函数接口继承和函数实现继承。以下面的这个例子来说明：\n\n``` cpp\nclass Shape {\npublic:\n    virtual void draw() const = 0;\n    virtual void error(const std::string& msg);\n    int getID() const;\n};\nclass Rect: public Shape {...};\nclass Circle: public Shape {...};\n```\n\n- 纯虚函数\n声明纯虚函数（如`draw()`函数）是为了让derived class只继承函数接口。乃是一种约定：“你一定要实现某某，但是我不管你如何实现”。\n不过，你仍然可以给纯虚函数提供函数定义。\n\n- 虚函数\n非纯虚函数（如`error()`函数）的目的是，让derived class继承该函数的接口和缺省实现。乃是约定“你必须支持XX，但是如果你不想自己实现，可以用我提供的这个”。\n然而可能会出现这样一种局面：derived class的表现与base class不同，但是又忘记了重写这个虚函数。为了避免这种情况，可以使用下面的技术来达到“除非你明确要求，否则我才不给你提供那个缺省定义”的目的。\n\n``` cpp\nclass Base {\npublic:\n    virtual void fun() = 0;   // 注意，我们改写成了纯虚函数\nprotected:\n    void default_fun() {...};   // 缺省实现\n};\n// 此时，若想使用缺省实现，就必须显式地调用\nclass Derived: public Base {\npublic:\n    virtual void fun() {\n        default_fun();\n    }\n};\n```\n不过这样导致一个多余的`default_fun()`函数。如果不想添加额外的函数，我们可以使用上述提到的拥有定义的纯虚函数来实现。\n\n``` cpp\nclass Base {\npublic:\n    virtual void fun() = 0;\n};\n// 为纯虚函数提供定义\nvoid Base::fun() {\n    // 缺省行为\n}\n\nclass Derived: public Base {\npublic:\n    // 显式调用基类的纯虚函数，实现缺省行为\n    virtual void fun() {Base::fun(); }\n};\nclass Derived2: public Base {\npublic:\n    // 实现自定义的行为\n    virtual void fun() {\n        // ...\n    }\n};\n```\n\n- 非虚函数\n这意味着你不应该在derived class中定义不同的行为（老老实实用我给你的！），使得其继承了一份接口和强制实现。\n\n## 35 考虑`virtual`函数之外的其他选择\n虚函数使得多态成为可能。不过在一些情况下，为了实现多态，不一定非要使用虚函数。本条款介绍了一些相关技术。\n\n在某游戏中，需要设计一个计算角色剩余血量的函数。下面是一种惯常的设计。\n``` cpp\nclass GameCharacter {\npublic:\n    virtual int healthValue() const;\n};\n```\n\n- 使用non-virtual interface实现template method模式\n这种流派主张`virtual`函数应该几乎总是私有的。较好的设计时将`healthValue()`函数设为非虚函数，并调用虚函数进行实现。这个调用函数中，可以做一些预先准备（互斥锁，日志等），后续可以做一些打扫工作。\n\n``` cpp\nclass GameCharacter {\npublic:\n    int healthValue() const {\n        // ... 前期准备\n        int ret_val = doHealthValue();\n        // ... 后续清理\n        return ret_val\n    }\nprivate:\n    virtual int doHealthValue() const {...}\n};\n```\n这样做的好处是基类明确定义了该如何实现求血量这个行为，同时又给了一定的自由，派生类可以重写`doHealthValue()`函数，针对自身的特点计算血量。\n\n- 使用函数指针实现策略模式\n上述方案实际上是对虚函数的调用进行了一次包装。我们还可以借由函数指针实现策略模式，为不同的派生类甚至不同的对象实例做出不同的实现。\n\n``` cpp\nclass GameCharacter;   // 前置声明\n// 计算血量的缺省方法\nint defaultHealthValue(const GameCharacter&);\n\nclass GameCharacter {\npublic:\n    typedef int (*HealthCalcFun) (const GameCharacter&);\n\n    explicit GameCharacter(HealthCalcFun f=defaultHealthValue\n        :healthFunc(f){\n        // ...\n    }\nprivate:\n    HealthCalcFun healthFunc;\n};\n```\n\n这样，我们通过在构造时候传入相应的函数指针，就可以实现计算血量的个性化设置。比如两个同样的boss，血量下降方式就可以不一样。\n或者我们可以在运行时候，通过设定`healthFunc`，来实现动态血量计算方法的变化。\n\n- 借由`std::function`实现策略模式\n作为上面的改进，我们可以使用`std::function`（C++11），这样，不止函数指针可以使用，函数对象等也都可以了。（关于`std::function`的大致介绍，可以看[这里](http://en.cppreference.com/w/cpp/utility/functional/function)）。\n\n我们只需将上面的`typedef`改掉即可。不再使用函数指针，而是更加高级更加通用的`std::function`。\n\n``` cpp\ntypedef std::function<int(const GameCharacter&)> HealthCalcFun;\n```\n\n- 使用古典的策略模式\n如下图所示。对于血量计算，我们单独抻出来一个基类，并有不同的实现。`GameCharacter`类中则含有一个指向`HealthCalcFun`类实例的指针。\n\n![使用UML表示的策略模式](/img/effectivecpp_strategy_pattern.png)\n\n``` cpp\n//我们首先定义HealthCalcFunc基类\nclass GameCharacter;    // 前向声明\nclass HealthCalcFunc {\npublic:\n    virtual int calc(const GameCharacter& gc) const {...}\n};\n\nHealthCalcFunc defaultCalcFunc;\n\nclass GameCharacter {\nprivate:\n    HealthCalcFunc* pfun;\npublic:\n    explicit GameCharacter(HealthCalcFunc* p=&defaultCalcFunc):\n        pfun(p) {}\n    int healthValue() const {\n        return pfun->calc(*this);\n    }\n};\n```\n\n该条款给出了虚函数的若干替代方案。\n\n## 36 绝不重新定义继承而来的非虚函数\n在条款34中已经指出，非虚函数是一种实现继承的约定。派生类不应该重新定义非虚函数。这破坏了约定。\n\n如下所示。\n``` cpp\nclass B {\npublic:\n    void mf() {...}\n};\nclass D: public B {\npublic:\n    void mf() {...}\n};\n\nD d;\nB* pb = &d;\nD* pd = &d;\n\npb->mf();   // 调用的是B::mf()\npd->mf();   // 调用的是D::mf()\n```\n\n这是因为非虚函数的绑定是编译期行为（和虚函数的动态绑定相对，其发生在运行时）。由于`pb`被声明为一个指向`B`的指针，所以其调用的是`B`的成员函数`mf()`。\n\n为了不至于让自己陷入精神分裂与背信弃义的境地，请不要重新定义继承而来的非虚函数。\n\n## 37 绝不重新定义继承而来的缺省参数值\n由于条款36的分析，所以我们只讨论继承而来的是带有缺省参数的虚函数。这样一来，本条款背后的逻辑就很清晰了：因为缺省参数同样是静态绑定的，而虚函数却是动态绑定。让我们再解释一下。\n\n静态类型是指在程序中被声明时的类型（不论其真实指向是什么）。\n``` cpp\n// Circle是Shape的派生类\nShape* ps;\nShape* pc = new Circle;   // 静态类型都是Shape\n```\n\n动态类型是指当前所指对象的类型。就上例来说，`pc`的动态类型是`Circle*`，而`ps`没有动态类型，因为它并没有指向任何对象实例。动态类型常常可以通过赋值改变。\n``` cpp\nps = new Circle;   // 现在ps的动态类型是Circle*\n```\n\n虚函数是运行时决定的，取决于发出调用的那个对象的动态类型。\n\n不过遵守此项条款，有时又会造成不便。看下例：\n\n``` cpp\nclass Shape {\npublic:\n    enum ShapeColor {RED, GREEN};\n    virtual void draw(ShapeColor c=RED) const=0;\n};\n\nclass Circle: public Shape {\npublic:\n    virtual void draw(ShapeColor c=RED) const;\n};\n```\n\n第一个问题，代码重复，我写了两遍缺省参数。第二造成了代码依存。比如我想换成`GREEN`为默认参数，需要在基类和派生类中同时修改。\n\n一种解决方法是采用条款35中的替代设计，如NVI方法。令基类中的一个public的非虚函数调用私有的虚函数，而后者可以被派生类重新定义。我们只需要在public的非虚函数中定义缺省参数即可。\n\n``` cpp\nclass Shape {\npublic:\n    void draw(ShapeColor c=RED) const {\n        doDraw(c);  // 调用私有的虚函数\n    }\nprivate:\n    //真正的工作在此完成\n    virtual void doDraw(ShapeColor c) const = 0;  \n};\n\nclass Circle: public Shape {\nprivate:\n    virtual void doDraw(ShapeColor c) const;  // 派生类重写这个真正的实现\n};\n```\n\n## 38 通过复合塑模has-a或“根据某物实现出”\n复合是指某种对象内含其他对象。复合实际有两层意义，一种较好理解，即has-a，如人有名字、性别等他类，一种是指根据某物实现（is-implemented-in-terms-of）。例如实现消息管理的某个类中含有队列作为实现。\n\n## 39 明智而审慎地使用`private`继承\n私有继承意味着条款38中的“根据某物实现出”。例如`D`私有继承自`B`，不是说`D`是某种`B`，私有继承完全是一种技术上的实现（和对现实的抽象没有半毛钱关系）。`B`的每样东西在`D`中都是不可见的，也就是成了黑箱，因为它们本身就是实现细节，你只是考虑用`B`来实现`D`的功能而已。\n\n但是复合也能达到相同的效果啊~我在`D`中加入一个`B`的对象实例不就好了？很多情况下的确是这样，如果没有必要，不建议使用私有继承。\n\n## 40 明智而审慎地使用多重继承\n使用多重继承有可能造成歧义。例如，`C`继承自`A`和`B`，而两个基类中都含有成员函数`mf()`。那么当`d.mf()`的时候，究竟是在调用哪个呢？你必须明确地指出,`d.A::mf()`。\n\n使用多重继承还可能会造成“钻石型”继承。任何时候继承体系中某个基类和派生类之间有一条以上的相通路线，就面临一个问题，是否要让基类中的每个成员变量经由每一条路线被复制？如果只想保留一份，那么需要将`File`定为虚基类，所有直接继承自它的类采用虚继承。\n![钻石型继承](/img/effectivecpp_diamond.png)\n\n``` cpp\nclass File {...};\nclass InputFile: virtual public File {...};\nclass OutputFile: virtual public File {...};\nclass IOFile: public InputFile, public OutputFile {...};\n```\n\n从正确的角度看，public的继承总应该是virtual的。不过这样会造成代码体积的膨胀和执行效率的下降。\n\n所以，如无必要，不要使用虚继承。即使使用，尽可能避免在其中放置数据（类似Java或C#中的接口Interface）\n\n## 附注 `std::function`的基本使用\n`std::function`的作用类似于函数指针，但是能力更加强大。我们可以将函数指针，函数对象，lambda表达式或者类中的成员函数作为`std::function`。\n如下所示：\n\n``` cpp\n#include <functional>\n#include <iostream>\n\nstruct Foo {\n    Foo(int num) : num_(num) {}\n    void print_add(int i) const { std::cout << num_+i << '\\n'; }\n    int num_;\n};\n\nvoid print_num(int i)\n{\n    std::cout << i << '\\n';\n}\n\nstruct PrintNum {\n    void operator()(int i) const\n    {\n        std::cout << i << '\\n';\n    }\n};\n\nint main()\n{\n    // store a free function\n    // 函数指针\n    std::function<void(int)> f_display = print_num;\n    f_display(-9);\n\n    // lambda表达式\n    // store a lambda\n    std::function<void()> f_display_42 = []() { print_num(42); };\n    f_display_42();\n\n    // store the result of a call to std::bind\n    // 绑定之后的函数对象\n    std::function<void()> f_display_31337 = std::bind(print_num, 31337);\n    f_display_31337();\n\n    // store a call to a member function\n    // 类中的成员函数，第一个参数为类实例的const reference\n    std::function<void(const Foo&, int)> f_add_display = &Foo::print_add;\n    const Foo foo(314159);\n    f_add_display(foo, 1);\n    f_add_display(314159, 1);\n\n    // store a call to a data member accessor\n    std::function<int(Foo const&)> f_num = &Foo::num_;\n    std::cout << \"num_: \" << f_num(foo) << '\\n';\n\n    // store a call to a member function and object\n    using std::placeholders::_1;\n    std::function<void(int)> f_add_display2 = std::bind( &Foo::print_add, foo, _1 );\n    f_add_display2(2);\n\n    // store a call to a member function and object ptr\n    std::function<void(int)> f_add_display3 = std::bind( &Foo::print_add, &foo, _1 );\n    f_add_display3(3);\n\n    // store a call to a function object\n    std::function<void(int)> f_display_obj = PrintNum();\n    f_display_obj(18);\n}\n```\n","slug":"effective-cpp-06","published":1,"updated":"2018-01-12T06:22:20.467Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjcu6vcss0010qu46djlehal0","content":"<p>C++中允许多重继承，并且可以指定继承是否是public or private等。成员函数也可以是虚函数或者非虚函数。如何在OOP这一C++联邦中的重要一员的规则下，写出易于拓展，易于维护且高效的代码？</p>\n<p>真•面向对象编程！<br><img src=\"/img/effectivecpp_06_joke.jpg\" alt=\"面向对象编程\"><br><a id=\"more\"></a></p>\n<h2 id=\"32-确定public继承塑模出Is-a的关系\"><a href=\"#32-确定public继承塑模出Is-a的关系\" class=\"headerlink\" title=\"32 确定public继承塑模出Is-a的关系\"></a>32 确定<code>public</code>继承塑模出Is-a的关系</h2><p>请把这条规则记在心中：<code>public</code>继承意味着Is-a（XX是X的一种）的关系。适用于base class上的东西也一定能够用在derived class身上。因为每一个derived class对象也是一个base class的对象。</p>\n<p>不过，在实际使用时，可能并不是那么简单。举个例子，在鸟类这个基类中定义了<code>fly()</code>这一虚函数，而企鹅很显然是一种鸟，但是却没有飞翔的能力。类似的情况需要在编程实践中灵活处理。</p>\n<h2 id=\"33-避免遮掩继承而来的名称\"><a href=\"#33-避免遮掩继承而来的名称\" class=\"headerlink\" title=\"33 避免遮掩继承而来的名称\"></a>33 避免遮掩继承而来的名称</h2><p>这个题材实际和作用域有关。当C++遇到某个名称时，会首先在local域中寻找，如果找到，就不再继续寻找。这样，derived class中的名称可能会遮盖base class中的名称。</p>\n<p>一种解决办法是使用<code>using</code>声明。如下所示：<br><figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">class</span> Base &#123;</div><div class=\"line\"><span class=\"keyword\">public</span>:</div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">virtual</span> <span class=\"keyword\">void</span> <span class=\"title\">f1</span><span class=\"params\">()</span> </span>= <span class=\"number\">0</span>;</div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">f3</span><span class=\"params\">()</span></span>;</div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">f3</span><span class=\"params\">(<span class=\"keyword\">double</span>)</span></span>;</div><div class=\"line\">&#125;;</div><div class=\"line\"></div><div class=\"line\"><span class=\"keyword\">class</span> Derived: <span class=\"keyword\">public</span> Base &#123;</div><div class=\"line\"><span class=\"keyword\">public</span>:</div><div class=\"line\">    <span class=\"keyword\">using</span> Base::f3;</div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">virtual</span> <span class=\"keyword\">void</span> <span class=\"title\">f1</span><span class=\"params\">()</span></span>;</div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">f3</span><span class=\"params\">()</span></span>;</div><div class=\"line\">&#125;;</div><div class=\"line\"></div><div class=\"line\">Derived d;</div><div class=\"line\">d.f1();  <span class=\"comment\">// 没问题，调用了Derived中的f1</span></div><div class=\"line\">d.f3();  <span class=\"comment\">// 没问题，调用了Derived中的f3</span></div><div class=\"line\"><span class=\"keyword\">double</span> x;</div><div class=\"line\">d.f3(x);  <span class=\"comment\">// 没问题，调用了Base中的f3。</span></div><div class=\"line\"><span class=\"comment\">// 但是如果没有using声明的话，Base::f3会被冲掉。</span></div></pre></td></tr></table></figure></p>\n<h2 id=\"34-区分接口继承和实现继承\"><a href=\"#34-区分接口继承和实现继承\" class=\"headerlink\" title=\"34 区分接口继承和实现继承\"></a>34 区分接口继承和实现继承</h2><p>表面上直截了当的<code>public</code>继承，可以细分为函数接口继承和函数实现继承。以下面的这个例子来说明：</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">class</span> Shape &#123;</div><div class=\"line\"><span class=\"keyword\">public</span>:</div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">virtual</span> <span class=\"keyword\">void</span> <span class=\"title\">draw</span><span class=\"params\">()</span> <span class=\"keyword\">const</span> </span>= <span class=\"number\">0</span>;</div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">virtual</span> <span class=\"keyword\">void</span> <span class=\"title\">error</span><span class=\"params\">(<span class=\"keyword\">const</span> <span class=\"built_in\">std</span>::<span class=\"built_in\">string</span>&amp; msg)</span></span>;</div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">int</span> <span class=\"title\">getID</span><span class=\"params\">()</span> <span class=\"keyword\">const</span></span>;</div><div class=\"line\">&#125;;</div><div class=\"line\"><span class=\"keyword\">class</span> Rect: <span class=\"keyword\">public</span> Shape &#123;...&#125;;</div><div class=\"line\"><span class=\"keyword\">class</span> Circle: <span class=\"keyword\">public</span> Shape &#123;...&#125;;</div></pre></td></tr></table></figure>\n<ul>\n<li><p>纯虚函数<br>声明纯虚函数（如<code>draw()</code>函数）是为了让derived class只继承函数接口。乃是一种约定：“你一定要实现某某，但是我不管你如何实现”。<br>不过，你仍然可以给纯虚函数提供函数定义。</p>\n</li>\n<li><p>虚函数<br>非纯虚函数（如<code>error()</code>函数）的目的是，让derived class继承该函数的接口和缺省实现。乃是约定“你必须支持XX，但是如果你不想自己实现，可以用我提供的这个”。<br>然而可能会出现这样一种局面：derived class的表现与base class不同，但是又忘记了重写这个虚函数。为了避免这种情况，可以使用下面的技术来达到“除非你明确要求，否则我才不给你提供那个缺省定义”的目的。</p>\n</li>\n</ul>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">class</span> Base &#123;</div><div class=\"line\"><span class=\"keyword\">public</span>:</div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">virtual</span> <span class=\"keyword\">void</span> <span class=\"title\">fun</span><span class=\"params\">()</span> </span>= <span class=\"number\">0</span>;   <span class=\"comment\">// 注意，我们改写成了纯虚函数</span></div><div class=\"line\"><span class=\"keyword\">protected</span>:</div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">default_fun</span><span class=\"params\">()</span> </span>&#123;...&#125;;   <span class=\"comment\">// 缺省实现</span></div><div class=\"line\">&#125;;</div><div class=\"line\"><span class=\"comment\">// 此时，若想使用缺省实现，就必须显式地调用</span></div><div class=\"line\"><span class=\"keyword\">class</span> Derived: <span class=\"keyword\">public</span> Base &#123;</div><div class=\"line\"><span class=\"keyword\">public</span>:</div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">virtual</span> <span class=\"keyword\">void</span> <span class=\"title\">fun</span><span class=\"params\">()</span> </span>&#123;</div><div class=\"line\">        default_fun();</div><div class=\"line\">    &#125;</div><div class=\"line\">&#125;;</div></pre></td></tr></table></figure>\n<p>不过这样导致一个多余的<code>default_fun()</code>函数。如果不想添加额外的函数，我们可以使用上述提到的拥有定义的纯虚函数来实现。</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">class</span> Base &#123;</div><div class=\"line\"><span class=\"keyword\">public</span>:</div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">virtual</span> <span class=\"keyword\">void</span> <span class=\"title\">fun</span><span class=\"params\">()</span> </span>= <span class=\"number\">0</span>;</div><div class=\"line\">&#125;;</div><div class=\"line\"><span class=\"comment\">// 为纯虚函数提供定义</span></div><div class=\"line\"><span class=\"keyword\">void</span> Base::fun() &#123;</div><div class=\"line\">    <span class=\"comment\">// 缺省行为</span></div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\"><span class=\"keyword\">class</span> Derived: <span class=\"keyword\">public</span> Base &#123;</div><div class=\"line\"><span class=\"keyword\">public</span>:</div><div class=\"line\">    <span class=\"comment\">// 显式调用基类的纯虚函数，实现缺省行为</span></div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">virtual</span> <span class=\"keyword\">void</span> <span class=\"title\">fun</span><span class=\"params\">()</span> </span>&#123;Base::fun(); &#125;</div><div class=\"line\">&#125;;</div><div class=\"line\"><span class=\"keyword\">class</span> Derived2: <span class=\"keyword\">public</span> Base &#123;</div><div class=\"line\"><span class=\"keyword\">public</span>:</div><div class=\"line\">    <span class=\"comment\">// 实现自定义的行为</span></div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">virtual</span> <span class=\"keyword\">void</span> <span class=\"title\">fun</span><span class=\"params\">()</span> </span>&#123;</div><div class=\"line\">        <span class=\"comment\">// ...</span></div><div class=\"line\">    &#125;</div><div class=\"line\">&#125;;</div></pre></td></tr></table></figure>\n<ul>\n<li>非虚函数<br>这意味着你不应该在derived class中定义不同的行为（老老实实用我给你的！），使得其继承了一份接口和强制实现。</li>\n</ul>\n<h2 id=\"35-考虑virtual函数之外的其他选择\"><a href=\"#35-考虑virtual函数之外的其他选择\" class=\"headerlink\" title=\"35 考虑virtual函数之外的其他选择\"></a>35 考虑<code>virtual</code>函数之外的其他选择</h2><p>虚函数使得多态成为可能。不过在一些情况下，为了实现多态，不一定非要使用虚函数。本条款介绍了一些相关技术。</p>\n<p>在某游戏中，需要设计一个计算角色剩余血量的函数。下面是一种惯常的设计。<br><figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">class</span> GameCharacter &#123;</div><div class=\"line\"><span class=\"keyword\">public</span>:</div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">virtual</span> <span class=\"keyword\">int</span> <span class=\"title\">healthValue</span><span class=\"params\">()</span> <span class=\"keyword\">const</span></span>;</div><div class=\"line\">&#125;;</div></pre></td></tr></table></figure></p>\n<ul>\n<li>使用non-virtual interface实现template method模式<br>这种流派主张<code>virtual</code>函数应该几乎总是私有的。较好的设计时将<code>healthValue()</code>函数设为非虚函数，并调用虚函数进行实现。这个调用函数中，可以做一些预先准备（互斥锁，日志等），后续可以做一些打扫工作。</li>\n</ul>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">class</span> GameCharacter &#123;</div><div class=\"line\"><span class=\"keyword\">public</span>:</div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">int</span> <span class=\"title\">healthValue</span><span class=\"params\">()</span> <span class=\"keyword\">const</span> </span>&#123;</div><div class=\"line\">        <span class=\"comment\">// ... 前期准备</span></div><div class=\"line\">        <span class=\"keyword\">int</span> ret_val = doHealthValue();</div><div class=\"line\">        <span class=\"comment\">// ... 后续清理</span></div><div class=\"line\">        <span class=\"keyword\">return</span> ret_val</div><div class=\"line\">    &#125;</div><div class=\"line\"><span class=\"keyword\">private</span>:</div><div class=\"line\">    <span class=\"keyword\">virtual</span> <span class=\"keyword\">int</span> doHealthValue() <span class=\"keyword\">const</span> &#123;...&#125;</div><div class=\"line\">&#125;;</div></pre></td></tr></table></figure>\n<p>这样做的好处是基类明确定义了该如何实现求血量这个行为，同时又给了一定的自由，派生类可以重写<code>doHealthValue()</code>函数，针对自身的特点计算血量。</p>\n<ul>\n<li>使用函数指针实现策略模式<br>上述方案实际上是对虚函数的调用进行了一次包装。我们还可以借由函数指针实现策略模式，为不同的派生类甚至不同的对象实例做出不同的实现。</li>\n</ul>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">class</span> GameCharacter;   <span class=\"comment\">// 前置声明</span></div><div class=\"line\"><span class=\"comment\">// 计算血量的缺省方法</span></div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">int</span> <span class=\"title\">defaultHealthValue</span><span class=\"params\">(<span class=\"keyword\">const</span> GameCharacter&amp;)</span></span>;</div><div class=\"line\"></div><div class=\"line\"><span class=\"keyword\">class</span> GameCharacter &#123;</div><div class=\"line\"><span class=\"keyword\">public</span>:</div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">typedef</span> <span class=\"title\">int</span> <span class=\"params\">(*HealthCalcFun)</span> <span class=\"params\">(<span class=\"keyword\">const</span> GameCharacter&amp;)</span></span>;</div><div class=\"line\"></div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">explicit</span> <span class=\"title\">GameCharacter</span><span class=\"params\">(HealthCalcFun f=defaultHealthValue</span></span></div><div class=\"line\">        :healthFunc(f)&#123;</div><div class=\"line\">        <span class=\"comment\">// ...</span></div><div class=\"line\">    &#125;</div><div class=\"line\"><span class=\"keyword\">private</span>:</div><div class=\"line\">    HealthCalcFun healthFunc;</div><div class=\"line\">&#125;;</div></pre></td></tr></table></figure>\n<p>这样，我们通过在构造时候传入相应的函数指针，就可以实现计算血量的个性化设置。比如两个同样的boss，血量下降方式就可以不一样。<br>或者我们可以在运行时候，通过设定<code>healthFunc</code>，来实现动态血量计算方法的变化。</p>\n<ul>\n<li>借由<code>std::function</code>实现策略模式<br>作为上面的改进，我们可以使用<code>std::function</code>（C++11），这样，不止函数指针可以使用，函数对象等也都可以了。（关于<code>std::function</code>的大致介绍，可以看<a href=\"http://en.cppreference.com/w/cpp/utility/functional/function\" target=\"_blank\" rel=\"external\">这里</a>）。</li>\n</ul>\n<p>我们只需将上面的<code>typedef</code>改掉即可。不再使用函数指针，而是更加高级更加通用的<code>std::function</code>。</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">typedef</span> <span class=\"built_in\">std</span>::function&lt;<span class=\"keyword\">int</span>(<span class=\"keyword\">const</span> GameCharacter&amp;)&gt; HealthCalcFun;</div></pre></td></tr></table></figure>\n<ul>\n<li>使用古典的策略模式<br>如下图所示。对于血量计算，我们单独抻出来一个基类，并有不同的实现。<code>GameCharacter</code>类中则含有一个指向<code>HealthCalcFun</code>类实例的指针。</li>\n</ul>\n<p><img src=\"/img/effectivecpp_strategy_pattern.png\" alt=\"使用UML表示的策略模式\"></p>\n<figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div></pre></td><td class=\"code\"><pre><div class=\"line\">//我们首先定义HealthCalcFunc基类</div><div class=\"line\">class GameCharacter;    // 前向声明</div><div class=\"line\">class HealthCalcFunc &#123;</div><div class=\"line\">public:</div><div class=\"line\">    virtual int calc(const GameCharacter&amp; gc) const &#123;...&#125;</div><div class=\"line\">&#125;;</div><div class=\"line\"></div><div class=\"line\">HealthCalcFunc defaultCalcFunc;</div><div class=\"line\"></div><div class=\"line\">class GameCharacter &#123;</div><div class=\"line\">private:</div><div class=\"line\">    HealthCalcFunc* pfun;</div><div class=\"line\">public:</div><div class=\"line\">    explicit GameCharacter(HealthCalcFunc* p=&amp;defaultCalcFunc):</div><div class=\"line\">        pfun(p) &#123;&#125;</div><div class=\"line\">    int healthValue() const &#123;</div><div class=\"line\">        return pfun-&gt;calc(*this);</div><div class=\"line\">    &#125;</div><div class=\"line\">&#125;;</div></pre></td></tr></table></figure>\n<p>该条款给出了虚函数的若干替代方案。</p>\n<h2 id=\"36-绝不重新定义继承而来的非虚函数\"><a href=\"#36-绝不重新定义继承而来的非虚函数\" class=\"headerlink\" title=\"36 绝不重新定义继承而来的非虚函数\"></a>36 绝不重新定义继承而来的非虚函数</h2><p>在条款34中已经指出，非虚函数是一种实现继承的约定。派生类不应该重新定义非虚函数。这破坏了约定。</p>\n<p>如下所示。<br><figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">class</span> B &#123;</div><div class=\"line\"><span class=\"keyword\">public</span>:</div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">mf</span><span class=\"params\">()</span> </span>&#123;...&#125;</div><div class=\"line\">&#125;;</div><div class=\"line\"><span class=\"keyword\">class</span> D: <span class=\"keyword\">public</span> B &#123;</div><div class=\"line\"><span class=\"keyword\">public</span>:</div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">mf</span><span class=\"params\">()</span> </span>&#123;...&#125;</div><div class=\"line\">&#125;;</div><div class=\"line\"></div><div class=\"line\">D d;</div><div class=\"line\">B* pb = &amp;d;</div><div class=\"line\">D* pd = &amp;d;</div><div class=\"line\"></div><div class=\"line\">pb-&gt;mf();   <span class=\"comment\">// 调用的是B::mf()</span></div><div class=\"line\">pd-&gt;mf();   <span class=\"comment\">// 调用的是D::mf()</span></div></pre></td></tr></table></figure></p>\n<p>这是因为非虚函数的绑定是编译期行为（和虚函数的动态绑定相对，其发生在运行时）。由于<code>pb</code>被声明为一个指向<code>B</code>的指针，所以其调用的是<code>B</code>的成员函数<code>mf()</code>。</p>\n<p>为了不至于让自己陷入精神分裂与背信弃义的境地，请不要重新定义继承而来的非虚函数。</p>\n<h2 id=\"37-绝不重新定义继承而来的缺省参数值\"><a href=\"#37-绝不重新定义继承而来的缺省参数值\" class=\"headerlink\" title=\"37 绝不重新定义继承而来的缺省参数值\"></a>37 绝不重新定义继承而来的缺省参数值</h2><p>由于条款36的分析，所以我们只讨论继承而来的是带有缺省参数的虚函数。这样一来，本条款背后的逻辑就很清晰了：因为缺省参数同样是静态绑定的，而虚函数却是动态绑定。让我们再解释一下。</p>\n<p>静态类型是指在程序中被声明时的类型（不论其真实指向是什么）。<br><figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\">// Circle是Shape的派生类</span></div><div class=\"line\">Shape* ps;</div><div class=\"line\">Shape* pc = <span class=\"keyword\">new</span> Circle;   <span class=\"comment\">// 静态类型都是Shape</span></div></pre></td></tr></table></figure></p>\n<p>动态类型是指当前所指对象的类型。就上例来说，<code>pc</code>的动态类型是<code>Circle*</code>，而<code>ps</code>没有动态类型，因为它并没有指向任何对象实例。动态类型常常可以通过赋值改变。<br><figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">ps = <span class=\"keyword\">new</span> Circle;   <span class=\"comment\">// 现在ps的动态类型是Circle*</span></div></pre></td></tr></table></figure></p>\n<p>虚函数是运行时决定的，取决于发出调用的那个对象的动态类型。</p>\n<p>不过遵守此项条款，有时又会造成不便。看下例：</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">class</span> Shape &#123;</div><div class=\"line\"><span class=\"keyword\">public</span>:</div><div class=\"line\">    <span class=\"keyword\">enum</span> ShapeColor &#123;RED, GREEN&#125;;</div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">virtual</span> <span class=\"keyword\">void</span> <span class=\"title\">draw</span><span class=\"params\">(ShapeColor c=RED)</span> <span class=\"keyword\">const</span></span>=<span class=\"number\">0</span>;</div><div class=\"line\">&#125;;</div><div class=\"line\"></div><div class=\"line\"><span class=\"keyword\">class</span> Circle: <span class=\"keyword\">public</span> Shape &#123;</div><div class=\"line\"><span class=\"keyword\">public</span>:</div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">virtual</span> <span class=\"keyword\">void</span> <span class=\"title\">draw</span><span class=\"params\">(ShapeColor c=RED)</span> <span class=\"keyword\">const</span></span>;</div><div class=\"line\">&#125;;</div></pre></td></tr></table></figure>\n<p>第一个问题，代码重复，我写了两遍缺省参数。第二造成了代码依存。比如我想换成<code>GREEN</code>为默认参数，需要在基类和派生类中同时修改。</p>\n<p>一种解决方法是采用条款35中的替代设计，如NVI方法。令基类中的一个public的非虚函数调用私有的虚函数，而后者可以被派生类重新定义。我们只需要在public的非虚函数中定义缺省参数即可。</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">class</span> Shape &#123;</div><div class=\"line\"><span class=\"keyword\">public</span>:</div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">draw</span><span class=\"params\">(ShapeColor c=RED)</span> <span class=\"keyword\">const</span> </span>&#123;</div><div class=\"line\">        doDraw(c);  <span class=\"comment\">// 调用私有的虚函数</span></div><div class=\"line\">    &#125;</div><div class=\"line\"><span class=\"keyword\">private</span>:</div><div class=\"line\">    <span class=\"comment\">//真正的工作在此完成</span></div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">virtual</span> <span class=\"keyword\">void</span> <span class=\"title\">doDraw</span><span class=\"params\">(ShapeColor c)</span> <span class=\"keyword\">const</span> </span>= <span class=\"number\">0</span>;  </div><div class=\"line\">&#125;;</div><div class=\"line\"></div><div class=\"line\"><span class=\"keyword\">class</span> Circle: <span class=\"keyword\">public</span> Shape &#123;</div><div class=\"line\"><span class=\"keyword\">private</span>:</div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">virtual</span> <span class=\"keyword\">void</span> <span class=\"title\">doDraw</span><span class=\"params\">(ShapeColor c)</span> <span class=\"keyword\">const</span></span>;  <span class=\"comment\">// 派生类重写这个真正的实现</span></div><div class=\"line\">&#125;;</div></pre></td></tr></table></figure>\n<h2 id=\"38-通过复合塑模has-a或“根据某物实现出”\"><a href=\"#38-通过复合塑模has-a或“根据某物实现出”\" class=\"headerlink\" title=\"38 通过复合塑模has-a或“根据某物实现出”\"></a>38 通过复合塑模has-a或“根据某物实现出”</h2><p>复合是指某种对象内含其他对象。复合实际有两层意义，一种较好理解，即has-a，如人有名字、性别等他类，一种是指根据某物实现（is-implemented-in-terms-of）。例如实现消息管理的某个类中含有队列作为实现。</p>\n<h2 id=\"39-明智而审慎地使用private继承\"><a href=\"#39-明智而审慎地使用private继承\" class=\"headerlink\" title=\"39 明智而审慎地使用private继承\"></a>39 明智而审慎地使用<code>private</code>继承</h2><p>私有继承意味着条款38中的“根据某物实现出”。例如<code>D</code>私有继承自<code>B</code>，不是说<code>D</code>是某种<code>B</code>，私有继承完全是一种技术上的实现（和对现实的抽象没有半毛钱关系）。<code>B</code>的每样东西在<code>D</code>中都是不可见的，也就是成了黑箱，因为它们本身就是实现细节，你只是考虑用<code>B</code>来实现<code>D</code>的功能而已。</p>\n<p>但是复合也能达到相同的效果啊~我在<code>D</code>中加入一个<code>B</code>的对象实例不就好了？很多情况下的确是这样，如果没有必要，不建议使用私有继承。</p>\n<h2 id=\"40-明智而审慎地使用多重继承\"><a href=\"#40-明智而审慎地使用多重继承\" class=\"headerlink\" title=\"40 明智而审慎地使用多重继承\"></a>40 明智而审慎地使用多重继承</h2><p>使用多重继承有可能造成歧义。例如，<code>C</code>继承自<code>A</code>和<code>B</code>，而两个基类中都含有成员函数<code>mf()</code>。那么当<code>d.mf()</code>的时候，究竟是在调用哪个呢？你必须明确地指出,<code>d.A::mf()</code>。</p>\n<p>使用多重继承还可能会造成“钻石型”继承。任何时候继承体系中某个基类和派生类之间有一条以上的相通路线，就面临一个问题，是否要让基类中的每个成员变量经由每一条路线被复制？如果只想保留一份，那么需要将<code>File</code>定为虚基类，所有直接继承自它的类采用虚继承。<br><img src=\"/img/effectivecpp_diamond.png\" alt=\"钻石型继承\"></p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">class</span> File &#123;...&#125;;</div><div class=\"line\"><span class=\"keyword\">class</span> InputFile: <span class=\"keyword\">virtual</span> <span class=\"keyword\">public</span> File &#123;...&#125;;</div><div class=\"line\"><span class=\"keyword\">class</span> OutputFile: <span class=\"keyword\">virtual</span> <span class=\"keyword\">public</span> File &#123;...&#125;;</div><div class=\"line\"><span class=\"keyword\">class</span> IOFile: <span class=\"keyword\">public</span> InputFile, <span class=\"keyword\">public</span> OutputFile &#123;...&#125;;</div></pre></td></tr></table></figure>\n<p>从正确的角度看，public的继承总应该是virtual的。不过这样会造成代码体积的膨胀和执行效率的下降。</p>\n<p>所以，如无必要，不要使用虚继承。即使使用，尽可能避免在其中放置数据（类似Java或C#中的接口Interface）</p>\n<h2 id=\"附注-std-function的基本使用\"><a href=\"#附注-std-function的基本使用\" class=\"headerlink\" title=\"附注 std::function的基本使用\"></a>附注 <code>std::function</code>的基本使用</h2><p><code>std::function</code>的作用类似于函数指针，但是能力更加强大。我们可以将函数指针，函数对象，lambda表达式或者类中的成员函数作为<code>std::function</code>。<br>如下所示：</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div><div class=\"line\">47</div><div class=\"line\">48</div><div class=\"line\">49</div><div class=\"line\">50</div><div class=\"line\">51</div><div class=\"line\">52</div><div class=\"line\">53</div><div class=\"line\">54</div><div class=\"line\">55</div><div class=\"line\">56</div><div class=\"line\">57</div><div class=\"line\">58</div><div class=\"line\">59</div><div class=\"line\">60</div><div class=\"line\">61</div><div class=\"line\">62</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">include</span> <span class=\"meta-string\">&lt;functional&gt;</span></span></div><div class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">include</span> <span class=\"meta-string\">&lt;iostream&gt;</span></span></div><div class=\"line\"></div><div class=\"line\"><span class=\"keyword\">struct</span> Foo &#123;</div><div class=\"line\">    Foo(<span class=\"keyword\">int</span> num) : num_(num) &#123;&#125;</div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">print_add</span><span class=\"params\">(<span class=\"keyword\">int</span> i)</span> <span class=\"keyword\">const</span> </span>&#123; <span class=\"built_in\">std</span>::<span class=\"built_in\">cout</span> &lt;&lt; num_+i &lt;&lt; <span class=\"string\">'\\n'</span>; &#125;</div><div class=\"line\">    <span class=\"keyword\">int</span> num_;</div><div class=\"line\">&#125;;</div><div class=\"line\"></div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">print_num</span><span class=\"params\">(<span class=\"keyword\">int</span> i)</span></span></div><div class=\"line\">&#123;</div><div class=\"line\">    <span class=\"built_in\">std</span>::<span class=\"built_in\">cout</span> &lt;&lt; i &lt;&lt; <span class=\"string\">'\\n'</span>;</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\"><span class=\"keyword\">struct</span> PrintNum &#123;</div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">operator</span><span class=\"params\">()</span><span class=\"params\">(<span class=\"keyword\">int</span> i)</span> <span class=\"keyword\">const</span></span></div><div class=\"line\">    &#123;</div><div class=\"line\">        <span class=\"built_in\">std</span>::<span class=\"built_in\">cout</span> &lt;&lt; i &lt;&lt; <span class=\"string\">'\\n'</span>;</div><div class=\"line\">    &#125;</div><div class=\"line\">&#125;;</div><div class=\"line\"></div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">int</span> <span class=\"title\">main</span><span class=\"params\">()</span></span></div><div class=\"line\">&#123;</div><div class=\"line\">    <span class=\"comment\">// store a free function</span></div><div class=\"line\">    <span class=\"comment\">// 函数指针</span></div><div class=\"line\">    <span class=\"built_in\">std</span>::function&lt;<span class=\"keyword\">void</span>(<span class=\"keyword\">int</span>)&gt; f_display = print_num;</div><div class=\"line\">    f_display(<span class=\"number\">-9</span>);</div><div class=\"line\"></div><div class=\"line\">    <span class=\"comment\">// lambda表达式</span></div><div class=\"line\">    <span class=\"comment\">// store a lambda</span></div><div class=\"line\">    <span class=\"built_in\">std</span>::function&lt;<span class=\"keyword\">void</span>()&gt; f_display_42 = []() &#123; print_num(<span class=\"number\">42</span>); &#125;;</div><div class=\"line\">    f_display_42();</div><div class=\"line\"></div><div class=\"line\">    <span class=\"comment\">// store the result of a call to std::bind</span></div><div class=\"line\">    <span class=\"comment\">// 绑定之后的函数对象</span></div><div class=\"line\">    <span class=\"built_in\">std</span>::function&lt;<span class=\"keyword\">void</span>()&gt; f_display_31337 = <span class=\"built_in\">std</span>::bind(print_num, <span class=\"number\">31337</span>);</div><div class=\"line\">    f_display_31337();</div><div class=\"line\"></div><div class=\"line\">    <span class=\"comment\">// store a call to a member function</span></div><div class=\"line\">    <span class=\"comment\">// 类中的成员函数，第一个参数为类实例的const reference</span></div><div class=\"line\">    <span class=\"built_in\">std</span>::function&lt;<span class=\"keyword\">void</span>(<span class=\"keyword\">const</span> Foo&amp;, <span class=\"keyword\">int</span>)&gt; f_add_display = &amp;Foo::print_add;</div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">const</span> Foo <span class=\"title\">foo</span><span class=\"params\">(<span class=\"number\">314159</span>)</span></span>;</div><div class=\"line\">    f_add_display(foo, <span class=\"number\">1</span>);</div><div class=\"line\">    f_add_display(<span class=\"number\">314159</span>, <span class=\"number\">1</span>);</div><div class=\"line\"></div><div class=\"line\">    <span class=\"comment\">// store a call to a data member accessor</span></div><div class=\"line\">    <span class=\"built_in\">std</span>::function&lt;<span class=\"keyword\">int</span>(Foo <span class=\"keyword\">const</span>&amp;)&gt; f_num = &amp;Foo::num_;</div><div class=\"line\">    <span class=\"built_in\">std</span>::<span class=\"built_in\">cout</span> &lt;&lt; <span class=\"string\">\"num_: \"</span> &lt;&lt; f_num(foo) &lt;&lt; <span class=\"string\">'\\n'</span>;</div><div class=\"line\"></div><div class=\"line\">    <span class=\"comment\">// store a call to a member function and object</span></div><div class=\"line\">    <span class=\"keyword\">using</span> <span class=\"built_in\">std</span>::placeholders::_1;</div><div class=\"line\">    <span class=\"built_in\">std</span>::function&lt;<span class=\"keyword\">void</span>(<span class=\"keyword\">int</span>)&gt; f_add_display2 = <span class=\"built_in\">std</span>::bind( &amp;Foo::print_add, foo, _1 );</div><div class=\"line\">    f_add_display2(<span class=\"number\">2</span>);</div><div class=\"line\"></div><div class=\"line\">    <span class=\"comment\">// store a call to a member function and object ptr</span></div><div class=\"line\">    <span class=\"built_in\">std</span>::function&lt;<span class=\"keyword\">void</span>(<span class=\"keyword\">int</span>)&gt; f_add_display3 = <span class=\"built_in\">std</span>::bind( &amp;Foo::print_add, &amp;foo, _1 );</div><div class=\"line\">    f_add_display3(<span class=\"number\">3</span>);</div><div class=\"line\"></div><div class=\"line\">    <span class=\"comment\">// store a call to a function object</span></div><div class=\"line\">    <span class=\"built_in\">std</span>::function&lt;<span class=\"keyword\">void</span>(<span class=\"keyword\">int</span>)&gt; f_display_obj = PrintNum();</div><div class=\"line\">    f_display_obj(<span class=\"number\">18</span>);</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n","excerpt":"<p>C++中允许多重继承，并且可以指定继承是否是public or private等。成员函数也可以是虚函数或者非虚函数。如何在OOP这一C++联邦中的重要一员的规则下，写出易于拓展，易于维护且高效的代码？</p>\n<p>真•面向对象编程！<br><img src=\"/img/effectivecpp_06_joke.jpg\" alt=\"面向对象编程\"><br>","more":"</p>\n<h2 id=\"32-确定public继承塑模出Is-a的关系\"><a href=\"#32-确定public继承塑模出Is-a的关系\" class=\"headerlink\" title=\"32 确定public继承塑模出Is-a的关系\"></a>32 确定<code>public</code>继承塑模出Is-a的关系</h2><p>请把这条规则记在心中：<code>public</code>继承意味着Is-a（XX是X的一种）的关系。适用于base class上的东西也一定能够用在derived class身上。因为每一个derived class对象也是一个base class的对象。</p>\n<p>不过，在实际使用时，可能并不是那么简单。举个例子，在鸟类这个基类中定义了<code>fly()</code>这一虚函数，而企鹅很显然是一种鸟，但是却没有飞翔的能力。类似的情况需要在编程实践中灵活处理。</p>\n<h2 id=\"33-避免遮掩继承而来的名称\"><a href=\"#33-避免遮掩继承而来的名称\" class=\"headerlink\" title=\"33 避免遮掩继承而来的名称\"></a>33 避免遮掩继承而来的名称</h2><p>这个题材实际和作用域有关。当C++遇到某个名称时，会首先在local域中寻找，如果找到，就不再继续寻找。这样，derived class中的名称可能会遮盖base class中的名称。</p>\n<p>一种解决办法是使用<code>using</code>声明。如下所示：<br><figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">class</span> Base &#123;</div><div class=\"line\"><span class=\"keyword\">public</span>:</div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">virtual</span> <span class=\"keyword\">void</span> <span class=\"title\">f1</span><span class=\"params\">()</span> </span>= <span class=\"number\">0</span>;</div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">f3</span><span class=\"params\">()</span></span>;</div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">f3</span><span class=\"params\">(<span class=\"keyword\">double</span>)</span></span>;</div><div class=\"line\">&#125;;</div><div class=\"line\"></div><div class=\"line\"><span class=\"keyword\">class</span> Derived: <span class=\"keyword\">public</span> Base &#123;</div><div class=\"line\"><span class=\"keyword\">public</span>:</div><div class=\"line\">    <span class=\"keyword\">using</span> Base::f3;</div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">virtual</span> <span class=\"keyword\">void</span> <span class=\"title\">f1</span><span class=\"params\">()</span></span>;</div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">f3</span><span class=\"params\">()</span></span>;</div><div class=\"line\">&#125;;</div><div class=\"line\"></div><div class=\"line\">Derived d;</div><div class=\"line\">d.f1();  <span class=\"comment\">// 没问题，调用了Derived中的f1</span></div><div class=\"line\">d.f3();  <span class=\"comment\">// 没问题，调用了Derived中的f3</span></div><div class=\"line\"><span class=\"keyword\">double</span> x;</div><div class=\"line\">d.f3(x);  <span class=\"comment\">// 没问题，调用了Base中的f3。</span></div><div class=\"line\"><span class=\"comment\">// 但是如果没有using声明的话，Base::f3会被冲掉。</span></div></pre></td></tr></table></figure></p>\n<h2 id=\"34-区分接口继承和实现继承\"><a href=\"#34-区分接口继承和实现继承\" class=\"headerlink\" title=\"34 区分接口继承和实现继承\"></a>34 区分接口继承和实现继承</h2><p>表面上直截了当的<code>public</code>继承，可以细分为函数接口继承和函数实现继承。以下面的这个例子来说明：</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">class</span> Shape &#123;</div><div class=\"line\"><span class=\"keyword\">public</span>:</div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">virtual</span> <span class=\"keyword\">void</span> <span class=\"title\">draw</span><span class=\"params\">()</span> <span class=\"keyword\">const</span> </span>= <span class=\"number\">0</span>;</div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">virtual</span> <span class=\"keyword\">void</span> <span class=\"title\">error</span><span class=\"params\">(<span class=\"keyword\">const</span> <span class=\"built_in\">std</span>::<span class=\"built_in\">string</span>&amp; msg)</span></span>;</div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">int</span> <span class=\"title\">getID</span><span class=\"params\">()</span> <span class=\"keyword\">const</span></span>;</div><div class=\"line\">&#125;;</div><div class=\"line\"><span class=\"keyword\">class</span> Rect: <span class=\"keyword\">public</span> Shape &#123;...&#125;;</div><div class=\"line\"><span class=\"keyword\">class</span> Circle: <span class=\"keyword\">public</span> Shape &#123;...&#125;;</div></pre></td></tr></table></figure>\n<ul>\n<li><p>纯虚函数<br>声明纯虚函数（如<code>draw()</code>函数）是为了让derived class只继承函数接口。乃是一种约定：“你一定要实现某某，但是我不管你如何实现”。<br>不过，你仍然可以给纯虚函数提供函数定义。</p>\n</li>\n<li><p>虚函数<br>非纯虚函数（如<code>error()</code>函数）的目的是，让derived class继承该函数的接口和缺省实现。乃是约定“你必须支持XX，但是如果你不想自己实现，可以用我提供的这个”。<br>然而可能会出现这样一种局面：derived class的表现与base class不同，但是又忘记了重写这个虚函数。为了避免这种情况，可以使用下面的技术来达到“除非你明确要求，否则我才不给你提供那个缺省定义”的目的。</p>\n</li>\n</ul>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">class</span> Base &#123;</div><div class=\"line\"><span class=\"keyword\">public</span>:</div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">virtual</span> <span class=\"keyword\">void</span> <span class=\"title\">fun</span><span class=\"params\">()</span> </span>= <span class=\"number\">0</span>;   <span class=\"comment\">// 注意，我们改写成了纯虚函数</span></div><div class=\"line\"><span class=\"keyword\">protected</span>:</div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">default_fun</span><span class=\"params\">()</span> </span>&#123;...&#125;;   <span class=\"comment\">// 缺省实现</span></div><div class=\"line\">&#125;;</div><div class=\"line\"><span class=\"comment\">// 此时，若想使用缺省实现，就必须显式地调用</span></div><div class=\"line\"><span class=\"keyword\">class</span> Derived: <span class=\"keyword\">public</span> Base &#123;</div><div class=\"line\"><span class=\"keyword\">public</span>:</div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">virtual</span> <span class=\"keyword\">void</span> <span class=\"title\">fun</span><span class=\"params\">()</span> </span>&#123;</div><div class=\"line\">        default_fun();</div><div class=\"line\">    &#125;</div><div class=\"line\">&#125;;</div></pre></td></tr></table></figure>\n<p>不过这样导致一个多余的<code>default_fun()</code>函数。如果不想添加额外的函数，我们可以使用上述提到的拥有定义的纯虚函数来实现。</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">class</span> Base &#123;</div><div class=\"line\"><span class=\"keyword\">public</span>:</div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">virtual</span> <span class=\"keyword\">void</span> <span class=\"title\">fun</span><span class=\"params\">()</span> </span>= <span class=\"number\">0</span>;</div><div class=\"line\">&#125;;</div><div class=\"line\"><span class=\"comment\">// 为纯虚函数提供定义</span></div><div class=\"line\"><span class=\"keyword\">void</span> Base::fun() &#123;</div><div class=\"line\">    <span class=\"comment\">// 缺省行为</span></div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\"><span class=\"keyword\">class</span> Derived: <span class=\"keyword\">public</span> Base &#123;</div><div class=\"line\"><span class=\"keyword\">public</span>:</div><div class=\"line\">    <span class=\"comment\">// 显式调用基类的纯虚函数，实现缺省行为</span></div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">virtual</span> <span class=\"keyword\">void</span> <span class=\"title\">fun</span><span class=\"params\">()</span> </span>&#123;Base::fun(); &#125;</div><div class=\"line\">&#125;;</div><div class=\"line\"><span class=\"keyword\">class</span> Derived2: <span class=\"keyword\">public</span> Base &#123;</div><div class=\"line\"><span class=\"keyword\">public</span>:</div><div class=\"line\">    <span class=\"comment\">// 实现自定义的行为</span></div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">virtual</span> <span class=\"keyword\">void</span> <span class=\"title\">fun</span><span class=\"params\">()</span> </span>&#123;</div><div class=\"line\">        <span class=\"comment\">// ...</span></div><div class=\"line\">    &#125;</div><div class=\"line\">&#125;;</div></pre></td></tr></table></figure>\n<ul>\n<li>非虚函数<br>这意味着你不应该在derived class中定义不同的行为（老老实实用我给你的！），使得其继承了一份接口和强制实现。</li>\n</ul>\n<h2 id=\"35-考虑virtual函数之外的其他选择\"><a href=\"#35-考虑virtual函数之外的其他选择\" class=\"headerlink\" title=\"35 考虑virtual函数之外的其他选择\"></a>35 考虑<code>virtual</code>函数之外的其他选择</h2><p>虚函数使得多态成为可能。不过在一些情况下，为了实现多态，不一定非要使用虚函数。本条款介绍了一些相关技术。</p>\n<p>在某游戏中，需要设计一个计算角色剩余血量的函数。下面是一种惯常的设计。<br><figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">class</span> GameCharacter &#123;</div><div class=\"line\"><span class=\"keyword\">public</span>:</div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">virtual</span> <span class=\"keyword\">int</span> <span class=\"title\">healthValue</span><span class=\"params\">()</span> <span class=\"keyword\">const</span></span>;</div><div class=\"line\">&#125;;</div></pre></td></tr></table></figure></p>\n<ul>\n<li>使用non-virtual interface实现template method模式<br>这种流派主张<code>virtual</code>函数应该几乎总是私有的。较好的设计时将<code>healthValue()</code>函数设为非虚函数，并调用虚函数进行实现。这个调用函数中，可以做一些预先准备（互斥锁，日志等），后续可以做一些打扫工作。</li>\n</ul>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">class</span> GameCharacter &#123;</div><div class=\"line\"><span class=\"keyword\">public</span>:</div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">int</span> <span class=\"title\">healthValue</span><span class=\"params\">()</span> <span class=\"keyword\">const</span> </span>&#123;</div><div class=\"line\">        <span class=\"comment\">// ... 前期准备</span></div><div class=\"line\">        <span class=\"keyword\">int</span> ret_val = doHealthValue();</div><div class=\"line\">        <span class=\"comment\">// ... 后续清理</span></div><div class=\"line\">        <span class=\"keyword\">return</span> ret_val</div><div class=\"line\">    &#125;</div><div class=\"line\"><span class=\"keyword\">private</span>:</div><div class=\"line\">    <span class=\"keyword\">virtual</span> <span class=\"keyword\">int</span> doHealthValue() <span class=\"keyword\">const</span> &#123;...&#125;</div><div class=\"line\">&#125;;</div></pre></td></tr></table></figure>\n<p>这样做的好处是基类明确定义了该如何实现求血量这个行为，同时又给了一定的自由，派生类可以重写<code>doHealthValue()</code>函数，针对自身的特点计算血量。</p>\n<ul>\n<li>使用函数指针实现策略模式<br>上述方案实际上是对虚函数的调用进行了一次包装。我们还可以借由函数指针实现策略模式，为不同的派生类甚至不同的对象实例做出不同的实现。</li>\n</ul>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">class</span> GameCharacter;   <span class=\"comment\">// 前置声明</span></div><div class=\"line\"><span class=\"comment\">// 计算血量的缺省方法</span></div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">int</span> <span class=\"title\">defaultHealthValue</span><span class=\"params\">(<span class=\"keyword\">const</span> GameCharacter&amp;)</span></span>;</div><div class=\"line\"></div><div class=\"line\"><span class=\"keyword\">class</span> GameCharacter &#123;</div><div class=\"line\"><span class=\"keyword\">public</span>:</div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">typedef</span> <span class=\"title\">int</span> <span class=\"params\">(*HealthCalcFun)</span> <span class=\"params\">(<span class=\"keyword\">const</span> GameCharacter&amp;)</span></span>;</div><div class=\"line\"></div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">explicit</span> <span class=\"title\">GameCharacter</span><span class=\"params\">(HealthCalcFun f=defaultHealthValue</div><div class=\"line\">        :healthFunc(f)</span></span>&#123;</div><div class=\"line\">        <span class=\"comment\">// ...</span></div><div class=\"line\">    &#125;</div><div class=\"line\"><span class=\"keyword\">private</span>:</div><div class=\"line\">    HealthCalcFun healthFunc;</div><div class=\"line\">&#125;;</div></pre></td></tr></table></figure>\n<p>这样，我们通过在构造时候传入相应的函数指针，就可以实现计算血量的个性化设置。比如两个同样的boss，血量下降方式就可以不一样。<br>或者我们可以在运行时候，通过设定<code>healthFunc</code>，来实现动态血量计算方法的变化。</p>\n<ul>\n<li>借由<code>std::function</code>实现策略模式<br>作为上面的改进，我们可以使用<code>std::function</code>（C++11），这样，不止函数指针可以使用，函数对象等也都可以了。（关于<code>std::function</code>的大致介绍，可以看<a href=\"http://en.cppreference.com/w/cpp/utility/functional/function\">这里</a>）。</li>\n</ul>\n<p>我们只需将上面的<code>typedef</code>改掉即可。不再使用函数指针，而是更加高级更加通用的<code>std::function</code>。</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">typedef</span> <span class=\"built_in\">std</span>::function&lt;<span class=\"keyword\">int</span>(<span class=\"keyword\">const</span> GameCharacter&amp;)&gt; HealthCalcFun;</div></pre></td></tr></table></figure>\n<ul>\n<li>使用古典的策略模式<br>如下图所示。对于血量计算，我们单独抻出来一个基类，并有不同的实现。<code>GameCharacter</code>类中则含有一个指向<code>HealthCalcFun</code>类实例的指针。</li>\n</ul>\n<p><img src=\"/img/effectivecpp_strategy_pattern.png\" alt=\"使用UML表示的策略模式\"></p>\n<figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div></pre></td><td class=\"code\"><pre><div class=\"line\">//我们首先定义HealthCalcFunc基类</div><div class=\"line\">class GameCharacter;    // 前向声明</div><div class=\"line\">class HealthCalcFunc &#123;</div><div class=\"line\">public:</div><div class=\"line\">    virtual int calc(const GameCharacter&amp; gc) const &#123;...&#125;</div><div class=\"line\">&#125;;</div><div class=\"line\"></div><div class=\"line\">HealthCalcFunc defaultCalcFunc;</div><div class=\"line\"></div><div class=\"line\">class GameCharacter &#123;</div><div class=\"line\">private:</div><div class=\"line\">    HealthCalcFunc* pfun;</div><div class=\"line\">public:</div><div class=\"line\">    explicit GameCharacter(HealthCalcFunc* p=&amp;defaultCalcFunc):</div><div class=\"line\">        pfun(p) &#123;&#125;</div><div class=\"line\">    int healthValue() const &#123;</div><div class=\"line\">        return pfun-&gt;calc(*this);</div><div class=\"line\">    &#125;</div><div class=\"line\">&#125;;</div></pre></td></tr></table></figure>\n<p>该条款给出了虚函数的若干替代方案。</p>\n<h2 id=\"36-绝不重新定义继承而来的非虚函数\"><a href=\"#36-绝不重新定义继承而来的非虚函数\" class=\"headerlink\" title=\"36 绝不重新定义继承而来的非虚函数\"></a>36 绝不重新定义继承而来的非虚函数</h2><p>在条款34中已经指出，非虚函数是一种实现继承的约定。派生类不应该重新定义非虚函数。这破坏了约定。</p>\n<p>如下所示。<br><figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">class</span> B &#123;</div><div class=\"line\"><span class=\"keyword\">public</span>:</div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">mf</span><span class=\"params\">()</span> </span>&#123;...&#125;</div><div class=\"line\">&#125;;</div><div class=\"line\"><span class=\"keyword\">class</span> D: <span class=\"keyword\">public</span> B &#123;</div><div class=\"line\"><span class=\"keyword\">public</span>:</div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">mf</span><span class=\"params\">()</span> </span>&#123;...&#125;</div><div class=\"line\">&#125;;</div><div class=\"line\"></div><div class=\"line\">D d;</div><div class=\"line\">B* pb = &amp;d;</div><div class=\"line\">D* pd = &amp;d;</div><div class=\"line\"></div><div class=\"line\">pb-&gt;mf();   <span class=\"comment\">// 调用的是B::mf()</span></div><div class=\"line\">pd-&gt;mf();   <span class=\"comment\">// 调用的是D::mf()</span></div></pre></td></tr></table></figure></p>\n<p>这是因为非虚函数的绑定是编译期行为（和虚函数的动态绑定相对，其发生在运行时）。由于<code>pb</code>被声明为一个指向<code>B</code>的指针，所以其调用的是<code>B</code>的成员函数<code>mf()</code>。</p>\n<p>为了不至于让自己陷入精神分裂与背信弃义的境地，请不要重新定义继承而来的非虚函数。</p>\n<h2 id=\"37-绝不重新定义继承而来的缺省参数值\"><a href=\"#37-绝不重新定义继承而来的缺省参数值\" class=\"headerlink\" title=\"37 绝不重新定义继承而来的缺省参数值\"></a>37 绝不重新定义继承而来的缺省参数值</h2><p>由于条款36的分析，所以我们只讨论继承而来的是带有缺省参数的虚函数。这样一来，本条款背后的逻辑就很清晰了：因为缺省参数同样是静态绑定的，而虚函数却是动态绑定。让我们再解释一下。</p>\n<p>静态类型是指在程序中被声明时的类型（不论其真实指向是什么）。<br><figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\">// Circle是Shape的派生类</span></div><div class=\"line\">Shape* ps;</div><div class=\"line\">Shape* pc = <span class=\"keyword\">new</span> Circle;   <span class=\"comment\">// 静态类型都是Shape</span></div></pre></td></tr></table></figure></p>\n<p>动态类型是指当前所指对象的类型。就上例来说，<code>pc</code>的动态类型是<code>Circle*</code>，而<code>ps</code>没有动态类型，因为它并没有指向任何对象实例。动态类型常常可以通过赋值改变。<br><figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">ps = <span class=\"keyword\">new</span> Circle;   <span class=\"comment\">// 现在ps的动态类型是Circle*</span></div></pre></td></tr></table></figure></p>\n<p>虚函数是运行时决定的，取决于发出调用的那个对象的动态类型。</p>\n<p>不过遵守此项条款，有时又会造成不便。看下例：</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">class</span> Shape &#123;</div><div class=\"line\"><span class=\"keyword\">public</span>:</div><div class=\"line\">    <span class=\"keyword\">enum</span> ShapeColor &#123;RED, GREEN&#125;;</div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">virtual</span> <span class=\"keyword\">void</span> <span class=\"title\">draw</span><span class=\"params\">(ShapeColor c=RED)</span> <span class=\"keyword\">const</span></span>=<span class=\"number\">0</span>;</div><div class=\"line\">&#125;;</div><div class=\"line\"></div><div class=\"line\"><span class=\"keyword\">class</span> Circle: <span class=\"keyword\">public</span> Shape &#123;</div><div class=\"line\"><span class=\"keyword\">public</span>:</div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">virtual</span> <span class=\"keyword\">void</span> <span class=\"title\">draw</span><span class=\"params\">(ShapeColor c=RED)</span> <span class=\"keyword\">const</span></span>;</div><div class=\"line\">&#125;;</div></pre></td></tr></table></figure>\n<p>第一个问题，代码重复，我写了两遍缺省参数。第二造成了代码依存。比如我想换成<code>GREEN</code>为默认参数，需要在基类和派生类中同时修改。</p>\n<p>一种解决方法是采用条款35中的替代设计，如NVI方法。令基类中的一个public的非虚函数调用私有的虚函数，而后者可以被派生类重新定义。我们只需要在public的非虚函数中定义缺省参数即可。</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">class</span> Shape &#123;</div><div class=\"line\"><span class=\"keyword\">public</span>:</div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">draw</span><span class=\"params\">(ShapeColor c=RED)</span> <span class=\"keyword\">const</span> </span>&#123;</div><div class=\"line\">        doDraw(c);  <span class=\"comment\">// 调用私有的虚函数</span></div><div class=\"line\">    &#125;</div><div class=\"line\"><span class=\"keyword\">private</span>:</div><div class=\"line\">    <span class=\"comment\">//真正的工作在此完成</span></div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">virtual</span> <span class=\"keyword\">void</span> <span class=\"title\">doDraw</span><span class=\"params\">(ShapeColor c)</span> <span class=\"keyword\">const</span> </span>= <span class=\"number\">0</span>;  </div><div class=\"line\">&#125;;</div><div class=\"line\"></div><div class=\"line\"><span class=\"keyword\">class</span> Circle: <span class=\"keyword\">public</span> Shape &#123;</div><div class=\"line\"><span class=\"keyword\">private</span>:</div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">virtual</span> <span class=\"keyword\">void</span> <span class=\"title\">doDraw</span><span class=\"params\">(ShapeColor c)</span> <span class=\"keyword\">const</span></span>;  <span class=\"comment\">// 派生类重写这个真正的实现</span></div><div class=\"line\">&#125;;</div></pre></td></tr></table></figure>\n<h2 id=\"38-通过复合塑模has-a或“根据某物实现出”\"><a href=\"#38-通过复合塑模has-a或“根据某物实现出”\" class=\"headerlink\" title=\"38 通过复合塑模has-a或“根据某物实现出”\"></a>38 通过复合塑模has-a或“根据某物实现出”</h2><p>复合是指某种对象内含其他对象。复合实际有两层意义，一种较好理解，即has-a，如人有名字、性别等他类，一种是指根据某物实现（is-implemented-in-terms-of）。例如实现消息管理的某个类中含有队列作为实现。</p>\n<h2 id=\"39-明智而审慎地使用private继承\"><a href=\"#39-明智而审慎地使用private继承\" class=\"headerlink\" title=\"39 明智而审慎地使用private继承\"></a>39 明智而审慎地使用<code>private</code>继承</h2><p>私有继承意味着条款38中的“根据某物实现出”。例如<code>D</code>私有继承自<code>B</code>，不是说<code>D</code>是某种<code>B</code>，私有继承完全是一种技术上的实现（和对现实的抽象没有半毛钱关系）。<code>B</code>的每样东西在<code>D</code>中都是不可见的，也就是成了黑箱，因为它们本身就是实现细节，你只是考虑用<code>B</code>来实现<code>D</code>的功能而已。</p>\n<p>但是复合也能达到相同的效果啊~我在<code>D</code>中加入一个<code>B</code>的对象实例不就好了？很多情况下的确是这样，如果没有必要，不建议使用私有继承。</p>\n<h2 id=\"40-明智而审慎地使用多重继承\"><a href=\"#40-明智而审慎地使用多重继承\" class=\"headerlink\" title=\"40 明智而审慎地使用多重继承\"></a>40 明智而审慎地使用多重继承</h2><p>使用多重继承有可能造成歧义。例如，<code>C</code>继承自<code>A</code>和<code>B</code>，而两个基类中都含有成员函数<code>mf()</code>。那么当<code>d.mf()</code>的时候，究竟是在调用哪个呢？你必须明确地指出,<code>d.A::mf()</code>。</p>\n<p>使用多重继承还可能会造成“钻石型”继承。任何时候继承体系中某个基类和派生类之间有一条以上的相通路线，就面临一个问题，是否要让基类中的每个成员变量经由每一条路线被复制？如果只想保留一份，那么需要将<code>File</code>定为虚基类，所有直接继承自它的类采用虚继承。<br><img src=\"/img/effectivecpp_diamond.png\" alt=\"钻石型继承\"></p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">class</span> File &#123;...&#125;;</div><div class=\"line\"><span class=\"keyword\">class</span> InputFile: <span class=\"keyword\">virtual</span> <span class=\"keyword\">public</span> File &#123;...&#125;;</div><div class=\"line\"><span class=\"keyword\">class</span> OutputFile: <span class=\"keyword\">virtual</span> <span class=\"keyword\">public</span> File &#123;...&#125;;</div><div class=\"line\"><span class=\"keyword\">class</span> IOFile: <span class=\"keyword\">public</span> InputFile, <span class=\"keyword\">public</span> OutputFile &#123;...&#125;;</div></pre></td></tr></table></figure>\n<p>从正确的角度看，public的继承总应该是virtual的。不过这样会造成代码体积的膨胀和执行效率的下降。</p>\n<p>所以，如无必要，不要使用虚继承。即使使用，尽可能避免在其中放置数据（类似Java或C#中的接口Interface）</p>\n<h2 id=\"附注-std-function的基本使用\"><a href=\"#附注-std-function的基本使用\" class=\"headerlink\" title=\"附注 std::function的基本使用\"></a>附注 <code>std::function</code>的基本使用</h2><p><code>std::function</code>的作用类似于函数指针，但是能力更加强大。我们可以将函数指针，函数对象，lambda表达式或者类中的成员函数作为<code>std::function</code>。<br>如下所示：</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div><div class=\"line\">47</div><div class=\"line\">48</div><div class=\"line\">49</div><div class=\"line\">50</div><div class=\"line\">51</div><div class=\"line\">52</div><div class=\"line\">53</div><div class=\"line\">54</div><div class=\"line\">55</div><div class=\"line\">56</div><div class=\"line\">57</div><div class=\"line\">58</div><div class=\"line\">59</div><div class=\"line\">60</div><div class=\"line\">61</div><div class=\"line\">62</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">include</span> <span class=\"meta-string\">&lt;functional&gt;</span></span></div><div class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">include</span> <span class=\"meta-string\">&lt;iostream&gt;</span></span></div><div class=\"line\"></div><div class=\"line\"><span class=\"keyword\">struct</span> Foo &#123;</div><div class=\"line\">    Foo(<span class=\"keyword\">int</span> num) : num_(num) &#123;&#125;</div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">print_add</span><span class=\"params\">(<span class=\"keyword\">int</span> i)</span> <span class=\"keyword\">const</span> </span>&#123; <span class=\"built_in\">std</span>::<span class=\"built_in\">cout</span> &lt;&lt; num_+i &lt;&lt; <span class=\"string\">'\\n'</span>; &#125;</div><div class=\"line\">    <span class=\"keyword\">int</span> num_;</div><div class=\"line\">&#125;;</div><div class=\"line\"></div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">print_num</span><span class=\"params\">(<span class=\"keyword\">int</span> i)</span></div><div class=\"line\"></span>&#123;</div><div class=\"line\">    <span class=\"built_in\">std</span>::<span class=\"built_in\">cout</span> &lt;&lt; i &lt;&lt; <span class=\"string\">'\\n'</span>;</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\"><span class=\"keyword\">struct</span> PrintNum &#123;</div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">operator</span><span class=\"params\">()</span><span class=\"params\">(<span class=\"keyword\">int</span> i)</span> <span class=\"keyword\">const</span></div><div class=\"line\">    </span>&#123;</div><div class=\"line\">        <span class=\"built_in\">std</span>::<span class=\"built_in\">cout</span> &lt;&lt; i &lt;&lt; <span class=\"string\">'\\n'</span>;</div><div class=\"line\">    &#125;</div><div class=\"line\">&#125;;</div><div class=\"line\"></div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">int</span> <span class=\"title\">main</span><span class=\"params\">()</span></div><div class=\"line\"></span>&#123;</div><div class=\"line\">    <span class=\"comment\">// store a free function</span></div><div class=\"line\">    <span class=\"comment\">// 函数指针</span></div><div class=\"line\">    <span class=\"built_in\">std</span>::function&lt;<span class=\"keyword\">void</span>(<span class=\"keyword\">int</span>)&gt; f_display = print_num;</div><div class=\"line\">    f_display(<span class=\"number\">-9</span>);</div><div class=\"line\"></div><div class=\"line\">    <span class=\"comment\">// lambda表达式</span></div><div class=\"line\">    <span class=\"comment\">// store a lambda</span></div><div class=\"line\">    <span class=\"built_in\">std</span>::function&lt;<span class=\"keyword\">void</span>()&gt; f_display_42 = []() &#123; print_num(<span class=\"number\">42</span>); &#125;;</div><div class=\"line\">    f_display_42();</div><div class=\"line\"></div><div class=\"line\">    <span class=\"comment\">// store the result of a call to std::bind</span></div><div class=\"line\">    <span class=\"comment\">// 绑定之后的函数对象</span></div><div class=\"line\">    <span class=\"built_in\">std</span>::function&lt;<span class=\"keyword\">void</span>()&gt; f_display_31337 = <span class=\"built_in\">std</span>::bind(print_num, <span class=\"number\">31337</span>);</div><div class=\"line\">    f_display_31337();</div><div class=\"line\"></div><div class=\"line\">    <span class=\"comment\">// store a call to a member function</span></div><div class=\"line\">    <span class=\"comment\">// 类中的成员函数，第一个参数为类实例的const reference</span></div><div class=\"line\">    <span class=\"built_in\">std</span>::function&lt;<span class=\"keyword\">void</span>(<span class=\"keyword\">const</span> Foo&amp;, <span class=\"keyword\">int</span>)&gt; f_add_display = &amp;Foo::print_add;</div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">const</span> Foo <span class=\"title\">foo</span><span class=\"params\">(<span class=\"number\">314159</span>)</span></span>;</div><div class=\"line\">    f_add_display(foo, <span class=\"number\">1</span>);</div><div class=\"line\">    f_add_display(<span class=\"number\">314159</span>, <span class=\"number\">1</span>);</div><div class=\"line\"></div><div class=\"line\">    <span class=\"comment\">// store a call to a data member accessor</span></div><div class=\"line\">    <span class=\"built_in\">std</span>::function&lt;<span class=\"keyword\">int</span>(Foo <span class=\"keyword\">const</span>&amp;)&gt; f_num = &amp;Foo::num_;</div><div class=\"line\">    <span class=\"built_in\">std</span>::<span class=\"built_in\">cout</span> &lt;&lt; <span class=\"string\">\"num_: \"</span> &lt;&lt; f_num(foo) &lt;&lt; <span class=\"string\">'\\n'</span>;</div><div class=\"line\"></div><div class=\"line\">    <span class=\"comment\">// store a call to a member function and object</span></div><div class=\"line\">    <span class=\"keyword\">using</span> <span class=\"built_in\">std</span>::placeholders::_1;</div><div class=\"line\">    <span class=\"built_in\">std</span>::function&lt;<span class=\"keyword\">void</span>(<span class=\"keyword\">int</span>)&gt; f_add_display2 = <span class=\"built_in\">std</span>::bind( &amp;Foo::print_add, foo, _1 );</div><div class=\"line\">    f_add_display2(<span class=\"number\">2</span>);</div><div class=\"line\"></div><div class=\"line\">    <span class=\"comment\">// store a call to a member function and object ptr</span></div><div class=\"line\">    <span class=\"built_in\">std</span>::function&lt;<span class=\"keyword\">void</span>(<span class=\"keyword\">int</span>)&gt; f_add_display3 = <span class=\"built_in\">std</span>::bind( &amp;Foo::print_add, &amp;foo, _1 );</div><div class=\"line\">    f_add_display3(<span class=\"number\">3</span>);</div><div class=\"line\"></div><div class=\"line\">    <span class=\"comment\">// store a call to a function object</span></div><div class=\"line\">    <span class=\"built_in\">std</span>::function&lt;<span class=\"keyword\">void</span>(<span class=\"keyword\">int</span>)&gt; f_display_obj = PrintNum();</div><div class=\"line\">    f_display_obj(<span class=\"number\">18</span>);</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>"},{"title":"Effective CPP 阅读 - Chapter 2 构造拷贝和赋值函数","date":"2017-04-24T12:50:10.000Z","_content":"在第二章中，作者主要关注了在C++的OOP“联邦”中行事的注意事项。主要包括有虚函数的情况下的继承以及copying function（拷贝构造函数和拷贝赋值函数）的处理。\n<!-- more-->\n\n## 05 了解C++默默编写并调用了哪些函数\n\nC++编译器会自动为类添加默认构造函数和拷贝构造函数和析构函数，以及拷贝赋值函数。\n\n在默认构造函数中，会调用基类的构造函数以及各个成员变量的构造函数。\n\n在拷贝构造函数和拷贝赋值函数中，将单纯地将源对象中的non-static的成员变量拷贝到目标对象（浅拷贝）。\n\n## 06 若不想使用编译器自动生成的函数，那就明确拒绝\n\n有的时候，我们的类故意设计为不能拷贝构造或赋值。这时候，可以将拷贝构造函数和拷贝赋值函数声明为`private`，并且不提供函数的实现。\n\n## 07 为多态基类声明虚析构函数\n\n多态是OOP的基本概念之一。在C++中，可能遇到这样的情景：使用base class的指针或引用指向derived派生类对象，以此实现运行时的不同逻辑。这种情况下，要为基类声明虚析构函数。这是因为，当派生类对象经由一个base class指针被删除时，如果该base class带有非虚的析构函数，其结果是未定义的。常常会造成派生类自己的成员变量不能被销毁，造成内存泄露。\n\n而那些意图并非是用来当做base class的类来说，随意将其虚构函数声明为`virtual`也是不恰当的。因为这会额外引入虚函数表，造成对象体积的无谓增大，给性能造成影响，而且丧失了对C语言的移植性。\n\n由于那些被用来作为base class等待派生类继承的类通常情况下都有虚函数存在（派生类正是对虚函数重写，实现了多态），所以这一条款可以归纳如下：\n\n> 那些有虚函数的类，几乎确定都应该有一个虚析构函数。\n\n对于STL而言，记住其中的容器都不是为了继承而设计的，不要继承STL中的容器，包括`vector`，`string`等。\n\n有的时候，我们可能会想声明一个抽象基类作为接口。当手上没有纯虚函数时候，可以将析构函数声明为纯虚的。然而这时候问题来了，我们需要为这个纯虚析构函数提供了一个空定义。\n\n``` cpp\nclass ABC {\npublic:\n    virtual ~ABC() = 0;    \n};\nABC::~ABC() {}\n```\n\n这看上去很违背常理，因为一般情况下纯虚函数不需要实现。这里是因为当派生类被销毁时，其析构函数中会调用`~ABC()`，所以必须为这个函数提供一份定义。\n\n## 08 别让异常逃离析构函数\n如果在析构函数中抛出异常，会导致未定义的行为。\n\n析构函数不应该抛出异常。如果析构函数调用的函数可能会抛出异常，要在析构函数中捕获异常，然后结束程序（这不是未定义行为）或者吞掉它们。\n\n## 09 绝不在构造函数和析构函数中调用虚函数\n你不应该在构造函数和析构函数中调用虚函数，这样的调用通常不会导致你想要的结果。\n\n为什么呢？\n\n如果我们在派生类的构造函数中使用了从基类中继承而来的虚函数。在派生类的构造函数之前，基类的构造函数被调用，这时候，所调用的虚函数实际是基类中的那个版本！析构函数同理。\n\n直接在构造函数中调用虚函数看上去很容易避免。然而在构造函数中你可能会调用其他的初始化函数，你应该确保这些初始化函数中没有调用虚函数。否则，你可能会陷入到苦涩头疼的调试中去。编译器通常不会发现此类问题，但是在程序运行中，如果基类的虚函数是纯虚函数，程序很可能中止（和后面的相比，也许这还算好的）。如果基类中的虚函数有自己的实现，那么你可能就会头疼于程序的表现为何出乎意料（期望调用派生类的重写版本，实际仍是基类的原始版本）。\n\n## 10 令`operator=`返回一个`*this`的引用\n为了实现连锁赋值，赋值操作符必须返回一个引用，指向操作符左侧的赋值实参。这条规范被大多数人遵守。除非有确实好的理由，否则最好按规范办事。\n\n所谓连锁赋值，是指下面这样的情况：\n\n``` cpp\na = b = c;\n```\n\n这一条款不仅适用于赋值运算符，也适用于`+=`等。\n\n``` cpp\nclass A {\npublic:\n    A& operator=(const A& rhs) {\n        // ...\n        return *this;\n    }\n};\n```\n\n## 11 在`operator=`中处理自我赋值\n所谓自我赋值，是指：\n\n``` cpp\nWidget w;\n// ...\nw = w;\n```\n\n自我赋值常常发生在同一个对象的不同别名之间。在实现赋值运算符时，应注意处理这一现象。\n\n一种方法是进行“证同测试”，如下：\n\n``` cpp\nA& operator=(const A& rhs) {\n    if(&rhs == this) return *this; //证同测试\n    // ...\n}\n```\n不过作者指出，这种方法不具备异常安全性。另一种方法是在函数内部，合理安排语句顺序，防止提前释放该对象本身的资源。\n\n还有一种方法，使用copy-swap方法，首先拷贝构造一个`rhs`的拷贝，然后交换该拷贝和`*this`，\n\n毫无疑问，使用证同测试方法和作者后文的方法都会造成性能的些许下降，这需要根据具体情况具体分析，合理采用。\n\n## 12 复制对象时勿忘每一个成分\n这一条款是指存在继承时，实现copying函数（指拷贝构造函数和拷贝赋值函数）不要忘记base class部分成员。看下面的例子：\n\n``` cpp\nclass Derived: public Base{\nprivate:\n    int a;\npublic:\n    Derived(const Derived& rhs):a(rhs.a) {}\n    Derived& operator=(const Derived& rhs) {\n        a = rhs.a;\n        return *this;\n    }\n};\n```\n\n上面的代码只是拷贝了派生类新加入的成员`a`，而对基类中已有的成员未作处理。要记住，\n>任何时候自己实现copying函数时，要担起重责大任，小心地复制其基类的成员。由于基类成员往往声明为`private`，所以，一般调用基类的成员函数进行拷贝。将上面的代码修改为：\n\n``` cpp\nDerived(const Derived& rhs):Base(rhs), a(rhs.a) {}\nDerived& operator=(const Derived& rhs) {\n    Base::operator=(rhs);\n    a = rhs.a;\n    return *this;\n}\n```\n\n此外，两种copying函数的实现往往是相似的。然而，不要试图在一个函数中调用另一个函数。把相似代码提取出来，写成一个独立的`init()`函数是一个更好的选择。\n","source":"_posts/effective-cpp-02.md","raw":"---\ntitle: Effective CPP 阅读 - Chapter 2 构造拷贝和赋值函数\ndate: 2017-04-24 20:50:10\ntags:\n     - cpp\n---\n在第二章中，作者主要关注了在C++的OOP“联邦”中行事的注意事项。主要包括有虚函数的情况下的继承以及copying function（拷贝构造函数和拷贝赋值函数）的处理。\n<!-- more-->\n\n## 05 了解C++默默编写并调用了哪些函数\n\nC++编译器会自动为类添加默认构造函数和拷贝构造函数和析构函数，以及拷贝赋值函数。\n\n在默认构造函数中，会调用基类的构造函数以及各个成员变量的构造函数。\n\n在拷贝构造函数和拷贝赋值函数中，将单纯地将源对象中的non-static的成员变量拷贝到目标对象（浅拷贝）。\n\n## 06 若不想使用编译器自动生成的函数，那就明确拒绝\n\n有的时候，我们的类故意设计为不能拷贝构造或赋值。这时候，可以将拷贝构造函数和拷贝赋值函数声明为`private`，并且不提供函数的实现。\n\n## 07 为多态基类声明虚析构函数\n\n多态是OOP的基本概念之一。在C++中，可能遇到这样的情景：使用base class的指针或引用指向derived派生类对象，以此实现运行时的不同逻辑。这种情况下，要为基类声明虚析构函数。这是因为，当派生类对象经由一个base class指针被删除时，如果该base class带有非虚的析构函数，其结果是未定义的。常常会造成派生类自己的成员变量不能被销毁，造成内存泄露。\n\n而那些意图并非是用来当做base class的类来说，随意将其虚构函数声明为`virtual`也是不恰当的。因为这会额外引入虚函数表，造成对象体积的无谓增大，给性能造成影响，而且丧失了对C语言的移植性。\n\n由于那些被用来作为base class等待派生类继承的类通常情况下都有虚函数存在（派生类正是对虚函数重写，实现了多态），所以这一条款可以归纳如下：\n\n> 那些有虚函数的类，几乎确定都应该有一个虚析构函数。\n\n对于STL而言，记住其中的容器都不是为了继承而设计的，不要继承STL中的容器，包括`vector`，`string`等。\n\n有的时候，我们可能会想声明一个抽象基类作为接口。当手上没有纯虚函数时候，可以将析构函数声明为纯虚的。然而这时候问题来了，我们需要为这个纯虚析构函数提供了一个空定义。\n\n``` cpp\nclass ABC {\npublic:\n    virtual ~ABC() = 0;    \n};\nABC::~ABC() {}\n```\n\n这看上去很违背常理，因为一般情况下纯虚函数不需要实现。这里是因为当派生类被销毁时，其析构函数中会调用`~ABC()`，所以必须为这个函数提供一份定义。\n\n## 08 别让异常逃离析构函数\n如果在析构函数中抛出异常，会导致未定义的行为。\n\n析构函数不应该抛出异常。如果析构函数调用的函数可能会抛出异常，要在析构函数中捕获异常，然后结束程序（这不是未定义行为）或者吞掉它们。\n\n## 09 绝不在构造函数和析构函数中调用虚函数\n你不应该在构造函数和析构函数中调用虚函数，这样的调用通常不会导致你想要的结果。\n\n为什么呢？\n\n如果我们在派生类的构造函数中使用了从基类中继承而来的虚函数。在派生类的构造函数之前，基类的构造函数被调用，这时候，所调用的虚函数实际是基类中的那个版本！析构函数同理。\n\n直接在构造函数中调用虚函数看上去很容易避免。然而在构造函数中你可能会调用其他的初始化函数，你应该确保这些初始化函数中没有调用虚函数。否则，你可能会陷入到苦涩头疼的调试中去。编译器通常不会发现此类问题，但是在程序运行中，如果基类的虚函数是纯虚函数，程序很可能中止（和后面的相比，也许这还算好的）。如果基类中的虚函数有自己的实现，那么你可能就会头疼于程序的表现为何出乎意料（期望调用派生类的重写版本，实际仍是基类的原始版本）。\n\n## 10 令`operator=`返回一个`*this`的引用\n为了实现连锁赋值，赋值操作符必须返回一个引用，指向操作符左侧的赋值实参。这条规范被大多数人遵守。除非有确实好的理由，否则最好按规范办事。\n\n所谓连锁赋值，是指下面这样的情况：\n\n``` cpp\na = b = c;\n```\n\n这一条款不仅适用于赋值运算符，也适用于`+=`等。\n\n``` cpp\nclass A {\npublic:\n    A& operator=(const A& rhs) {\n        // ...\n        return *this;\n    }\n};\n```\n\n## 11 在`operator=`中处理自我赋值\n所谓自我赋值，是指：\n\n``` cpp\nWidget w;\n// ...\nw = w;\n```\n\n自我赋值常常发生在同一个对象的不同别名之间。在实现赋值运算符时，应注意处理这一现象。\n\n一种方法是进行“证同测试”，如下：\n\n``` cpp\nA& operator=(const A& rhs) {\n    if(&rhs == this) return *this; //证同测试\n    // ...\n}\n```\n不过作者指出，这种方法不具备异常安全性。另一种方法是在函数内部，合理安排语句顺序，防止提前释放该对象本身的资源。\n\n还有一种方法，使用copy-swap方法，首先拷贝构造一个`rhs`的拷贝，然后交换该拷贝和`*this`，\n\n毫无疑问，使用证同测试方法和作者后文的方法都会造成性能的些许下降，这需要根据具体情况具体分析，合理采用。\n\n## 12 复制对象时勿忘每一个成分\n这一条款是指存在继承时，实现copying函数（指拷贝构造函数和拷贝赋值函数）不要忘记base class部分成员。看下面的例子：\n\n``` cpp\nclass Derived: public Base{\nprivate:\n    int a;\npublic:\n    Derived(const Derived& rhs):a(rhs.a) {}\n    Derived& operator=(const Derived& rhs) {\n        a = rhs.a;\n        return *this;\n    }\n};\n```\n\n上面的代码只是拷贝了派生类新加入的成员`a`，而对基类中已有的成员未作处理。要记住，\n>任何时候自己实现copying函数时，要担起重责大任，小心地复制其基类的成员。由于基类成员往往声明为`private`，所以，一般调用基类的成员函数进行拷贝。将上面的代码修改为：\n\n``` cpp\nDerived(const Derived& rhs):Base(rhs), a(rhs.a) {}\nDerived& operator=(const Derived& rhs) {\n    Base::operator=(rhs);\n    a = rhs.a;\n    return *this;\n}\n```\n\n此外，两种copying函数的实现往往是相似的。然而，不要试图在一个函数中调用另一个函数。把相似代码提取出来，写成一个独立的`init()`函数是一个更好的选择。\n","slug":"effective-cpp-02","published":1,"updated":"2018-01-12T06:22:20.464Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjcu6vcsw0011qu4696wxtc7d","content":"<p>在第二章中，作者主要关注了在C++的OOP“联邦”中行事的注意事项。主要包括有虚函数的情况下的继承以及copying function（拷贝构造函数和拷贝赋值函数）的处理。<br><a id=\"more\"></a></p>\n<h2 id=\"05-了解C-默默编写并调用了哪些函数\"><a href=\"#05-了解C-默默编写并调用了哪些函数\" class=\"headerlink\" title=\"05 了解C++默默编写并调用了哪些函数\"></a>05 了解C++默默编写并调用了哪些函数</h2><p>C++编译器会自动为类添加默认构造函数和拷贝构造函数和析构函数，以及拷贝赋值函数。</p>\n<p>在默认构造函数中，会调用基类的构造函数以及各个成员变量的构造函数。</p>\n<p>在拷贝构造函数和拷贝赋值函数中，将单纯地将源对象中的non-static的成员变量拷贝到目标对象（浅拷贝）。</p>\n<h2 id=\"06-若不想使用编译器自动生成的函数，那就明确拒绝\"><a href=\"#06-若不想使用编译器自动生成的函数，那就明确拒绝\" class=\"headerlink\" title=\"06 若不想使用编译器自动生成的函数，那就明确拒绝\"></a>06 若不想使用编译器自动生成的函数，那就明确拒绝</h2><p>有的时候，我们的类故意设计为不能拷贝构造或赋值。这时候，可以将拷贝构造函数和拷贝赋值函数声明为<code>private</code>，并且不提供函数的实现。</p>\n<h2 id=\"07-为多态基类声明虚析构函数\"><a href=\"#07-为多态基类声明虚析构函数\" class=\"headerlink\" title=\"07 为多态基类声明虚析构函数\"></a>07 为多态基类声明虚析构函数</h2><p>多态是OOP的基本概念之一。在C++中，可能遇到这样的情景：使用base class的指针或引用指向derived派生类对象，以此实现运行时的不同逻辑。这种情况下，要为基类声明虚析构函数。这是因为，当派生类对象经由一个base class指针被删除时，如果该base class带有非虚的析构函数，其结果是未定义的。常常会造成派生类自己的成员变量不能被销毁，造成内存泄露。</p>\n<p>而那些意图并非是用来当做base class的类来说，随意将其虚构函数声明为<code>virtual</code>也是不恰当的。因为这会额外引入虚函数表，造成对象体积的无谓增大，给性能造成影响，而且丧失了对C语言的移植性。</p>\n<p>由于那些被用来作为base class等待派生类继承的类通常情况下都有虚函数存在（派生类正是对虚函数重写，实现了多态），所以这一条款可以归纳如下：</p>\n<blockquote>\n<p>那些有虚函数的类，几乎确定都应该有一个虚析构函数。</p>\n</blockquote>\n<p>对于STL而言，记住其中的容器都不是为了继承而设计的，不要继承STL中的容器，包括<code>vector</code>，<code>string</code>等。</p>\n<p>有的时候，我们可能会想声明一个抽象基类作为接口。当手上没有纯虚函数时候，可以将析构函数声明为纯虚的。然而这时候问题来了，我们需要为这个纯虚析构函数提供了一个空定义。</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">class</span> ABC &#123;</div><div class=\"line\"><span class=\"keyword\">public</span>:</div><div class=\"line\">    <span class=\"keyword\">virtual</span> ~ABC() = <span class=\"number\">0</span>;    </div><div class=\"line\">&#125;;</div><div class=\"line\">ABC::~ABC() &#123;&#125;</div></pre></td></tr></table></figure>\n<p>这看上去很违背常理，因为一般情况下纯虚函数不需要实现。这里是因为当派生类被销毁时，其析构函数中会调用<code>~ABC()</code>，所以必须为这个函数提供一份定义。</p>\n<h2 id=\"08-别让异常逃离析构函数\"><a href=\"#08-别让异常逃离析构函数\" class=\"headerlink\" title=\"08 别让异常逃离析构函数\"></a>08 别让异常逃离析构函数</h2><p>如果在析构函数中抛出异常，会导致未定义的行为。</p>\n<p>析构函数不应该抛出异常。如果析构函数调用的函数可能会抛出异常，要在析构函数中捕获异常，然后结束程序（这不是未定义行为）或者吞掉它们。</p>\n<h2 id=\"09-绝不在构造函数和析构函数中调用虚函数\"><a href=\"#09-绝不在构造函数和析构函数中调用虚函数\" class=\"headerlink\" title=\"09 绝不在构造函数和析构函数中调用虚函数\"></a>09 绝不在构造函数和析构函数中调用虚函数</h2><p>你不应该在构造函数和析构函数中调用虚函数，这样的调用通常不会导致你想要的结果。</p>\n<p>为什么呢？</p>\n<p>如果我们在派生类的构造函数中使用了从基类中继承而来的虚函数。在派生类的构造函数之前，基类的构造函数被调用，这时候，所调用的虚函数实际是基类中的那个版本！析构函数同理。</p>\n<p>直接在构造函数中调用虚函数看上去很容易避免。然而在构造函数中你可能会调用其他的初始化函数，你应该确保这些初始化函数中没有调用虚函数。否则，你可能会陷入到苦涩头疼的调试中去。编译器通常不会发现此类问题，但是在程序运行中，如果基类的虚函数是纯虚函数，程序很可能中止（和后面的相比，也许这还算好的）。如果基类中的虚函数有自己的实现，那么你可能就会头疼于程序的表现为何出乎意料（期望调用派生类的重写版本，实际仍是基类的原始版本）。</p>\n<h2 id=\"10-令operator-返回一个-this的引用\"><a href=\"#10-令operator-返回一个-this的引用\" class=\"headerlink\" title=\"10 令operator=返回一个*this的引用\"></a>10 令<code>operator=</code>返回一个<code>*this</code>的引用</h2><p>为了实现连锁赋值，赋值操作符必须返回一个引用，指向操作符左侧的赋值实参。这条规范被大多数人遵守。除非有确实好的理由，否则最好按规范办事。</p>\n<p>所谓连锁赋值，是指下面这样的情况：</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">a = b = c;</div></pre></td></tr></table></figure>\n<p>这一条款不仅适用于赋值运算符，也适用于<code>+=</code>等。</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">class</span> A &#123;</div><div class=\"line\"><span class=\"keyword\">public</span>:</div><div class=\"line\">    A&amp; <span class=\"keyword\">operator</span>=(<span class=\"keyword\">const</span> A&amp; rhs) &#123;</div><div class=\"line\">        <span class=\"comment\">// ...</span></div><div class=\"line\">        <span class=\"keyword\">return</span> *<span class=\"keyword\">this</span>;</div><div class=\"line\">    &#125;</div><div class=\"line\">&#125;;</div></pre></td></tr></table></figure>\n<h2 id=\"11-在operator-中处理自我赋值\"><a href=\"#11-在operator-中处理自我赋值\" class=\"headerlink\" title=\"11 在operator=中处理自我赋值\"></a>11 在<code>operator=</code>中处理自我赋值</h2><p>所谓自我赋值，是指：</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\">Widget w;</div><div class=\"line\"><span class=\"comment\">// ...</span></div><div class=\"line\">w = w;</div></pre></td></tr></table></figure>\n<p>自我赋值常常发生在同一个对象的不同别名之间。在实现赋值运算符时，应注意处理这一现象。</p>\n<p>一种方法是进行“证同测试”，如下：</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\">A&amp; <span class=\"keyword\">operator</span>=(<span class=\"keyword\">const</span> A&amp; rhs) &#123;</div><div class=\"line\">    <span class=\"keyword\">if</span>(&amp;rhs == <span class=\"keyword\">this</span>) <span class=\"keyword\">return</span> *<span class=\"keyword\">this</span>; <span class=\"comment\">//证同测试</span></div><div class=\"line\">    <span class=\"comment\">// ...</span></div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<p>不过作者指出，这种方法不具备异常安全性。另一种方法是在函数内部，合理安排语句顺序，防止提前释放该对象本身的资源。</p>\n<p>还有一种方法，使用copy-swap方法，首先拷贝构造一个<code>rhs</code>的拷贝，然后交换该拷贝和<code>*this</code>，</p>\n<p>毫无疑问，使用证同测试方法和作者后文的方法都会造成性能的些许下降，这需要根据具体情况具体分析，合理采用。</p>\n<h2 id=\"12-复制对象时勿忘每一个成分\"><a href=\"#12-复制对象时勿忘每一个成分\" class=\"headerlink\" title=\"12 复制对象时勿忘每一个成分\"></a>12 复制对象时勿忘每一个成分</h2><p>这一条款是指存在继承时，实现copying函数（指拷贝构造函数和拷贝赋值函数）不要忘记base class部分成员。看下面的例子：</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">class</span> Derived: <span class=\"keyword\">public</span> Base&#123;</div><div class=\"line\"><span class=\"keyword\">private</span>:</div><div class=\"line\">    <span class=\"keyword\">int</span> a;</div><div class=\"line\"><span class=\"keyword\">public</span>:</div><div class=\"line\">    Derived(<span class=\"keyword\">const</span> Derived&amp; rhs):a(rhs.a) &#123;&#125;</div><div class=\"line\">    Derived&amp; <span class=\"keyword\">operator</span>=(<span class=\"keyword\">const</span> Derived&amp; rhs) &#123;</div><div class=\"line\">        a = rhs.a;</div><div class=\"line\">        <span class=\"keyword\">return</span> *<span class=\"keyword\">this</span>;</div><div class=\"line\">    &#125;</div><div class=\"line\">&#125;;</div></pre></td></tr></table></figure>\n<p>上面的代码只是拷贝了派生类新加入的成员<code>a</code>，而对基类中已有的成员未作处理。要记住，</p>\n<blockquote>\n<p>任何时候自己实现copying函数时，要担起重责大任，小心地复制其基类的成员。由于基类成员往往声明为<code>private</code>，所以，一般调用基类的成员函数进行拷贝。将上面的代码修改为：</p>\n</blockquote>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div></pre></td><td class=\"code\"><pre><div class=\"line\">Derived(<span class=\"keyword\">const</span> Derived&amp; rhs):Base(rhs), a(rhs.a) &#123;&#125;</div><div class=\"line\">Derived&amp; <span class=\"keyword\">operator</span>=(<span class=\"keyword\">const</span> Derived&amp; rhs) &#123;</div><div class=\"line\">    Base::<span class=\"keyword\">operator</span>=(rhs);</div><div class=\"line\">    a = rhs.a;</div><div class=\"line\">    <span class=\"keyword\">return</span> *<span class=\"keyword\">this</span>;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<p>此外，两种copying函数的实现往往是相似的。然而，不要试图在一个函数中调用另一个函数。把相似代码提取出来，写成一个独立的<code>init()</code>函数是一个更好的选择。</p>\n","excerpt":"<p>在第二章中，作者主要关注了在C++的OOP“联邦”中行事的注意事项。主要包括有虚函数的情况下的继承以及copying function（拷贝构造函数和拷贝赋值函数）的处理。<br>","more":"</p>\n<h2 id=\"05-了解C-默默编写并调用了哪些函数\"><a href=\"#05-了解C-默默编写并调用了哪些函数\" class=\"headerlink\" title=\"05 了解C++默默编写并调用了哪些函数\"></a>05 了解C++默默编写并调用了哪些函数</h2><p>C++编译器会自动为类添加默认构造函数和拷贝构造函数和析构函数，以及拷贝赋值函数。</p>\n<p>在默认构造函数中，会调用基类的构造函数以及各个成员变量的构造函数。</p>\n<p>在拷贝构造函数和拷贝赋值函数中，将单纯地将源对象中的non-static的成员变量拷贝到目标对象（浅拷贝）。</p>\n<h2 id=\"06-若不想使用编译器自动生成的函数，那就明确拒绝\"><a href=\"#06-若不想使用编译器自动生成的函数，那就明确拒绝\" class=\"headerlink\" title=\"06 若不想使用编译器自动生成的函数，那就明确拒绝\"></a>06 若不想使用编译器自动生成的函数，那就明确拒绝</h2><p>有的时候，我们的类故意设计为不能拷贝构造或赋值。这时候，可以将拷贝构造函数和拷贝赋值函数声明为<code>private</code>，并且不提供函数的实现。</p>\n<h2 id=\"07-为多态基类声明虚析构函数\"><a href=\"#07-为多态基类声明虚析构函数\" class=\"headerlink\" title=\"07 为多态基类声明虚析构函数\"></a>07 为多态基类声明虚析构函数</h2><p>多态是OOP的基本概念之一。在C++中，可能遇到这样的情景：使用base class的指针或引用指向derived派生类对象，以此实现运行时的不同逻辑。这种情况下，要为基类声明虚析构函数。这是因为，当派生类对象经由一个base class指针被删除时，如果该base class带有非虚的析构函数，其结果是未定义的。常常会造成派生类自己的成员变量不能被销毁，造成内存泄露。</p>\n<p>而那些意图并非是用来当做base class的类来说，随意将其虚构函数声明为<code>virtual</code>也是不恰当的。因为这会额外引入虚函数表，造成对象体积的无谓增大，给性能造成影响，而且丧失了对C语言的移植性。</p>\n<p>由于那些被用来作为base class等待派生类继承的类通常情况下都有虚函数存在（派生类正是对虚函数重写，实现了多态），所以这一条款可以归纳如下：</p>\n<blockquote>\n<p>那些有虚函数的类，几乎确定都应该有一个虚析构函数。</p>\n</blockquote>\n<p>对于STL而言，记住其中的容器都不是为了继承而设计的，不要继承STL中的容器，包括<code>vector</code>，<code>string</code>等。</p>\n<p>有的时候，我们可能会想声明一个抽象基类作为接口。当手上没有纯虚函数时候，可以将析构函数声明为纯虚的。然而这时候问题来了，我们需要为这个纯虚析构函数提供了一个空定义。</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">class</span> ABC &#123;</div><div class=\"line\"><span class=\"keyword\">public</span>:</div><div class=\"line\">    <span class=\"keyword\">virtual</span> ~ABC() = <span class=\"number\">0</span>;    </div><div class=\"line\">&#125;;</div><div class=\"line\">ABC::~ABC() &#123;&#125;</div></pre></td></tr></table></figure>\n<p>这看上去很违背常理，因为一般情况下纯虚函数不需要实现。这里是因为当派生类被销毁时，其析构函数中会调用<code>~ABC()</code>，所以必须为这个函数提供一份定义。</p>\n<h2 id=\"08-别让异常逃离析构函数\"><a href=\"#08-别让异常逃离析构函数\" class=\"headerlink\" title=\"08 别让异常逃离析构函数\"></a>08 别让异常逃离析构函数</h2><p>如果在析构函数中抛出异常，会导致未定义的行为。</p>\n<p>析构函数不应该抛出异常。如果析构函数调用的函数可能会抛出异常，要在析构函数中捕获异常，然后结束程序（这不是未定义行为）或者吞掉它们。</p>\n<h2 id=\"09-绝不在构造函数和析构函数中调用虚函数\"><a href=\"#09-绝不在构造函数和析构函数中调用虚函数\" class=\"headerlink\" title=\"09 绝不在构造函数和析构函数中调用虚函数\"></a>09 绝不在构造函数和析构函数中调用虚函数</h2><p>你不应该在构造函数和析构函数中调用虚函数，这样的调用通常不会导致你想要的结果。</p>\n<p>为什么呢？</p>\n<p>如果我们在派生类的构造函数中使用了从基类中继承而来的虚函数。在派生类的构造函数之前，基类的构造函数被调用，这时候，所调用的虚函数实际是基类中的那个版本！析构函数同理。</p>\n<p>直接在构造函数中调用虚函数看上去很容易避免。然而在构造函数中你可能会调用其他的初始化函数，你应该确保这些初始化函数中没有调用虚函数。否则，你可能会陷入到苦涩头疼的调试中去。编译器通常不会发现此类问题，但是在程序运行中，如果基类的虚函数是纯虚函数，程序很可能中止（和后面的相比，也许这还算好的）。如果基类中的虚函数有自己的实现，那么你可能就会头疼于程序的表现为何出乎意料（期望调用派生类的重写版本，实际仍是基类的原始版本）。</p>\n<h2 id=\"10-令operator-返回一个-this的引用\"><a href=\"#10-令operator-返回一个-this的引用\" class=\"headerlink\" title=\"10 令operator=返回一个*this的引用\"></a>10 令<code>operator=</code>返回一个<code>*this</code>的引用</h2><p>为了实现连锁赋值，赋值操作符必须返回一个引用，指向操作符左侧的赋值实参。这条规范被大多数人遵守。除非有确实好的理由，否则最好按规范办事。</p>\n<p>所谓连锁赋值，是指下面这样的情况：</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">a = b = c;</div></pre></td></tr></table></figure>\n<p>这一条款不仅适用于赋值运算符，也适用于<code>+=</code>等。</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">class</span> A &#123;</div><div class=\"line\"><span class=\"keyword\">public</span>:</div><div class=\"line\">    A&amp; <span class=\"keyword\">operator</span>=(<span class=\"keyword\">const</span> A&amp; rhs) &#123;</div><div class=\"line\">        <span class=\"comment\">// ...</span></div><div class=\"line\">        <span class=\"keyword\">return</span> *<span class=\"keyword\">this</span>;</div><div class=\"line\">    &#125;</div><div class=\"line\">&#125;;</div></pre></td></tr></table></figure>\n<h2 id=\"11-在operator-中处理自我赋值\"><a href=\"#11-在operator-中处理自我赋值\" class=\"headerlink\" title=\"11 在operator=中处理自我赋值\"></a>11 在<code>operator=</code>中处理自我赋值</h2><p>所谓自我赋值，是指：</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\">Widget w;</div><div class=\"line\"><span class=\"comment\">// ...</span></div><div class=\"line\">w = w;</div></pre></td></tr></table></figure>\n<p>自我赋值常常发生在同一个对象的不同别名之间。在实现赋值运算符时，应注意处理这一现象。</p>\n<p>一种方法是进行“证同测试”，如下：</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\">A&amp; <span class=\"keyword\">operator</span>=(<span class=\"keyword\">const</span> A&amp; rhs) &#123;</div><div class=\"line\">    <span class=\"keyword\">if</span>(&amp;rhs == <span class=\"keyword\">this</span>) <span class=\"keyword\">return</span> *<span class=\"keyword\">this</span>; <span class=\"comment\">//证同测试</span></div><div class=\"line\">    <span class=\"comment\">// ...</span></div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<p>不过作者指出，这种方法不具备异常安全性。另一种方法是在函数内部，合理安排语句顺序，防止提前释放该对象本身的资源。</p>\n<p>还有一种方法，使用copy-swap方法，首先拷贝构造一个<code>rhs</code>的拷贝，然后交换该拷贝和<code>*this</code>，</p>\n<p>毫无疑问，使用证同测试方法和作者后文的方法都会造成性能的些许下降，这需要根据具体情况具体分析，合理采用。</p>\n<h2 id=\"12-复制对象时勿忘每一个成分\"><a href=\"#12-复制对象时勿忘每一个成分\" class=\"headerlink\" title=\"12 复制对象时勿忘每一个成分\"></a>12 复制对象时勿忘每一个成分</h2><p>这一条款是指存在继承时，实现copying函数（指拷贝构造函数和拷贝赋值函数）不要忘记base class部分成员。看下面的例子：</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">class</span> Derived: <span class=\"keyword\">public</span> Base&#123;</div><div class=\"line\"><span class=\"keyword\">private</span>:</div><div class=\"line\">    <span class=\"keyword\">int</span> a;</div><div class=\"line\"><span class=\"keyword\">public</span>:</div><div class=\"line\">    Derived(<span class=\"keyword\">const</span> Derived&amp; rhs):a(rhs.a) &#123;&#125;</div><div class=\"line\">    Derived&amp; <span class=\"keyword\">operator</span>=(<span class=\"keyword\">const</span> Derived&amp; rhs) &#123;</div><div class=\"line\">        a = rhs.a;</div><div class=\"line\">        <span class=\"keyword\">return</span> *<span class=\"keyword\">this</span>;</div><div class=\"line\">    &#125;</div><div class=\"line\">&#125;;</div></pre></td></tr></table></figure>\n<p>上面的代码只是拷贝了派生类新加入的成员<code>a</code>，而对基类中已有的成员未作处理。要记住，</p>\n<blockquote>\n<p>任何时候自己实现copying函数时，要担起重责大任，小心地复制其基类的成员。由于基类成员往往声明为<code>private</code>，所以，一般调用基类的成员函数进行拷贝。将上面的代码修改为：</p>\n</blockquote>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div></pre></td><td class=\"code\"><pre><div class=\"line\">Derived(<span class=\"keyword\">const</span> Derived&amp; rhs):Base(rhs), a(rhs.a) &#123;&#125;</div><div class=\"line\">Derived&amp; <span class=\"keyword\">operator</span>=(<span class=\"keyword\">const</span> Derived&amp; rhs) &#123;</div><div class=\"line\">    Base::<span class=\"keyword\">operator</span>=(rhs);</div><div class=\"line\">    a = rhs.a;</div><div class=\"line\">    <span class=\"keyword\">return</span> *<span class=\"keyword\">this</span>;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<p>此外，两种copying函数的实现往往是相似的。然而，不要试图在一个函数中调用另一个函数。把相似代码提取出来，写成一个独立的<code>init()</code>函数是一个更好的选择。</p>"},{"title":"Effective CPP 阅读 - Chapter 7 模板与泛型编程","date":"2017-06-23T12:19:07.000Z","_content":"模板是C++联邦中的重要成员。要想用好STL，必须了解模板。同时，模板元编程也是C++中的黑科技。\n\n![来自大师的嘲讽~](/img/effective_cpp_07_joke.jpg)\n<!-- more -->\n\n## 41 了解隐式接口和编译器多态\nOOP总是以显式接口和运行期多态解决问题。通过虚函数，运行期将根据变量（指针或引用）的动态类型决定究竟调用哪一个函数。\n\n而在模板与泛型世界中，例如下面的代码。没有明确指出，但是要求类型`T`支持操作符`<`。隐式接口不基于函数的声明，而是由有效表达式组成。\n\n同时，模板的具现化（instantiated）是在编译期发生的。通过模板具现化和函数重载，实现了多态。\n``` cpp\ntemplate <typename T>\nbool fun(const T& a, const T& b) {return a < b;}\n```\n\n## 42 了解`typename`的双重意义\n`typename`和`class`一样，都可以用来表明模板函数或者模板类。这时候两者完全相同，如下：\n``` cpp\ntemplate <typename/class T>\nvoid fun(T& a) {...}\n```\n\n但是有一个地方只能用`typename`，即标识嵌套从属类型名称。所谓从属类型名称，是指依赖于某个模板参数的名称。如果该类型名称呈嵌套状态，则称为嵌套从属名称。如：\n``` cpp\n// 打印容器内的第二个元素\ntemplate <typename C>\nvoid print_2nd_element(const C& container) {\n    // 嵌套类型\n    typename C::const_iterator it(container.begin());\n    cout << *++it;\n}\n```\n\n不过不要在基类列或者成员初始化列表中以其作为基类的修饰符。如：\n\n``` cpp\ntemplate <typename T>\nclass Derived: public Base<T>::Nested { //即使Nest是嵌套类型，这里也不用typename\npublic:\n    explicit Derived(int x)\n     :Base<T>::Nested(x) { // 成员初始化列表也不用\n        typename Base<T>::Nested tmp; //这时候要使用\n     }\n};\n```\n\n## 43 学习处理模板化基类内的名称\n这个问题的根源在于模板特化，造成特化版本与一般版本接口不同。因为编译器不能够在模板化的基类中寻找继承而来的名称。例如下面的离子：\n\n``` cpp\nclass TypeA {\npublic:\n    void fun();\n};\n\nclass TypeB {\npublic:\n    void fun();\n};\n\ntemplate <typename Type>\nclass Base {\npublic:\n    void do_something() {\n        Type x;\n        x.fun();\n    }\n};\n\ntemplate <typename Type>\nclass Derived: public Base<Type> {\npublic:\n    void do_something_too() {\n        // ...\n        do_something();  // 调用基类的函数，这里无法编译\n    }\n};\n```\n\n这是因为存在如下可能，`Base<Tyep>`对某种`Type`进行了特化。\n``` cpp\ntemplate <>\nclass Base<TypeC> {\npublic:\n    // 这里没有实现 dom_somthing 函数\n};\n```\n\n所以存在这样的可能：`class Derived<TypeC>: public Base<TypeC>`，而这里面是没有`do_something`函数的。\n\n为了解决这个问题，有三种办法：\n\n- 在基类调用方法前面加上`this->`。\n``` cpp\nvoid do_something_too() {\n    // ...\n    this->do_something();  // 调用基类的函数，这里无法编译\n}\n```\n\n- 使用`using`声明式。虽然这里和条款33一样都是用了这一技术，但是目的是不一样的。条款33中是因为派生类名称掩盖了基类，而这里是因为编译器本身就不进入基类中进行查找。\n\n``` cpp\nusing Base<Type>::do_something;\nvoid do_something_too() {\n    // ...\n    this->do_something();  // 调用基类的函数，这里无法编译\n}\n```\n\n- 明白指出被调用的函数位于基类中。这种方法最不推荐，因为如果被调用的是虚函数，上述的明确资格修饰符会关闭虚函数的运行时绑定行为。\n\n``` cpp\nvoid do_something_too() {\n    // ...\n    Base<Type>::do_something();  // 调用基类的函数，这里无法编译\n}\n```\n\n## 44 将与参数无关的代码抽离模板\n由于模板会具象化生成多个类或者多个函数，所以最好将与模板参数无关的代码抽离出去，防止代码膨胀造成程序体积变大和效率下降。\n\n如下所示是一个$N$阶方阵，其中`n`是阶数。如果我们对每个不同阶数的矩阵都写一遍矩阵求逆操作，会造成代码膨胀。\n\n``` cpp\ntemplate <typename T, size_t n>\nclass Matrix {\npublic:\n    void invert();\n};\n```\n\n一种可行的解决方案是提取出一个公共的基类用于实现矩阵转置。\n\n``` cpp\ntemplate <typename T>\nclass MatrixBase {\nprotected:\n    MatrixBase(size_t n, T* pMem) // 存储矩阵大小和指针\n     :size(n), pData(pMem) {}\n    void setDataPtr(T* ptr) {pData = ptr;} // 设置指针\n    void invert();   // 实现求逆\nprivate:\n    size_t size;\n    T* pData;\n};\n```\n\n而矩阵类继承自刚才这个没有设定非类型参数的基类。我们这里使用`private`继承来显示新的矩阵派生类只是根据旧的基类实现，而不是想表示Is-a的关系。\n``` cpp\ntemplate <typename T, size_t n>\nclass Matrix: private MatrixBase<T> {\npublic:\n    Matrix(): MatrixBase<T>(n, 0), pData(new T[n*n]) {\n        this->setDataPtr(pData.get()); // 将指针副本传给基类\n    }\nprivate:\n    boost::scoped_array<T> pData;\n};\n```\n\n然而这样改动并不一定比原来的效率更高。因为按原来的写法，常量`n`是个编译器常量，编译器可以通过常量的广传做优化。所以，实际使用时，还是要以profile为准。\n\n上述的例子是由于非类型参数造成的代码膨胀，而类型参数有时也会出现这种问题。如有的平台上`int`和`long`有相同的二进制表述。那么`vector<int>`和`vector<long>`的成员函数可能完全相同，也会造成代码膨胀。\n\n在很多平台上，不同类型的指针二进制表述是一样的，所以凡是模板中含有指针，如`vector<int*>, list<const int*>`等，往往应该对成员函数使用唯一的底层实现。例如，当你在操作某个成员函数而它操作的是一个强类型指针（即`T*`）时，你应该让它调用另一个无类型指针`void*`的函数，由后者完成实际工作。\n\n## 45 运用成员函数模板接受所有兼容类型\n使用场景一，我们可以将某个类的拷贝构造函数写成模板函数，使其能够接受兼容类型。比如对于智能指针，我们希望能够实现原始指针那种向上转型的能力。如下所示，基类指针能够指向基类和派生类。\n``` cpp\nBase* p = new Base;\nBase* p = new Derived;\n```\n\n``` cpp\n// 一个通用的智能指针模板\ntemplate <typename T>\nclass SmartPointer {\npublic:\n    // 为了兼容类型，需要再引入一个模板参数 U\n    template <typename U>\n    SmartPointer(const SmartPointer<U>& other)\n        // 这里可能会发生指针之间的隐式类型转换\n       :ptr(other.get())  {...}\n    T* get() const { return ptr; }\nprivate:\n    T* ptr;\n};\n```\n\n成员函数模板还可以用来作为赋值操作。\n``` cpp\ntemplate <typename T>\nclass shared_ptr {\npublic:\n    ...\n    // 接受任意兼容的shared_ptr赋值\n    template <typename Y>\n    shared_ptr& operator = (shared_ptr<Y> const& r);\n    // 接受任意兼容的auto_ptr赋值\n    template <typename Y>\n    shared_ptr& operator = (auto_ptr<Y> const& r);\n};\n```\n\n不过声明泛化版本的拷贝构造函数和赋值运算符，并不会阻止编译器为你生成默认的版本。所以如果你想控制拷贝或赋值的方方面面，必须同时声明泛化版本和普通版本。即：\n``` cpp\nshared_ptr& operator = (shared_ptr const& r);\n```\n## 46 需要类型转换时请为模板定义（friend）非成员函数\n回顾条款24，在其中指出，只有非成员函数才有能力在所有实参身上实施隐式类型转换。当这一规则延伸到模板世界中时，情况又有不同。如下所示，我们将实数类`Rational`声明为模板。\n\n``` cpp\ntemplate <typename T>\nclass Rational {\npublic:\n    Rational(const T& numerator=0, const T& denominator=1);\n};\n\ntemplate <typename T>\nconst Rational<T> operator*(const Rational<T>& lhs,\n                            const Rational<T>& rhs)\n{...}\n\nRational<int> onehalf(1, 2);\nRational<int> res = onehalf * 2;  // 改成模板后便会编译错误！\n```\n\n这是因为在进行模板类型推导时，并未将`2`进行隐式类型转换（否则，就是一个鸡生蛋蛋生鸡的问题了）。所以编译器没法找到这样的一个函数。\n\n解决方法是将这个运算符重载函数声明为`Rational<T>`的友元函数。这样，在`onehalf`被声明时，`Rational<int>`类被具现化，则该友元函数也被声明出来了。\n\n然而这时也只能通过编译而链接出错。因为无法找到函数的定义。解决方法是将函数体移动到类内部（即声明时即定义）。对于更复杂的函数，我们可以定义一个在模板类外部的辅助函数，而由这个友元函数去调用。\n``` cpp\ntemplate <typename T> class Rational;   // 前向声明\ntemplate <typename T>\nconst Rational<T> doMultiply(const Rational<T>& lhs,\n                             const Rational<T>& rhs) {};\n\ntemplate <typename T>\nclass Rational {\npublic:\n    Rational(const T& numerator=0, const T& denominator=1) {\n    }\n    // ...\n    friend const Rational<T> operator*(const Rational& lhs,\n                                       const Rational& rhs)\n    {return doMultiply(lhs, rhs); }\n};\n```\n\n## 47 使用`trait`表现类型信息\nSTL中的`advance`函数可以将某个迭代器移动给定的距离。但是对于不同的迭代器，我们需要采用不同的策略。\n- 输入迭代器。输入迭代器只能前向移动，每次一步，而且是只读一次，模仿的是输入文件的指针。例如`istream_iterator`。\n- 输出迭代器。输出迭代器只能向前移动，每次一步，而且是只写一次，模仿的是输出文件的指针。例如`ostream_iterator`。\n- 前向迭代器。只能向前移动，每次一步，可以读或写所指物一次以上。例如单向链表。\n- 双向迭代器。可以向前向后移动，每次一步，可以读或写所指物一次以上，例如双向链表。\n- 随机迭代器。可以随意跳转任意距离，例如`vector`或原始指针。\n\n为了对它们进行分类，C++有对应的tag标签。\n``` cpp\nstruct input_iterator_tag {};\nstruct output_iterator_tag {};\nstruct forward_iterator_tag: public input_iterator_tag {};\nstruct bidirectional_iterator_tag: public forward_iterator_tag {};\nstruct random_access_iterator_tag: public bidirectional_iterator_tag {};\n```\n\n所以我们可以在`advance`的代码中，对迭代器的类型进行判断，从而采取不同的操作。`trait`就是能够让你在编译器获得类型信息。\n\n我们希望`trait`也能够应用于内建类型，所以直接类型内的嵌套信息这种方案被排除了。因为我们无法对内建类型，如原始指针塞进去这个类型信息（对用户自定义的类型倒是很简单）。STL采用的方案是将其放入模板及其特化版本中。STL中有好几个这样的`trait`（而且C++11加入了更多），其中针对迭代器的是`iterator_traits`。\n\n为了实现这一功能，我们要在定义相应迭代器的时候，指明其类型（通常通过`typedef`来实现）。如队列的迭代器支持随机访问，则：\n``` cpp\ntemplate <typename T>\nclass deque {\npublic:\n    class iterator {\n    public:\n        typedef random_access_iterator_tag iterator_category;\n        // ...\n    }\n};\n```\n\n这样，我们就能在`iterator_traits`内部通过访问迭代器的`iterator_category`来获得其类型信息啦~如下所示，`iterator_traits`只是鹦鹉学舌般地表现`IterT`说自己是什么。\n\n``` cpp\ntemplate <typename IterT>\nstruct iterator_traits {\n    typedef typename IterT::iterator_category iterator_category;\n};\n```\n\n如何支持原始指针呢？用模板特化就好了~\n\n``` cpp\ntemplate <typename T>\nstruct iterator_traits<T*> {\n    typedef random_access_iterator_tag iterator_category;\n};\n```\n\n总结起来，如何设计并实现一个`traits`呢？\n\n- 确认若干你想要获取到的类型相关信息，例如本例中我们想要获得迭代器的分类（category）。\n- 为该信息取一个名称，如`iterator_category`\n- 提供一个模板和相关的特化版本，内含你想要提供的类型相关信息。\n\n好了，下面我们可以实现`advance`了。\n``` cpp\ntemplate <typename IterT, typename DistT>\nvoid advance(IterT& iter, DistT d) {\n    if(typeid(typename std::iterator_traits<IterT>::iterator_category\n        == typeid(std::random_access_iterator_tag) {\n        // ...\n    }\n    // ...\n}\n```\n\n然而，为什么要将在编译期能确定的事情搞到运行时再确定呢？我们可以通过函数重载的方法实现编译期的`if-else`功能。\n\n我们为不同类型的迭代器实现不同的移动方法。\n``` cpp\ntemplate <typename IterT, typename DistT>\nvoid doAdvance(IterT& iter, Dist d, std::random_access_iterator_tag) {\n    iter += d;\n}\n\n// ...其他类型的迭代器对应的 doadvance\n\n// 用advance函数包装这些重载函数\ntemplate <typename Iter, typename DistT>\nvoid advance(IterT& iter, Dist d) {\n    doAdvance(iter, d, typename std::iterator_traits<IterT>::iterator_category());\n    // 注意 typename\n    // 注意传入的是对象实例，所以要 iterator_category()\n}\n```\n也就是说\n- 首先建立一组重载函数或函数模板（真正干活的劳工），彼此之间的差异只在`trait`参数。\n- 建立包装函数（包工头），调用上述劳工函数并传递`trait`信息。\n\n## 48 认识模板元编程\n模板元编程（Template Metaprogram， TMP）能够实现将计算前移到编译器，能够实现早期错误侦测（如科学计算上的量度单位是否正确）和更高的执行效率（MXNet利用模板实现懒惰求值，消除中间临时量）。\n\n条款47介绍了选择分支结构如何借由`trait`实现。这里介绍循环由递归模板具现化实现的方法。\n\n为了生成斐波那契数列，我们首先定义一个模板参数为`n`的模板类。然后指出其值可以递归地由模板具现化实现。并通过模板特化给出递归基。\n\n``` cpp\ntemplate <unsigned n>\nstruct F {\n    enum {value = n * F<n-1>::value };\n};\n\ntemplate <>\nstruct F<0> {\n    enum {value = 1 };\n};\n```\n\nTMP博大精深，想要深入学习，还是要参考相关书籍。\n","source":"_posts/effective-cpp-07.md","raw":"---\ntitle: Effective CPP 阅读 - Chapter 7 模板与泛型编程\ndate: 2017-06-23 20:19:07\ntags:\n    - cpp\n---\n模板是C++联邦中的重要成员。要想用好STL，必须了解模板。同时，模板元编程也是C++中的黑科技。\n\n![来自大师的嘲讽~](/img/effective_cpp_07_joke.jpg)\n<!-- more -->\n\n## 41 了解隐式接口和编译器多态\nOOP总是以显式接口和运行期多态解决问题。通过虚函数，运行期将根据变量（指针或引用）的动态类型决定究竟调用哪一个函数。\n\n而在模板与泛型世界中，例如下面的代码。没有明确指出，但是要求类型`T`支持操作符`<`。隐式接口不基于函数的声明，而是由有效表达式组成。\n\n同时，模板的具现化（instantiated）是在编译期发生的。通过模板具现化和函数重载，实现了多态。\n``` cpp\ntemplate <typename T>\nbool fun(const T& a, const T& b) {return a < b;}\n```\n\n## 42 了解`typename`的双重意义\n`typename`和`class`一样，都可以用来表明模板函数或者模板类。这时候两者完全相同，如下：\n``` cpp\ntemplate <typename/class T>\nvoid fun(T& a) {...}\n```\n\n但是有一个地方只能用`typename`，即标识嵌套从属类型名称。所谓从属类型名称，是指依赖于某个模板参数的名称。如果该类型名称呈嵌套状态，则称为嵌套从属名称。如：\n``` cpp\n// 打印容器内的第二个元素\ntemplate <typename C>\nvoid print_2nd_element(const C& container) {\n    // 嵌套类型\n    typename C::const_iterator it(container.begin());\n    cout << *++it;\n}\n```\n\n不过不要在基类列或者成员初始化列表中以其作为基类的修饰符。如：\n\n``` cpp\ntemplate <typename T>\nclass Derived: public Base<T>::Nested { //即使Nest是嵌套类型，这里也不用typename\npublic:\n    explicit Derived(int x)\n     :Base<T>::Nested(x) { // 成员初始化列表也不用\n        typename Base<T>::Nested tmp; //这时候要使用\n     }\n};\n```\n\n## 43 学习处理模板化基类内的名称\n这个问题的根源在于模板特化，造成特化版本与一般版本接口不同。因为编译器不能够在模板化的基类中寻找继承而来的名称。例如下面的离子：\n\n``` cpp\nclass TypeA {\npublic:\n    void fun();\n};\n\nclass TypeB {\npublic:\n    void fun();\n};\n\ntemplate <typename Type>\nclass Base {\npublic:\n    void do_something() {\n        Type x;\n        x.fun();\n    }\n};\n\ntemplate <typename Type>\nclass Derived: public Base<Type> {\npublic:\n    void do_something_too() {\n        // ...\n        do_something();  // 调用基类的函数，这里无法编译\n    }\n};\n```\n\n这是因为存在如下可能，`Base<Tyep>`对某种`Type`进行了特化。\n``` cpp\ntemplate <>\nclass Base<TypeC> {\npublic:\n    // 这里没有实现 dom_somthing 函数\n};\n```\n\n所以存在这样的可能：`class Derived<TypeC>: public Base<TypeC>`，而这里面是没有`do_something`函数的。\n\n为了解决这个问题，有三种办法：\n\n- 在基类调用方法前面加上`this->`。\n``` cpp\nvoid do_something_too() {\n    // ...\n    this->do_something();  // 调用基类的函数，这里无法编译\n}\n```\n\n- 使用`using`声明式。虽然这里和条款33一样都是用了这一技术，但是目的是不一样的。条款33中是因为派生类名称掩盖了基类，而这里是因为编译器本身就不进入基类中进行查找。\n\n``` cpp\nusing Base<Type>::do_something;\nvoid do_something_too() {\n    // ...\n    this->do_something();  // 调用基类的函数，这里无法编译\n}\n```\n\n- 明白指出被调用的函数位于基类中。这种方法最不推荐，因为如果被调用的是虚函数，上述的明确资格修饰符会关闭虚函数的运行时绑定行为。\n\n``` cpp\nvoid do_something_too() {\n    // ...\n    Base<Type>::do_something();  // 调用基类的函数，这里无法编译\n}\n```\n\n## 44 将与参数无关的代码抽离模板\n由于模板会具象化生成多个类或者多个函数，所以最好将与模板参数无关的代码抽离出去，防止代码膨胀造成程序体积变大和效率下降。\n\n如下所示是一个$N$阶方阵，其中`n`是阶数。如果我们对每个不同阶数的矩阵都写一遍矩阵求逆操作，会造成代码膨胀。\n\n``` cpp\ntemplate <typename T, size_t n>\nclass Matrix {\npublic:\n    void invert();\n};\n```\n\n一种可行的解决方案是提取出一个公共的基类用于实现矩阵转置。\n\n``` cpp\ntemplate <typename T>\nclass MatrixBase {\nprotected:\n    MatrixBase(size_t n, T* pMem) // 存储矩阵大小和指针\n     :size(n), pData(pMem) {}\n    void setDataPtr(T* ptr) {pData = ptr;} // 设置指针\n    void invert();   // 实现求逆\nprivate:\n    size_t size;\n    T* pData;\n};\n```\n\n而矩阵类继承自刚才这个没有设定非类型参数的基类。我们这里使用`private`继承来显示新的矩阵派生类只是根据旧的基类实现，而不是想表示Is-a的关系。\n``` cpp\ntemplate <typename T, size_t n>\nclass Matrix: private MatrixBase<T> {\npublic:\n    Matrix(): MatrixBase<T>(n, 0), pData(new T[n*n]) {\n        this->setDataPtr(pData.get()); // 将指针副本传给基类\n    }\nprivate:\n    boost::scoped_array<T> pData;\n};\n```\n\n然而这样改动并不一定比原来的效率更高。因为按原来的写法，常量`n`是个编译器常量，编译器可以通过常量的广传做优化。所以，实际使用时，还是要以profile为准。\n\n上述的例子是由于非类型参数造成的代码膨胀，而类型参数有时也会出现这种问题。如有的平台上`int`和`long`有相同的二进制表述。那么`vector<int>`和`vector<long>`的成员函数可能完全相同，也会造成代码膨胀。\n\n在很多平台上，不同类型的指针二进制表述是一样的，所以凡是模板中含有指针，如`vector<int*>, list<const int*>`等，往往应该对成员函数使用唯一的底层实现。例如，当你在操作某个成员函数而它操作的是一个强类型指针（即`T*`）时，你应该让它调用另一个无类型指针`void*`的函数，由后者完成实际工作。\n\n## 45 运用成员函数模板接受所有兼容类型\n使用场景一，我们可以将某个类的拷贝构造函数写成模板函数，使其能够接受兼容类型。比如对于智能指针，我们希望能够实现原始指针那种向上转型的能力。如下所示，基类指针能够指向基类和派生类。\n``` cpp\nBase* p = new Base;\nBase* p = new Derived;\n```\n\n``` cpp\n// 一个通用的智能指针模板\ntemplate <typename T>\nclass SmartPointer {\npublic:\n    // 为了兼容类型，需要再引入一个模板参数 U\n    template <typename U>\n    SmartPointer(const SmartPointer<U>& other)\n        // 这里可能会发生指针之间的隐式类型转换\n       :ptr(other.get())  {...}\n    T* get() const { return ptr; }\nprivate:\n    T* ptr;\n};\n```\n\n成员函数模板还可以用来作为赋值操作。\n``` cpp\ntemplate <typename T>\nclass shared_ptr {\npublic:\n    ...\n    // 接受任意兼容的shared_ptr赋值\n    template <typename Y>\n    shared_ptr& operator = (shared_ptr<Y> const& r);\n    // 接受任意兼容的auto_ptr赋值\n    template <typename Y>\n    shared_ptr& operator = (auto_ptr<Y> const& r);\n};\n```\n\n不过声明泛化版本的拷贝构造函数和赋值运算符，并不会阻止编译器为你生成默认的版本。所以如果你想控制拷贝或赋值的方方面面，必须同时声明泛化版本和普通版本。即：\n``` cpp\nshared_ptr& operator = (shared_ptr const& r);\n```\n## 46 需要类型转换时请为模板定义（friend）非成员函数\n回顾条款24，在其中指出，只有非成员函数才有能力在所有实参身上实施隐式类型转换。当这一规则延伸到模板世界中时，情况又有不同。如下所示，我们将实数类`Rational`声明为模板。\n\n``` cpp\ntemplate <typename T>\nclass Rational {\npublic:\n    Rational(const T& numerator=0, const T& denominator=1);\n};\n\ntemplate <typename T>\nconst Rational<T> operator*(const Rational<T>& lhs,\n                            const Rational<T>& rhs)\n{...}\n\nRational<int> onehalf(1, 2);\nRational<int> res = onehalf * 2;  // 改成模板后便会编译错误！\n```\n\n这是因为在进行模板类型推导时，并未将`2`进行隐式类型转换（否则，就是一个鸡生蛋蛋生鸡的问题了）。所以编译器没法找到这样的一个函数。\n\n解决方法是将这个运算符重载函数声明为`Rational<T>`的友元函数。这样，在`onehalf`被声明时，`Rational<int>`类被具现化，则该友元函数也被声明出来了。\n\n然而这时也只能通过编译而链接出错。因为无法找到函数的定义。解决方法是将函数体移动到类内部（即声明时即定义）。对于更复杂的函数，我们可以定义一个在模板类外部的辅助函数，而由这个友元函数去调用。\n``` cpp\ntemplate <typename T> class Rational;   // 前向声明\ntemplate <typename T>\nconst Rational<T> doMultiply(const Rational<T>& lhs,\n                             const Rational<T>& rhs) {};\n\ntemplate <typename T>\nclass Rational {\npublic:\n    Rational(const T& numerator=0, const T& denominator=1) {\n    }\n    // ...\n    friend const Rational<T> operator*(const Rational& lhs,\n                                       const Rational& rhs)\n    {return doMultiply(lhs, rhs); }\n};\n```\n\n## 47 使用`trait`表现类型信息\nSTL中的`advance`函数可以将某个迭代器移动给定的距离。但是对于不同的迭代器，我们需要采用不同的策略。\n- 输入迭代器。输入迭代器只能前向移动，每次一步，而且是只读一次，模仿的是输入文件的指针。例如`istream_iterator`。\n- 输出迭代器。输出迭代器只能向前移动，每次一步，而且是只写一次，模仿的是输出文件的指针。例如`ostream_iterator`。\n- 前向迭代器。只能向前移动，每次一步，可以读或写所指物一次以上。例如单向链表。\n- 双向迭代器。可以向前向后移动，每次一步，可以读或写所指物一次以上，例如双向链表。\n- 随机迭代器。可以随意跳转任意距离，例如`vector`或原始指针。\n\n为了对它们进行分类，C++有对应的tag标签。\n``` cpp\nstruct input_iterator_tag {};\nstruct output_iterator_tag {};\nstruct forward_iterator_tag: public input_iterator_tag {};\nstruct bidirectional_iterator_tag: public forward_iterator_tag {};\nstruct random_access_iterator_tag: public bidirectional_iterator_tag {};\n```\n\n所以我们可以在`advance`的代码中，对迭代器的类型进行判断，从而采取不同的操作。`trait`就是能够让你在编译器获得类型信息。\n\n我们希望`trait`也能够应用于内建类型，所以直接类型内的嵌套信息这种方案被排除了。因为我们无法对内建类型，如原始指针塞进去这个类型信息（对用户自定义的类型倒是很简单）。STL采用的方案是将其放入模板及其特化版本中。STL中有好几个这样的`trait`（而且C++11加入了更多），其中针对迭代器的是`iterator_traits`。\n\n为了实现这一功能，我们要在定义相应迭代器的时候，指明其类型（通常通过`typedef`来实现）。如队列的迭代器支持随机访问，则：\n``` cpp\ntemplate <typename T>\nclass deque {\npublic:\n    class iterator {\n    public:\n        typedef random_access_iterator_tag iterator_category;\n        // ...\n    }\n};\n```\n\n这样，我们就能在`iterator_traits`内部通过访问迭代器的`iterator_category`来获得其类型信息啦~如下所示，`iterator_traits`只是鹦鹉学舌般地表现`IterT`说自己是什么。\n\n``` cpp\ntemplate <typename IterT>\nstruct iterator_traits {\n    typedef typename IterT::iterator_category iterator_category;\n};\n```\n\n如何支持原始指针呢？用模板特化就好了~\n\n``` cpp\ntemplate <typename T>\nstruct iterator_traits<T*> {\n    typedef random_access_iterator_tag iterator_category;\n};\n```\n\n总结起来，如何设计并实现一个`traits`呢？\n\n- 确认若干你想要获取到的类型相关信息，例如本例中我们想要获得迭代器的分类（category）。\n- 为该信息取一个名称，如`iterator_category`\n- 提供一个模板和相关的特化版本，内含你想要提供的类型相关信息。\n\n好了，下面我们可以实现`advance`了。\n``` cpp\ntemplate <typename IterT, typename DistT>\nvoid advance(IterT& iter, DistT d) {\n    if(typeid(typename std::iterator_traits<IterT>::iterator_category\n        == typeid(std::random_access_iterator_tag) {\n        // ...\n    }\n    // ...\n}\n```\n\n然而，为什么要将在编译期能确定的事情搞到运行时再确定呢？我们可以通过函数重载的方法实现编译期的`if-else`功能。\n\n我们为不同类型的迭代器实现不同的移动方法。\n``` cpp\ntemplate <typename IterT, typename DistT>\nvoid doAdvance(IterT& iter, Dist d, std::random_access_iterator_tag) {\n    iter += d;\n}\n\n// ...其他类型的迭代器对应的 doadvance\n\n// 用advance函数包装这些重载函数\ntemplate <typename Iter, typename DistT>\nvoid advance(IterT& iter, Dist d) {\n    doAdvance(iter, d, typename std::iterator_traits<IterT>::iterator_category());\n    // 注意 typename\n    // 注意传入的是对象实例，所以要 iterator_category()\n}\n```\n也就是说\n- 首先建立一组重载函数或函数模板（真正干活的劳工），彼此之间的差异只在`trait`参数。\n- 建立包装函数（包工头），调用上述劳工函数并传递`trait`信息。\n\n## 48 认识模板元编程\n模板元编程（Template Metaprogram， TMP）能够实现将计算前移到编译器，能够实现早期错误侦测（如科学计算上的量度单位是否正确）和更高的执行效率（MXNet利用模板实现懒惰求值，消除中间临时量）。\n\n条款47介绍了选择分支结构如何借由`trait`实现。这里介绍循环由递归模板具现化实现的方法。\n\n为了生成斐波那契数列，我们首先定义一个模板参数为`n`的模板类。然后指出其值可以递归地由模板具现化实现。并通过模板特化给出递归基。\n\n``` cpp\ntemplate <unsigned n>\nstruct F {\n    enum {value = n * F<n-1>::value };\n};\n\ntemplate <>\nstruct F<0> {\n    enum {value = 1 };\n};\n```\n\nTMP博大精深，想要深入学习，还是要参考相关书籍。\n","slug":"effective-cpp-07","published":1,"updated":"2018-01-12T06:22:20.468Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjcu6vcsy0014qu46d0xac3tg","content":"<p>模板是C++联邦中的重要成员。要想用好STL，必须了解模板。同时，模板元编程也是C++中的黑科技。</p>\n<p><img src=\"/img/effective_cpp_07_joke.jpg\" alt=\"来自大师的嘲讽~\"><br><a id=\"more\"></a></p>\n<h2 id=\"41-了解隐式接口和编译器多态\"><a href=\"#41-了解隐式接口和编译器多态\" class=\"headerlink\" title=\"41 了解隐式接口和编译器多态\"></a>41 了解隐式接口和编译器多态</h2><p>OOP总是以显式接口和运行期多态解决问题。通过虚函数，运行期将根据变量（指针或引用）的动态类型决定究竟调用哪一个函数。</p>\n<p>而在模板与泛型世界中，例如下面的代码。没有明确指出，但是要求类型<code>T</code>支持操作符<code>&lt;</code>。隐式接口不基于函数的声明，而是由有效表达式组成。</p>\n<p>同时，模板的具现化（instantiated）是在编译期发生的。通过模板具现化和函数重载，实现了多态。<br><figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">template</span> &lt;<span class=\"keyword\">typename</span> T&gt;</div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">bool</span> <span class=\"title\">fun</span><span class=\"params\">(<span class=\"keyword\">const</span> T&amp; a, <span class=\"keyword\">const</span> T&amp; b)</span> </span>&#123;<span class=\"keyword\">return</span> a &lt; b;&#125;</div></pre></td></tr></table></figure></p>\n<h2 id=\"42-了解typename的双重意义\"><a href=\"#42-了解typename的双重意义\" class=\"headerlink\" title=\"42 了解typename的双重意义\"></a>42 了解<code>typename</code>的双重意义</h2><p><code>typename</code>和<code>class</code>一样，都可以用来表明模板函数或者模板类。这时候两者完全相同，如下：<br><figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">template</span> &lt;<span class=\"keyword\">typename</span>/<span class=\"keyword\">class</span> T&gt;</div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">fun</span><span class=\"params\">(T&amp; a)</span> </span>&#123;...&#125;</div></pre></td></tr></table></figure></p>\n<p>但是有一个地方只能用<code>typename</code>，即标识嵌套从属类型名称。所谓从属类型名称，是指依赖于某个模板参数的名称。如果该类型名称呈嵌套状态，则称为嵌套从属名称。如：<br><figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div></pre></td><td class=\"code\"><pre><div class=\"line\">// 打印容器内的第二个元素</div><div class=\"line\">template &lt;typename C&gt;</div><div class=\"line\">void print_2nd_element(const C&amp; container) &#123;</div><div class=\"line\">    // 嵌套类型</div><div class=\"line\">    typename C::const_iterator it(container.begin());</div><div class=\"line\">    cout &lt;&lt; *++it;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure></p>\n<p>不过不要在基类列或者成员初始化列表中以其作为基类的修饰符。如：</p>\n<figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div></pre></td><td class=\"code\"><pre><div class=\"line\">template &lt;typename T&gt;</div><div class=\"line\">class Derived: public Base&lt;T&gt;::Nested &#123; //即使Nest是嵌套类型，这里也不用typename</div><div class=\"line\">public:</div><div class=\"line\">    explicit Derived(int x)</div><div class=\"line\">     :Base&lt;T&gt;::Nested(x) &#123; // 成员初始化列表也不用</div><div class=\"line\">        typename Base&lt;T&gt;::Nested tmp; //这时候要使用</div><div class=\"line\">     &#125;</div><div class=\"line\">&#125;;</div></pre></td></tr></table></figure>\n<h2 id=\"43-学习处理模板化基类内的名称\"><a href=\"#43-学习处理模板化基类内的名称\" class=\"headerlink\" title=\"43 学习处理模板化基类内的名称\"></a>43 学习处理模板化基类内的名称</h2><p>这个问题的根源在于模板特化，造成特化版本与一般版本接口不同。因为编译器不能够在模板化的基类中寻找继承而来的名称。例如下面的离子：</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">class</span> TypeA &#123;</div><div class=\"line\"><span class=\"keyword\">public</span>:</div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">fun</span><span class=\"params\">()</span></span>;</div><div class=\"line\">&#125;;</div><div class=\"line\"></div><div class=\"line\"><span class=\"keyword\">class</span> TypeB &#123;</div><div class=\"line\"><span class=\"keyword\">public</span>:</div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">fun</span><span class=\"params\">()</span></span>;</div><div class=\"line\">&#125;;</div><div class=\"line\"></div><div class=\"line\"><span class=\"keyword\">template</span> &lt;<span class=\"keyword\">typename</span> Type&gt;</div><div class=\"line\"><span class=\"keyword\">class</span> Base &#123;</div><div class=\"line\"><span class=\"keyword\">public</span>:</div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">do_something</span><span class=\"params\">()</span> </span>&#123;</div><div class=\"line\">        Type x;</div><div class=\"line\">        x.fun();</div><div class=\"line\">    &#125;</div><div class=\"line\">&#125;;</div><div class=\"line\"></div><div class=\"line\"><span class=\"keyword\">template</span> &lt;<span class=\"keyword\">typename</span> Type&gt;</div><div class=\"line\"><span class=\"keyword\">class</span> Derived: <span class=\"keyword\">public</span> Base&lt;Type&gt; &#123;</div><div class=\"line\"><span class=\"keyword\">public</span>:</div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">do_something_too</span><span class=\"params\">()</span> </span>&#123;</div><div class=\"line\">        <span class=\"comment\">// ...</span></div><div class=\"line\">        do_something();  <span class=\"comment\">// 调用基类的函数，这里无法编译</span></div><div class=\"line\">    &#125;</div><div class=\"line\">&#125;;</div></pre></td></tr></table></figure>\n<p>这是因为存在如下可能，<code>Base&lt;Tyep&gt;</code>对某种<code>Type</code>进行了特化。<br><figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">template</span> &lt;&gt;</div><div class=\"line\"><span class=\"keyword\">class</span> Base&lt;TypeC&gt; &#123;</div><div class=\"line\"><span class=\"keyword\">public</span>:</div><div class=\"line\">    <span class=\"comment\">// 这里没有实现 dom_somthing 函数</span></div><div class=\"line\">&#125;;</div></pre></td></tr></table></figure></p>\n<p>所以存在这样的可能：<code>class Derived&lt;TypeC&gt;: public Base&lt;TypeC&gt;</code>，而这里面是没有<code>do_something</code>函数的。</p>\n<p>为了解决这个问题，有三种办法：</p>\n<ul>\n<li><p>在基类调用方法前面加上<code>this-&gt;</code>。</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">do_something_too</span><span class=\"params\">()</span> </span>&#123;</div><div class=\"line\">    <span class=\"comment\">// ...</span></div><div class=\"line\">    <span class=\"keyword\">this</span>-&gt;do_something();  <span class=\"comment\">// 调用基类的函数，这里无法编译</span></div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n</li>\n<li><p>使用<code>using</code>声明式。虽然这里和条款33一样都是用了这一技术，但是目的是不一样的。条款33中是因为派生类名称掩盖了基类，而这里是因为编译器本身就不进入基类中进行查找。</p>\n</li>\n</ul>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">using</span> Base&lt;Type&gt;::do_something;</div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">do_something_too</span><span class=\"params\">()</span> </span>&#123;</div><div class=\"line\">    <span class=\"comment\">// ...</span></div><div class=\"line\">    <span class=\"keyword\">this</span>-&gt;do_something();  <span class=\"comment\">// 调用基类的函数，这里无法编译</span></div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<ul>\n<li>明白指出被调用的函数位于基类中。这种方法最不推荐，因为如果被调用的是虚函数，上述的明确资格修饰符会关闭虚函数的运行时绑定行为。</li>\n</ul>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">do_something_too</span><span class=\"params\">()</span> </span>&#123;</div><div class=\"line\">    <span class=\"comment\">// ...</span></div><div class=\"line\">    Base&lt;Type&gt;::do_something();  <span class=\"comment\">// 调用基类的函数，这里无法编译</span></div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<h2 id=\"44-将与参数无关的代码抽离模板\"><a href=\"#44-将与参数无关的代码抽离模板\" class=\"headerlink\" title=\"44 将与参数无关的代码抽离模板\"></a>44 将与参数无关的代码抽离模板</h2><p>由于模板会具象化生成多个类或者多个函数，所以最好将与模板参数无关的代码抽离出去，防止代码膨胀造成程序体积变大和效率下降。</p>\n<p>如下所示是一个$N$阶方阵，其中<code>n</code>是阶数。如果我们对每个不同阶数的矩阵都写一遍矩阵求逆操作，会造成代码膨胀。</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">template</span> &lt;<span class=\"keyword\">typename</span> T, <span class=\"keyword\">size_t</span> n&gt;</div><div class=\"line\"><span class=\"keyword\">class</span> Matrix &#123;</div><div class=\"line\"><span class=\"keyword\">public</span>:</div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">invert</span><span class=\"params\">()</span></span>;</div><div class=\"line\">&#125;;</div></pre></td></tr></table></figure>\n<p>一种可行的解决方案是提取出一个公共的基类用于实现矩阵转置。</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">template</span> &lt;<span class=\"keyword\">typename</span> T&gt;</div><div class=\"line\"><span class=\"keyword\">class</span> MatrixBase &#123;</div><div class=\"line\"><span class=\"keyword\">protected</span>:</div><div class=\"line\">    MatrixBase(<span class=\"keyword\">size_t</span> n, T* pMem) <span class=\"comment\">// 存储矩阵大小和指针</span></div><div class=\"line\">     :size(n), pData(pMem) &#123;&#125;</div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">setDataPtr</span><span class=\"params\">(T* ptr)</span> </span>&#123;pData = ptr;&#125; <span class=\"comment\">// 设置指针</span></div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">invert</span><span class=\"params\">()</span></span>;   <span class=\"comment\">// 实现求逆</span></div><div class=\"line\"><span class=\"keyword\">private</span>:</div><div class=\"line\">    <span class=\"keyword\">size_t</span> size;</div><div class=\"line\">    T* pData;</div><div class=\"line\">&#125;;</div></pre></td></tr></table></figure>\n<p>而矩阵类继承自刚才这个没有设定非类型参数的基类。我们这里使用<code>private</code>继承来显示新的矩阵派生类只是根据旧的基类实现，而不是想表示Is-a的关系。<br><figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">template</span> &lt;<span class=\"keyword\">typename</span> T, <span class=\"keyword\">size_t</span> n&gt;</div><div class=\"line\"><span class=\"keyword\">class</span> Matrix: <span class=\"keyword\">private</span> MatrixBase&lt;T&gt; &#123;</div><div class=\"line\"><span class=\"keyword\">public</span>:</div><div class=\"line\">    Matrix(): MatrixBase&lt;T&gt;(n, <span class=\"number\">0</span>), pData(<span class=\"keyword\">new</span> T[n*n]) &#123;</div><div class=\"line\">        <span class=\"keyword\">this</span>-&gt;setDataPtr(pData.get()); <span class=\"comment\">// 将指针副本传给基类</span></div><div class=\"line\">    &#125;</div><div class=\"line\"><span class=\"keyword\">private</span>:</div><div class=\"line\">    boost::scoped_array&lt;T&gt; pData;</div><div class=\"line\">&#125;;</div></pre></td></tr></table></figure></p>\n<p>然而这样改动并不一定比原来的效率更高。因为按原来的写法，常量<code>n</code>是个编译器常量，编译器可以通过常量的广传做优化。所以，实际使用时，还是要以profile为准。</p>\n<p>上述的例子是由于非类型参数造成的代码膨胀，而类型参数有时也会出现这种问题。如有的平台上<code>int</code>和<code>long</code>有相同的二进制表述。那么<code>vector&lt;int&gt;</code>和<code>vector&lt;long&gt;</code>的成员函数可能完全相同，也会造成代码膨胀。</p>\n<p>在很多平台上，不同类型的指针二进制表述是一样的，所以凡是模板中含有指针，如<code>vector&lt;int*&gt;, list&lt;const int*&gt;</code>等，往往应该对成员函数使用唯一的底层实现。例如，当你在操作某个成员函数而它操作的是一个强类型指针（即<code>T*</code>）时，你应该让它调用另一个无类型指针<code>void*</code>的函数，由后者完成实际工作。</p>\n<h2 id=\"45-运用成员函数模板接受所有兼容类型\"><a href=\"#45-运用成员函数模板接受所有兼容类型\" class=\"headerlink\" title=\"45 运用成员函数模板接受所有兼容类型\"></a>45 运用成员函数模板接受所有兼容类型</h2><p>使用场景一，我们可以将某个类的拷贝构造函数写成模板函数，使其能够接受兼容类型。比如对于智能指针，我们希望能够实现原始指针那种向上转型的能力。如下所示，基类指针能够指向基类和派生类。<br><figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">Base* p = <span class=\"keyword\">new</span> Base;</div><div class=\"line\">Base* p = <span class=\"keyword\">new</span> Derived;</div></pre></td></tr></table></figure></p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\">// 一个通用的智能指针模板</span></div><div class=\"line\"><span class=\"keyword\">template</span> &lt;<span class=\"keyword\">typename</span> T&gt;</div><div class=\"line\"><span class=\"keyword\">class</span> SmartPointer &#123;</div><div class=\"line\"><span class=\"keyword\">public</span>:</div><div class=\"line\">    <span class=\"comment\">// 为了兼容类型，需要再引入一个模板参数 U</span></div><div class=\"line\">    <span class=\"keyword\">template</span> &lt;<span class=\"keyword\">typename</span> U&gt;</div><div class=\"line\">    SmartPointer(<span class=\"keyword\">const</span> SmartPointer&lt;U&gt;&amp; other)</div><div class=\"line\">        <span class=\"comment\">// 这里可能会发生指针之间的隐式类型转换</span></div><div class=\"line\">       :ptr(other.get())  &#123;...&#125;</div><div class=\"line\">    <span class=\"function\">T* <span class=\"title\">get</span><span class=\"params\">()</span> <span class=\"keyword\">const</span> </span>&#123; <span class=\"keyword\">return</span> ptr; &#125;</div><div class=\"line\"><span class=\"keyword\">private</span>:</div><div class=\"line\">    T* ptr;</div><div class=\"line\">&#125;;</div></pre></td></tr></table></figure>\n<p>成员函数模板还可以用来作为赋值操作。<br><figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">template</span> &lt;<span class=\"keyword\">typename</span> T&gt;</div><div class=\"line\"><span class=\"keyword\">class</span> <span class=\"built_in\">shared_ptr</span> &#123;</div><div class=\"line\"><span class=\"keyword\">public</span>:</div><div class=\"line\">    ...</div><div class=\"line\">    <span class=\"comment\">// 接受任意兼容的shared_ptr赋值</span></div><div class=\"line\">    <span class=\"keyword\">template</span> &lt;<span class=\"keyword\">typename</span> Y&gt;</div><div class=\"line\">    <span class=\"built_in\">shared_ptr</span>&amp; <span class=\"keyword\">operator</span> = (<span class=\"built_in\">shared_ptr</span>&lt;Y&gt; <span class=\"keyword\">const</span>&amp; r);</div><div class=\"line\">    <span class=\"comment\">// 接受任意兼容的auto_ptr赋值</span></div><div class=\"line\">    <span class=\"keyword\">template</span> &lt;<span class=\"keyword\">typename</span> Y&gt;</div><div class=\"line\">    <span class=\"built_in\">shared_ptr</span>&amp; <span class=\"keyword\">operator</span> = (<span class=\"built_in\">auto_ptr</span>&lt;Y&gt; <span class=\"keyword\">const</span>&amp; r);</div><div class=\"line\">&#125;;</div></pre></td></tr></table></figure></p>\n<p>不过声明泛化版本的拷贝构造函数和赋值运算符，并不会阻止编译器为你生成默认的版本。所以如果你想控制拷贝或赋值的方方面面，必须同时声明泛化版本和普通版本。即：<br><figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"built_in\">shared_ptr</span>&amp; <span class=\"keyword\">operator</span> = (<span class=\"built_in\">shared_ptr</span> <span class=\"keyword\">const</span>&amp; r);</div></pre></td></tr></table></figure></p>\n<h2 id=\"46-需要类型转换时请为模板定义（friend）非成员函数\"><a href=\"#46-需要类型转换时请为模板定义（friend）非成员函数\" class=\"headerlink\" title=\"46 需要类型转换时请为模板定义（friend）非成员函数\"></a>46 需要类型转换时请为模板定义（friend）非成员函数</h2><p>回顾条款24，在其中指出，只有非成员函数才有能力在所有实参身上实施隐式类型转换。当这一规则延伸到模板世界中时，情况又有不同。如下所示，我们将实数类<code>Rational</code>声明为模板。</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">template</span> &lt;<span class=\"keyword\">typename</span> T&gt;</div><div class=\"line\"><span class=\"keyword\">class</span> Rational &#123;</div><div class=\"line\"><span class=\"keyword\">public</span>:</div><div class=\"line\">    Rational(<span class=\"keyword\">const</span> T&amp; numerator=<span class=\"number\">0</span>, <span class=\"keyword\">const</span> T&amp; denominator=<span class=\"number\">1</span>);</div><div class=\"line\">&#125;;</div><div class=\"line\"></div><div class=\"line\"><span class=\"keyword\">template</span> &lt;<span class=\"keyword\">typename</span> T&gt;</div><div class=\"line\"><span class=\"keyword\">const</span> Rational&lt;T&gt; <span class=\"keyword\">operator</span>*(<span class=\"keyword\">const</span> Rational&lt;T&gt;&amp; lhs,</div><div class=\"line\">                            <span class=\"keyword\">const</span> Rational&lt;T&gt;&amp; rhs)</div><div class=\"line\">&#123;...&#125;</div><div class=\"line\"></div><div class=\"line\">Rational&lt;<span class=\"keyword\">int</span>&gt; onehalf(<span class=\"number\">1</span>, <span class=\"number\">2</span>);</div><div class=\"line\">Rational&lt;<span class=\"keyword\">int</span>&gt; res = onehalf * <span class=\"number\">2</span>;  <span class=\"comment\">// 改成模板后便会编译错误！</span></div></pre></td></tr></table></figure>\n<p>这是因为在进行模板类型推导时，并未将<code>2</code>进行隐式类型转换（否则，就是一个鸡生蛋蛋生鸡的问题了）。所以编译器没法找到这样的一个函数。</p>\n<p>解决方法是将这个运算符重载函数声明为<code>Rational&lt;T&gt;</code>的友元函数。这样，在<code>onehalf</code>被声明时，<code>Rational&lt;int&gt;</code>类被具现化，则该友元函数也被声明出来了。</p>\n<p>然而这时也只能通过编译而链接出错。因为无法找到函数的定义。解决方法是将函数体移动到类内部（即声明时即定义）。对于更复杂的函数，我们可以定义一个在模板类外部的辅助函数，而由这个友元函数去调用。<br><figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">template</span> &lt;<span class=\"keyword\">typename</span> T&gt; <span class=\"keyword\">class</span> Rational;   <span class=\"comment\">// 前向声明</span></div><div class=\"line\"><span class=\"keyword\">template</span> &lt;<span class=\"keyword\">typename</span> T&gt;</div><div class=\"line\"><span class=\"keyword\">const</span> Rational&lt;T&gt; doMultiply(<span class=\"keyword\">const</span> Rational&lt;T&gt;&amp; lhs,</div><div class=\"line\">                             <span class=\"keyword\">const</span> Rational&lt;T&gt;&amp; rhs) &#123;&#125;;</div><div class=\"line\"></div><div class=\"line\"><span class=\"keyword\">template</span> &lt;<span class=\"keyword\">typename</span> T&gt;</div><div class=\"line\"><span class=\"keyword\">class</span> Rational &#123;</div><div class=\"line\"><span class=\"keyword\">public</span>:</div><div class=\"line\">    Rational(<span class=\"keyword\">const</span> T&amp; numerator=<span class=\"number\">0</span>, <span class=\"keyword\">const</span> T&amp; denominator=<span class=\"number\">1</span>) &#123;</div><div class=\"line\">    &#125;</div><div class=\"line\">    <span class=\"comment\">// ...</span></div><div class=\"line\">    <span class=\"keyword\">friend</span> <span class=\"keyword\">const</span> Rational&lt;T&gt; <span class=\"keyword\">operator</span>*(<span class=\"keyword\">const</span> Rational&amp; lhs,</div><div class=\"line\">                                       <span class=\"keyword\">const</span> Rational&amp; rhs)</div><div class=\"line\">    &#123;<span class=\"keyword\">return</span> doMultiply(lhs, rhs); &#125;</div><div class=\"line\">&#125;;</div></pre></td></tr></table></figure></p>\n<h2 id=\"47-使用trait表现类型信息\"><a href=\"#47-使用trait表现类型信息\" class=\"headerlink\" title=\"47 使用trait表现类型信息\"></a>47 使用<code>trait</code>表现类型信息</h2><p>STL中的<code>advance</code>函数可以将某个迭代器移动给定的距离。但是对于不同的迭代器，我们需要采用不同的策略。</p>\n<ul>\n<li>输入迭代器。输入迭代器只能前向移动，每次一步，而且是只读一次，模仿的是输入文件的指针。例如<code>istream_iterator</code>。</li>\n<li>输出迭代器。输出迭代器只能向前移动，每次一步，而且是只写一次，模仿的是输出文件的指针。例如<code>ostream_iterator</code>。</li>\n<li>前向迭代器。只能向前移动，每次一步，可以读或写所指物一次以上。例如单向链表。</li>\n<li>双向迭代器。可以向前向后移动，每次一步，可以读或写所指物一次以上，例如双向链表。</li>\n<li>随机迭代器。可以随意跳转任意距离，例如<code>vector</code>或原始指针。</li>\n</ul>\n<p>为了对它们进行分类，C++有对应的tag标签。<br><figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">struct</span> input_iterator_tag &#123;&#125;;</div><div class=\"line\"><span class=\"keyword\">struct</span> output_iterator_tag &#123;&#125;;</div><div class=\"line\"><span class=\"keyword\">struct</span> forward_iterator_tag: <span class=\"keyword\">public</span> input_iterator_tag &#123;&#125;;</div><div class=\"line\"><span class=\"keyword\">struct</span> bidirectional_iterator_tag: <span class=\"keyword\">public</span> forward_iterator_tag &#123;&#125;;</div><div class=\"line\"><span class=\"keyword\">struct</span> random_access_iterator_tag: <span class=\"keyword\">public</span> bidirectional_iterator_tag &#123;&#125;;</div></pre></td></tr></table></figure></p>\n<p>所以我们可以在<code>advance</code>的代码中，对迭代器的类型进行判断，从而采取不同的操作。<code>trait</code>就是能够让你在编译器获得类型信息。</p>\n<p>我们希望<code>trait</code>也能够应用于内建类型，所以直接类型内的嵌套信息这种方案被排除了。因为我们无法对内建类型，如原始指针塞进去这个类型信息（对用户自定义的类型倒是很简单）。STL采用的方案是将其放入模板及其特化版本中。STL中有好几个这样的<code>trait</code>（而且C++11加入了更多），其中针对迭代器的是<code>iterator_traits</code>。</p>\n<p>为了实现这一功能，我们要在定义相应迭代器的时候，指明其类型（通常通过<code>typedef</code>来实现）。如队列的迭代器支持随机访问，则：<br><figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">template</span> &lt;<span class=\"keyword\">typename</span> T&gt;</div><div class=\"line\"><span class=\"keyword\">class</span> <span class=\"built_in\">deque</span> &#123;</div><div class=\"line\"><span class=\"keyword\">public</span>:</div><div class=\"line\">    <span class=\"keyword\">class</span> iterator &#123;</div><div class=\"line\">    <span class=\"keyword\">public</span>:</div><div class=\"line\">        <span class=\"keyword\">typedef</span> random_access_iterator_tag iterator_category;</div><div class=\"line\">        <span class=\"comment\">// ...</span></div><div class=\"line\">    &#125;</div><div class=\"line\">&#125;;</div></pre></td></tr></table></figure></p>\n<p>这样，我们就能在<code>iterator_traits</code>内部通过访问迭代器的<code>iterator_category</code>来获得其类型信息啦~如下所示，<code>iterator_traits</code>只是鹦鹉学舌般地表现<code>IterT</code>说自己是什么。</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">template</span> &lt;<span class=\"keyword\">typename</span> IterT&gt;</div><div class=\"line\"><span class=\"keyword\">struct</span> iterator_traits &#123;</div><div class=\"line\">    <span class=\"keyword\">typedef</span> <span class=\"keyword\">typename</span> IterT::iterator_category iterator_category;</div><div class=\"line\">&#125;;</div></pre></td></tr></table></figure>\n<p>如何支持原始指针呢？用模板特化就好了~</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">template</span> &lt;<span class=\"keyword\">typename</span> T&gt;</div><div class=\"line\"><span class=\"keyword\">struct</span> iterator_traits&lt;T*&gt; &#123;</div><div class=\"line\">    <span class=\"keyword\">typedef</span> random_access_iterator_tag iterator_category;</div><div class=\"line\">&#125;;</div></pre></td></tr></table></figure>\n<p>总结起来，如何设计并实现一个<code>traits</code>呢？</p>\n<ul>\n<li>确认若干你想要获取到的类型相关信息，例如本例中我们想要获得迭代器的分类（category）。</li>\n<li>为该信息取一个名称，如<code>iterator_category</code></li>\n<li>提供一个模板和相关的特化版本，内含你想要提供的类型相关信息。</li>\n</ul>\n<p>好了，下面我们可以实现<code>advance</code>了。<br><figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">template</span> &lt;<span class=\"keyword\">typename</span> IterT, <span class=\"keyword\">typename</span> DistT&gt;</div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">advance</span><span class=\"params\">(IterT&amp; iter, DistT d)</span> </span>&#123;</div><div class=\"line\">    <span class=\"keyword\">if</span>(<span class=\"keyword\">typeid</span>(<span class=\"keyword\">typename</span> <span class=\"built_in\">std</span>::iterator_traits&lt;IterT&gt;::iterator_category</div><div class=\"line\">        == <span class=\"keyword\">typeid</span>(<span class=\"built_in\">std</span>::random_access_iterator_tag) &#123;</div><div class=\"line\">        <span class=\"comment\">// ...</span></div><div class=\"line\">    &#125;</div><div class=\"line\">    <span class=\"comment\">// ...</span></div><div class=\"line\">&#125;</div></pre></td></tr></table></figure></p>\n<p>然而，为什么要将在编译期能确定的事情搞到运行时再确定呢？我们可以通过函数重载的方法实现编译期的<code>if-else</code>功能。</p>\n<p>我们为不同类型的迭代器实现不同的移动方法。<br><figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">template</span> &lt;<span class=\"keyword\">typename</span> IterT, <span class=\"keyword\">typename</span> DistT&gt;</div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">doAdvance</span><span class=\"params\">(IterT&amp; iter, Dist d, <span class=\"built_in\">std</span>::random_access_iterator_tag)</span> </span>&#123;</div><div class=\"line\">    iter += d;</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">// ...其他类型的迭代器对应的 doadvance</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">// 用advance函数包装这些重载函数</span></div><div class=\"line\"><span class=\"keyword\">template</span> &lt;<span class=\"keyword\">typename</span> Iter, <span class=\"keyword\">typename</span> DistT&gt;</div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">advance</span><span class=\"params\">(IterT&amp; iter, Dist d)</span> </span>&#123;</div><div class=\"line\">    doAdvance(iter, d, <span class=\"keyword\">typename</span> <span class=\"built_in\">std</span>::iterator_traits&lt;IterT&gt;::iterator_category());</div><div class=\"line\">    <span class=\"comment\">// 注意 typename</span></div><div class=\"line\">    <span class=\"comment\">// 注意传入的是对象实例，所以要 iterator_category()</span></div><div class=\"line\">&#125;</div></pre></td></tr></table></figure></p>\n<p>也就是说</p>\n<ul>\n<li>首先建立一组重载函数或函数模板（真正干活的劳工），彼此之间的差异只在<code>trait</code>参数。</li>\n<li>建立包装函数（包工头），调用上述劳工函数并传递<code>trait</code>信息。</li>\n</ul>\n<h2 id=\"48-认识模板元编程\"><a href=\"#48-认识模板元编程\" class=\"headerlink\" title=\"48 认识模板元编程\"></a>48 认识模板元编程</h2><p>模板元编程（Template Metaprogram， TMP）能够实现将计算前移到编译器，能够实现早期错误侦测（如科学计算上的量度单位是否正确）和更高的执行效率（MXNet利用模板实现懒惰求值，消除中间临时量）。</p>\n<p>条款47介绍了选择分支结构如何借由<code>trait</code>实现。这里介绍循环由递归模板具现化实现的方法。</p>\n<p>为了生成斐波那契数列，我们首先定义一个模板参数为<code>n</code>的模板类。然后指出其值可以递归地由模板具现化实现。并通过模板特化给出递归基。</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">template</span> &lt;<span class=\"keyword\">unsigned</span> n&gt;</div><div class=\"line\"><span class=\"keyword\">struct</span> F &#123;</div><div class=\"line\">    <span class=\"keyword\">enum</span> &#123;value = n * F&lt;n<span class=\"number\">-1</span>&gt;::value &#125;;</div><div class=\"line\">&#125;;</div><div class=\"line\"></div><div class=\"line\"><span class=\"keyword\">template</span> &lt;&gt;</div><div class=\"line\"><span class=\"keyword\">struct</span> F&lt;<span class=\"number\">0</span>&gt; &#123;</div><div class=\"line\">    <span class=\"keyword\">enum</span> &#123;value = <span class=\"number\">1</span> &#125;;</div><div class=\"line\">&#125;;</div></pre></td></tr></table></figure>\n<p>TMP博大精深，想要深入学习，还是要参考相关书籍。</p>\n","excerpt":"<p>模板是C++联邦中的重要成员。要想用好STL，必须了解模板。同时，模板元编程也是C++中的黑科技。</p>\n<p><img src=\"/img/effective_cpp_07_joke.jpg\" alt=\"来自大师的嘲讽~\"><br>","more":"</p>\n<h2 id=\"41-了解隐式接口和编译器多态\"><a href=\"#41-了解隐式接口和编译器多态\" class=\"headerlink\" title=\"41 了解隐式接口和编译器多态\"></a>41 了解隐式接口和编译器多态</h2><p>OOP总是以显式接口和运行期多态解决问题。通过虚函数，运行期将根据变量（指针或引用）的动态类型决定究竟调用哪一个函数。</p>\n<p>而在模板与泛型世界中，例如下面的代码。没有明确指出，但是要求类型<code>T</code>支持操作符<code>&lt;</code>。隐式接口不基于函数的声明，而是由有效表达式组成。</p>\n<p>同时，模板的具现化（instantiated）是在编译期发生的。通过模板具现化和函数重载，实现了多态。<br><figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">template</span> &lt;<span class=\"keyword\">typename</span> T&gt;</div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">bool</span> <span class=\"title\">fun</span><span class=\"params\">(<span class=\"keyword\">const</span> T&amp; a, <span class=\"keyword\">const</span> T&amp; b)</span> </span>&#123;<span class=\"keyword\">return</span> a &lt; b;&#125;</div></pre></td></tr></table></figure></p>\n<h2 id=\"42-了解typename的双重意义\"><a href=\"#42-了解typename的双重意义\" class=\"headerlink\" title=\"42 了解typename的双重意义\"></a>42 了解<code>typename</code>的双重意义</h2><p><code>typename</code>和<code>class</code>一样，都可以用来表明模板函数或者模板类。这时候两者完全相同，如下：<br><figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">template</span> &lt;<span class=\"keyword\">typename</span>/<span class=\"keyword\">class</span> T&gt;</div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">fun</span><span class=\"params\">(T&amp; a)</span> </span>&#123;...&#125;</div></pre></td></tr></table></figure></p>\n<p>但是有一个地方只能用<code>typename</code>，即标识嵌套从属类型名称。所谓从属类型名称，是指依赖于某个模板参数的名称。如果该类型名称呈嵌套状态，则称为嵌套从属名称。如：<br><figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div></pre></td><td class=\"code\"><pre><div class=\"line\">// 打印容器内的第二个元素</div><div class=\"line\">template &lt;typename C&gt;</div><div class=\"line\">void print_2nd_element(const C&amp; container) &#123;</div><div class=\"line\">    // 嵌套类型</div><div class=\"line\">    typename C::const_iterator it(container.begin());</div><div class=\"line\">    cout &lt;&lt; *++it;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure></p>\n<p>不过不要在基类列或者成员初始化列表中以其作为基类的修饰符。如：</p>\n<figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div></pre></td><td class=\"code\"><pre><div class=\"line\">template &lt;typename T&gt;</div><div class=\"line\">class Derived: public Base&lt;T&gt;::Nested &#123; //即使Nest是嵌套类型，这里也不用typename</div><div class=\"line\">public:</div><div class=\"line\">    explicit Derived(int x)</div><div class=\"line\">     :Base&lt;T&gt;::Nested(x) &#123; // 成员初始化列表也不用</div><div class=\"line\">        typename Base&lt;T&gt;::Nested tmp; //这时候要使用</div><div class=\"line\">     &#125;</div><div class=\"line\">&#125;;</div></pre></td></tr></table></figure>\n<h2 id=\"43-学习处理模板化基类内的名称\"><a href=\"#43-学习处理模板化基类内的名称\" class=\"headerlink\" title=\"43 学习处理模板化基类内的名称\"></a>43 学习处理模板化基类内的名称</h2><p>这个问题的根源在于模板特化，造成特化版本与一般版本接口不同。因为编译器不能够在模板化的基类中寻找继承而来的名称。例如下面的离子：</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">class</span> TypeA &#123;</div><div class=\"line\"><span class=\"keyword\">public</span>:</div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">fun</span><span class=\"params\">()</span></span>;</div><div class=\"line\">&#125;;</div><div class=\"line\"></div><div class=\"line\"><span class=\"keyword\">class</span> TypeB &#123;</div><div class=\"line\"><span class=\"keyword\">public</span>:</div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">fun</span><span class=\"params\">()</span></span>;</div><div class=\"line\">&#125;;</div><div class=\"line\"></div><div class=\"line\"><span class=\"keyword\">template</span> &lt;<span class=\"keyword\">typename</span> Type&gt;</div><div class=\"line\"><span class=\"keyword\">class</span> Base &#123;</div><div class=\"line\"><span class=\"keyword\">public</span>:</div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">do_something</span><span class=\"params\">()</span> </span>&#123;</div><div class=\"line\">        Type x;</div><div class=\"line\">        x.fun();</div><div class=\"line\">    &#125;</div><div class=\"line\">&#125;;</div><div class=\"line\"></div><div class=\"line\"><span class=\"keyword\">template</span> &lt;<span class=\"keyword\">typename</span> Type&gt;</div><div class=\"line\"><span class=\"keyword\">class</span> Derived: <span class=\"keyword\">public</span> Base&lt;Type&gt; &#123;</div><div class=\"line\"><span class=\"keyword\">public</span>:</div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">do_something_too</span><span class=\"params\">()</span> </span>&#123;</div><div class=\"line\">        <span class=\"comment\">// ...</span></div><div class=\"line\">        do_something();  <span class=\"comment\">// 调用基类的函数，这里无法编译</span></div><div class=\"line\">    &#125;</div><div class=\"line\">&#125;;</div></pre></td></tr></table></figure>\n<p>这是因为存在如下可能，<code>Base&lt;Tyep&gt;</code>对某种<code>Type</code>进行了特化。<br><figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">template</span> &lt;&gt;</div><div class=\"line\"><span class=\"keyword\">class</span> Base&lt;TypeC&gt; &#123;</div><div class=\"line\"><span class=\"keyword\">public</span>:</div><div class=\"line\">    <span class=\"comment\">// 这里没有实现 dom_somthing 函数</span></div><div class=\"line\">&#125;;</div></pre></td></tr></table></figure></p>\n<p>所以存在这样的可能：<code>class Derived&lt;TypeC&gt;: public Base&lt;TypeC&gt;</code>，而这里面是没有<code>do_something</code>函数的。</p>\n<p>为了解决这个问题，有三种办法：</p>\n<ul>\n<li><p>在基类调用方法前面加上<code>this-&gt;</code>。</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">do_something_too</span><span class=\"params\">()</span> </span>&#123;</div><div class=\"line\">    <span class=\"comment\">// ...</span></div><div class=\"line\">    <span class=\"keyword\">this</span>-&gt;do_something();  <span class=\"comment\">// 调用基类的函数，这里无法编译</span></div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n</li>\n<li><p>使用<code>using</code>声明式。虽然这里和条款33一样都是用了这一技术，但是目的是不一样的。条款33中是因为派生类名称掩盖了基类，而这里是因为编译器本身就不进入基类中进行查找。</p>\n</li>\n</ul>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">using</span> Base&lt;Type&gt;::do_something;</div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">do_something_too</span><span class=\"params\">()</span> </span>&#123;</div><div class=\"line\">    <span class=\"comment\">// ...</span></div><div class=\"line\">    <span class=\"keyword\">this</span>-&gt;do_something();  <span class=\"comment\">// 调用基类的函数，这里无法编译</span></div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<ul>\n<li>明白指出被调用的函数位于基类中。这种方法最不推荐，因为如果被调用的是虚函数，上述的明确资格修饰符会关闭虚函数的运行时绑定行为。</li>\n</ul>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">do_something_too</span><span class=\"params\">()</span> </span>&#123;</div><div class=\"line\">    <span class=\"comment\">// ...</span></div><div class=\"line\">    Base&lt;Type&gt;::do_something();  <span class=\"comment\">// 调用基类的函数，这里无法编译</span></div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<h2 id=\"44-将与参数无关的代码抽离模板\"><a href=\"#44-将与参数无关的代码抽离模板\" class=\"headerlink\" title=\"44 将与参数无关的代码抽离模板\"></a>44 将与参数无关的代码抽离模板</h2><p>由于模板会具象化生成多个类或者多个函数，所以最好将与模板参数无关的代码抽离出去，防止代码膨胀造成程序体积变大和效率下降。</p>\n<p>如下所示是一个$N$阶方阵，其中<code>n</code>是阶数。如果我们对每个不同阶数的矩阵都写一遍矩阵求逆操作，会造成代码膨胀。</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">template</span> &lt;<span class=\"keyword\">typename</span> T, <span class=\"keyword\">size_t</span> n&gt;</div><div class=\"line\"><span class=\"keyword\">class</span> Matrix &#123;</div><div class=\"line\"><span class=\"keyword\">public</span>:</div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">invert</span><span class=\"params\">()</span></span>;</div><div class=\"line\">&#125;;</div></pre></td></tr></table></figure>\n<p>一种可行的解决方案是提取出一个公共的基类用于实现矩阵转置。</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">template</span> &lt;<span class=\"keyword\">typename</span> T&gt;</div><div class=\"line\"><span class=\"keyword\">class</span> MatrixBase &#123;</div><div class=\"line\"><span class=\"keyword\">protected</span>:</div><div class=\"line\">    MatrixBase(<span class=\"keyword\">size_t</span> n, T* pMem) <span class=\"comment\">// 存储矩阵大小和指针</span></div><div class=\"line\">     :size(n), pData(pMem) &#123;&#125;</div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">setDataPtr</span><span class=\"params\">(T* ptr)</span> </span>&#123;pData = ptr;&#125; <span class=\"comment\">// 设置指针</span></div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">invert</span><span class=\"params\">()</span></span>;   <span class=\"comment\">// 实现求逆</span></div><div class=\"line\"><span class=\"keyword\">private</span>:</div><div class=\"line\">    <span class=\"keyword\">size_t</span> size;</div><div class=\"line\">    T* pData;</div><div class=\"line\">&#125;;</div></pre></td></tr></table></figure>\n<p>而矩阵类继承自刚才这个没有设定非类型参数的基类。我们这里使用<code>private</code>继承来显示新的矩阵派生类只是根据旧的基类实现，而不是想表示Is-a的关系。<br><figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">template</span> &lt;<span class=\"keyword\">typename</span> T, <span class=\"keyword\">size_t</span> n&gt;</div><div class=\"line\"><span class=\"keyword\">class</span> Matrix: <span class=\"keyword\">private</span> MatrixBase&lt;T&gt; &#123;</div><div class=\"line\"><span class=\"keyword\">public</span>:</div><div class=\"line\">    Matrix(): MatrixBase&lt;T&gt;(n, <span class=\"number\">0</span>), pData(<span class=\"keyword\">new</span> T[n*n]) &#123;</div><div class=\"line\">        <span class=\"keyword\">this</span>-&gt;setDataPtr(pData.get()); <span class=\"comment\">// 将指针副本传给基类</span></div><div class=\"line\">    &#125;</div><div class=\"line\"><span class=\"keyword\">private</span>:</div><div class=\"line\">    boost::scoped_array&lt;T&gt; pData;</div><div class=\"line\">&#125;;</div></pre></td></tr></table></figure></p>\n<p>然而这样改动并不一定比原来的效率更高。因为按原来的写法，常量<code>n</code>是个编译器常量，编译器可以通过常量的广传做优化。所以，实际使用时，还是要以profile为准。</p>\n<p>上述的例子是由于非类型参数造成的代码膨胀，而类型参数有时也会出现这种问题。如有的平台上<code>int</code>和<code>long</code>有相同的二进制表述。那么<code>vector&lt;int&gt;</code>和<code>vector&lt;long&gt;</code>的成员函数可能完全相同，也会造成代码膨胀。</p>\n<p>在很多平台上，不同类型的指针二进制表述是一样的，所以凡是模板中含有指针，如<code>vector&lt;int*&gt;, list&lt;const int*&gt;</code>等，往往应该对成员函数使用唯一的底层实现。例如，当你在操作某个成员函数而它操作的是一个强类型指针（即<code>T*</code>）时，你应该让它调用另一个无类型指针<code>void*</code>的函数，由后者完成实际工作。</p>\n<h2 id=\"45-运用成员函数模板接受所有兼容类型\"><a href=\"#45-运用成员函数模板接受所有兼容类型\" class=\"headerlink\" title=\"45 运用成员函数模板接受所有兼容类型\"></a>45 运用成员函数模板接受所有兼容类型</h2><p>使用场景一，我们可以将某个类的拷贝构造函数写成模板函数，使其能够接受兼容类型。比如对于智能指针，我们希望能够实现原始指针那种向上转型的能力。如下所示，基类指针能够指向基类和派生类。<br><figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">Base* p = <span class=\"keyword\">new</span> Base;</div><div class=\"line\">Base* p = <span class=\"keyword\">new</span> Derived;</div></pre></td></tr></table></figure></p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\">// 一个通用的智能指针模板</span></div><div class=\"line\"><span class=\"keyword\">template</span> &lt;<span class=\"keyword\">typename</span> T&gt;</div><div class=\"line\"><span class=\"keyword\">class</span> SmartPointer &#123;</div><div class=\"line\"><span class=\"keyword\">public</span>:</div><div class=\"line\">    <span class=\"comment\">// 为了兼容类型，需要再引入一个模板参数 U</span></div><div class=\"line\">    <span class=\"keyword\">template</span> &lt;<span class=\"keyword\">typename</span> U&gt;</div><div class=\"line\">    SmartPointer(<span class=\"keyword\">const</span> SmartPointer&lt;U&gt;&amp; other)</div><div class=\"line\">        <span class=\"comment\">// 这里可能会发生指针之间的隐式类型转换</span></div><div class=\"line\">       :ptr(other.get())  &#123;...&#125;</div><div class=\"line\">    <span class=\"function\">T* <span class=\"title\">get</span><span class=\"params\">()</span> <span class=\"keyword\">const</span> </span>&#123; <span class=\"keyword\">return</span> ptr; &#125;</div><div class=\"line\"><span class=\"keyword\">private</span>:</div><div class=\"line\">    T* ptr;</div><div class=\"line\">&#125;;</div></pre></td></tr></table></figure>\n<p>成员函数模板还可以用来作为赋值操作。<br><figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">template</span> &lt;<span class=\"keyword\">typename</span> T&gt;</div><div class=\"line\"><span class=\"keyword\">class</span> <span class=\"built_in\">shared_ptr</span> &#123;</div><div class=\"line\"><span class=\"keyword\">public</span>:</div><div class=\"line\">    ...</div><div class=\"line\">    <span class=\"comment\">// 接受任意兼容的shared_ptr赋值</span></div><div class=\"line\">    <span class=\"keyword\">template</span> &lt;<span class=\"keyword\">typename</span> Y&gt;</div><div class=\"line\">    <span class=\"built_in\">shared_ptr</span>&amp; <span class=\"keyword\">operator</span> = (<span class=\"built_in\">shared_ptr</span>&lt;Y&gt; <span class=\"keyword\">const</span>&amp; r);</div><div class=\"line\">    <span class=\"comment\">// 接受任意兼容的auto_ptr赋值</span></div><div class=\"line\">    <span class=\"keyword\">template</span> &lt;<span class=\"keyword\">typename</span> Y&gt;</div><div class=\"line\">    <span class=\"built_in\">shared_ptr</span>&amp; <span class=\"keyword\">operator</span> = (<span class=\"built_in\">auto_ptr</span>&lt;Y&gt; <span class=\"keyword\">const</span>&amp; r);</div><div class=\"line\">&#125;;</div></pre></td></tr></table></figure></p>\n<p>不过声明泛化版本的拷贝构造函数和赋值运算符，并不会阻止编译器为你生成默认的版本。所以如果你想控制拷贝或赋值的方方面面，必须同时声明泛化版本和普通版本。即：<br><figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"built_in\">shared_ptr</span>&amp; <span class=\"keyword\">operator</span> = (<span class=\"built_in\">shared_ptr</span> <span class=\"keyword\">const</span>&amp; r);</div></pre></td></tr></table></figure></p>\n<h2 id=\"46-需要类型转换时请为模板定义（friend）非成员函数\"><a href=\"#46-需要类型转换时请为模板定义（friend）非成员函数\" class=\"headerlink\" title=\"46 需要类型转换时请为模板定义（friend）非成员函数\"></a>46 需要类型转换时请为模板定义（friend）非成员函数</h2><p>回顾条款24，在其中指出，只有非成员函数才有能力在所有实参身上实施隐式类型转换。当这一规则延伸到模板世界中时，情况又有不同。如下所示，我们将实数类<code>Rational</code>声明为模板。</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">template</span> &lt;<span class=\"keyword\">typename</span> T&gt;</div><div class=\"line\"><span class=\"keyword\">class</span> Rational &#123;</div><div class=\"line\"><span class=\"keyword\">public</span>:</div><div class=\"line\">    Rational(<span class=\"keyword\">const</span> T&amp; numerator=<span class=\"number\">0</span>, <span class=\"keyword\">const</span> T&amp; denominator=<span class=\"number\">1</span>);</div><div class=\"line\">&#125;;</div><div class=\"line\"></div><div class=\"line\"><span class=\"keyword\">template</span> &lt;<span class=\"keyword\">typename</span> T&gt;</div><div class=\"line\"><span class=\"keyword\">const</span> Rational&lt;T&gt; <span class=\"keyword\">operator</span>*(<span class=\"keyword\">const</span> Rational&lt;T&gt;&amp; lhs,</div><div class=\"line\">                            <span class=\"keyword\">const</span> Rational&lt;T&gt;&amp; rhs)</div><div class=\"line\">&#123;...&#125;</div><div class=\"line\"></div><div class=\"line\">Rational&lt;<span class=\"keyword\">int</span>&gt; onehalf(<span class=\"number\">1</span>, <span class=\"number\">2</span>);</div><div class=\"line\">Rational&lt;<span class=\"keyword\">int</span>&gt; res = onehalf * <span class=\"number\">2</span>;  <span class=\"comment\">// 改成模板后便会编译错误！</span></div></pre></td></tr></table></figure>\n<p>这是因为在进行模板类型推导时，并未将<code>2</code>进行隐式类型转换（否则，就是一个鸡生蛋蛋生鸡的问题了）。所以编译器没法找到这样的一个函数。</p>\n<p>解决方法是将这个运算符重载函数声明为<code>Rational&lt;T&gt;</code>的友元函数。这样，在<code>onehalf</code>被声明时，<code>Rational&lt;int&gt;</code>类被具现化，则该友元函数也被声明出来了。</p>\n<p>然而这时也只能通过编译而链接出错。因为无法找到函数的定义。解决方法是将函数体移动到类内部（即声明时即定义）。对于更复杂的函数，我们可以定义一个在模板类外部的辅助函数，而由这个友元函数去调用。<br><figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">template</span> &lt;<span class=\"keyword\">typename</span> T&gt; <span class=\"keyword\">class</span> Rational;   <span class=\"comment\">// 前向声明</span></div><div class=\"line\"><span class=\"keyword\">template</span> &lt;<span class=\"keyword\">typename</span> T&gt;</div><div class=\"line\"><span class=\"keyword\">const</span> Rational&lt;T&gt; doMultiply(<span class=\"keyword\">const</span> Rational&lt;T&gt;&amp; lhs,</div><div class=\"line\">                             <span class=\"keyword\">const</span> Rational&lt;T&gt;&amp; rhs) &#123;&#125;;</div><div class=\"line\"></div><div class=\"line\"><span class=\"keyword\">template</span> &lt;<span class=\"keyword\">typename</span> T&gt;</div><div class=\"line\"><span class=\"keyword\">class</span> Rational &#123;</div><div class=\"line\"><span class=\"keyword\">public</span>:</div><div class=\"line\">    Rational(<span class=\"keyword\">const</span> T&amp; numerator=<span class=\"number\">0</span>, <span class=\"keyword\">const</span> T&amp; denominator=<span class=\"number\">1</span>) &#123;</div><div class=\"line\">    &#125;</div><div class=\"line\">    <span class=\"comment\">// ...</span></div><div class=\"line\">    <span class=\"keyword\">friend</span> <span class=\"keyword\">const</span> Rational&lt;T&gt; <span class=\"keyword\">operator</span>*(<span class=\"keyword\">const</span> Rational&amp; lhs,</div><div class=\"line\">                                       <span class=\"keyword\">const</span> Rational&amp; rhs)</div><div class=\"line\">    &#123;<span class=\"keyword\">return</span> doMultiply(lhs, rhs); &#125;</div><div class=\"line\">&#125;;</div></pre></td></tr></table></figure></p>\n<h2 id=\"47-使用trait表现类型信息\"><a href=\"#47-使用trait表现类型信息\" class=\"headerlink\" title=\"47 使用trait表现类型信息\"></a>47 使用<code>trait</code>表现类型信息</h2><p>STL中的<code>advance</code>函数可以将某个迭代器移动给定的距离。但是对于不同的迭代器，我们需要采用不同的策略。</p>\n<ul>\n<li>输入迭代器。输入迭代器只能前向移动，每次一步，而且是只读一次，模仿的是输入文件的指针。例如<code>istream_iterator</code>。</li>\n<li>输出迭代器。输出迭代器只能向前移动，每次一步，而且是只写一次，模仿的是输出文件的指针。例如<code>ostream_iterator</code>。</li>\n<li>前向迭代器。只能向前移动，每次一步，可以读或写所指物一次以上。例如单向链表。</li>\n<li>双向迭代器。可以向前向后移动，每次一步，可以读或写所指物一次以上，例如双向链表。</li>\n<li>随机迭代器。可以随意跳转任意距离，例如<code>vector</code>或原始指针。</li>\n</ul>\n<p>为了对它们进行分类，C++有对应的tag标签。<br><figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">struct</span> input_iterator_tag &#123;&#125;;</div><div class=\"line\"><span class=\"keyword\">struct</span> output_iterator_tag &#123;&#125;;</div><div class=\"line\"><span class=\"keyword\">struct</span> forward_iterator_tag: <span class=\"keyword\">public</span> input_iterator_tag &#123;&#125;;</div><div class=\"line\"><span class=\"keyword\">struct</span> bidirectional_iterator_tag: <span class=\"keyword\">public</span> forward_iterator_tag &#123;&#125;;</div><div class=\"line\"><span class=\"keyword\">struct</span> random_access_iterator_tag: <span class=\"keyword\">public</span> bidirectional_iterator_tag &#123;&#125;;</div></pre></td></tr></table></figure></p>\n<p>所以我们可以在<code>advance</code>的代码中，对迭代器的类型进行判断，从而采取不同的操作。<code>trait</code>就是能够让你在编译器获得类型信息。</p>\n<p>我们希望<code>trait</code>也能够应用于内建类型，所以直接类型内的嵌套信息这种方案被排除了。因为我们无法对内建类型，如原始指针塞进去这个类型信息（对用户自定义的类型倒是很简单）。STL采用的方案是将其放入模板及其特化版本中。STL中有好几个这样的<code>trait</code>（而且C++11加入了更多），其中针对迭代器的是<code>iterator_traits</code>。</p>\n<p>为了实现这一功能，我们要在定义相应迭代器的时候，指明其类型（通常通过<code>typedef</code>来实现）。如队列的迭代器支持随机访问，则：<br><figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">template</span> &lt;<span class=\"keyword\">typename</span> T&gt;</div><div class=\"line\"><span class=\"keyword\">class</span> <span class=\"built_in\">deque</span> &#123;</div><div class=\"line\"><span class=\"keyword\">public</span>:</div><div class=\"line\">    <span class=\"keyword\">class</span> iterator &#123;</div><div class=\"line\">    <span class=\"keyword\">public</span>:</div><div class=\"line\">        <span class=\"keyword\">typedef</span> random_access_iterator_tag iterator_category;</div><div class=\"line\">        <span class=\"comment\">// ...</span></div><div class=\"line\">    &#125;</div><div class=\"line\">&#125;;</div></pre></td></tr></table></figure></p>\n<p>这样，我们就能在<code>iterator_traits</code>内部通过访问迭代器的<code>iterator_category</code>来获得其类型信息啦~如下所示，<code>iterator_traits</code>只是鹦鹉学舌般地表现<code>IterT</code>说自己是什么。</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">template</span> &lt;<span class=\"keyword\">typename</span> IterT&gt;</div><div class=\"line\"><span class=\"keyword\">struct</span> iterator_traits &#123;</div><div class=\"line\">    <span class=\"keyword\">typedef</span> <span class=\"keyword\">typename</span> IterT::iterator_category iterator_category;</div><div class=\"line\">&#125;;</div></pre></td></tr></table></figure>\n<p>如何支持原始指针呢？用模板特化就好了~</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">template</span> &lt;<span class=\"keyword\">typename</span> T&gt;</div><div class=\"line\"><span class=\"keyword\">struct</span> iterator_traits&lt;T*&gt; &#123;</div><div class=\"line\">    <span class=\"keyword\">typedef</span> random_access_iterator_tag iterator_category;</div><div class=\"line\">&#125;;</div></pre></td></tr></table></figure>\n<p>总结起来，如何设计并实现一个<code>traits</code>呢？</p>\n<ul>\n<li>确认若干你想要获取到的类型相关信息，例如本例中我们想要获得迭代器的分类（category）。</li>\n<li>为该信息取一个名称，如<code>iterator_category</code></li>\n<li>提供一个模板和相关的特化版本，内含你想要提供的类型相关信息。</li>\n</ul>\n<p>好了，下面我们可以实现<code>advance</code>了。<br><figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">template</span> &lt;<span class=\"keyword\">typename</span> IterT, <span class=\"keyword\">typename</span> DistT&gt;</div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">advance</span><span class=\"params\">(IterT&amp; iter, DistT d)</span> </span>&#123;</div><div class=\"line\">    <span class=\"keyword\">if</span>(<span class=\"keyword\">typeid</span>(<span class=\"keyword\">typename</span> <span class=\"built_in\">std</span>::iterator_traits&lt;IterT&gt;::iterator_category</div><div class=\"line\">        == <span class=\"keyword\">typeid</span>(<span class=\"built_in\">std</span>::random_access_iterator_tag) &#123;</div><div class=\"line\">        <span class=\"comment\">// ...</span></div><div class=\"line\">    &#125;</div><div class=\"line\">    <span class=\"comment\">// ...</span></div><div class=\"line\">&#125;</div></pre></td></tr></table></figure></p>\n<p>然而，为什么要将在编译期能确定的事情搞到运行时再确定呢？我们可以通过函数重载的方法实现编译期的<code>if-else</code>功能。</p>\n<p>我们为不同类型的迭代器实现不同的移动方法。<br><figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">template</span> &lt;<span class=\"keyword\">typename</span> IterT, <span class=\"keyword\">typename</span> DistT&gt;</div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">doAdvance</span><span class=\"params\">(IterT&amp; iter, Dist d, <span class=\"built_in\">std</span>::random_access_iterator_tag)</span> </span>&#123;</div><div class=\"line\">    iter += d;</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">// ...其他类型的迭代器对应的 doadvance</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">// 用advance函数包装这些重载函数</span></div><div class=\"line\"><span class=\"keyword\">template</span> &lt;<span class=\"keyword\">typename</span> Iter, <span class=\"keyword\">typename</span> DistT&gt;</div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">advance</span><span class=\"params\">(IterT&amp; iter, Dist d)</span> </span>&#123;</div><div class=\"line\">    doAdvance(iter, d, <span class=\"keyword\">typename</span> <span class=\"built_in\">std</span>::iterator_traits&lt;IterT&gt;::iterator_category());</div><div class=\"line\">    <span class=\"comment\">// 注意 typename</span></div><div class=\"line\">    <span class=\"comment\">// 注意传入的是对象实例，所以要 iterator_category()</span></div><div class=\"line\">&#125;</div></pre></td></tr></table></figure></p>\n<p>也就是说</p>\n<ul>\n<li>首先建立一组重载函数或函数模板（真正干活的劳工），彼此之间的差异只在<code>trait</code>参数。</li>\n<li>建立包装函数（包工头），调用上述劳工函数并传递<code>trait</code>信息。</li>\n</ul>\n<h2 id=\"48-认识模板元编程\"><a href=\"#48-认识模板元编程\" class=\"headerlink\" title=\"48 认识模板元编程\"></a>48 认识模板元编程</h2><p>模板元编程（Template Metaprogram， TMP）能够实现将计算前移到编译器，能够实现早期错误侦测（如科学计算上的量度单位是否正确）和更高的执行效率（MXNet利用模板实现懒惰求值，消除中间临时量）。</p>\n<p>条款47介绍了选择分支结构如何借由<code>trait</code>实现。这里介绍循环由递归模板具现化实现的方法。</p>\n<p>为了生成斐波那契数列，我们首先定义一个模板参数为<code>n</code>的模板类。然后指出其值可以递归地由模板具现化实现。并通过模板特化给出递归基。</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">template</span> &lt;<span class=\"keyword\">unsigned</span> n&gt;</div><div class=\"line\"><span class=\"keyword\">struct</span> F &#123;</div><div class=\"line\">    <span class=\"keyword\">enum</span> &#123;value = n * F&lt;n<span class=\"number\">-1</span>&gt;::value &#125;;</div><div class=\"line\">&#125;;</div><div class=\"line\"></div><div class=\"line\"><span class=\"keyword\">template</span> &lt;&gt;</div><div class=\"line\"><span class=\"keyword\">struct</span> F&lt;<span class=\"number\">0</span>&gt; &#123;</div><div class=\"line\">    <span class=\"keyword\">enum</span> &#123;value = <span class=\"number\">1</span> &#125;;</div><div class=\"line\">&#125;;</div></pre></td></tr></table></figure>\n<p>TMP博大精深，想要深入学习，还是要参考相关书籍。</p>"},{"title":"Effective CPP 阅读 - Chapter 5 实现","date":"2017-05-01T06:56:54.000Z","_content":"第四章中探讨了如何更好地提出类的定义和函数声明，精心设计的接口能让后续工作轻松不少。然而如何能够正确高效地实现，也是一件重要的事情。行百里者半九十。\n\n<!-- more -->\n\n## 26 尽可能延后变量定义式的出现时间\n变量，尤其是自定义对象，一旦被定义，就会调用构造函数；一旦生命周期结束，又要调用析构函数。所以，延后变量定义的时间，并且最好能够给定恰当的初始值，这样能够提高代码执行效率。\n\n## 27 尽量少做转型动作\n第一，安全考虑。C++的类型系统保证类型错误不会发生。理论上如果你的代码“很干净”地通过编译，就表明它不意图在任何对象上执行不安全和无意义的操作，这是一个很有价值的保险。不要轻易打破。\n\n第二，效率问题。这里展开说。\n\nC++中的转型动作有以下三种：\n\n- `(T)expression`，C时代的风格\n- `T(expression)`，函数风格\n- 新式风格，包括`static_cast`,`const_cast`,`dynamic_cast`和`reinterpret_cast`四种。\n\n作者不提倡使用两种旧风格，而使用下面的四种。一种理由是它们容易被自动化代码检查工具匹配检查。\n\n对于 `const_cast`，用于脱掉某对象的`const`属性。\n\n对于`static_cast`，用于进行强制类型转换。例如将non-const类型转换为const类型，或者将`double`类型转换为`int`等。\n\n对于`dynamic_cast`，用于执行安全向下转换，也就是用于决定某对象是否归属继承体系中的某个类型。注意，`dynamic_cast`的很多实现都很慢，尤其是继承深度较深时，也是这几个里面唯一可能造成重大（注意，并非其他三者不会带来执行时间开销）运行成本的动作。\n\n很多时候，之所以使用`dynamic_cast`是因为你想在一个（你认为是）某个派生类对象中执行派生类中（并非从基类继承）的成员函数，但你的手上只有指向这个派生类对象的base指针或引用。这种情况下，也许将派生类的这个函数在基类中也定义一个空函数体的函数，再由派生类重写可能更好。\n\n对于`reinterpret_cast`，它用来实现低级转型，实际动作和结果依赖于编译器，表示它不可移植。例如将指针转换为`int`，此转换是在bit层面的转换，更详细的信息可以参见[cpp reference的介绍](http://en.cppreference.com/w/cpp/language/reinterpret_cast)。\n\n在它们之中，`reinterpret_cast`和`const_cast`完全是编译器层面的东西。`static_cast`会导致编译器生成对应CPU指令，但是是在编译器就能决定的。`dynamic_cast`是在运行时多态的一种手段。\n\n## 28 避免返回指向对象内部成分的句柄（handle）\n当成员函数返回类内部私有成员变量（或者私有成员函数，较少见）的句柄（如指针，引用或迭代器），而且成员变量的资源又存储于对象之外，这时候，虽然可以将此成员函数声明为`const`，但是实际上并不能避免通过此句柄修改资源的情况发生。\n\n例如在自定义的`string`类中，使用堆上的数组储存字符串。如果某个成员函数能够返回字符串数组，那么可以使用这个指针修改数组内的值，而这也是符合`const`约定的。\n\n所以，若无十分必要，不要返回对象内部的句柄。有此需要时，首先考虑是否应返回`const handle&`。\n\n即使这样，还有可能造成返回的句柄比变量本身生命周期更长，也就是句柄所指之物已经被析构，句柄此时成为空悬状态，造成问题。\n\n## 29 为“异常安全”努力是值得的\n异常安全函数是指即使发生异常，也不会泄露资源或者允许任何数据结构被破坏。由于在代码执行过程中，可能发生内存申请失败等等异常，导致我们逻辑上已经设想好的程序控制流被中断，造成内存泄漏（后续的`delete`操作没有执行）。此外，我们希望如果异常发生，变量的值（程序的状态）能够恢复到异常发生之前。\n\n让我们先看一个不满足异常安全的函数例子：\n``` cpp\n// 自定义`Menu`类中修改背景图片的成员函数\nvoid Menu::changeBg(std::istream& imgSrc) {\n    lock(&this->mutex);  // 互斥锁 1\n    delete this->bg;     // 释放本已有的bg2\n    ++this->imgChangeCnt;      // 计数器  3\n    bg = new Image(imgSrc);    // 新图片  4\n    unlock(&this->mutex);      // 释放锁  5\n}\n```\n\n上面的代码存在以下问题，使得它不满足异常安全性。\n\n- 资源泄漏。一旦第4行内存申请失败，那么第五行无法执行，互斥锁永远把持住了。\n- 数据被破坏。还是上面的情况，则`bg`此时的资源已经被析构，而且计数器的值也增加了。·\n\n解决第一个问题，可以考虑使用智能指针，即条款13中的使用对象管理资源。\n\n我们将异常安全分为以下三类：\n\n- 基本型。异常被抛出后，程序内的数据不会被破坏。但是并不保证程序的现实状态（究竟`bg`是何值）\n- 强烈保证。异常抛出后，程序恢复到该函数调用前的状态。copy-and-swap策略是达成这一目标的常见方法。首先为待修改的对象原件做出一份副本，然后在副本上做一切修改。若有任何修改抛出异常，则原件不受影响。待所有修改完成后，再将修改过后的副本和原件在一个不抛出异常的swap操作中交换。\n\n在这里，常常采用pImpl技术，也就是在对象中仅存储资源的（智能）指针，在swap中只操作该指针即可。\n\n- 绝不抛出异常。作用于内置类型（`int`或指针等）身上的所有操作都提供了nothrow保证。\n\n## 30 透彻了解内联的方方面面\n`inline`函数在编译期实现函数本体的替换，避免了函数调用的开销，还可能使得编译器基于上下文进行优化，鼓励使用`inline`替换函数宏定义。\n\n然而，`inline`不要乱用。首先，`inline`会使得目标码体积变大。可能造成额外的换页行为，降低高速缓存的命中概率，反而造成性能的损失。\n\n另一方面，`inline`只是对编译器的申请，不是真的一定内联。\n\n`inline`函数通常定义在头文件中（或者直接定义在类的内部，这样无需加入`inline`关键字），这是因为在编译中编译器需要知道这个函数具体长什么样子，才能够实现内联。\n\n有时候，虽然编译器有意愿内敛某函数，但是还是会为它产生一个函数本体。这常常发生在取某个内联函数地址时。与此并提，编译器通常不对通过函数指针调用的内联函数进行内联。也就是说，是否真的内联，还与函数的调用方式有关。\n\n作者给出的建议是，一开始不要将任何函数内联，之后使用profile工具进行优化。不要忘记28法则，80%的程序执行时间花在了20%的代码上。除非找对了目标，否则优化都是无用功。将内联函数应用于调用频繁且小型的函数身上。\n\n## 31 将文件间的编译依存关系降至最低\nC++的头文件包含机制饱受批评。连串的编译依存关系常常使得项目的编译时间大大加长。\n\n首先，程序库头文件应该“完全且仅有声明式”的存在，将实现代码放入cpp文件中。\n\n另外，之所以C++编译时容易出现“牵一发而动全身”的情况，是因为C++与Java等语言不同。在Java中编译器只分配一个指针指向实际对象，也就不需要知道对象的实际大小。而C++编译器却需要知道对象中每个成员变量的明确定义，才能知道对象的实际大小，从而在内存中分配空间。\n\n从这里出发，我们可以参考Java等语言中的思路，建立一个handle类，在其中包含原来那个类的完全数据，而在新的类中定义一个指向该handle类的指针，这也就是前面所提到的pImpl方法。\n\n使用这种思虑，定义的包含有`Date`类型对象（指明这个人的生日）的`Person`类如下：\n\n``` cpp\n#include <string>   // for string\n#include <memory>   // for shared_ptr\n\nclass PersonImpl;   // Person实现类的前置声明\nclass Date;         // Person接口用到的类的前置声明\n\nclass Person {\npublic:\n    Person(const std::string& name, const Date& birthday);\n    std::string name() const;\nprivate:\n    std::shared_ptr<PersonImpl> pImpl;   // 指向实现类\n};\n\n/*****************实现文件****************/\n#include \"Person.h\"\n#include \"PersonImpl.h\"\nPerson::Person(const std::string& name, const Date& birthday):\n    pImpl(new PersonImpl(name, birthday)) {}\n\nstd::string Person::name() const {\n    return pImpl->name();\n}\n```\n\n在上面的代码中，通过构造handle类`PersonImpl`，在`Person`中我们只需要前置声明`Date`，而无需包含头文件`date.hpp`。这样，即使`Date`或者`Person`有修改，影响也仅限于`Date`的实现文件和`PersonImpl`而已，不会传导到`Person`和使用了`Person`的其他代码文件。通过这种做法，实际上`Person`成为了一个单纯的接口，具体的实现在`PersonImpl`中完成，实现了“接口与实现的分离”。\n\n综上：\n\n- 如果使用object pointer或者object reference可以完成任务，就不要使用object。只要前置声明就可以定义出指向该类型的pointer或者reference，但是需要完整地定义式才能定义object。\n- 如果能够，尽量用类的声明式替换定义式 。注意，当声明某个函数而它用到某个类时，你并不需要这个类的定义式。即使函数以pass-by-value方式传递参数（通常情况下这也不是一个好主意）或返回值。\n- 为声明式和定义式提供不同的头文件（`Person`本身和`PersonImpl`）。这两个文件应该保持一致。声明式改变了，需要修改定义式头文件。程序库客户应该包含声明文件。\n\n除了上面的方法，也可以将`Person`定义为抽象基类（Caffe中的`Layer`就是类似的模式）。为了达成这一目标，`Person`需要一个虚构造函数（见条款7）和一系列的纯虚函数（作为接口，等待派生类重写实现）。如下所示：\n\n``` cpp\nclass Person {\npublic:\n    virtual ~Person();\n    virtual string name() const = 0;\n};\n```\n\n客户必须能够为这种类创建对象。通常的做法是调用一个工厂函数，返回派生类的（智能）指针。这样的函数常常在抽象基类中声明为`static`。\n\n``` cpp\nclass Person {\npublic:\n    static shared_ptr<Person> create(const string& name, const Date& birthday);\n// ... 刚才的其他代码\n};\n```\n\n当然，要想使用，我们还必须定义派生类实现相应的接口。\n\n``` cpp\nclass RealPerson: public Person {\npublic:\n    RealPerson(const string& name, const Date& birthday): name(name), birthDate(birthday) {}\n    virtual ~RealPerson() {}\n    string name() const { return this->name; }\nprivate:\n    string name;\n    Date birthDate;\n};\n```\n\n上面的工厂函数`create()`的实现：\n\n``` cpp\nshared_ptr<Person> Person::create(const string& name, const Date& date) {\n    return shared_ptr<Person>(new RealPerson(name, date));\n}\n```\n\n实际应用中的工厂函数会像工厂一样，根据客户需要，产出不同的派生类对象。\n\n当然，使用上述技术增大了程序运行时间开销和内存空间。这需要在工程中分情况讨论。是否这部分的开销大到了需要无视接口实现分离原则的地步？如果是的，那就用具象的类代替他们。但是，不要因噎废食。\n","source":"_posts/effective-cpp-05.md","raw":"---\ntitle: Effective CPP 阅读 - Chapter 5 实现\ndate: 2017-05-01 14:56:54\ntags:\n    - cpp\n---\n第四章中探讨了如何更好地提出类的定义和函数声明，精心设计的接口能让后续工作轻松不少。然而如何能够正确高效地实现，也是一件重要的事情。行百里者半九十。\n\n<!-- more -->\n\n## 26 尽可能延后变量定义式的出现时间\n变量，尤其是自定义对象，一旦被定义，就会调用构造函数；一旦生命周期结束，又要调用析构函数。所以，延后变量定义的时间，并且最好能够给定恰当的初始值，这样能够提高代码执行效率。\n\n## 27 尽量少做转型动作\n第一，安全考虑。C++的类型系统保证类型错误不会发生。理论上如果你的代码“很干净”地通过编译，就表明它不意图在任何对象上执行不安全和无意义的操作，这是一个很有价值的保险。不要轻易打破。\n\n第二，效率问题。这里展开说。\n\nC++中的转型动作有以下三种：\n\n- `(T)expression`，C时代的风格\n- `T(expression)`，函数风格\n- 新式风格，包括`static_cast`,`const_cast`,`dynamic_cast`和`reinterpret_cast`四种。\n\n作者不提倡使用两种旧风格，而使用下面的四种。一种理由是它们容易被自动化代码检查工具匹配检查。\n\n对于 `const_cast`，用于脱掉某对象的`const`属性。\n\n对于`static_cast`，用于进行强制类型转换。例如将non-const类型转换为const类型，或者将`double`类型转换为`int`等。\n\n对于`dynamic_cast`，用于执行安全向下转换，也就是用于决定某对象是否归属继承体系中的某个类型。注意，`dynamic_cast`的很多实现都很慢，尤其是继承深度较深时，也是这几个里面唯一可能造成重大（注意，并非其他三者不会带来执行时间开销）运行成本的动作。\n\n很多时候，之所以使用`dynamic_cast`是因为你想在一个（你认为是）某个派生类对象中执行派生类中（并非从基类继承）的成员函数，但你的手上只有指向这个派生类对象的base指针或引用。这种情况下，也许将派生类的这个函数在基类中也定义一个空函数体的函数，再由派生类重写可能更好。\n\n对于`reinterpret_cast`，它用来实现低级转型，实际动作和结果依赖于编译器，表示它不可移植。例如将指针转换为`int`，此转换是在bit层面的转换，更详细的信息可以参见[cpp reference的介绍](http://en.cppreference.com/w/cpp/language/reinterpret_cast)。\n\n在它们之中，`reinterpret_cast`和`const_cast`完全是编译器层面的东西。`static_cast`会导致编译器生成对应CPU指令，但是是在编译器就能决定的。`dynamic_cast`是在运行时多态的一种手段。\n\n## 28 避免返回指向对象内部成分的句柄（handle）\n当成员函数返回类内部私有成员变量（或者私有成员函数，较少见）的句柄（如指针，引用或迭代器），而且成员变量的资源又存储于对象之外，这时候，虽然可以将此成员函数声明为`const`，但是实际上并不能避免通过此句柄修改资源的情况发生。\n\n例如在自定义的`string`类中，使用堆上的数组储存字符串。如果某个成员函数能够返回字符串数组，那么可以使用这个指针修改数组内的值，而这也是符合`const`约定的。\n\n所以，若无十分必要，不要返回对象内部的句柄。有此需要时，首先考虑是否应返回`const handle&`。\n\n即使这样，还有可能造成返回的句柄比变量本身生命周期更长，也就是句柄所指之物已经被析构，句柄此时成为空悬状态，造成问题。\n\n## 29 为“异常安全”努力是值得的\n异常安全函数是指即使发生异常，也不会泄露资源或者允许任何数据结构被破坏。由于在代码执行过程中，可能发生内存申请失败等等异常，导致我们逻辑上已经设想好的程序控制流被中断，造成内存泄漏（后续的`delete`操作没有执行）。此外，我们希望如果异常发生，变量的值（程序的状态）能够恢复到异常发生之前。\n\n让我们先看一个不满足异常安全的函数例子：\n``` cpp\n// 自定义`Menu`类中修改背景图片的成员函数\nvoid Menu::changeBg(std::istream& imgSrc) {\n    lock(&this->mutex);  // 互斥锁 1\n    delete this->bg;     // 释放本已有的bg2\n    ++this->imgChangeCnt;      // 计数器  3\n    bg = new Image(imgSrc);    // 新图片  4\n    unlock(&this->mutex);      // 释放锁  5\n}\n```\n\n上面的代码存在以下问题，使得它不满足异常安全性。\n\n- 资源泄漏。一旦第4行内存申请失败，那么第五行无法执行，互斥锁永远把持住了。\n- 数据被破坏。还是上面的情况，则`bg`此时的资源已经被析构，而且计数器的值也增加了。·\n\n解决第一个问题，可以考虑使用智能指针，即条款13中的使用对象管理资源。\n\n我们将异常安全分为以下三类：\n\n- 基本型。异常被抛出后，程序内的数据不会被破坏。但是并不保证程序的现实状态（究竟`bg`是何值）\n- 强烈保证。异常抛出后，程序恢复到该函数调用前的状态。copy-and-swap策略是达成这一目标的常见方法。首先为待修改的对象原件做出一份副本，然后在副本上做一切修改。若有任何修改抛出异常，则原件不受影响。待所有修改完成后，再将修改过后的副本和原件在一个不抛出异常的swap操作中交换。\n\n在这里，常常采用pImpl技术，也就是在对象中仅存储资源的（智能）指针，在swap中只操作该指针即可。\n\n- 绝不抛出异常。作用于内置类型（`int`或指针等）身上的所有操作都提供了nothrow保证。\n\n## 30 透彻了解内联的方方面面\n`inline`函数在编译期实现函数本体的替换，避免了函数调用的开销，还可能使得编译器基于上下文进行优化，鼓励使用`inline`替换函数宏定义。\n\n然而，`inline`不要乱用。首先，`inline`会使得目标码体积变大。可能造成额外的换页行为，降低高速缓存的命中概率，反而造成性能的损失。\n\n另一方面，`inline`只是对编译器的申请，不是真的一定内联。\n\n`inline`函数通常定义在头文件中（或者直接定义在类的内部，这样无需加入`inline`关键字），这是因为在编译中编译器需要知道这个函数具体长什么样子，才能够实现内联。\n\n有时候，虽然编译器有意愿内敛某函数，但是还是会为它产生一个函数本体。这常常发生在取某个内联函数地址时。与此并提，编译器通常不对通过函数指针调用的内联函数进行内联。也就是说，是否真的内联，还与函数的调用方式有关。\n\n作者给出的建议是，一开始不要将任何函数内联，之后使用profile工具进行优化。不要忘记28法则，80%的程序执行时间花在了20%的代码上。除非找对了目标，否则优化都是无用功。将内联函数应用于调用频繁且小型的函数身上。\n\n## 31 将文件间的编译依存关系降至最低\nC++的头文件包含机制饱受批评。连串的编译依存关系常常使得项目的编译时间大大加长。\n\n首先，程序库头文件应该“完全且仅有声明式”的存在，将实现代码放入cpp文件中。\n\n另外，之所以C++编译时容易出现“牵一发而动全身”的情况，是因为C++与Java等语言不同。在Java中编译器只分配一个指针指向实际对象，也就不需要知道对象的实际大小。而C++编译器却需要知道对象中每个成员变量的明确定义，才能知道对象的实际大小，从而在内存中分配空间。\n\n从这里出发，我们可以参考Java等语言中的思路，建立一个handle类，在其中包含原来那个类的完全数据，而在新的类中定义一个指向该handle类的指针，这也就是前面所提到的pImpl方法。\n\n使用这种思虑，定义的包含有`Date`类型对象（指明这个人的生日）的`Person`类如下：\n\n``` cpp\n#include <string>   // for string\n#include <memory>   // for shared_ptr\n\nclass PersonImpl;   // Person实现类的前置声明\nclass Date;         // Person接口用到的类的前置声明\n\nclass Person {\npublic:\n    Person(const std::string& name, const Date& birthday);\n    std::string name() const;\nprivate:\n    std::shared_ptr<PersonImpl> pImpl;   // 指向实现类\n};\n\n/*****************实现文件****************/\n#include \"Person.h\"\n#include \"PersonImpl.h\"\nPerson::Person(const std::string& name, const Date& birthday):\n    pImpl(new PersonImpl(name, birthday)) {}\n\nstd::string Person::name() const {\n    return pImpl->name();\n}\n```\n\n在上面的代码中，通过构造handle类`PersonImpl`，在`Person`中我们只需要前置声明`Date`，而无需包含头文件`date.hpp`。这样，即使`Date`或者`Person`有修改，影响也仅限于`Date`的实现文件和`PersonImpl`而已，不会传导到`Person`和使用了`Person`的其他代码文件。通过这种做法，实际上`Person`成为了一个单纯的接口，具体的实现在`PersonImpl`中完成，实现了“接口与实现的分离”。\n\n综上：\n\n- 如果使用object pointer或者object reference可以完成任务，就不要使用object。只要前置声明就可以定义出指向该类型的pointer或者reference，但是需要完整地定义式才能定义object。\n- 如果能够，尽量用类的声明式替换定义式 。注意，当声明某个函数而它用到某个类时，你并不需要这个类的定义式。即使函数以pass-by-value方式传递参数（通常情况下这也不是一个好主意）或返回值。\n- 为声明式和定义式提供不同的头文件（`Person`本身和`PersonImpl`）。这两个文件应该保持一致。声明式改变了，需要修改定义式头文件。程序库客户应该包含声明文件。\n\n除了上面的方法，也可以将`Person`定义为抽象基类（Caffe中的`Layer`就是类似的模式）。为了达成这一目标，`Person`需要一个虚构造函数（见条款7）和一系列的纯虚函数（作为接口，等待派生类重写实现）。如下所示：\n\n``` cpp\nclass Person {\npublic:\n    virtual ~Person();\n    virtual string name() const = 0;\n};\n```\n\n客户必须能够为这种类创建对象。通常的做法是调用一个工厂函数，返回派生类的（智能）指针。这样的函数常常在抽象基类中声明为`static`。\n\n``` cpp\nclass Person {\npublic:\n    static shared_ptr<Person> create(const string& name, const Date& birthday);\n// ... 刚才的其他代码\n};\n```\n\n当然，要想使用，我们还必须定义派生类实现相应的接口。\n\n``` cpp\nclass RealPerson: public Person {\npublic:\n    RealPerson(const string& name, const Date& birthday): name(name), birthDate(birthday) {}\n    virtual ~RealPerson() {}\n    string name() const { return this->name; }\nprivate:\n    string name;\n    Date birthDate;\n};\n```\n\n上面的工厂函数`create()`的实现：\n\n``` cpp\nshared_ptr<Person> Person::create(const string& name, const Date& date) {\n    return shared_ptr<Person>(new RealPerson(name, date));\n}\n```\n\n实际应用中的工厂函数会像工厂一样，根据客户需要，产出不同的派生类对象。\n\n当然，使用上述技术增大了程序运行时间开销和内存空间。这需要在工程中分情况讨论。是否这部分的开销大到了需要无视接口实现分离原则的地步？如果是的，那就用具象的类代替他们。但是，不要因噎废食。\n","slug":"effective-cpp-05","published":1,"updated":"2018-01-12T06:22:20.467Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjcu6vct00016qu46ixydzv8h","content":"<p>第四章中探讨了如何更好地提出类的定义和函数声明，精心设计的接口能让后续工作轻松不少。然而如何能够正确高效地实现，也是一件重要的事情。行百里者半九十。</p>\n<a id=\"more\"></a>\n<h2 id=\"26-尽可能延后变量定义式的出现时间\"><a href=\"#26-尽可能延后变量定义式的出现时间\" class=\"headerlink\" title=\"26 尽可能延后变量定义式的出现时间\"></a>26 尽可能延后变量定义式的出现时间</h2><p>变量，尤其是自定义对象，一旦被定义，就会调用构造函数；一旦生命周期结束，又要调用析构函数。所以，延后变量定义的时间，并且最好能够给定恰当的初始值，这样能够提高代码执行效率。</p>\n<h2 id=\"27-尽量少做转型动作\"><a href=\"#27-尽量少做转型动作\" class=\"headerlink\" title=\"27 尽量少做转型动作\"></a>27 尽量少做转型动作</h2><p>第一，安全考虑。C++的类型系统保证类型错误不会发生。理论上如果你的代码“很干净”地通过编译，就表明它不意图在任何对象上执行不安全和无意义的操作，这是一个很有价值的保险。不要轻易打破。</p>\n<p>第二，效率问题。这里展开说。</p>\n<p>C++中的转型动作有以下三种：</p>\n<ul>\n<li><code>(T)expression</code>，C时代的风格</li>\n<li><code>T(expression)</code>，函数风格</li>\n<li>新式风格，包括<code>static_cast</code>,<code>const_cast</code>,<code>dynamic_cast</code>和<code>reinterpret_cast</code>四种。</li>\n</ul>\n<p>作者不提倡使用两种旧风格，而使用下面的四种。一种理由是它们容易被自动化代码检查工具匹配检查。</p>\n<p>对于 <code>const_cast</code>，用于脱掉某对象的<code>const</code>属性。</p>\n<p>对于<code>static_cast</code>，用于进行强制类型转换。例如将non-const类型转换为const类型，或者将<code>double</code>类型转换为<code>int</code>等。</p>\n<p>对于<code>dynamic_cast</code>，用于执行安全向下转换，也就是用于决定某对象是否归属继承体系中的某个类型。注意，<code>dynamic_cast</code>的很多实现都很慢，尤其是继承深度较深时，也是这几个里面唯一可能造成重大（注意，并非其他三者不会带来执行时间开销）运行成本的动作。</p>\n<p>很多时候，之所以使用<code>dynamic_cast</code>是因为你想在一个（你认为是）某个派生类对象中执行派生类中（并非从基类继承）的成员函数，但你的手上只有指向这个派生类对象的base指针或引用。这种情况下，也许将派生类的这个函数在基类中也定义一个空函数体的函数，再由派生类重写可能更好。</p>\n<p>对于<code>reinterpret_cast</code>，它用来实现低级转型，实际动作和结果依赖于编译器，表示它不可移植。例如将指针转换为<code>int</code>，此转换是在bit层面的转换，更详细的信息可以参见<a href=\"http://en.cppreference.com/w/cpp/language/reinterpret_cast\" target=\"_blank\" rel=\"external\">cpp reference的介绍</a>。</p>\n<p>在它们之中，<code>reinterpret_cast</code>和<code>const_cast</code>完全是编译器层面的东西。<code>static_cast</code>会导致编译器生成对应CPU指令，但是是在编译器就能决定的。<code>dynamic_cast</code>是在运行时多态的一种手段。</p>\n<h2 id=\"28-避免返回指向对象内部成分的句柄（handle）\"><a href=\"#28-避免返回指向对象内部成分的句柄（handle）\" class=\"headerlink\" title=\"28 避免返回指向对象内部成分的句柄（handle）\"></a>28 避免返回指向对象内部成分的句柄（handle）</h2><p>当成员函数返回类内部私有成员变量（或者私有成员函数，较少见）的句柄（如指针，引用或迭代器），而且成员变量的资源又存储于对象之外，这时候，虽然可以将此成员函数声明为<code>const</code>，但是实际上并不能避免通过此句柄修改资源的情况发生。</p>\n<p>例如在自定义的<code>string</code>类中，使用堆上的数组储存字符串。如果某个成员函数能够返回字符串数组，那么可以使用这个指针修改数组内的值，而这也是符合<code>const</code>约定的。</p>\n<p>所以，若无十分必要，不要返回对象内部的句柄。有此需要时，首先考虑是否应返回<code>const handle&amp;</code>。</p>\n<p>即使这样，还有可能造成返回的句柄比变量本身生命周期更长，也就是句柄所指之物已经被析构，句柄此时成为空悬状态，造成问题。</p>\n<h2 id=\"29-为“异常安全”努力是值得的\"><a href=\"#29-为“异常安全”努力是值得的\" class=\"headerlink\" title=\"29 为“异常安全”努力是值得的\"></a>29 为“异常安全”努力是值得的</h2><p>异常安全函数是指即使发生异常，也不会泄露资源或者允许任何数据结构被破坏。由于在代码执行过程中，可能发生内存申请失败等等异常，导致我们逻辑上已经设想好的程序控制流被中断，造成内存泄漏（后续的<code>delete</code>操作没有执行）。此外，我们希望如果异常发生，变量的值（程序的状态）能够恢复到异常发生之前。</p>\n<p>让我们先看一个不满足异常安全的函数例子：<br><figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\">// 自定义`Menu`类中修改背景图片的成员函数</span></div><div class=\"line\"><span class=\"keyword\">void</span> Menu::changeBg(<span class=\"built_in\">std</span>::istream&amp; imgSrc) &#123;</div><div class=\"line\">    lock(&amp;<span class=\"keyword\">this</span>-&gt;mutex);  <span class=\"comment\">// 互斥锁 1</span></div><div class=\"line\">    <span class=\"keyword\">delete</span> <span class=\"keyword\">this</span>-&gt;bg;     <span class=\"comment\">// 释放本已有的bg2</span></div><div class=\"line\">    ++<span class=\"keyword\">this</span>-&gt;imgChangeCnt;      <span class=\"comment\">// 计数器  3</span></div><div class=\"line\">    bg = <span class=\"keyword\">new</span> Image(imgSrc);    <span class=\"comment\">// 新图片  4</span></div><div class=\"line\">    unlock(&amp;<span class=\"keyword\">this</span>-&gt;mutex);      <span class=\"comment\">// 释放锁  5</span></div><div class=\"line\">&#125;</div></pre></td></tr></table></figure></p>\n<p>上面的代码存在以下问题，使得它不满足异常安全性。</p>\n<ul>\n<li>资源泄漏。一旦第4行内存申请失败，那么第五行无法执行，互斥锁永远把持住了。</li>\n<li>数据被破坏。还是上面的情况，则<code>bg</code>此时的资源已经被析构，而且计数器的值也增加了。·</li>\n</ul>\n<p>解决第一个问题，可以考虑使用智能指针，即条款13中的使用对象管理资源。</p>\n<p>我们将异常安全分为以下三类：</p>\n<ul>\n<li>基本型。异常被抛出后，程序内的数据不会被破坏。但是并不保证程序的现实状态（究竟<code>bg</code>是何值）</li>\n<li>强烈保证。异常抛出后，程序恢复到该函数调用前的状态。copy-and-swap策略是达成这一目标的常见方法。首先为待修改的对象原件做出一份副本，然后在副本上做一切修改。若有任何修改抛出异常，则原件不受影响。待所有修改完成后，再将修改过后的副本和原件在一个不抛出异常的swap操作中交换。</li>\n</ul>\n<p>在这里，常常采用pImpl技术，也就是在对象中仅存储资源的（智能）指针，在swap中只操作该指针即可。</p>\n<ul>\n<li>绝不抛出异常。作用于内置类型（<code>int</code>或指针等）身上的所有操作都提供了nothrow保证。</li>\n</ul>\n<h2 id=\"30-透彻了解内联的方方面面\"><a href=\"#30-透彻了解内联的方方面面\" class=\"headerlink\" title=\"30 透彻了解内联的方方面面\"></a>30 透彻了解内联的方方面面</h2><p><code>inline</code>函数在编译期实现函数本体的替换，避免了函数调用的开销，还可能使得编译器基于上下文进行优化，鼓励使用<code>inline</code>替换函数宏定义。</p>\n<p>然而，<code>inline</code>不要乱用。首先，<code>inline</code>会使得目标码体积变大。可能造成额外的换页行为，降低高速缓存的命中概率，反而造成性能的损失。</p>\n<p>另一方面，<code>inline</code>只是对编译器的申请，不是真的一定内联。</p>\n<p><code>inline</code>函数通常定义在头文件中（或者直接定义在类的内部，这样无需加入<code>inline</code>关键字），这是因为在编译中编译器需要知道这个函数具体长什么样子，才能够实现内联。</p>\n<p>有时候，虽然编译器有意愿内敛某函数，但是还是会为它产生一个函数本体。这常常发生在取某个内联函数地址时。与此并提，编译器通常不对通过函数指针调用的内联函数进行内联。也就是说，是否真的内联，还与函数的调用方式有关。</p>\n<p>作者给出的建议是，一开始不要将任何函数内联，之后使用profile工具进行优化。不要忘记28法则，80%的程序执行时间花在了20%的代码上。除非找对了目标，否则优化都是无用功。将内联函数应用于调用频繁且小型的函数身上。</p>\n<h2 id=\"31-将文件间的编译依存关系降至最低\"><a href=\"#31-将文件间的编译依存关系降至最低\" class=\"headerlink\" title=\"31 将文件间的编译依存关系降至最低\"></a>31 将文件间的编译依存关系降至最低</h2><p>C++的头文件包含机制饱受批评。连串的编译依存关系常常使得项目的编译时间大大加长。</p>\n<p>首先，程序库头文件应该“完全且仅有声明式”的存在，将实现代码放入cpp文件中。</p>\n<p>另外，之所以C++编译时容易出现“牵一发而动全身”的情况，是因为C++与Java等语言不同。在Java中编译器只分配一个指针指向实际对象，也就不需要知道对象的实际大小。而C++编译器却需要知道对象中每个成员变量的明确定义，才能知道对象的实际大小，从而在内存中分配空间。</p>\n<p>从这里出发，我们可以参考Java等语言中的思路，建立一个handle类，在其中包含原来那个类的完全数据，而在新的类中定义一个指向该handle类的指针，这也就是前面所提到的pImpl方法。</p>\n<p>使用这种思虑，定义的包含有<code>Date</code>类型对象（指明这个人的生日）的<code>Person</code>类如下：</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">include</span> <span class=\"meta-string\">&lt;string&gt;</span>   <span class=\"comment\">// for string</span></span></div><div class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">include</span> <span class=\"meta-string\">&lt;memory&gt;</span>   <span class=\"comment\">// for shared_ptr</span></span></div><div class=\"line\"></div><div class=\"line\"><span class=\"keyword\">class</span> PersonImpl;   <span class=\"comment\">// Person实现类的前置声明</span></div><div class=\"line\"><span class=\"keyword\">class</span> Date;         <span class=\"comment\">// Person接口用到的类的前置声明</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"keyword\">class</span> Person &#123;</div><div class=\"line\"><span class=\"keyword\">public</span>:</div><div class=\"line\">    Person(<span class=\"keyword\">const</span> <span class=\"built_in\">std</span>::<span class=\"built_in\">string</span>&amp; name, <span class=\"keyword\">const</span> Date&amp; birthday);</div><div class=\"line\">    <span class=\"built_in\">std</span>::<span class=\"function\"><span class=\"built_in\">string</span> <span class=\"title\">name</span><span class=\"params\">()</span> <span class=\"keyword\">const</span></span>;</div><div class=\"line\"><span class=\"keyword\">private</span>:</div><div class=\"line\">    <span class=\"built_in\">std</span>::<span class=\"built_in\">shared_ptr</span>&lt;PersonImpl&gt; pImpl;   <span class=\"comment\">// 指向实现类</span></div><div class=\"line\">&#125;;</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">/*****************实现文件****************/</span></div><div class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">include</span> <span class=\"meta-string\">\"Person.h\"</span></span></div><div class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">include</span> <span class=\"meta-string\">\"PersonImpl.h\"</span></span></div><div class=\"line\">Person::Person(<span class=\"keyword\">const</span> <span class=\"built_in\">std</span>::<span class=\"built_in\">string</span>&amp; name, <span class=\"keyword\">const</span> Date&amp; birthday):</div><div class=\"line\">    pImpl(<span class=\"keyword\">new</span> PersonImpl(name, birthday)) &#123;&#125;</div><div class=\"line\"></div><div class=\"line\"><span class=\"built_in\">std</span>::<span class=\"built_in\">string</span> Person::name() <span class=\"keyword\">const</span> &#123;</div><div class=\"line\">    <span class=\"keyword\">return</span> pImpl-&gt;name();</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<p>在上面的代码中，通过构造handle类<code>PersonImpl</code>，在<code>Person</code>中我们只需要前置声明<code>Date</code>，而无需包含头文件<code>date.hpp</code>。这样，即使<code>Date</code>或者<code>Person</code>有修改，影响也仅限于<code>Date</code>的实现文件和<code>PersonImpl</code>而已，不会传导到<code>Person</code>和使用了<code>Person</code>的其他代码文件。通过这种做法，实际上<code>Person</code>成为了一个单纯的接口，具体的实现在<code>PersonImpl</code>中完成，实现了“接口与实现的分离”。</p>\n<p>综上：</p>\n<ul>\n<li>如果使用object pointer或者object reference可以完成任务，就不要使用object。只要前置声明就可以定义出指向该类型的pointer或者reference，但是需要完整地定义式才能定义object。</li>\n<li>如果能够，尽量用类的声明式替换定义式 。注意，当声明某个函数而它用到某个类时，你并不需要这个类的定义式。即使函数以pass-by-value方式传递参数（通常情况下这也不是一个好主意）或返回值。</li>\n<li>为声明式和定义式提供不同的头文件（<code>Person</code>本身和<code>PersonImpl</code>）。这两个文件应该保持一致。声明式改变了，需要修改定义式头文件。程序库客户应该包含声明文件。</li>\n</ul>\n<p>除了上面的方法，也可以将<code>Person</code>定义为抽象基类（Caffe中的<code>Layer</code>就是类似的模式）。为了达成这一目标，<code>Person</code>需要一个虚构造函数（见条款7）和一系列的纯虚函数（作为接口，等待派生类重写实现）。如下所示：</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">class</span> Person &#123;</div><div class=\"line\"><span class=\"keyword\">public</span>:</div><div class=\"line\">    <span class=\"keyword\">virtual</span> ~Person();</div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">virtual</span> <span class=\"built_in\">string</span> <span class=\"title\">name</span><span class=\"params\">()</span> <span class=\"keyword\">const</span> </span>= <span class=\"number\">0</span>;</div><div class=\"line\">&#125;;</div></pre></td></tr></table></figure>\n<p>客户必须能够为这种类创建对象。通常的做法是调用一个工厂函数，返回派生类的（智能）指针。这样的函数常常在抽象基类中声明为<code>static</code>。</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">class</span> Person &#123;</div><div class=\"line\"><span class=\"keyword\">public</span>:</div><div class=\"line\">    <span class=\"keyword\">static</span> <span class=\"built_in\">shared_ptr</span>&lt;Person&gt; create(<span class=\"keyword\">const</span> <span class=\"built_in\">string</span>&amp; name, <span class=\"keyword\">const</span> Date&amp; birthday);</div><div class=\"line\"><span class=\"comment\">// ... 刚才的其他代码</span></div><div class=\"line\">&#125;;</div></pre></td></tr></table></figure>\n<p>当然，要想使用，我们还必须定义派生类实现相应的接口。</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">class</span> RealPerson: <span class=\"keyword\">public</span> Person &#123;</div><div class=\"line\"><span class=\"keyword\">public</span>:</div><div class=\"line\">    RealPerson(<span class=\"keyword\">const</span> <span class=\"built_in\">string</span>&amp; name, <span class=\"keyword\">const</span> Date&amp; birthday): name(name), birthDate(birthday) &#123;&#125;</div><div class=\"line\">    <span class=\"keyword\">virtual</span> ~RealPerson() &#123;&#125;</div><div class=\"line\">    <span class=\"function\"><span class=\"built_in\">string</span> <span class=\"title\">name</span><span class=\"params\">()</span> <span class=\"keyword\">const</span> </span>&#123; <span class=\"keyword\">return</span> <span class=\"keyword\">this</span>-&gt;name; &#125;</div><div class=\"line\"><span class=\"keyword\">private</span>:</div><div class=\"line\">    <span class=\"built_in\">string</span> name;</div><div class=\"line\">    Date birthDate;</div><div class=\"line\">&#125;;</div></pre></td></tr></table></figure>\n<p>上面的工厂函数<code>create()</code>的实现：</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"built_in\">shared_ptr</span>&lt;Person&gt; Person::create(<span class=\"keyword\">const</span> <span class=\"built_in\">string</span>&amp; name, <span class=\"keyword\">const</span> Date&amp; date) &#123;</div><div class=\"line\">    <span class=\"keyword\">return</span> <span class=\"built_in\">shared_ptr</span>&lt;Person&gt;(<span class=\"keyword\">new</span> RealPerson(name, date));</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<p>实际应用中的工厂函数会像工厂一样，根据客户需要，产出不同的派生类对象。</p>\n<p>当然，使用上述技术增大了程序运行时间开销和内存空间。这需要在工程中分情况讨论。是否这部分的开销大到了需要无视接口实现分离原则的地步？如果是的，那就用具象的类代替他们。但是，不要因噎废食。</p>\n","excerpt":"<p>第四章中探讨了如何更好地提出类的定义和函数声明，精心设计的接口能让后续工作轻松不少。然而如何能够正确高效地实现，也是一件重要的事情。行百里者半九十。</p>","more":"<h2 id=\"26-尽可能延后变量定义式的出现时间\"><a href=\"#26-尽可能延后变量定义式的出现时间\" class=\"headerlink\" title=\"26 尽可能延后变量定义式的出现时间\"></a>26 尽可能延后变量定义式的出现时间</h2><p>变量，尤其是自定义对象，一旦被定义，就会调用构造函数；一旦生命周期结束，又要调用析构函数。所以，延后变量定义的时间，并且最好能够给定恰当的初始值，这样能够提高代码执行效率。</p>\n<h2 id=\"27-尽量少做转型动作\"><a href=\"#27-尽量少做转型动作\" class=\"headerlink\" title=\"27 尽量少做转型动作\"></a>27 尽量少做转型动作</h2><p>第一，安全考虑。C++的类型系统保证类型错误不会发生。理论上如果你的代码“很干净”地通过编译，就表明它不意图在任何对象上执行不安全和无意义的操作，这是一个很有价值的保险。不要轻易打破。</p>\n<p>第二，效率问题。这里展开说。</p>\n<p>C++中的转型动作有以下三种：</p>\n<ul>\n<li><code>(T)expression</code>，C时代的风格</li>\n<li><code>T(expression)</code>，函数风格</li>\n<li>新式风格，包括<code>static_cast</code>,<code>const_cast</code>,<code>dynamic_cast</code>和<code>reinterpret_cast</code>四种。</li>\n</ul>\n<p>作者不提倡使用两种旧风格，而使用下面的四种。一种理由是它们容易被自动化代码检查工具匹配检查。</p>\n<p>对于 <code>const_cast</code>，用于脱掉某对象的<code>const</code>属性。</p>\n<p>对于<code>static_cast</code>，用于进行强制类型转换。例如将non-const类型转换为const类型，或者将<code>double</code>类型转换为<code>int</code>等。</p>\n<p>对于<code>dynamic_cast</code>，用于执行安全向下转换，也就是用于决定某对象是否归属继承体系中的某个类型。注意，<code>dynamic_cast</code>的很多实现都很慢，尤其是继承深度较深时，也是这几个里面唯一可能造成重大（注意，并非其他三者不会带来执行时间开销）运行成本的动作。</p>\n<p>很多时候，之所以使用<code>dynamic_cast</code>是因为你想在一个（你认为是）某个派生类对象中执行派生类中（并非从基类继承）的成员函数，但你的手上只有指向这个派生类对象的base指针或引用。这种情况下，也许将派生类的这个函数在基类中也定义一个空函数体的函数，再由派生类重写可能更好。</p>\n<p>对于<code>reinterpret_cast</code>，它用来实现低级转型，实际动作和结果依赖于编译器，表示它不可移植。例如将指针转换为<code>int</code>，此转换是在bit层面的转换，更详细的信息可以参见<a href=\"http://en.cppreference.com/w/cpp/language/reinterpret_cast\">cpp reference的介绍</a>。</p>\n<p>在它们之中，<code>reinterpret_cast</code>和<code>const_cast</code>完全是编译器层面的东西。<code>static_cast</code>会导致编译器生成对应CPU指令，但是是在编译器就能决定的。<code>dynamic_cast</code>是在运行时多态的一种手段。</p>\n<h2 id=\"28-避免返回指向对象内部成分的句柄（handle）\"><a href=\"#28-避免返回指向对象内部成分的句柄（handle）\" class=\"headerlink\" title=\"28 避免返回指向对象内部成分的句柄（handle）\"></a>28 避免返回指向对象内部成分的句柄（handle）</h2><p>当成员函数返回类内部私有成员变量（或者私有成员函数，较少见）的句柄（如指针，引用或迭代器），而且成员变量的资源又存储于对象之外，这时候，虽然可以将此成员函数声明为<code>const</code>，但是实际上并不能避免通过此句柄修改资源的情况发生。</p>\n<p>例如在自定义的<code>string</code>类中，使用堆上的数组储存字符串。如果某个成员函数能够返回字符串数组，那么可以使用这个指针修改数组内的值，而这也是符合<code>const</code>约定的。</p>\n<p>所以，若无十分必要，不要返回对象内部的句柄。有此需要时，首先考虑是否应返回<code>const handle&amp;</code>。</p>\n<p>即使这样，还有可能造成返回的句柄比变量本身生命周期更长，也就是句柄所指之物已经被析构，句柄此时成为空悬状态，造成问题。</p>\n<h2 id=\"29-为“异常安全”努力是值得的\"><a href=\"#29-为“异常安全”努力是值得的\" class=\"headerlink\" title=\"29 为“异常安全”努力是值得的\"></a>29 为“异常安全”努力是值得的</h2><p>异常安全函数是指即使发生异常，也不会泄露资源或者允许任何数据结构被破坏。由于在代码执行过程中，可能发生内存申请失败等等异常，导致我们逻辑上已经设想好的程序控制流被中断，造成内存泄漏（后续的<code>delete</code>操作没有执行）。此外，我们希望如果异常发生，变量的值（程序的状态）能够恢复到异常发生之前。</p>\n<p>让我们先看一个不满足异常安全的函数例子：<br><figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\">// 自定义`Menu`类中修改背景图片的成员函数</span></div><div class=\"line\"><span class=\"keyword\">void</span> Menu::changeBg(<span class=\"built_in\">std</span>::istream&amp; imgSrc) &#123;</div><div class=\"line\">    lock(&amp;<span class=\"keyword\">this</span>-&gt;mutex);  <span class=\"comment\">// 互斥锁 1</span></div><div class=\"line\">    <span class=\"keyword\">delete</span> <span class=\"keyword\">this</span>-&gt;bg;     <span class=\"comment\">// 释放本已有的bg2</span></div><div class=\"line\">    ++<span class=\"keyword\">this</span>-&gt;imgChangeCnt;      <span class=\"comment\">// 计数器  3</span></div><div class=\"line\">    bg = <span class=\"keyword\">new</span> Image(imgSrc);    <span class=\"comment\">// 新图片  4</span></div><div class=\"line\">    unlock(&amp;<span class=\"keyword\">this</span>-&gt;mutex);      <span class=\"comment\">// 释放锁  5</span></div><div class=\"line\">&#125;</div></pre></td></tr></table></figure></p>\n<p>上面的代码存在以下问题，使得它不满足异常安全性。</p>\n<ul>\n<li>资源泄漏。一旦第4行内存申请失败，那么第五行无法执行，互斥锁永远把持住了。</li>\n<li>数据被破坏。还是上面的情况，则<code>bg</code>此时的资源已经被析构，而且计数器的值也增加了。·</li>\n</ul>\n<p>解决第一个问题，可以考虑使用智能指针，即条款13中的使用对象管理资源。</p>\n<p>我们将异常安全分为以下三类：</p>\n<ul>\n<li>基本型。异常被抛出后，程序内的数据不会被破坏。但是并不保证程序的现实状态（究竟<code>bg</code>是何值）</li>\n<li>强烈保证。异常抛出后，程序恢复到该函数调用前的状态。copy-and-swap策略是达成这一目标的常见方法。首先为待修改的对象原件做出一份副本，然后在副本上做一切修改。若有任何修改抛出异常，则原件不受影响。待所有修改完成后，再将修改过后的副本和原件在一个不抛出异常的swap操作中交换。</li>\n</ul>\n<p>在这里，常常采用pImpl技术，也就是在对象中仅存储资源的（智能）指针，在swap中只操作该指针即可。</p>\n<ul>\n<li>绝不抛出异常。作用于内置类型（<code>int</code>或指针等）身上的所有操作都提供了nothrow保证。</li>\n</ul>\n<h2 id=\"30-透彻了解内联的方方面面\"><a href=\"#30-透彻了解内联的方方面面\" class=\"headerlink\" title=\"30 透彻了解内联的方方面面\"></a>30 透彻了解内联的方方面面</h2><p><code>inline</code>函数在编译期实现函数本体的替换，避免了函数调用的开销，还可能使得编译器基于上下文进行优化，鼓励使用<code>inline</code>替换函数宏定义。</p>\n<p>然而，<code>inline</code>不要乱用。首先，<code>inline</code>会使得目标码体积变大。可能造成额外的换页行为，降低高速缓存的命中概率，反而造成性能的损失。</p>\n<p>另一方面，<code>inline</code>只是对编译器的申请，不是真的一定内联。</p>\n<p><code>inline</code>函数通常定义在头文件中（或者直接定义在类的内部，这样无需加入<code>inline</code>关键字），这是因为在编译中编译器需要知道这个函数具体长什么样子，才能够实现内联。</p>\n<p>有时候，虽然编译器有意愿内敛某函数，但是还是会为它产生一个函数本体。这常常发生在取某个内联函数地址时。与此并提，编译器通常不对通过函数指针调用的内联函数进行内联。也就是说，是否真的内联，还与函数的调用方式有关。</p>\n<p>作者给出的建议是，一开始不要将任何函数内联，之后使用profile工具进行优化。不要忘记28法则，80%的程序执行时间花在了20%的代码上。除非找对了目标，否则优化都是无用功。将内联函数应用于调用频繁且小型的函数身上。</p>\n<h2 id=\"31-将文件间的编译依存关系降至最低\"><a href=\"#31-将文件间的编译依存关系降至最低\" class=\"headerlink\" title=\"31 将文件间的编译依存关系降至最低\"></a>31 将文件间的编译依存关系降至最低</h2><p>C++的头文件包含机制饱受批评。连串的编译依存关系常常使得项目的编译时间大大加长。</p>\n<p>首先，程序库头文件应该“完全且仅有声明式”的存在，将实现代码放入cpp文件中。</p>\n<p>另外，之所以C++编译时容易出现“牵一发而动全身”的情况，是因为C++与Java等语言不同。在Java中编译器只分配一个指针指向实际对象，也就不需要知道对象的实际大小。而C++编译器却需要知道对象中每个成员变量的明确定义，才能知道对象的实际大小，从而在内存中分配空间。</p>\n<p>从这里出发，我们可以参考Java等语言中的思路，建立一个handle类，在其中包含原来那个类的完全数据，而在新的类中定义一个指向该handle类的指针，这也就是前面所提到的pImpl方法。</p>\n<p>使用这种思虑，定义的包含有<code>Date</code>类型对象（指明这个人的生日）的<code>Person</code>类如下：</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">include</span> <span class=\"meta-string\">&lt;string&gt;</span>   <span class=\"comment\">// for string</span></span></div><div class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">include</span> <span class=\"meta-string\">&lt;memory&gt;</span>   <span class=\"comment\">// for shared_ptr</span></span></div><div class=\"line\"></div><div class=\"line\"><span class=\"keyword\">class</span> PersonImpl;   <span class=\"comment\">// Person实现类的前置声明</span></div><div class=\"line\"><span class=\"keyword\">class</span> Date;         <span class=\"comment\">// Person接口用到的类的前置声明</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"keyword\">class</span> Person &#123;</div><div class=\"line\"><span class=\"keyword\">public</span>:</div><div class=\"line\">    Person(<span class=\"keyword\">const</span> <span class=\"built_in\">std</span>::<span class=\"built_in\">string</span>&amp; name, <span class=\"keyword\">const</span> Date&amp; birthday);</div><div class=\"line\">    <span class=\"built_in\">std</span>::<span class=\"function\"><span class=\"built_in\">string</span> <span class=\"title\">name</span><span class=\"params\">()</span> <span class=\"keyword\">const</span></span>;</div><div class=\"line\"><span class=\"keyword\">private</span>:</div><div class=\"line\">    <span class=\"built_in\">std</span>::<span class=\"built_in\">shared_ptr</span>&lt;PersonImpl&gt; pImpl;   <span class=\"comment\">// 指向实现类</span></div><div class=\"line\">&#125;;</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">/*****************实现文件****************/</span></div><div class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">include</span> <span class=\"meta-string\">\"Person.h\"</span></span></div><div class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">include</span> <span class=\"meta-string\">\"PersonImpl.h\"</span></span></div><div class=\"line\">Person::Person(<span class=\"keyword\">const</span> <span class=\"built_in\">std</span>::<span class=\"built_in\">string</span>&amp; name, <span class=\"keyword\">const</span> Date&amp; birthday):</div><div class=\"line\">    pImpl(<span class=\"keyword\">new</span> PersonImpl(name, birthday)) &#123;&#125;</div><div class=\"line\"></div><div class=\"line\"><span class=\"built_in\">std</span>::<span class=\"built_in\">string</span> Person::name() <span class=\"keyword\">const</span> &#123;</div><div class=\"line\">    <span class=\"keyword\">return</span> pImpl-&gt;name();</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<p>在上面的代码中，通过构造handle类<code>PersonImpl</code>，在<code>Person</code>中我们只需要前置声明<code>Date</code>，而无需包含头文件<code>date.hpp</code>。这样，即使<code>Date</code>或者<code>Person</code>有修改，影响也仅限于<code>Date</code>的实现文件和<code>PersonImpl</code>而已，不会传导到<code>Person</code>和使用了<code>Person</code>的其他代码文件。通过这种做法，实际上<code>Person</code>成为了一个单纯的接口，具体的实现在<code>PersonImpl</code>中完成，实现了“接口与实现的分离”。</p>\n<p>综上：</p>\n<ul>\n<li>如果使用object pointer或者object reference可以完成任务，就不要使用object。只要前置声明就可以定义出指向该类型的pointer或者reference，但是需要完整地定义式才能定义object。</li>\n<li>如果能够，尽量用类的声明式替换定义式 。注意，当声明某个函数而它用到某个类时，你并不需要这个类的定义式。即使函数以pass-by-value方式传递参数（通常情况下这也不是一个好主意）或返回值。</li>\n<li>为声明式和定义式提供不同的头文件（<code>Person</code>本身和<code>PersonImpl</code>）。这两个文件应该保持一致。声明式改变了，需要修改定义式头文件。程序库客户应该包含声明文件。</li>\n</ul>\n<p>除了上面的方法，也可以将<code>Person</code>定义为抽象基类（Caffe中的<code>Layer</code>就是类似的模式）。为了达成这一目标，<code>Person</code>需要一个虚构造函数（见条款7）和一系列的纯虚函数（作为接口，等待派生类重写实现）。如下所示：</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">class</span> Person &#123;</div><div class=\"line\"><span class=\"keyword\">public</span>:</div><div class=\"line\">    <span class=\"keyword\">virtual</span> ~Person();</div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">virtual</span> <span class=\"built_in\">string</span> <span class=\"title\">name</span><span class=\"params\">()</span> <span class=\"keyword\">const</span> </span>= <span class=\"number\">0</span>;</div><div class=\"line\">&#125;;</div></pre></td></tr></table></figure>\n<p>客户必须能够为这种类创建对象。通常的做法是调用一个工厂函数，返回派生类的（智能）指针。这样的函数常常在抽象基类中声明为<code>static</code>。</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">class</span> Person &#123;</div><div class=\"line\"><span class=\"keyword\">public</span>:</div><div class=\"line\">    <span class=\"keyword\">static</span> <span class=\"built_in\">shared_ptr</span>&lt;Person&gt; create(<span class=\"keyword\">const</span> <span class=\"built_in\">string</span>&amp; name, <span class=\"keyword\">const</span> Date&amp; birthday);</div><div class=\"line\"><span class=\"comment\">// ... 刚才的其他代码</span></div><div class=\"line\">&#125;;</div></pre></td></tr></table></figure>\n<p>当然，要想使用，我们还必须定义派生类实现相应的接口。</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">class</span> RealPerson: <span class=\"keyword\">public</span> Person &#123;</div><div class=\"line\"><span class=\"keyword\">public</span>:</div><div class=\"line\">    RealPerson(<span class=\"keyword\">const</span> <span class=\"built_in\">string</span>&amp; name, <span class=\"keyword\">const</span> Date&amp; birthday): name(name), birthDate(birthday) &#123;&#125;</div><div class=\"line\">    <span class=\"keyword\">virtual</span> ~RealPerson() &#123;&#125;</div><div class=\"line\">    <span class=\"function\"><span class=\"built_in\">string</span> <span class=\"title\">name</span><span class=\"params\">()</span> <span class=\"keyword\">const</span> </span>&#123; <span class=\"keyword\">return</span> <span class=\"keyword\">this</span>-&gt;name; &#125;</div><div class=\"line\"><span class=\"keyword\">private</span>:</div><div class=\"line\">    <span class=\"built_in\">string</span> name;</div><div class=\"line\">    Date birthDate;</div><div class=\"line\">&#125;;</div></pre></td></tr></table></figure>\n<p>上面的工厂函数<code>create()</code>的实现：</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"built_in\">shared_ptr</span>&lt;Person&gt; Person::create(<span class=\"keyword\">const</span> <span class=\"built_in\">string</span>&amp; name, <span class=\"keyword\">const</span> Date&amp; date) &#123;</div><div class=\"line\">    <span class=\"keyword\">return</span> <span class=\"built_in\">shared_ptr</span>&lt;Person&gt;(<span class=\"keyword\">new</span> RealPerson(name, date));</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<p>实际应用中的工厂函数会像工厂一样，根据客户需要，产出不同的派生类对象。</p>\n<p>当然，使用上述技术增大了程序运行时间开销和内存空间。这需要在工程中分情况讨论。是否这部分的开销大到了需要无视接口实现分离原则的地步？如果是的，那就用具象的类代替他们。但是，不要因噎废食。</p>"},{"title":"使用 Visual Studio 编译 GSL 科学计算库","date":"2016-12-16T11:00:00.000Z","_content":"\nGSL是一个GNU支持的科学计算库，提供了很丰富的数值计算方法。本文介绍了GSL库在Windows环境下使用VisualStudio进行编译构建的过程。\n\n![GSL is GNU Sentific Library](/img/gsl_picture.jpg)\n<!-- more -->\n[GSL 的项目主页](http://www.gnu.org/software/gsl/)提供的说明来看，GSL支持如下的科学计算：\n\n\n（下面的这张表格的HTML使用的是[No-Cruft Excel to HTML Table Converter](http://pressbin.com/tools/excel_to_html_table/index.html)生成的）\n{% raw %}\n<table>\n   <tr>\n      <td>Complex Numbers </td>\n      <td>Roots of Polynomials</td>\n   </tr>\n   <tr>\n      <td>Special Functions </td>\n      <td>Vectors and Matrices</td>\n   </tr>\n   <tr>\n      <td>Permutations </td>\n      <td>Sorting</td>\n   </tr>\n   <tr>\n      <td>BLAS Support </td>\n      <td>Linear Algebra</td>\n   </tr>\n   <tr>\n      <td>Eigensystems </td>\n      <td>Fast Fourier Transforms</td>\n   </tr>\n   <tr>\n      <td>Quadrature </td>\n      <td>Random Numbers</td>\n   </tr>\n   <tr>\n      <td>Quasi-Random Sequences </td>\n      <td>Random Distributions</td>\n   </tr>\n   <tr>\n      <td>Statistics </td>\n      <td>Histograms</td>\n   </tr>\n   <tr>\n      <td>N-Tuples </td>\n      <td>Monte Carlo Integration</td>\n   </tr>\n   <tr>\n      <td>Simulated Annealing </td>\n      <td>Differential Equations</td>\n   </tr>\n   <tr>\n      <td>Interpolation </td>\n      <td>Numerical Differentiation</td>\n   </tr>\n   <tr>\n      <td>Chebyshev Approximation </td>\n      <td>Series Acceleration</td>\n   </tr>\n   <tr>\n      <td>Discrete Hankel Transforms </td>\n      <td>Root-Finding</td>\n   </tr>\n   <tr>\n      <td>Minimization </td>\n      <td>Least-Squares Fitting</td>\n   </tr>\n   <tr>\n      <td>Physical Constants </td>\n      <td>IEEE Floating-Point</td>\n   </tr>\n   <tr>\n      <td>Discrete Wavelet Transforms </td>\n      <td>Basis splines</td>\n   </tr>\n</table>\n{% endraw %}\n\nGSL的Linux下的配置很简单，照着它的INSTALL文件一步一步来就可以了。CMAKE大法HAO!\n\n``` bash\n./configure\nmake\nmake install\nmake clean\n```\n\n同样的，GSL也可以在Windows环境下配置，下面记录了如何在Windows环境下使用 Visual Studio 和 CMakeGUI 编译测试GSL。\n\n## 使用CMAKE编译成.SLN文件\n\n打开CMAKEGUI，将输入代码路径选为GSL源代码地址，输出路径设为自己想要的输出路径。点击 “Configure“，选择Visual Studio2013为编译器，点击Finish后会进行必要的配置。然后将表格里面的选项都打上勾，再次点击”Configure“，等待完成之后点击”Generate“。完成之后，就可以在输出路径下看到GSL.sln文件了。\n\n## 使用Visual Studio生成解决方案\n\n使用 Visual Studio 打开刚才生成的.SLN文件，分别在Debug和Release模式下生成解决方案，等待完成即可。\n\n当完成后，你应该可以在路径下看到这样一张图，我们主要关注的文件夹是\\bin，\\gsl，\\Debug和\\Release。\n\n\n## 加入环境变量\n\n修改环境变量的Path，将\\GSL_Build_Path\\bin\\Debug加入，这主要是为了\\Debug文件夹下面的gsl.dll文件。如果不进行这一步的话，一会虽然可以编译，但是却不能运行。\n\n这里顺便注释一句，当使用第三方库的时候，如果需要动态链接库的支持，其中一种方法就是将DLL文件的路径加入到Path中去。\n\n## 建立Visual Studio属性表\n\nVisual Studio可以通过建立工程属性表的方法来配置工程选项，一个OpenCV的例子可以参见Yuanbo She的这篇博文 [Opencv 完美配置攻略 2014 (Win8.1 + Opencv 2.4.8 + VS 2013)](http://my.phirobot.com/blog/2014-02-opencv_configuration_in_vs.html)。\n\n配置文件中主要是包含文件和静态链接库LIB的路径设置。下面把我的贴出来，只需要根据GSL的生成路径做相应修改即可。注意我的属性表中保留了OpenCV的内容，如果不需要的话，尽可以删掉。上面的博文对这张属性表如何配置讲得很清楚，有问题可以去参考。\n\n``` html\n<?xml version=\"1.0\" encoding=\"utf-8\"?>\n<Project ToolsVersion=\"4.0\" xmlns=\"http://schemas.microsoft.com/developer/msbuild/2003\">\n  <ImportGroup Label=\"PropertySheets\" />\n  <PropertyGroup Label=\"UserMacros\" />\n  <PropertyGroup>\n        <IncludePath>$(OPENCV249)\\include;E:\\GSLCode\\gsl-build\\;$(IncludePath)</IncludePath>\n        <LibraryPath Condition=\"'$(Platform)'=='Win32'\">$(OPENCV249)\\x86\\vc12\\lib;E:\\GSLCode\\gsl-build\\Debug;$(LibraryPath)</LibraryPath>\n        <LibraryPath Condition=\"'$(Platform)'=='X64'\">$(OPENCV249)\\x64\\vc12\\lib;E:\\GSLCode\\gsl-build\\Debug;$(LibraryPath)</LibraryPath>\n  </PropertyGroup>\n  <ItemDefinitionGroup>\n        <Link Condition=\"'$(Configuration)'=='Debug'\">\n          <AdditionalDependencies>opencv_calib3d249d.lib;opencv_contrib249d.lib;opencv_core249d.lib;opencv_features2d249d.lib;opencv_flann249d.lib;opencv_gpu249d.lib;opencv_highgui249d.lib;opencv_imgproc249d.lib;opencv_legacy249d.lib;opencv_ml249d.lib;opencv_nonfree249d.lib;opencv_objdetect249d.lib;opencv_ocl249d.lib;opencv_photo249d.lib;opencv_stitching249d.lib;opencv_superres249d.lib;opencv_ts249d.lib;opencv_video249d.lib;opencv_videostab249d.lib;gsl.lib;gslcblas.lib;%(AdditionalDependencies)</AdditionalDependencies>\n        </Link>\n        <Link Condition=\"'$(Configuration)'=='Release'\">\n          <AdditionalDependencies>opencv_calib3d249.lib;opencv_contrib249.lib;opencv_core249.lib;opencv_features2d249.lib;opencv_flann249.lib;opencv_gpu249.lib;opencv_highgui249.lib;opencv_imgproc249.lib;opencv_legacy249.lib;opencv_ml249.lib;opencv_nonfree249.lib;opencv_objdetect249.lib;opencv_ocl249.lib;opencv_photo249.lib;opencv_stitching249.lib;opencv_superres249.lib;opencv_ts249.lib;opencv_video249.lib;opencv_videostab249.lib;gsl.lib;gslcblas.lib;%(AdditionalDependencies)</AdditionalDependencies>\n        </Link>\n  </ItemDefinitionGroup>\n  <ItemGroup />\n</Project>\n```\n\n在以后建立Visual Studio工程的时候，在属性窗口直接添加现有属性表就可以了！\n\n## 测试\n\n在项目网站的教程上直接找到一段代码，进行测试，输出贝塞尔函数的值。\n\n\n``` cpp\n#include <stdio.h>\n#include <gsl/gsl_sf_bessel.h>\nint main(void)\n{\n\tdouble x = 5.0;\n\tdouble y = gsl_sf_bessel_J0(x);\n\tprintf(\"J0(%g) = %.18e\\n\", x, y);\n\treturn 0;\n}\n```\n\n控制台输出正确：\n{% raw %}\n<p><img src=\"http://i.imgur.com/uXhVvwS.jpg\" width=\"600\" height=\"200\"></p>\n{% endraw %}\n","source":"_posts/gsl-with-vs.md","raw":"---\ntitle: 使用 Visual Studio 编译 GSL 科学计算库\ndate: 2016-12-16 19:00:00\ntags:\n    - tool\n    - gsl\n---\n\nGSL是一个GNU支持的科学计算库，提供了很丰富的数值计算方法。本文介绍了GSL库在Windows环境下使用VisualStudio进行编译构建的过程。\n\n![GSL is GNU Sentific Library](/img/gsl_picture.jpg)\n<!-- more -->\n[GSL 的项目主页](http://www.gnu.org/software/gsl/)提供的说明来看，GSL支持如下的科学计算：\n\n\n（下面的这张表格的HTML使用的是[No-Cruft Excel to HTML Table Converter](http://pressbin.com/tools/excel_to_html_table/index.html)生成的）\n{% raw %}\n<table>\n   <tr>\n      <td>Complex Numbers </td>\n      <td>Roots of Polynomials</td>\n   </tr>\n   <tr>\n      <td>Special Functions </td>\n      <td>Vectors and Matrices</td>\n   </tr>\n   <tr>\n      <td>Permutations </td>\n      <td>Sorting</td>\n   </tr>\n   <tr>\n      <td>BLAS Support </td>\n      <td>Linear Algebra</td>\n   </tr>\n   <tr>\n      <td>Eigensystems </td>\n      <td>Fast Fourier Transforms</td>\n   </tr>\n   <tr>\n      <td>Quadrature </td>\n      <td>Random Numbers</td>\n   </tr>\n   <tr>\n      <td>Quasi-Random Sequences </td>\n      <td>Random Distributions</td>\n   </tr>\n   <tr>\n      <td>Statistics </td>\n      <td>Histograms</td>\n   </tr>\n   <tr>\n      <td>N-Tuples </td>\n      <td>Monte Carlo Integration</td>\n   </tr>\n   <tr>\n      <td>Simulated Annealing </td>\n      <td>Differential Equations</td>\n   </tr>\n   <tr>\n      <td>Interpolation </td>\n      <td>Numerical Differentiation</td>\n   </tr>\n   <tr>\n      <td>Chebyshev Approximation </td>\n      <td>Series Acceleration</td>\n   </tr>\n   <tr>\n      <td>Discrete Hankel Transforms </td>\n      <td>Root-Finding</td>\n   </tr>\n   <tr>\n      <td>Minimization </td>\n      <td>Least-Squares Fitting</td>\n   </tr>\n   <tr>\n      <td>Physical Constants </td>\n      <td>IEEE Floating-Point</td>\n   </tr>\n   <tr>\n      <td>Discrete Wavelet Transforms </td>\n      <td>Basis splines</td>\n   </tr>\n</table>\n{% endraw %}\n\nGSL的Linux下的配置很简单，照着它的INSTALL文件一步一步来就可以了。CMAKE大法HAO!\n\n``` bash\n./configure\nmake\nmake install\nmake clean\n```\n\n同样的，GSL也可以在Windows环境下配置，下面记录了如何在Windows环境下使用 Visual Studio 和 CMakeGUI 编译测试GSL。\n\n## 使用CMAKE编译成.SLN文件\n\n打开CMAKEGUI，将输入代码路径选为GSL源代码地址，输出路径设为自己想要的输出路径。点击 “Configure“，选择Visual Studio2013为编译器，点击Finish后会进行必要的配置。然后将表格里面的选项都打上勾，再次点击”Configure“，等待完成之后点击”Generate“。完成之后，就可以在输出路径下看到GSL.sln文件了。\n\n## 使用Visual Studio生成解决方案\n\n使用 Visual Studio 打开刚才生成的.SLN文件，分别在Debug和Release模式下生成解决方案，等待完成即可。\n\n当完成后，你应该可以在路径下看到这样一张图，我们主要关注的文件夹是\\bin，\\gsl，\\Debug和\\Release。\n\n\n## 加入环境变量\n\n修改环境变量的Path，将\\GSL_Build_Path\\bin\\Debug加入，这主要是为了\\Debug文件夹下面的gsl.dll文件。如果不进行这一步的话，一会虽然可以编译，但是却不能运行。\n\n这里顺便注释一句，当使用第三方库的时候，如果需要动态链接库的支持，其中一种方法就是将DLL文件的路径加入到Path中去。\n\n## 建立Visual Studio属性表\n\nVisual Studio可以通过建立工程属性表的方法来配置工程选项，一个OpenCV的例子可以参见Yuanbo She的这篇博文 [Opencv 完美配置攻略 2014 (Win8.1 + Opencv 2.4.8 + VS 2013)](http://my.phirobot.com/blog/2014-02-opencv_configuration_in_vs.html)。\n\n配置文件中主要是包含文件和静态链接库LIB的路径设置。下面把我的贴出来，只需要根据GSL的生成路径做相应修改即可。注意我的属性表中保留了OpenCV的内容，如果不需要的话，尽可以删掉。上面的博文对这张属性表如何配置讲得很清楚，有问题可以去参考。\n\n``` html\n<?xml version=\"1.0\" encoding=\"utf-8\"?>\n<Project ToolsVersion=\"4.0\" xmlns=\"http://schemas.microsoft.com/developer/msbuild/2003\">\n  <ImportGroup Label=\"PropertySheets\" />\n  <PropertyGroup Label=\"UserMacros\" />\n  <PropertyGroup>\n        <IncludePath>$(OPENCV249)\\include;E:\\GSLCode\\gsl-build\\;$(IncludePath)</IncludePath>\n        <LibraryPath Condition=\"'$(Platform)'=='Win32'\">$(OPENCV249)\\x86\\vc12\\lib;E:\\GSLCode\\gsl-build\\Debug;$(LibraryPath)</LibraryPath>\n        <LibraryPath Condition=\"'$(Platform)'=='X64'\">$(OPENCV249)\\x64\\vc12\\lib;E:\\GSLCode\\gsl-build\\Debug;$(LibraryPath)</LibraryPath>\n  </PropertyGroup>\n  <ItemDefinitionGroup>\n        <Link Condition=\"'$(Configuration)'=='Debug'\">\n          <AdditionalDependencies>opencv_calib3d249d.lib;opencv_contrib249d.lib;opencv_core249d.lib;opencv_features2d249d.lib;opencv_flann249d.lib;opencv_gpu249d.lib;opencv_highgui249d.lib;opencv_imgproc249d.lib;opencv_legacy249d.lib;opencv_ml249d.lib;opencv_nonfree249d.lib;opencv_objdetect249d.lib;opencv_ocl249d.lib;opencv_photo249d.lib;opencv_stitching249d.lib;opencv_superres249d.lib;opencv_ts249d.lib;opencv_video249d.lib;opencv_videostab249d.lib;gsl.lib;gslcblas.lib;%(AdditionalDependencies)</AdditionalDependencies>\n        </Link>\n        <Link Condition=\"'$(Configuration)'=='Release'\">\n          <AdditionalDependencies>opencv_calib3d249.lib;opencv_contrib249.lib;opencv_core249.lib;opencv_features2d249.lib;opencv_flann249.lib;opencv_gpu249.lib;opencv_highgui249.lib;opencv_imgproc249.lib;opencv_legacy249.lib;opencv_ml249.lib;opencv_nonfree249.lib;opencv_objdetect249.lib;opencv_ocl249.lib;opencv_photo249.lib;opencv_stitching249.lib;opencv_superres249.lib;opencv_ts249.lib;opencv_video249.lib;opencv_videostab249.lib;gsl.lib;gslcblas.lib;%(AdditionalDependencies)</AdditionalDependencies>\n        </Link>\n  </ItemDefinitionGroup>\n  <ItemGroup />\n</Project>\n```\n\n在以后建立Visual Studio工程的时候，在属性窗口直接添加现有属性表就可以了！\n\n## 测试\n\n在项目网站的教程上直接找到一段代码，进行测试，输出贝塞尔函数的值。\n\n\n``` cpp\n#include <stdio.h>\n#include <gsl/gsl_sf_bessel.h>\nint main(void)\n{\n\tdouble x = 5.0;\n\tdouble y = gsl_sf_bessel_J0(x);\n\tprintf(\"J0(%g) = %.18e\\n\", x, y);\n\treturn 0;\n}\n```\n\n控制台输出正确：\n{% raw %}\n<p><img src=\"http://i.imgur.com/uXhVvwS.jpg\" width=\"600\" height=\"200\"></p>\n{% endraw %}\n","slug":"gsl-with-vs","published":1,"updated":"2018-01-12T06:22:20.470Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjcu6vct20018qu46pn80hrcg","content":"<p>GSL是一个GNU支持的科学计算库，提供了很丰富的数值计算方法。本文介绍了GSL库在Windows环境下使用VisualStudio进行编译构建的过程。</p>\n<p><img src=\"/img/gsl_picture.jpg\" alt=\"GSL is GNU Sentific Library\"><br><a id=\"more\"></a><br><a href=\"http://www.gnu.org/software/gsl/\" target=\"_blank\" rel=\"external\">GSL 的项目主页</a>提供的说明来看，GSL支持如下的科学计算：</p>\n<p>（下面的这张表格的HTML使用的是<a href=\"http://pressbin.com/tools/excel_to_html_table/index.html\" target=\"_blank\" rel=\"external\">No-Cruft Excel to HTML Table Converter</a>生成的）<br>\n<table>\n   <tr>\n      <td>Complex Numbers </td>\n      <td>Roots of Polynomials</td>\n   </tr>\n   <tr>\n      <td>Special Functions </td>\n      <td>Vectors and Matrices</td>\n   </tr>\n   <tr>\n      <td>Permutations </td>\n      <td>Sorting</td>\n   </tr>\n   <tr>\n      <td>BLAS Support </td>\n      <td>Linear Algebra</td>\n   </tr>\n   <tr>\n      <td>Eigensystems </td>\n      <td>Fast Fourier Transforms</td>\n   </tr>\n   <tr>\n      <td>Quadrature </td>\n      <td>Random Numbers</td>\n   </tr>\n   <tr>\n      <td>Quasi-Random Sequences </td>\n      <td>Random Distributions</td>\n   </tr>\n   <tr>\n      <td>Statistics </td>\n      <td>Histograms</td>\n   </tr>\n   <tr>\n      <td>N-Tuples </td>\n      <td>Monte Carlo Integration</td>\n   </tr>\n   <tr>\n      <td>Simulated Annealing </td>\n      <td>Differential Equations</td>\n   </tr>\n   <tr>\n      <td>Interpolation </td>\n      <td>Numerical Differentiation</td>\n   </tr>\n   <tr>\n      <td>Chebyshev Approximation </td>\n      <td>Series Acceleration</td>\n   </tr>\n   <tr>\n      <td>Discrete Hankel Transforms </td>\n      <td>Root-Finding</td>\n   </tr>\n   <tr>\n      <td>Minimization </td>\n      <td>Least-Squares Fitting</td>\n   </tr>\n   <tr>\n      <td>Physical Constants </td>\n      <td>IEEE Floating-Point</td>\n   </tr>\n   <tr>\n      <td>Discrete Wavelet Transforms </td>\n      <td>Basis splines</td>\n   </tr>\n</table>\n</p>\n<p>GSL的Linux下的配置很简单，照着它的INSTALL文件一步一步来就可以了。CMAKE大法HAO!</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\">./configure</div><div class=\"line\">make</div><div class=\"line\">make install</div><div class=\"line\">make clean</div></pre></td></tr></table></figure>\n<p>同样的，GSL也可以在Windows环境下配置，下面记录了如何在Windows环境下使用 Visual Studio 和 CMakeGUI 编译测试GSL。</p>\n<h2 id=\"使用CMAKE编译成-SLN文件\"><a href=\"#使用CMAKE编译成-SLN文件\" class=\"headerlink\" title=\"使用CMAKE编译成.SLN文件\"></a>使用CMAKE编译成.SLN文件</h2><p>打开CMAKEGUI，将输入代码路径选为GSL源代码地址，输出路径设为自己想要的输出路径。点击 “Configure“，选择Visual Studio2013为编译器，点击Finish后会进行必要的配置。然后将表格里面的选项都打上勾，再次点击”Configure“，等待完成之后点击”Generate“。完成之后，就可以在输出路径下看到GSL.sln文件了。</p>\n<h2 id=\"使用Visual-Studio生成解决方案\"><a href=\"#使用Visual-Studio生成解决方案\" class=\"headerlink\" title=\"使用Visual Studio生成解决方案\"></a>使用Visual Studio生成解决方案</h2><p>使用 Visual Studio 打开刚才生成的.SLN文件，分别在Debug和Release模式下生成解决方案，等待完成即可。</p>\n<p>当完成后，你应该可以在路径下看到这样一张图，我们主要关注的文件夹是\\bin，\\gsl，\\Debug和\\Release。</p>\n<h2 id=\"加入环境变量\"><a href=\"#加入环境变量\" class=\"headerlink\" title=\"加入环境变量\"></a>加入环境变量</h2><p>修改环境变量的Path，将\\GSL_Build_Path\\bin\\Debug加入，这主要是为了\\Debug文件夹下面的gsl.dll文件。如果不进行这一步的话，一会虽然可以编译，但是却不能运行。</p>\n<p>这里顺便注释一句，当使用第三方库的时候，如果需要动态链接库的支持，其中一种方法就是将DLL文件的路径加入到Path中去。</p>\n<h2 id=\"建立Visual-Studio属性表\"><a href=\"#建立Visual-Studio属性表\" class=\"headerlink\" title=\"建立Visual Studio属性表\"></a>建立Visual Studio属性表</h2><p>Visual Studio可以通过建立工程属性表的方法来配置工程选项，一个OpenCV的例子可以参见Yuanbo She的这篇博文 <a href=\"http://my.phirobot.com/blog/2014-02-opencv_configuration_in_vs.html\" target=\"_blank\" rel=\"external\">Opencv 完美配置攻略 2014 (Win8.1 + Opencv 2.4.8 + VS 2013)</a>。</p>\n<p>配置文件中主要是包含文件和静态链接库LIB的路径设置。下面把我的贴出来，只需要根据GSL的生成路径做相应修改即可。注意我的属性表中保留了OpenCV的内容，如果不需要的话，尽可以删掉。上面的博文对这张属性表如何配置讲得很清楚，有问题可以去参考。</p>\n<figure class=\"highlight html\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div></pre></td><td class=\"code\"><pre><div class=\"line\">&lt;?xml version=\"1.0\" encoding=\"utf-8\"?&gt;</div><div class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">Project</span> <span class=\"attr\">ToolsVersion</span>=<span class=\"string\">\"4.0\"</span> <span class=\"attr\">xmlns</span>=<span class=\"string\">\"http://schemas.microsoft.com/developer/msbuild/2003\"</span>&gt;</span></div><div class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">ImportGroup</span> <span class=\"attr\">Label</span>=<span class=\"string\">\"PropertySheets\"</span> /&gt;</span></div><div class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">PropertyGroup</span> <span class=\"attr\">Label</span>=<span class=\"string\">\"UserMacros\"</span> /&gt;</span></div><div class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">PropertyGroup</span>&gt;</span></div><div class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">IncludePath</span>&gt;</span>$(OPENCV249)\\include;E:\\GSLCode\\gsl-build\\;$(IncludePath)<span class=\"tag\">&lt;/<span class=\"name\">IncludePath</span>&gt;</span></div><div class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">LibraryPath</span> <span class=\"attr\">Condition</span>=<span class=\"string\">\"'$(Platform)'=='Win32'\"</span>&gt;</span>$(OPENCV249)\\x86\\vc12\\lib;E:\\GSLCode\\gsl-build\\Debug;$(LibraryPath)<span class=\"tag\">&lt;/<span class=\"name\">LibraryPath</span>&gt;</span></div><div class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">LibraryPath</span> <span class=\"attr\">Condition</span>=<span class=\"string\">\"'$(Platform)'=='X64'\"</span>&gt;</span>$(OPENCV249)\\x64\\vc12\\lib;E:\\GSLCode\\gsl-build\\Debug;$(LibraryPath)<span class=\"tag\">&lt;/<span class=\"name\">LibraryPath</span>&gt;</span></div><div class=\"line\">  <span class=\"tag\">&lt;/<span class=\"name\">PropertyGroup</span>&gt;</span></div><div class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">ItemDefinitionGroup</span>&gt;</span></div><div class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">Link</span> <span class=\"attr\">Condition</span>=<span class=\"string\">\"'$(Configuration)'=='Debug'\"</span>&gt;</span></div><div class=\"line\">          <span class=\"tag\">&lt;<span class=\"name\">AdditionalDependencies</span>&gt;</span>opencv_calib3d249d.lib;opencv_contrib249d.lib;opencv_core249d.lib;opencv_features2d249d.lib;opencv_flann249d.lib;opencv_gpu249d.lib;opencv_highgui249d.lib;opencv_imgproc249d.lib;opencv_legacy249d.lib;opencv_ml249d.lib;opencv_nonfree249d.lib;opencv_objdetect249d.lib;opencv_ocl249d.lib;opencv_photo249d.lib;opencv_stitching249d.lib;opencv_superres249d.lib;opencv_ts249d.lib;opencv_video249d.lib;opencv_videostab249d.lib;gsl.lib;gslcblas.lib;%(AdditionalDependencies)<span class=\"tag\">&lt;/<span class=\"name\">AdditionalDependencies</span>&gt;</span></div><div class=\"line\">        <span class=\"tag\">&lt;/<span class=\"name\">Link</span>&gt;</span></div><div class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">Link</span> <span class=\"attr\">Condition</span>=<span class=\"string\">\"'$(Configuration)'=='Release'\"</span>&gt;</span></div><div class=\"line\">          <span class=\"tag\">&lt;<span class=\"name\">AdditionalDependencies</span>&gt;</span>opencv_calib3d249.lib;opencv_contrib249.lib;opencv_core249.lib;opencv_features2d249.lib;opencv_flann249.lib;opencv_gpu249.lib;opencv_highgui249.lib;opencv_imgproc249.lib;opencv_legacy249.lib;opencv_ml249.lib;opencv_nonfree249.lib;opencv_objdetect249.lib;opencv_ocl249.lib;opencv_photo249.lib;opencv_stitching249.lib;opencv_superres249.lib;opencv_ts249.lib;opencv_video249.lib;opencv_videostab249.lib;gsl.lib;gslcblas.lib;%(AdditionalDependencies)<span class=\"tag\">&lt;/<span class=\"name\">AdditionalDependencies</span>&gt;</span></div><div class=\"line\">        <span class=\"tag\">&lt;/<span class=\"name\">Link</span>&gt;</span></div><div class=\"line\">  <span class=\"tag\">&lt;/<span class=\"name\">ItemDefinitionGroup</span>&gt;</span></div><div class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">ItemGroup</span> /&gt;</span></div><div class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">Project</span>&gt;</span></div></pre></td></tr></table></figure>\n<p>在以后建立Visual Studio工程的时候，在属性窗口直接添加现有属性表就可以了！</p>\n<h2 id=\"测试\"><a href=\"#测试\" class=\"headerlink\" title=\"测试\"></a>测试</h2><p>在项目网站的教程上直接找到一段代码，进行测试，输出贝塞尔函数的值。</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">include</span> <span class=\"meta-string\">&lt;stdio.h&gt;</span></span></div><div class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">include</span> <span class=\"meta-string\">&lt;gsl/gsl_sf_bessel.h&gt;</span></span></div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">int</span> <span class=\"title\">main</span><span class=\"params\">(<span class=\"keyword\">void</span>)</span></span></div><div class=\"line\">&#123;</div><div class=\"line\">\t<span class=\"keyword\">double</span> x = <span class=\"number\">5.0</span>;</div><div class=\"line\">\t<span class=\"keyword\">double</span> y = gsl_sf_bessel_J0(x);</div><div class=\"line\">\t<span class=\"built_in\">printf</span>(<span class=\"string\">\"J0(%g) = %.18e\\n\"</span>, x, y);</div><div class=\"line\">\t<span class=\"keyword\">return</span> <span class=\"number\">0</span>;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<p>控制台输出正确：<br>\n</p><p><img src=\"http://i.imgur.com/uXhVvwS.jpg\" width=\"600\" height=\"200\"></p>\n<p></p>\n","excerpt":"<p>GSL是一个GNU支持的科学计算库，提供了很丰富的数值计算方法。本文介绍了GSL库在Windows环境下使用VisualStudio进行编译构建的过程。</p>\n<p><img src=\"/img/gsl_picture.jpg\" alt=\"GSL is GNU Sentific Library\"><br>","more":"<br><a href=\"http://www.gnu.org/software/gsl/\">GSL 的项目主页</a>提供的说明来看，GSL支持如下的科学计算：</p>\n<p>（下面的这张表格的HTML使用的是<a href=\"http://pressbin.com/tools/excel_to_html_table/index.html\">No-Cruft Excel to HTML Table Converter</a>生成的）<br>\n<table>\n   <tr>\n      <td>Complex Numbers </td>\n      <td>Roots of Polynomials</td>\n   </tr>\n   <tr>\n      <td>Special Functions </td>\n      <td>Vectors and Matrices</td>\n   </tr>\n   <tr>\n      <td>Permutations </td>\n      <td>Sorting</td>\n   </tr>\n   <tr>\n      <td>BLAS Support </td>\n      <td>Linear Algebra</td>\n   </tr>\n   <tr>\n      <td>Eigensystems </td>\n      <td>Fast Fourier Transforms</td>\n   </tr>\n   <tr>\n      <td>Quadrature </td>\n      <td>Random Numbers</td>\n   </tr>\n   <tr>\n      <td>Quasi-Random Sequences </td>\n      <td>Random Distributions</td>\n   </tr>\n   <tr>\n      <td>Statistics </td>\n      <td>Histograms</td>\n   </tr>\n   <tr>\n      <td>N-Tuples </td>\n      <td>Monte Carlo Integration</td>\n   </tr>\n   <tr>\n      <td>Simulated Annealing </td>\n      <td>Differential Equations</td>\n   </tr>\n   <tr>\n      <td>Interpolation </td>\n      <td>Numerical Differentiation</td>\n   </tr>\n   <tr>\n      <td>Chebyshev Approximation </td>\n      <td>Series Acceleration</td>\n   </tr>\n   <tr>\n      <td>Discrete Hankel Transforms </td>\n      <td>Root-Finding</td>\n   </tr>\n   <tr>\n      <td>Minimization </td>\n      <td>Least-Squares Fitting</td>\n   </tr>\n   <tr>\n      <td>Physical Constants </td>\n      <td>IEEE Floating-Point</td>\n   </tr>\n   <tr>\n      <td>Discrete Wavelet Transforms </td>\n      <td>Basis splines</td>\n   </tr>\n</table>\n</p>\n<p>GSL的Linux下的配置很简单，照着它的INSTALL文件一步一步来就可以了。CMAKE大法HAO!</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\">./configure</div><div class=\"line\">make</div><div class=\"line\">make install</div><div class=\"line\">make clean</div></pre></td></tr></table></figure>\n<p>同样的，GSL也可以在Windows环境下配置，下面记录了如何在Windows环境下使用 Visual Studio 和 CMakeGUI 编译测试GSL。</p>\n<h2 id=\"使用CMAKE编译成-SLN文件\"><a href=\"#使用CMAKE编译成-SLN文件\" class=\"headerlink\" title=\"使用CMAKE编译成.SLN文件\"></a>使用CMAKE编译成.SLN文件</h2><p>打开CMAKEGUI，将输入代码路径选为GSL源代码地址，输出路径设为自己想要的输出路径。点击 “Configure“，选择Visual Studio2013为编译器，点击Finish后会进行必要的配置。然后将表格里面的选项都打上勾，再次点击”Configure“，等待完成之后点击”Generate“。完成之后，就可以在输出路径下看到GSL.sln文件了。</p>\n<h2 id=\"使用Visual-Studio生成解决方案\"><a href=\"#使用Visual-Studio生成解决方案\" class=\"headerlink\" title=\"使用Visual Studio生成解决方案\"></a>使用Visual Studio生成解决方案</h2><p>使用 Visual Studio 打开刚才生成的.SLN文件，分别在Debug和Release模式下生成解决方案，等待完成即可。</p>\n<p>当完成后，你应该可以在路径下看到这样一张图，我们主要关注的文件夹是\\bin，\\gsl，\\Debug和\\Release。</p>\n<h2 id=\"加入环境变量\"><a href=\"#加入环境变量\" class=\"headerlink\" title=\"加入环境变量\"></a>加入环境变量</h2><p>修改环境变量的Path，将\\GSL_Build_Path\\bin\\Debug加入，这主要是为了\\Debug文件夹下面的gsl.dll文件。如果不进行这一步的话，一会虽然可以编译，但是却不能运行。</p>\n<p>这里顺便注释一句，当使用第三方库的时候，如果需要动态链接库的支持，其中一种方法就是将DLL文件的路径加入到Path中去。</p>\n<h2 id=\"建立Visual-Studio属性表\"><a href=\"#建立Visual-Studio属性表\" class=\"headerlink\" title=\"建立Visual Studio属性表\"></a>建立Visual Studio属性表</h2><p>Visual Studio可以通过建立工程属性表的方法来配置工程选项，一个OpenCV的例子可以参见Yuanbo She的这篇博文 <a href=\"http://my.phirobot.com/blog/2014-02-opencv_configuration_in_vs.html\">Opencv 完美配置攻略 2014 (Win8.1 + Opencv 2.4.8 + VS 2013)</a>。</p>\n<p>配置文件中主要是包含文件和静态链接库LIB的路径设置。下面把我的贴出来，只需要根据GSL的生成路径做相应修改即可。注意我的属性表中保留了OpenCV的内容，如果不需要的话，尽可以删掉。上面的博文对这张属性表如何配置讲得很清楚，有问题可以去参考。</p>\n<figure class=\"highlight html\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div></pre></td><td class=\"code\"><pre><div class=\"line\">&lt;?xml version=\"1.0\" encoding=\"utf-8\"?&gt;</div><div class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">Project</span> <span class=\"attr\">ToolsVersion</span>=<span class=\"string\">\"4.0\"</span> <span class=\"attr\">xmlns</span>=<span class=\"string\">\"http://schemas.microsoft.com/developer/msbuild/2003\"</span>&gt;</span></div><div class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">ImportGroup</span> <span class=\"attr\">Label</span>=<span class=\"string\">\"PropertySheets\"</span> /&gt;</span></div><div class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">PropertyGroup</span> <span class=\"attr\">Label</span>=<span class=\"string\">\"UserMacros\"</span> /&gt;</span></div><div class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">PropertyGroup</span>&gt;</span></div><div class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">IncludePath</span>&gt;</span>$(OPENCV249)\\include;E:\\GSLCode\\gsl-build\\;$(IncludePath)<span class=\"tag\">&lt;/<span class=\"name\">IncludePath</span>&gt;</span></div><div class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">LibraryPath</span> <span class=\"attr\">Condition</span>=<span class=\"string\">\"'$(Platform)'=='Win32'\"</span>&gt;</span>$(OPENCV249)\\x86\\vc12\\lib;E:\\GSLCode\\gsl-build\\Debug;$(LibraryPath)<span class=\"tag\">&lt;/<span class=\"name\">LibraryPath</span>&gt;</span></div><div class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">LibraryPath</span> <span class=\"attr\">Condition</span>=<span class=\"string\">\"'$(Platform)'=='X64'\"</span>&gt;</span>$(OPENCV249)\\x64\\vc12\\lib;E:\\GSLCode\\gsl-build\\Debug;$(LibraryPath)<span class=\"tag\">&lt;/<span class=\"name\">LibraryPath</span>&gt;</span></div><div class=\"line\">  <span class=\"tag\">&lt;/<span class=\"name\">PropertyGroup</span>&gt;</span></div><div class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">ItemDefinitionGroup</span>&gt;</span></div><div class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">Link</span> <span class=\"attr\">Condition</span>=<span class=\"string\">\"'$(Configuration)'=='Debug'\"</span>&gt;</span></div><div class=\"line\">          <span class=\"tag\">&lt;<span class=\"name\">AdditionalDependencies</span>&gt;</span>opencv_calib3d249d.lib;opencv_contrib249d.lib;opencv_core249d.lib;opencv_features2d249d.lib;opencv_flann249d.lib;opencv_gpu249d.lib;opencv_highgui249d.lib;opencv_imgproc249d.lib;opencv_legacy249d.lib;opencv_ml249d.lib;opencv_nonfree249d.lib;opencv_objdetect249d.lib;opencv_ocl249d.lib;opencv_photo249d.lib;opencv_stitching249d.lib;opencv_superres249d.lib;opencv_ts249d.lib;opencv_video249d.lib;opencv_videostab249d.lib;gsl.lib;gslcblas.lib;%(AdditionalDependencies)<span class=\"tag\">&lt;/<span class=\"name\">AdditionalDependencies</span>&gt;</span></div><div class=\"line\">        <span class=\"tag\">&lt;/<span class=\"name\">Link</span>&gt;</span></div><div class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">Link</span> <span class=\"attr\">Condition</span>=<span class=\"string\">\"'$(Configuration)'=='Release'\"</span>&gt;</span></div><div class=\"line\">          <span class=\"tag\">&lt;<span class=\"name\">AdditionalDependencies</span>&gt;</span>opencv_calib3d249.lib;opencv_contrib249.lib;opencv_core249.lib;opencv_features2d249.lib;opencv_flann249.lib;opencv_gpu249.lib;opencv_highgui249.lib;opencv_imgproc249.lib;opencv_legacy249.lib;opencv_ml249.lib;opencv_nonfree249.lib;opencv_objdetect249.lib;opencv_ocl249.lib;opencv_photo249.lib;opencv_stitching249.lib;opencv_superres249.lib;opencv_ts249.lib;opencv_video249.lib;opencv_videostab249.lib;gsl.lib;gslcblas.lib;%(AdditionalDependencies)<span class=\"tag\">&lt;/<span class=\"name\">AdditionalDependencies</span>&gt;</span></div><div class=\"line\">        <span class=\"tag\">&lt;/<span class=\"name\">Link</span>&gt;</span></div><div class=\"line\">  <span class=\"tag\">&lt;/<span class=\"name\">ItemDefinitionGroup</span>&gt;</span></div><div class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">ItemGroup</span> /&gt;</span></div><div class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">Project</span>&gt;</span></div></pre></td></tr></table></figure>\n<p>在以后建立Visual Studio工程的时候，在属性窗口直接添加现有属性表就可以了！</p>\n<h2 id=\"测试\"><a href=\"#测试\" class=\"headerlink\" title=\"测试\"></a>测试</h2><p>在项目网站的教程上直接找到一段代码，进行测试，输出贝塞尔函数的值。</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">include</span> <span class=\"meta-string\">&lt;stdio.h&gt;</span></span></div><div class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">include</span> <span class=\"meta-string\">&lt;gsl/gsl_sf_bessel.h&gt;</span></span></div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">int</span> <span class=\"title\">main</span><span class=\"params\">(<span class=\"keyword\">void</span>)</span></div><div class=\"line\"></span>&#123;</div><div class=\"line\">\t<span class=\"keyword\">double</span> x = <span class=\"number\">5.0</span>;</div><div class=\"line\">\t<span class=\"keyword\">double</span> y = gsl_sf_bessel_J0(x);</div><div class=\"line\">\t<span class=\"built_in\">printf</span>(<span class=\"string\">\"J0(%g) = %.18e\\n\"</span>, x, y);</div><div class=\"line\">\t<span class=\"keyword\">return</span> <span class=\"number\">0</span>;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<p>控制台输出正确：<br>\n<p><img src=\"http://i.imgur.com/uXhVvwS.jpg\" width=\"600\" height=\"200\"></p>\n</p>"},{"title":"Focal Loss论文阅读 - Focal Loss for Dense Object Detection","date":"2017-08-14T14:43:55.000Z","_content":"Focal Loss这篇文章是He Kaiming和Ross发表在ICCV2017上的文章。关于这篇文章在知乎上有相关的[讨论](https://www.zhihu.com/question/63581984)。最近一直在做强化学习相关的东西，目标检测方面很长时间不看新的东西了，把自己阅读论文的要点记录如下，也是一次对这方面进展的回顾。\n\n下图来自于论文，是各种主流模型的比较。其中横轴是前向推断的时间，纵轴是检测器的精度。作者提出的RetinaNet在单独某个维度上都可以吊打其他模型。不过图上没有加入YOLO的对比。YOLO的速度仍然是其一大优势，但是精度和其他方法相比，仍然不高。\n\n![不同模型关于精度和速度的比较](/img/focal_loss_different_model_comparison.jpg)\n<!-- more -->\n\n## 为什么要有Focal Loss？\n目前主流的检测算法可以分为两类：one-state和two-stage。前者以YOLO和SSD为代表，后者以RCNN系列为代表。后者的特点是分类器是在一个稀疏的候选目标中进行分类（背景和对应类别），而这是通过前面的proposal过程实现的。例如Seletive Search或者RPN。相对于后者，这种方法是在一个稀疏集合内做分类。与之相反，前者是输出一个稠密的proposal，然后丢进分类器中，直接进行类别分类。后者使用的方法结构一般较为简单，速度较快，但是目前存在的问题是精度不高，普遍不如前者的方法。\n\n论文作者指出，之所以做稠密分类的后者精度不够高，核心问题（central issus）是稠密proposal中前景和背景的极度不平衡。以我更熟悉的YOLO举例子，比如在PASCAL VOC数据集中，每张图片上标注的目标可能也就几个。但是YOLO V2最后一层的输出是$13 \\times 13 \\times 5$，也就是$845$个候选目标！大量（简单易区分）的负样本在loss中占据了很大比重，使得有用的loss不能回传回来。\n\n基于此，作者将经典的交叉熵损失做了变形（见下），给那些易于被分类的简单例子小的权重，给不易区分的难例更大的权重。同时，作者提出了一个新的one-stage的检测器RetinaNet，达到了速度和精度很好地trade-off。\n\n$$\\text{FL}(p_t) = -(1-p_t)^\\gamma \\log(p_t)$$\n\n## 物体检测的两种主流方法\n在深度学习之前，经典的物体检测方法为滑动窗，并使用人工设计的特征。HoG和DPM等方法是其中比较有名的。\n\nR-CNN系的方法是目前最为流行的物体检测方法之一，同时也是目前精度最高的方法。在R-CNN系方法中，正负类别不平衡这个问题通过前面的proposal解决了。通过EdgeBoxes，Selective Search，DeepMask，RPN等方法，过滤掉了大多数的背景，实际传给后续网络的proposal的数量是比较少的（1-2K）。\n\n在YOLO，SSD等方法中，需要直接对feature map的大量proposal（100K）进行检测，而且这些proposal很多都在feature map上重叠。大量的负样本带来两个问题：\n- 过于简单，有效信息过少，使得训练效率低；\n- 简单的负样本在训练过程中压倒性优势，使得模型发生退化。\n\n在Faster-RCNN方法中，Huber Loss被用来降低outlier的影响（较大error的样本，也就是难例，传回来的梯度做了clipping，也只能是$1$）。而FocalLoss是对inner中简单的那些样本对loss的贡献进行限制。即使这些简单样本数量很多，也不让它们在训练中占到优势。\n\n## Focal Loss\nFocal Loss从交叉熵损失而来。二分类的交叉熵损失如下：\n\n$$\\text{CE}(p, y) = \\begin{cases}-\\log(p) \\quad &\\text{if}\\quad y = 1\\\\ -\\log(1-p) &\\text{otherwise}\\end{cases}$$\n\n对应的，多分类的交叉熵损失是这样的：\n$$\\text{CE}(p, y) = -\\log(p_y)$$\n\n如下图所示，蓝色线为交叉熵损失函数随着$p_t$变化的曲线($p_t$意为ground truth，是标注类别所对应的概率)。可以看到，当概率大于$.5$，即认为是易分类的简单样本时，值仍然较大。这样，很多简单样本累加起来，就很可能盖住那些稀少的不易正确分类的类别。\n![FL vs CELoss](/img/focal_loss_vs_ce_loss.jpg)\n\n为了改善类别样本分布不均衡的问题，已经有人提出了使用加上权重的交叉熵损失，如下（即用参数$\\alpha_t$来平衡，这组参数可以是超参数，也可以由类别的比例倒数决定）。作者将其作为比较的baseline。\n$$\\text{CE}(p) = -\\alpha_t\\log(p_t)$$\n\n作者提出的则是一个自适应调节的权重，即Focal Loss，定义如下。由上图可以看到$\\gamma$取不同值的时候的函数值变化。作者发现，$\\gamma=2$时能够获得最佳的效果提升。\n$$\\text{FL}(p_t) = -(1-p_t)^\\gamma\\log(p_t)$$\n\n在实际实验中，作者使用的是加权之后的Focal Loss，作者发现这样能够带来些微的性能提升。\n\n## 模型初始化\n对于一般的分类网络，初始化之后，往往其输出的预测结果是均等的（随机猜测）。然而作者认为，这种初始化方式在类别极度不均衡的时候是有害的。作者提出，应该初始化模型参数，使得初始化之后，模型输出稀有类别的概率变小（如$0.01$）。作者发现这种初始化方法对于交叉熵损失和Focal Loss的性能提升都有帮助。\n\n在后续的模型介绍部分，作者较为详细地说明了模型初始化方法。首先，从imagenet预训练得到的base net不做调整，新加入的卷积层权重均初始化为$\\sigma=0.01$的高斯分布，偏置项为$0$。对于分类网络的最后一个卷积层，将偏置项置为$b=-\\log((1-\\pi)/\\pi)$。这里的$\\pi$参数是一个超参数，其意义是在训练的初始阶段，每个anchor被分类为前景的概率。在实验中，作者实际使用的大小是$0.01$。\n\n这样进行模型初始化造成的结果就是，在初始阶段，不会产生大量的False Positive，使得训练更加稳定。\n\n## RetinaNet\n作者利用前面介绍的发现和结论，基于ResNet和Feature Pyramid Net（FPN）设计了一种新的one-stage检测框架，命名为RetinaNet。\n","source":"_posts/focal-loss-paper.md","raw":"---\ntitle: Focal Loss论文阅读 - Focal Loss for Dense Object Detection\ndate: 2017-08-14 22:43:55\ntags:\n    - paper\n    - deep learning\n---\nFocal Loss这篇文章是He Kaiming和Ross发表在ICCV2017上的文章。关于这篇文章在知乎上有相关的[讨论](https://www.zhihu.com/question/63581984)。最近一直在做强化学习相关的东西，目标检测方面很长时间不看新的东西了，把自己阅读论文的要点记录如下，也是一次对这方面进展的回顾。\n\n下图来自于论文，是各种主流模型的比较。其中横轴是前向推断的时间，纵轴是检测器的精度。作者提出的RetinaNet在单独某个维度上都可以吊打其他模型。不过图上没有加入YOLO的对比。YOLO的速度仍然是其一大优势，但是精度和其他方法相比，仍然不高。\n\n![不同模型关于精度和速度的比较](/img/focal_loss_different_model_comparison.jpg)\n<!-- more -->\n\n## 为什么要有Focal Loss？\n目前主流的检测算法可以分为两类：one-state和two-stage。前者以YOLO和SSD为代表，后者以RCNN系列为代表。后者的特点是分类器是在一个稀疏的候选目标中进行分类（背景和对应类别），而这是通过前面的proposal过程实现的。例如Seletive Search或者RPN。相对于后者，这种方法是在一个稀疏集合内做分类。与之相反，前者是输出一个稠密的proposal，然后丢进分类器中，直接进行类别分类。后者使用的方法结构一般较为简单，速度较快，但是目前存在的问题是精度不高，普遍不如前者的方法。\n\n论文作者指出，之所以做稠密分类的后者精度不够高，核心问题（central issus）是稠密proposal中前景和背景的极度不平衡。以我更熟悉的YOLO举例子，比如在PASCAL VOC数据集中，每张图片上标注的目标可能也就几个。但是YOLO V2最后一层的输出是$13 \\times 13 \\times 5$，也就是$845$个候选目标！大量（简单易区分）的负样本在loss中占据了很大比重，使得有用的loss不能回传回来。\n\n基于此，作者将经典的交叉熵损失做了变形（见下），给那些易于被分类的简单例子小的权重，给不易区分的难例更大的权重。同时，作者提出了一个新的one-stage的检测器RetinaNet，达到了速度和精度很好地trade-off。\n\n$$\\text{FL}(p_t) = -(1-p_t)^\\gamma \\log(p_t)$$\n\n## 物体检测的两种主流方法\n在深度学习之前，经典的物体检测方法为滑动窗，并使用人工设计的特征。HoG和DPM等方法是其中比较有名的。\n\nR-CNN系的方法是目前最为流行的物体检测方法之一，同时也是目前精度最高的方法。在R-CNN系方法中，正负类别不平衡这个问题通过前面的proposal解决了。通过EdgeBoxes，Selective Search，DeepMask，RPN等方法，过滤掉了大多数的背景，实际传给后续网络的proposal的数量是比较少的（1-2K）。\n\n在YOLO，SSD等方法中，需要直接对feature map的大量proposal（100K）进行检测，而且这些proposal很多都在feature map上重叠。大量的负样本带来两个问题：\n- 过于简单，有效信息过少，使得训练效率低；\n- 简单的负样本在训练过程中压倒性优势，使得模型发生退化。\n\n在Faster-RCNN方法中，Huber Loss被用来降低outlier的影响（较大error的样本，也就是难例，传回来的梯度做了clipping，也只能是$1$）。而FocalLoss是对inner中简单的那些样本对loss的贡献进行限制。即使这些简单样本数量很多，也不让它们在训练中占到优势。\n\n## Focal Loss\nFocal Loss从交叉熵损失而来。二分类的交叉熵损失如下：\n\n$$\\text{CE}(p, y) = \\begin{cases}-\\log(p) \\quad &\\text{if}\\quad y = 1\\\\ -\\log(1-p) &\\text{otherwise}\\end{cases}$$\n\n对应的，多分类的交叉熵损失是这样的：\n$$\\text{CE}(p, y) = -\\log(p_y)$$\n\n如下图所示，蓝色线为交叉熵损失函数随着$p_t$变化的曲线($p_t$意为ground truth，是标注类别所对应的概率)。可以看到，当概率大于$.5$，即认为是易分类的简单样本时，值仍然较大。这样，很多简单样本累加起来，就很可能盖住那些稀少的不易正确分类的类别。\n![FL vs CELoss](/img/focal_loss_vs_ce_loss.jpg)\n\n为了改善类别样本分布不均衡的问题，已经有人提出了使用加上权重的交叉熵损失，如下（即用参数$\\alpha_t$来平衡，这组参数可以是超参数，也可以由类别的比例倒数决定）。作者将其作为比较的baseline。\n$$\\text{CE}(p) = -\\alpha_t\\log(p_t)$$\n\n作者提出的则是一个自适应调节的权重，即Focal Loss，定义如下。由上图可以看到$\\gamma$取不同值的时候的函数值变化。作者发现，$\\gamma=2$时能够获得最佳的效果提升。\n$$\\text{FL}(p_t) = -(1-p_t)^\\gamma\\log(p_t)$$\n\n在实际实验中，作者使用的是加权之后的Focal Loss，作者发现这样能够带来些微的性能提升。\n\n## 模型初始化\n对于一般的分类网络，初始化之后，往往其输出的预测结果是均等的（随机猜测）。然而作者认为，这种初始化方式在类别极度不均衡的时候是有害的。作者提出，应该初始化模型参数，使得初始化之后，模型输出稀有类别的概率变小（如$0.01$）。作者发现这种初始化方法对于交叉熵损失和Focal Loss的性能提升都有帮助。\n\n在后续的模型介绍部分，作者较为详细地说明了模型初始化方法。首先，从imagenet预训练得到的base net不做调整，新加入的卷积层权重均初始化为$\\sigma=0.01$的高斯分布，偏置项为$0$。对于分类网络的最后一个卷积层，将偏置项置为$b=-\\log((1-\\pi)/\\pi)$。这里的$\\pi$参数是一个超参数，其意义是在训练的初始阶段，每个anchor被分类为前景的概率。在实验中，作者实际使用的大小是$0.01$。\n\n这样进行模型初始化造成的结果就是，在初始阶段，不会产生大量的False Positive，使得训练更加稳定。\n\n## RetinaNet\n作者利用前面介绍的发现和结论，基于ResNet和Feature Pyramid Net（FPN）设计了一种新的one-stage检测框架，命名为RetinaNet。\n","slug":"focal-loss-paper","published":1,"updated":"2018-01-12T06:22:20.469Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjcu6vct30019qu467r7zgtet","content":"<p>Focal Loss这篇文章是He Kaiming和Ross发表在ICCV2017上的文章。关于这篇文章在知乎上有相关的<a href=\"https://www.zhihu.com/question/63581984\" target=\"_blank\" rel=\"external\">讨论</a>。最近一直在做强化学习相关的东西，目标检测方面很长时间不看新的东西了，把自己阅读论文的要点记录如下，也是一次对这方面进展的回顾。</p>\n<p>下图来自于论文，是各种主流模型的比较。其中横轴是前向推断的时间，纵轴是检测器的精度。作者提出的RetinaNet在单独某个维度上都可以吊打其他模型。不过图上没有加入YOLO的对比。YOLO的速度仍然是其一大优势，但是精度和其他方法相比，仍然不高。</p>\n<p><img src=\"/img/focal_loss_different_model_comparison.jpg\" alt=\"不同模型关于精度和速度的比较\"><br><a id=\"more\"></a></p>\n<h2 id=\"为什么要有Focal-Loss？\"><a href=\"#为什么要有Focal-Loss？\" class=\"headerlink\" title=\"为什么要有Focal Loss？\"></a>为什么要有Focal Loss？</h2><p>目前主流的检测算法可以分为两类：one-state和two-stage。前者以YOLO和SSD为代表，后者以RCNN系列为代表。后者的特点是分类器是在一个稀疏的候选目标中进行分类（背景和对应类别），而这是通过前面的proposal过程实现的。例如Seletive Search或者RPN。相对于后者，这种方法是在一个稀疏集合内做分类。与之相反，前者是输出一个稠密的proposal，然后丢进分类器中，直接进行类别分类。后者使用的方法结构一般较为简单，速度较快，但是目前存在的问题是精度不高，普遍不如前者的方法。</p>\n<p>论文作者指出，之所以做稠密分类的后者精度不够高，核心问题（central issus）是稠密proposal中前景和背景的极度不平衡。以我更熟悉的YOLO举例子，比如在PASCAL VOC数据集中，每张图片上标注的目标可能也就几个。但是YOLO V2最后一层的输出是$13 \\times 13 \\times 5$，也就是$845$个候选目标！大量（简单易区分）的负样本在loss中占据了很大比重，使得有用的loss不能回传回来。</p>\n<p>基于此，作者将经典的交叉熵损失做了变形（见下），给那些易于被分类的简单例子小的权重，给不易区分的难例更大的权重。同时，作者提出了一个新的one-stage的检测器RetinaNet，达到了速度和精度很好地trade-off。</p>\n<script type=\"math/tex; mode=display\">\\text{FL}(p_t) = -(1-p_t)^\\gamma \\log(p_t)</script><h2 id=\"物体检测的两种主流方法\"><a href=\"#物体检测的两种主流方法\" class=\"headerlink\" title=\"物体检测的两种主流方法\"></a>物体检测的两种主流方法</h2><p>在深度学习之前，经典的物体检测方法为滑动窗，并使用人工设计的特征。HoG和DPM等方法是其中比较有名的。</p>\n<p>R-CNN系的方法是目前最为流行的物体检测方法之一，同时也是目前精度最高的方法。在R-CNN系方法中，正负类别不平衡这个问题通过前面的proposal解决了。通过EdgeBoxes，Selective Search，DeepMask，RPN等方法，过滤掉了大多数的背景，实际传给后续网络的proposal的数量是比较少的（1-2K）。</p>\n<p>在YOLO，SSD等方法中，需要直接对feature map的大量proposal（100K）进行检测，而且这些proposal很多都在feature map上重叠。大量的负样本带来两个问题：</p>\n<ul>\n<li>过于简单，有效信息过少，使得训练效率低；</li>\n<li>简单的负样本在训练过程中压倒性优势，使得模型发生退化。</li>\n</ul>\n<p>在Faster-RCNN方法中，Huber Loss被用来降低outlier的影响（较大error的样本，也就是难例，传回来的梯度做了clipping，也只能是$1$）。而FocalLoss是对inner中简单的那些样本对loss的贡献进行限制。即使这些简单样本数量很多，也不让它们在训练中占到优势。</p>\n<h2 id=\"Focal-Loss\"><a href=\"#Focal-Loss\" class=\"headerlink\" title=\"Focal Loss\"></a>Focal Loss</h2><p>Focal Loss从交叉熵损失而来。二分类的交叉熵损失如下：</p>\n<script type=\"math/tex; mode=display\">\\text{CE}(p, y) = \\begin{cases}-\\log(p) \\quad &\\text{if}\\quad y = 1\\\\ -\\log(1-p) &\\text{otherwise}\\end{cases}</script><p>对应的，多分类的交叉熵损失是这样的：</p>\n<script type=\"math/tex; mode=display\">\\text{CE}(p, y) = -\\log(p_y)</script><p>如下图所示，蓝色线为交叉熵损失函数随着$p_t$变化的曲线($p_t$意为ground truth，是标注类别所对应的概率)。可以看到，当概率大于$.5$，即认为是易分类的简单样本时，值仍然较大。这样，很多简单样本累加起来，就很可能盖住那些稀少的不易正确分类的类别。<br><img src=\"/img/focal_loss_vs_ce_loss.jpg\" alt=\"FL vs CELoss\"></p>\n<p>为了改善类别样本分布不均衡的问题，已经有人提出了使用加上权重的交叉熵损失，如下（即用参数$\\alpha_t$来平衡，这组参数可以是超参数，也可以由类别的比例倒数决定）。作者将其作为比较的baseline。</p>\n<script type=\"math/tex; mode=display\">\\text{CE}(p) = -\\alpha_t\\log(p_t)</script><p>作者提出的则是一个自适应调节的权重，即Focal Loss，定义如下。由上图可以看到$\\gamma$取不同值的时候的函数值变化。作者发现，$\\gamma=2$时能够获得最佳的效果提升。</p>\n<script type=\"math/tex; mode=display\">\\text{FL}(p_t) = -(1-p_t)^\\gamma\\log(p_t)</script><p>在实际实验中，作者使用的是加权之后的Focal Loss，作者发现这样能够带来些微的性能提升。</p>\n<h2 id=\"模型初始化\"><a href=\"#模型初始化\" class=\"headerlink\" title=\"模型初始化\"></a>模型初始化</h2><p>对于一般的分类网络，初始化之后，往往其输出的预测结果是均等的（随机猜测）。然而作者认为，这种初始化方式在类别极度不均衡的时候是有害的。作者提出，应该初始化模型参数，使得初始化之后，模型输出稀有类别的概率变小（如$0.01$）。作者发现这种初始化方法对于交叉熵损失和Focal Loss的性能提升都有帮助。</p>\n<p>在后续的模型介绍部分，作者较为详细地说明了模型初始化方法。首先，从imagenet预训练得到的base net不做调整，新加入的卷积层权重均初始化为$\\sigma=0.01$的高斯分布，偏置项为$0$。对于分类网络的最后一个卷积层，将偏置项置为$b=-\\log((1-\\pi)/\\pi)$。这里的$\\pi$参数是一个超参数，其意义是在训练的初始阶段，每个anchor被分类为前景的概率。在实验中，作者实际使用的大小是$0.01$。</p>\n<p>这样进行模型初始化造成的结果就是，在初始阶段，不会产生大量的False Positive，使得训练更加稳定。</p>\n<h2 id=\"RetinaNet\"><a href=\"#RetinaNet\" class=\"headerlink\" title=\"RetinaNet\"></a>RetinaNet</h2><p>作者利用前面介绍的发现和结论，基于ResNet和Feature Pyramid Net（FPN）设计了一种新的one-stage检测框架，命名为RetinaNet。</p>\n","excerpt":"<p>Focal Loss这篇文章是He Kaiming和Ross发表在ICCV2017上的文章。关于这篇文章在知乎上有相关的<a href=\"https://www.zhihu.com/question/63581984\">讨论</a>。最近一直在做强化学习相关的东西，目标检测方面很长时间不看新的东西了，把自己阅读论文的要点记录如下，也是一次对这方面进展的回顾。</p>\n<p>下图来自于论文，是各种主流模型的比较。其中横轴是前向推断的时间，纵轴是检测器的精度。作者提出的RetinaNet在单独某个维度上都可以吊打其他模型。不过图上没有加入YOLO的对比。YOLO的速度仍然是其一大优势，但是精度和其他方法相比，仍然不高。</p>\n<p><img src=\"/img/focal_loss_different_model_comparison.jpg\" alt=\"不同模型关于精度和速度的比较\"><br>","more":"</p>\n<h2 id=\"为什么要有Focal-Loss？\"><a href=\"#为什么要有Focal-Loss？\" class=\"headerlink\" title=\"为什么要有Focal Loss？\"></a>为什么要有Focal Loss？</h2><p>目前主流的检测算法可以分为两类：one-state和two-stage。前者以YOLO和SSD为代表，后者以RCNN系列为代表。后者的特点是分类器是在一个稀疏的候选目标中进行分类（背景和对应类别），而这是通过前面的proposal过程实现的。例如Seletive Search或者RPN。相对于后者，这种方法是在一个稀疏集合内做分类。与之相反，前者是输出一个稠密的proposal，然后丢进分类器中，直接进行类别分类。后者使用的方法结构一般较为简单，速度较快，但是目前存在的问题是精度不高，普遍不如前者的方法。</p>\n<p>论文作者指出，之所以做稠密分类的后者精度不够高，核心问题（central issus）是稠密proposal中前景和背景的极度不平衡。以我更熟悉的YOLO举例子，比如在PASCAL VOC数据集中，每张图片上标注的目标可能也就几个。但是YOLO V2最后一层的输出是$13 \\times 13 \\times 5$，也就是$845$个候选目标！大量（简单易区分）的负样本在loss中占据了很大比重，使得有用的loss不能回传回来。</p>\n<p>基于此，作者将经典的交叉熵损失做了变形（见下），给那些易于被分类的简单例子小的权重，给不易区分的难例更大的权重。同时，作者提出了一个新的one-stage的检测器RetinaNet，达到了速度和精度很好地trade-off。</p>\n<script type=\"math/tex; mode=display\">\\text{FL}(p_t) = -(1-p_t)^\\gamma \\log(p_t)</script><h2 id=\"物体检测的两种主流方法\"><a href=\"#物体检测的两种主流方法\" class=\"headerlink\" title=\"物体检测的两种主流方法\"></a>物体检测的两种主流方法</h2><p>在深度学习之前，经典的物体检测方法为滑动窗，并使用人工设计的特征。HoG和DPM等方法是其中比较有名的。</p>\n<p>R-CNN系的方法是目前最为流行的物体检测方法之一，同时也是目前精度最高的方法。在R-CNN系方法中，正负类别不平衡这个问题通过前面的proposal解决了。通过EdgeBoxes，Selective Search，DeepMask，RPN等方法，过滤掉了大多数的背景，实际传给后续网络的proposal的数量是比较少的（1-2K）。</p>\n<p>在YOLO，SSD等方法中，需要直接对feature map的大量proposal（100K）进行检测，而且这些proposal很多都在feature map上重叠。大量的负样本带来两个问题：</p>\n<ul>\n<li>过于简单，有效信息过少，使得训练效率低；</li>\n<li>简单的负样本在训练过程中压倒性优势，使得模型发生退化。</li>\n</ul>\n<p>在Faster-RCNN方法中，Huber Loss被用来降低outlier的影响（较大error的样本，也就是难例，传回来的梯度做了clipping，也只能是$1$）。而FocalLoss是对inner中简单的那些样本对loss的贡献进行限制。即使这些简单样本数量很多，也不让它们在训练中占到优势。</p>\n<h2 id=\"Focal-Loss\"><a href=\"#Focal-Loss\" class=\"headerlink\" title=\"Focal Loss\"></a>Focal Loss</h2><p>Focal Loss从交叉熵损失而来。二分类的交叉熵损失如下：</p>\n<script type=\"math/tex; mode=display\">\\text{CE}(p, y) = \\begin{cases}-\\log(p) \\quad &\\text{if}\\quad y = 1\\\\ -\\log(1-p) &\\text{otherwise}\\end{cases}</script><p>对应的，多分类的交叉熵损失是这样的：</p>\n<script type=\"math/tex; mode=display\">\\text{CE}(p, y) = -\\log(p_y)</script><p>如下图所示，蓝色线为交叉熵损失函数随着$p_t$变化的曲线($p_t$意为ground truth，是标注类别所对应的概率)。可以看到，当概率大于$.5$，即认为是易分类的简单样本时，值仍然较大。这样，很多简单样本累加起来，就很可能盖住那些稀少的不易正确分类的类别。<br><img src=\"/img/focal_loss_vs_ce_loss.jpg\" alt=\"FL vs CELoss\"></p>\n<p>为了改善类别样本分布不均衡的问题，已经有人提出了使用加上权重的交叉熵损失，如下（即用参数$\\alpha_t$来平衡，这组参数可以是超参数，也可以由类别的比例倒数决定）。作者将其作为比较的baseline。</p>\n<script type=\"math/tex; mode=display\">\\text{CE}(p) = -\\alpha_t\\log(p_t)</script><p>作者提出的则是一个自适应调节的权重，即Focal Loss，定义如下。由上图可以看到$\\gamma$取不同值的时候的函数值变化。作者发现，$\\gamma=2$时能够获得最佳的效果提升。</p>\n<script type=\"math/tex; mode=display\">\\text{FL}(p_t) = -(1-p_t)^\\gamma\\log(p_t)</script><p>在实际实验中，作者使用的是加权之后的Focal Loss，作者发现这样能够带来些微的性能提升。</p>\n<h2 id=\"模型初始化\"><a href=\"#模型初始化\" class=\"headerlink\" title=\"模型初始化\"></a>模型初始化</h2><p>对于一般的分类网络，初始化之后，往往其输出的预测结果是均等的（随机猜测）。然而作者认为，这种初始化方式在类别极度不均衡的时候是有害的。作者提出，应该初始化模型参数，使得初始化之后，模型输出稀有类别的概率变小（如$0.01$）。作者发现这种初始化方法对于交叉熵损失和Focal Loss的性能提升都有帮助。</p>\n<p>在后续的模型介绍部分，作者较为详细地说明了模型初始化方法。首先，从imagenet预训练得到的base net不做调整，新加入的卷积层权重均初始化为$\\sigma=0.01$的高斯分布，偏置项为$0$。对于分类网络的最后一个卷积层，将偏置项置为$b=-\\log((1-\\pi)/\\pi)$。这里的$\\pi$参数是一个超参数，其意义是在训练的初始阶段，每个anchor被分类为前景的概率。在实验中，作者实际使用的大小是$0.01$。</p>\n<p>这样进行模型初始化造成的结果就是，在初始阶段，不会产生大量的False Positive，使得训练更加稳定。</p>\n<h2 id=\"RetinaNet\"><a href=\"#RetinaNet\" class=\"headerlink\" title=\"RetinaNet\"></a>RetinaNet</h2><p>作者利用前面介绍的发现和结论，基于ResNet和Feature Pyramid Net（FPN）设计了一种新的one-stage检测框架，命名为RetinaNet。</p>"},{"title":"Effective CPP 阅读 - Chapter 8 定制new和delete","date":"2017-07-03T11:47:43.000Z","_content":"手动管理内存，这既是C++的优点，也是C++中很容易出问题的地方。本章主要给出分配内存和归还时候的注意事项，主角是`operator new`和`operator delete`，配角是new_handler，它在当`operator new`无法满足客户内存需求时候被调用。\n\n另外，`operator new`和`operator delete`只用于分配单一对象内存。对于数组，应使用`operator new[]`，并通过`operator delete[]`归还。除非特别指定，本章中的各项既适用于单一`operator new`，也适用于`operator new[]`。\n\n最后，STL中容器使用的堆内存是由容器拥有的分配器对象（allocator objects）来管理的。本章不讨论。\n![memory_leak_everywhere](/img/effectivecpp_08_memory_leak_everywherre.jpg)\n<!-- more -->\n\n## 49 了解new-handler的行为\n什么是new-handler？当`operator new`无法满足内存分配需求时，会抛出异常。在抛出异常之前，会先调用一个客户指定的错误处理函数，这就是所谓的new-handler，也就是一个擦屁股的角色。\n\n为了指定new-handler，必须调用位于标准库`<new>`的函数`set_new_handler`。其声明如下：\n``` cpp\nnamespace std {\n    typedef void (*new_handler) ();\n    new_handler set_new_handler(new_handler p) throw();\n}\n```\n\n其中，传入参数`p`是你要指定的那个擦屁股函数的指针，返回参数是被取代的那个原始处理函数。`throw()`表示该函数不抛出任何异常。\n\n当`operator new`无法满足内存需求时，会不断调用`set_new_handler()`，直到找到足够的内存。更加具体的介绍见条款51.\n\n一个设计良好的new_handler函数可以是以下的设计策略：\n\n- 设法找到更多的内存可供使用，以便使得下一次的`operator new`成功。\n- 安装另一个new_handler函数。即在其中再次调用`set_new_handler`，找到其他的擦屁股函数接盘。\n- 卸载new_handler函数。即将`NULL`指针传进`set_new_handler()`中去。这样，`operator new`会抛出异常。\n- 抛出`bad_alloc`（或其派生类）异常。\n- 不返回（放弃治疗），直接告诉程序exit或abort。\n\n有的时候想为不同的类定制不同的擦屁股函数。这时候，需要为每个类提供自己的`set_new_handler()`函数和`operator new`。如下所示，由于对类的不同对象而言，擦屁股机制都是相同的，所以我们将擦屁股函数声明为类内的静态成员。\n\n``` cpp\nclass A {\npublic:\n    static std::new_handler set_new_handler(std::new_handler p) throw();\n    static void* operator new(std::size_t size) throw(std::bad_alloc);\n\nprivate:\n    static std::new_handler current_handler;\n};\n\n// 实现文件\nstd::new_handler A::set_new_handler(std::new_handler p) throw() {\n    std::new_hanlder old = current_handler;\n    current_handler = p;\n    return old;\n}\n```\n静态成员变量必须在类外进行定义（除非是`const`且为整数型），所以需要在类外定义：\n``` cpp\n// 实现文件\nstd::new_handler A::current_handler = 0;\n```\n\n在实现自定义的`operator new`的时候，首先调用`set_new_handler()`将自己的擦屁股函数安装为默认，然后调用global的`operator new`进行内存分配，最后恢复，把原来的擦屁股函数复原回去。书中，作者使用了一个类进行包装，利用类在scope的自动构造与析构，实现了自动化处理：\n\n``` cpp\n// 这个类实现了自动安装与恢复new_handler\nclass Helper {\npublic:\n    explicit Helper(std::new_handler p): handler(p) {}\n    ~Helper() {std::set_new_handler(handler); }\nprivate:\n    std::new_handler handler;\n    // 禁止拷贝构造与赋值\n    Helper(const Helper&);\n    Helper& operator= (const Helper&);\n};\n// 实现类A自定义的operator new\nvoid* A::operator new(std::size_t size) throw(std::bad_alloc) {\n    // 存储了函数返回值，也就是原始的 new_handler\n    Helper h(std::set_new_handler(current_handler));\n    return ::operator new(size);\n}\n```\n\n新的问题随之而来。如果我们想方便地复用上述代码呢？一个简单的方法是建立一个mixin风格的基类，这种基类用来让派生类继承某个唯一的能力（本例中是设定类的专属new_handler的能力）。而为了让不同的类获得不同的`current_handler`变量，我们把这个基类做成模板。\n\n``` cpp\ntemplate <typename T>\nclass HandlerHelper {\npublic:\n    static std::new_handler set_new_handler(std::new_handler p) throw();\n    static void* operator new(std::size_t size) throw(std::bad_alloc);\n    ... // 其他的new版本，见条款52\nprivate:\n    static std::new_handler current_handler;\n};\n// 实现部分的代码不写了，和上面的Helper和A中的对应内容基本完全一样\n```\n\n这样，我们只要让类`A`继承自`HandlerHelper<A>`即可（看上去很怪异。。。）：\n``` cpp\nclass A: public HandlerHelper<A> {\n    ...\n};\n```\n\n## 50 了解替换`new`和`delete`的合适时机\n最常见的理由（替换之后你能得到什么好处）：\n- 检测运用上的错误。比如缓冲区越界，我们可以在`delete`的时候进行检查。\n- 强化效能。编译器实现的`operator new`是为了普适性的功能，改成自定义版本可能提升效能。\n- 收集使用上的统计数据。为了优化程序性能，理当先收集你的软件如何使用动态内存。自定义的`operator new`和`delete`能够收集到这些信息。\n\n但是，写出能正常工作的`new`却不一定获得很好的性能。（各种细节上的问题，例如内存的对齐。也正因为如此，这里不再重复书上的一个具体实现）例如Boost库中的`Pool`对分配大量小型对象很有帮助。\n\n## 51 编写`new`和`delete`时候需要遵守常规\n自定义的`operator new`需要满足以下几点：\n- 如果有足够的内存，则返回其指针；否则，遵循条款49的约定。\n- 具体地，如果内存不足，那么应该循环调用new_handling函数（里面可能会清理出一些内存以供使用）。只有当指向new_handling的指针为`NULL`时，才抛出异常`bad_alloc`。\n- C++规定，即使用户申请的内存大小为0，也要返回一个合法指针。这个看似诡异的行为是为了简化语言的其他部分。\n- 还要避免掩盖正常的`operator new`。\n\n下面就是一个自定义`operator new`的例子：\n``` cpp\nvoid* operator new(size_t size) throw(bad_alloc) {\n    // 你的operator new也可能接受额外参数\n    using namespace std;\n    if(size == 0) {\n        size = 1; // 处理0byte申请\n    }\n    while(true) {\n        // ... try to alloc memory\n        if(success) {\n            return the pointer;\n        }\n        // 处理分配失败，找出当前的handler\n        // 我们没有诸如get_new_handler()的方法来获取new_handler函数句柄\n        // 所以只能用下面这种方法，利用set_new_handler的返回值获取当前处理函数\n        new_handler globalHandler = set_new_handler(0);\n        set_new_handler(globalHandler);\n\n        if(globalHandler) {\n            (*globalHandler)();\n        } else {\n            throw bad_alloc();\n        }\n    }\n}\n```\n\n在自定义`operator delete`时候，注意处理空指针的情况。C++确保delete NULL pointer是永远安全的。\n``` cpp\nvoid operator delete(void* memory) throw() {\n    if(memory == 0) return;\n    // ...\n}\n```\n\n## 52 写了placement new也要写placement delete\n如果`operator new`接受的参数除了一定会有的那个`size_t`之外还有其他参数，那么它就叫做placement new。一个特别有用的placement new的用法是接受一个指针指向对象该被构造之处。声明如下所示：\n``` cpp\nvoid* operator new(size_t size, void* memory) throw();\n\n```\n\n上述placement new已经被纳入C++规范（可以在头文件`<new>`中找到它。）这个函数常用来在`vector`的未使用空间上构造对象。实际上这是placement的得来：特定位置上的new。有的时候，人们谈论placement new时，实际是在专指这个函数。\n\n本条款主要探讨与placement new使用不当相关的内存泄漏问题。\n当你写一个`new`表达式时，共有两个函数被调用：\n- 分配内存的`operator new`\n- 该类的构造函数\n\n假设第一个函数调用成功，第二个函数却抛出异常。这时候我们需要将第一步申请得到的内存返还并恢复旧观，否则就会造成内存泄漏。具体来说，系统会调用和刚才申请内存的`operator new`对应的delete版本。\n\n如果目前面对的是正常签名的`operator new delete`，不会有问题。不过若是当时调用的是修改过签名形式的placement new时，就可能出现问题。例如，我们有下面的placement new，它的功能是在分配内存的时候做一些logging工作。\n``` cpp\n// 某个类Wedget内部有自定义的placement new如下\nstatic void* operator new(size_t size, ostream& logStream) throw (bad_alloc);\n\nWidget* pw = new (std::cerr) Widget;\n```\n如果系统找不到相应的placement delete版本，就会什么都不做。这样，就无法归还已经申请的内存，造成内存泄漏。所以有必要声明一个placement delete，对应那个有logging功能的placement new。\n``` cpp\nstatic void operator delete(void* memory, ostream& logStream) throw();\n// 这样，即使下式抛出异常，也能正确处理\nWidget* pw = new (std::cerr) Widget;\n```\n\n然而，如果什么异常都没有抛出，而客户又使用了下面的表达式返还内存：\n``` cpp\ndelete pw;\n```\n那么它调用的是正常版本的delete。所以，除了相对应的placement delete，还有必要同时提供正常版本的delete。前者为了解决构造过程中有异常抛出的情况，后者处理无异常抛出。\n\n一个比较简单的做法是，建立一个基类，其中有所有正常形式的new和delete。\n``` cpp\nclass StdNewDeleteForms {\npublic:\n    // 正常的new和delete\n    static void* operator new(std::size_t size) throw std::bad_alloc) {\n        return ::operator new(size);\n    }\n\n    static void operator delete(void* memory) throw() {\n        ::operator delete(memory);\n    }\n    // placement new 和 delete\n    static void* operator new(std::size_t size, void* p) throw() {\n        ::operator new(size, p);\n    }\n    static void operator delete(void* memory, void* p) throw() {\n        ::operator delete(memory, p);\n    }\n    // nothrow new 和 delete\n    static void* operator new(std::size_t size, const std::nothrow_t& nt) throw() {\n        return ::operator new(size, nt);\n    }\n    static void operator delete(void* memory, const std::nothrow_t&) throw() {\n        ::operator delete(mempry);\n    }\n};\n```\n\n上面这个类中包含了C++标准中已经规定好的三种形式的new和delete。那么，凡是想以自定义方式扩充标准形式，可利用继承机制和`using`声明（见条款39），取得标准形式。\n``` cpp\nclass Widget: public StdNewDeleteForms {\npublic:\n    // 使用标准new 和 delete\n    using StdNewDeleteForms::operator new;\n    using StdNetDeleteForms::operator delete;\n    // 添加自定义的placement new 和 delete\n    static void* operator new(std::size_t size,\n        std::ostream& logStream) throw(std::bad_alloc);\n    static void operator delete(void* memory, std::ostream& logStream) throw();\n};\n```\n","source":"_posts/effective-cpp-08.md","raw":"---\ntitle: Effective CPP 阅读 - Chapter 8 定制new和delete\ndate: 2017-07-03 19:47:43\ntags:\n    - cpp\n---\n手动管理内存，这既是C++的优点，也是C++中很容易出问题的地方。本章主要给出分配内存和归还时候的注意事项，主角是`operator new`和`operator delete`，配角是new_handler，它在当`operator new`无法满足客户内存需求时候被调用。\n\n另外，`operator new`和`operator delete`只用于分配单一对象内存。对于数组，应使用`operator new[]`，并通过`operator delete[]`归还。除非特别指定，本章中的各项既适用于单一`operator new`，也适用于`operator new[]`。\n\n最后，STL中容器使用的堆内存是由容器拥有的分配器对象（allocator objects）来管理的。本章不讨论。\n![memory_leak_everywhere](/img/effectivecpp_08_memory_leak_everywherre.jpg)\n<!-- more -->\n\n## 49 了解new-handler的行为\n什么是new-handler？当`operator new`无法满足内存分配需求时，会抛出异常。在抛出异常之前，会先调用一个客户指定的错误处理函数，这就是所谓的new-handler，也就是一个擦屁股的角色。\n\n为了指定new-handler，必须调用位于标准库`<new>`的函数`set_new_handler`。其声明如下：\n``` cpp\nnamespace std {\n    typedef void (*new_handler) ();\n    new_handler set_new_handler(new_handler p) throw();\n}\n```\n\n其中，传入参数`p`是你要指定的那个擦屁股函数的指针，返回参数是被取代的那个原始处理函数。`throw()`表示该函数不抛出任何异常。\n\n当`operator new`无法满足内存需求时，会不断调用`set_new_handler()`，直到找到足够的内存。更加具体的介绍见条款51.\n\n一个设计良好的new_handler函数可以是以下的设计策略：\n\n- 设法找到更多的内存可供使用，以便使得下一次的`operator new`成功。\n- 安装另一个new_handler函数。即在其中再次调用`set_new_handler`，找到其他的擦屁股函数接盘。\n- 卸载new_handler函数。即将`NULL`指针传进`set_new_handler()`中去。这样，`operator new`会抛出异常。\n- 抛出`bad_alloc`（或其派生类）异常。\n- 不返回（放弃治疗），直接告诉程序exit或abort。\n\n有的时候想为不同的类定制不同的擦屁股函数。这时候，需要为每个类提供自己的`set_new_handler()`函数和`operator new`。如下所示，由于对类的不同对象而言，擦屁股机制都是相同的，所以我们将擦屁股函数声明为类内的静态成员。\n\n``` cpp\nclass A {\npublic:\n    static std::new_handler set_new_handler(std::new_handler p) throw();\n    static void* operator new(std::size_t size) throw(std::bad_alloc);\n\nprivate:\n    static std::new_handler current_handler;\n};\n\n// 实现文件\nstd::new_handler A::set_new_handler(std::new_handler p) throw() {\n    std::new_hanlder old = current_handler;\n    current_handler = p;\n    return old;\n}\n```\n静态成员变量必须在类外进行定义（除非是`const`且为整数型），所以需要在类外定义：\n``` cpp\n// 实现文件\nstd::new_handler A::current_handler = 0;\n```\n\n在实现自定义的`operator new`的时候，首先调用`set_new_handler()`将自己的擦屁股函数安装为默认，然后调用global的`operator new`进行内存分配，最后恢复，把原来的擦屁股函数复原回去。书中，作者使用了一个类进行包装，利用类在scope的自动构造与析构，实现了自动化处理：\n\n``` cpp\n// 这个类实现了自动安装与恢复new_handler\nclass Helper {\npublic:\n    explicit Helper(std::new_handler p): handler(p) {}\n    ~Helper() {std::set_new_handler(handler); }\nprivate:\n    std::new_handler handler;\n    // 禁止拷贝构造与赋值\n    Helper(const Helper&);\n    Helper& operator= (const Helper&);\n};\n// 实现类A自定义的operator new\nvoid* A::operator new(std::size_t size) throw(std::bad_alloc) {\n    // 存储了函数返回值，也就是原始的 new_handler\n    Helper h(std::set_new_handler(current_handler));\n    return ::operator new(size);\n}\n```\n\n新的问题随之而来。如果我们想方便地复用上述代码呢？一个简单的方法是建立一个mixin风格的基类，这种基类用来让派生类继承某个唯一的能力（本例中是设定类的专属new_handler的能力）。而为了让不同的类获得不同的`current_handler`变量，我们把这个基类做成模板。\n\n``` cpp\ntemplate <typename T>\nclass HandlerHelper {\npublic:\n    static std::new_handler set_new_handler(std::new_handler p) throw();\n    static void* operator new(std::size_t size) throw(std::bad_alloc);\n    ... // 其他的new版本，见条款52\nprivate:\n    static std::new_handler current_handler;\n};\n// 实现部分的代码不写了，和上面的Helper和A中的对应内容基本完全一样\n```\n\n这样，我们只要让类`A`继承自`HandlerHelper<A>`即可（看上去很怪异。。。）：\n``` cpp\nclass A: public HandlerHelper<A> {\n    ...\n};\n```\n\n## 50 了解替换`new`和`delete`的合适时机\n最常见的理由（替换之后你能得到什么好处）：\n- 检测运用上的错误。比如缓冲区越界，我们可以在`delete`的时候进行检查。\n- 强化效能。编译器实现的`operator new`是为了普适性的功能，改成自定义版本可能提升效能。\n- 收集使用上的统计数据。为了优化程序性能，理当先收集你的软件如何使用动态内存。自定义的`operator new`和`delete`能够收集到这些信息。\n\n但是，写出能正常工作的`new`却不一定获得很好的性能。（各种细节上的问题，例如内存的对齐。也正因为如此，这里不再重复书上的一个具体实现）例如Boost库中的`Pool`对分配大量小型对象很有帮助。\n\n## 51 编写`new`和`delete`时候需要遵守常规\n自定义的`operator new`需要满足以下几点：\n- 如果有足够的内存，则返回其指针；否则，遵循条款49的约定。\n- 具体地，如果内存不足，那么应该循环调用new_handling函数（里面可能会清理出一些内存以供使用）。只有当指向new_handling的指针为`NULL`时，才抛出异常`bad_alloc`。\n- C++规定，即使用户申请的内存大小为0，也要返回一个合法指针。这个看似诡异的行为是为了简化语言的其他部分。\n- 还要避免掩盖正常的`operator new`。\n\n下面就是一个自定义`operator new`的例子：\n``` cpp\nvoid* operator new(size_t size) throw(bad_alloc) {\n    // 你的operator new也可能接受额外参数\n    using namespace std;\n    if(size == 0) {\n        size = 1; // 处理0byte申请\n    }\n    while(true) {\n        // ... try to alloc memory\n        if(success) {\n            return the pointer;\n        }\n        // 处理分配失败，找出当前的handler\n        // 我们没有诸如get_new_handler()的方法来获取new_handler函数句柄\n        // 所以只能用下面这种方法，利用set_new_handler的返回值获取当前处理函数\n        new_handler globalHandler = set_new_handler(0);\n        set_new_handler(globalHandler);\n\n        if(globalHandler) {\n            (*globalHandler)();\n        } else {\n            throw bad_alloc();\n        }\n    }\n}\n```\n\n在自定义`operator delete`时候，注意处理空指针的情况。C++确保delete NULL pointer是永远安全的。\n``` cpp\nvoid operator delete(void* memory) throw() {\n    if(memory == 0) return;\n    // ...\n}\n```\n\n## 52 写了placement new也要写placement delete\n如果`operator new`接受的参数除了一定会有的那个`size_t`之外还有其他参数，那么它就叫做placement new。一个特别有用的placement new的用法是接受一个指针指向对象该被构造之处。声明如下所示：\n``` cpp\nvoid* operator new(size_t size, void* memory) throw();\n\n```\n\n上述placement new已经被纳入C++规范（可以在头文件`<new>`中找到它。）这个函数常用来在`vector`的未使用空间上构造对象。实际上这是placement的得来：特定位置上的new。有的时候，人们谈论placement new时，实际是在专指这个函数。\n\n本条款主要探讨与placement new使用不当相关的内存泄漏问题。\n当你写一个`new`表达式时，共有两个函数被调用：\n- 分配内存的`operator new`\n- 该类的构造函数\n\n假设第一个函数调用成功，第二个函数却抛出异常。这时候我们需要将第一步申请得到的内存返还并恢复旧观，否则就会造成内存泄漏。具体来说，系统会调用和刚才申请内存的`operator new`对应的delete版本。\n\n如果目前面对的是正常签名的`operator new delete`，不会有问题。不过若是当时调用的是修改过签名形式的placement new时，就可能出现问题。例如，我们有下面的placement new，它的功能是在分配内存的时候做一些logging工作。\n``` cpp\n// 某个类Wedget内部有自定义的placement new如下\nstatic void* operator new(size_t size, ostream& logStream) throw (bad_alloc);\n\nWidget* pw = new (std::cerr) Widget;\n```\n如果系统找不到相应的placement delete版本，就会什么都不做。这样，就无法归还已经申请的内存，造成内存泄漏。所以有必要声明一个placement delete，对应那个有logging功能的placement new。\n``` cpp\nstatic void operator delete(void* memory, ostream& logStream) throw();\n// 这样，即使下式抛出异常，也能正确处理\nWidget* pw = new (std::cerr) Widget;\n```\n\n然而，如果什么异常都没有抛出，而客户又使用了下面的表达式返还内存：\n``` cpp\ndelete pw;\n```\n那么它调用的是正常版本的delete。所以，除了相对应的placement delete，还有必要同时提供正常版本的delete。前者为了解决构造过程中有异常抛出的情况，后者处理无异常抛出。\n\n一个比较简单的做法是，建立一个基类，其中有所有正常形式的new和delete。\n``` cpp\nclass StdNewDeleteForms {\npublic:\n    // 正常的new和delete\n    static void* operator new(std::size_t size) throw std::bad_alloc) {\n        return ::operator new(size);\n    }\n\n    static void operator delete(void* memory) throw() {\n        ::operator delete(memory);\n    }\n    // placement new 和 delete\n    static void* operator new(std::size_t size, void* p) throw() {\n        ::operator new(size, p);\n    }\n    static void operator delete(void* memory, void* p) throw() {\n        ::operator delete(memory, p);\n    }\n    // nothrow new 和 delete\n    static void* operator new(std::size_t size, const std::nothrow_t& nt) throw() {\n        return ::operator new(size, nt);\n    }\n    static void operator delete(void* memory, const std::nothrow_t&) throw() {\n        ::operator delete(mempry);\n    }\n};\n```\n\n上面这个类中包含了C++标准中已经规定好的三种形式的new和delete。那么，凡是想以自定义方式扩充标准形式，可利用继承机制和`using`声明（见条款39），取得标准形式。\n``` cpp\nclass Widget: public StdNewDeleteForms {\npublic:\n    // 使用标准new 和 delete\n    using StdNewDeleteForms::operator new;\n    using StdNetDeleteForms::operator delete;\n    // 添加自定义的placement new 和 delete\n    static void* operator new(std::size_t size,\n        std::ostream& logStream) throw(std::bad_alloc);\n    static void operator delete(void* memory, std::ostream& logStream) throw();\n};\n```\n","slug":"effective-cpp-08","published":1,"updated":"2018-01-12T06:22:20.469Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjcu6vct6001cqu4663b946a8","content":"<p>手动管理内存，这既是C++的优点，也是C++中很容易出问题的地方。本章主要给出分配内存和归还时候的注意事项，主角是<code>operator new</code>和<code>operator delete</code>，配角是new_handler，它在当<code>operator new</code>无法满足客户内存需求时候被调用。</p>\n<p>另外，<code>operator new</code>和<code>operator delete</code>只用于分配单一对象内存。对于数组，应使用<code>operator new[]</code>，并通过<code>operator delete[]</code>归还。除非特别指定，本章中的各项既适用于单一<code>operator new</code>，也适用于<code>operator new[]</code>。</p>\n<p>最后，STL中容器使用的堆内存是由容器拥有的分配器对象（allocator objects）来管理的。本章不讨论。<br><img src=\"/img/effectivecpp_08_memory_leak_everywherre.jpg\" alt=\"memory_leak_everywhere\"><br><a id=\"more\"></a></p>\n<h2 id=\"49-了解new-handler的行为\"><a href=\"#49-了解new-handler的行为\" class=\"headerlink\" title=\"49 了解new-handler的行为\"></a>49 了解new-handler的行为</h2><p>什么是new-handler？当<code>operator new</code>无法满足内存分配需求时，会抛出异常。在抛出异常之前，会先调用一个客户指定的错误处理函数，这就是所谓的new-handler，也就是一个擦屁股的角色。</p>\n<p>为了指定new-handler，必须调用位于标准库<code>&lt;new&gt;</code>的函数<code>set_new_handler</code>。其声明如下：<br><figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">namespace</span> <span class=\"built_in\">std</span> &#123;</div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">typedef</span> <span class=\"title\">void</span> <span class=\"params\">(*new_handler)</span> <span class=\"params\">()</span></span>;</div><div class=\"line\">    <span class=\"function\">new_handler <span class=\"title\">set_new_handler</span><span class=\"params\">(new_handler p)</span> <span class=\"title\">throw</span><span class=\"params\">()</span></span>;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure></p>\n<p>其中，传入参数<code>p</code>是你要指定的那个擦屁股函数的指针，返回参数是被取代的那个原始处理函数。<code>throw()</code>表示该函数不抛出任何异常。</p>\n<p>当<code>operator new</code>无法满足内存需求时，会不断调用<code>set_new_handler()</code>，直到找到足够的内存。更加具体的介绍见条款51.</p>\n<p>一个设计良好的new_handler函数可以是以下的设计策略：</p>\n<ul>\n<li>设法找到更多的内存可供使用，以便使得下一次的<code>operator new</code>成功。</li>\n<li>安装另一个new_handler函数。即在其中再次调用<code>set_new_handler</code>，找到其他的擦屁股函数接盘。</li>\n<li>卸载new_handler函数。即将<code>NULL</code>指针传进<code>set_new_handler()</code>中去。这样，<code>operator new</code>会抛出异常。</li>\n<li>抛出<code>bad_alloc</code>（或其派生类）异常。</li>\n<li>不返回（放弃治疗），直接告诉程序exit或abort。</li>\n</ul>\n<p>有的时候想为不同的类定制不同的擦屁股函数。这时候，需要为每个类提供自己的<code>set_new_handler()</code>函数和<code>operator new</code>。如下所示，由于对类的不同对象而言，擦屁股机制都是相同的，所以我们将擦屁股函数声明为类内的静态成员。</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">class</span> A &#123;</div><div class=\"line\"><span class=\"keyword\">public</span>:</div><div class=\"line\">    <span class=\"keyword\">static</span> <span class=\"built_in\">std</span>::<span class=\"function\">new_handler <span class=\"title\">set_new_handler</span><span class=\"params\">(<span class=\"built_in\">std</span>::new_handler p)</span> <span class=\"title\">throw</span><span class=\"params\">()</span></span>;</div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">static</span> <span class=\"keyword\">void</span>* <span class=\"keyword\">operator</span> <span class=\"title\">new</span><span class=\"params\">(<span class=\"built_in\">std</span>::<span class=\"keyword\">size_t</span> size)</span> <span class=\"title\">throw</span><span class=\"params\">(<span class=\"built_in\">std</span>::bad_alloc)</span></span>;</div><div class=\"line\"></div><div class=\"line\"><span class=\"keyword\">private</span>:</div><div class=\"line\">    <span class=\"keyword\">static</span> <span class=\"built_in\">std</span>::new_handler current_handler;</div><div class=\"line\">&#125;;</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">// 实现文件</span></div><div class=\"line\"><span class=\"built_in\">std</span>::new_handler A::set_new_handler(<span class=\"built_in\">std</span>::new_handler p) <span class=\"keyword\">throw</span>() &#123;</div><div class=\"line\">    <span class=\"built_in\">std</span>::new_hanlder old = current_handler;</div><div class=\"line\">    current_handler = p;</div><div class=\"line\">    <span class=\"keyword\">return</span> old;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<p>静态成员变量必须在类外进行定义（除非是<code>const</code>且为整数型），所以需要在类外定义：<br><figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\">// 实现文件</span></div><div class=\"line\"><span class=\"built_in\">std</span>::new_handler A::current_handler = <span class=\"number\">0</span>;</div></pre></td></tr></table></figure></p>\n<p>在实现自定义的<code>operator new</code>的时候，首先调用<code>set_new_handler()</code>将自己的擦屁股函数安装为默认，然后调用global的<code>operator new</code>进行内存分配，最后恢复，把原来的擦屁股函数复原回去。书中，作者使用了一个类进行包装，利用类在scope的自动构造与析构，实现了自动化处理：</p>\n<figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div></pre></td><td class=\"code\"><pre><div class=\"line\">// 这个类实现了自动安装与恢复new_handler</div><div class=\"line\">class Helper &#123;</div><div class=\"line\">public:</div><div class=\"line\">    explicit Helper(std::new_handler p): handler(p) &#123;&#125;</div><div class=\"line\">    ~Helper() &#123;std::set_new_handler(handler); &#125;</div><div class=\"line\">private:</div><div class=\"line\">    std::new_handler handler;</div><div class=\"line\">    // 禁止拷贝构造与赋值</div><div class=\"line\">    Helper(const Helper&amp;);</div><div class=\"line\">    Helper&amp; operator= (const Helper&amp;);</div><div class=\"line\">&#125;;</div><div class=\"line\">// 实现类A自定义的operator new</div><div class=\"line\">void* A::operator new(std::size_t size) throw(std::bad_alloc) &#123;</div><div class=\"line\">    // 存储了函数返回值，也就是原始的 new_handler</div><div class=\"line\">    Helper h(std::set_new_handler(current_handler));</div><div class=\"line\">    return ::operator new(size);</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<p>新的问题随之而来。如果我们想方便地复用上述代码呢？一个简单的方法是建立一个mixin风格的基类，这种基类用来让派生类继承某个唯一的能力（本例中是设定类的专属new_handler的能力）。而为了让不同的类获得不同的<code>current_handler</code>变量，我们把这个基类做成模板。</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">template</span> &lt;<span class=\"keyword\">typename</span> T&gt;</div><div class=\"line\"><span class=\"keyword\">class</span> HandlerHelper &#123;</div><div class=\"line\"><span class=\"keyword\">public</span>:</div><div class=\"line\">    <span class=\"keyword\">static</span> <span class=\"built_in\">std</span>::<span class=\"function\">new_handler <span class=\"title\">set_new_handler</span><span class=\"params\">(<span class=\"built_in\">std</span>::new_handler p)</span> <span class=\"title\">throw</span><span class=\"params\">()</span></span>;</div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">static</span> <span class=\"keyword\">void</span>* <span class=\"keyword\">operator</span> <span class=\"title\">new</span><span class=\"params\">(<span class=\"built_in\">std</span>::<span class=\"keyword\">size_t</span> size)</span> <span class=\"title\">throw</span><span class=\"params\">(<span class=\"built_in\">std</span>::bad_alloc)</span></span>;</div><div class=\"line\">    ... <span class=\"comment\">// 其他的new版本，见条款52</span></div><div class=\"line\"><span class=\"keyword\">private</span>:</div><div class=\"line\">    <span class=\"keyword\">static</span> <span class=\"built_in\">std</span>::new_handler current_handler;</div><div class=\"line\">&#125;;</div><div class=\"line\"><span class=\"comment\">// 实现部分的代码不写了，和上面的Helper和A中的对应内容基本完全一样</span></div></pre></td></tr></table></figure>\n<p>这样，我们只要让类<code>A</code>继承自<code>HandlerHelper&lt;A&gt;</code>即可（看上去很怪异。。。）：<br><figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">class</span> A: <span class=\"keyword\">public</span> HandlerHelper&lt;A&gt; &#123;</div><div class=\"line\">    ...</div><div class=\"line\">&#125;;</div></pre></td></tr></table></figure></p>\n<h2 id=\"50-了解替换new和delete的合适时机\"><a href=\"#50-了解替换new和delete的合适时机\" class=\"headerlink\" title=\"50 了解替换new和delete的合适时机\"></a>50 了解替换<code>new</code>和<code>delete</code>的合适时机</h2><p>最常见的理由（替换之后你能得到什么好处）：</p>\n<ul>\n<li>检测运用上的错误。比如缓冲区越界，我们可以在<code>delete</code>的时候进行检查。</li>\n<li>强化效能。编译器实现的<code>operator new</code>是为了普适性的功能，改成自定义版本可能提升效能。</li>\n<li>收集使用上的统计数据。为了优化程序性能，理当先收集你的软件如何使用动态内存。自定义的<code>operator new</code>和<code>delete</code>能够收集到这些信息。</li>\n</ul>\n<p>但是，写出能正常工作的<code>new</code>却不一定获得很好的性能。（各种细节上的问题，例如内存的对齐。也正因为如此，这里不再重复书上的一个具体实现）例如Boost库中的<code>Pool</code>对分配大量小型对象很有帮助。</p>\n<h2 id=\"51-编写new和delete时候需要遵守常规\"><a href=\"#51-编写new和delete时候需要遵守常规\" class=\"headerlink\" title=\"51 编写new和delete时候需要遵守常规\"></a>51 编写<code>new</code>和<code>delete</code>时候需要遵守常规</h2><p>自定义的<code>operator new</code>需要满足以下几点：</p>\n<ul>\n<li>如果有足够的内存，则返回其指针；否则，遵循条款49的约定。</li>\n<li>具体地，如果内存不足，那么应该循环调用new_handling函数（里面可能会清理出一些内存以供使用）。只有当指向new_handling的指针为<code>NULL</code>时，才抛出异常<code>bad_alloc</code>。</li>\n<li>C++规定，即使用户申请的内存大小为0，也要返回一个合法指针。这个看似诡异的行为是为了简化语言的其他部分。</li>\n<li>还要避免掩盖正常的<code>operator new</code>。</li>\n</ul>\n<p>下面就是一个自定义<code>operator new</code>的例子：<br><figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"function\"><span class=\"keyword\">void</span>* <span class=\"keyword\">operator</span> <span class=\"title\">new</span><span class=\"params\">(<span class=\"keyword\">size_t</span> size)</span> <span class=\"title\">throw</span><span class=\"params\">(bad_alloc)</span> </span>&#123;</div><div class=\"line\">    <span class=\"comment\">// 你的operator new也可能接受额外参数</span></div><div class=\"line\">    <span class=\"keyword\">using</span> <span class=\"keyword\">namespace</span> <span class=\"built_in\">std</span>;</div><div class=\"line\">    <span class=\"keyword\">if</span>(size == <span class=\"number\">0</span>) &#123;</div><div class=\"line\">        size = <span class=\"number\">1</span>; <span class=\"comment\">// 处理0byte申请</span></div><div class=\"line\">    &#125;</div><div class=\"line\">    <span class=\"keyword\">while</span>(<span class=\"literal\">true</span>) &#123;</div><div class=\"line\">        <span class=\"comment\">// ... try to alloc memory</span></div><div class=\"line\">        <span class=\"keyword\">if</span>(success) &#123;</div><div class=\"line\">            <span class=\"keyword\">return</span> the pointer;</div><div class=\"line\">        &#125;</div><div class=\"line\">        <span class=\"comment\">// 处理分配失败，找出当前的handler</span></div><div class=\"line\">        <span class=\"comment\">// 我们没有诸如get_new_handler()的方法来获取new_handler函数句柄</span></div><div class=\"line\">        <span class=\"comment\">// 所以只能用下面这种方法，利用set_new_handler的返回值获取当前处理函数</span></div><div class=\"line\">        new_handler globalHandler = set_new_handler(<span class=\"number\">0</span>);</div><div class=\"line\">        set_new_handler(globalHandler);</div><div class=\"line\"></div><div class=\"line\">        <span class=\"keyword\">if</span>(globalHandler) &#123;</div><div class=\"line\">            (*globalHandler)();</div><div class=\"line\">        &#125; <span class=\"keyword\">else</span> &#123;</div><div class=\"line\">            <span class=\"keyword\">throw</span> bad_alloc();</div><div class=\"line\">        &#125;</div><div class=\"line\">    &#125;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure></p>\n<p>在自定义<code>operator delete</code>时候，注意处理空指针的情况。C++确保delete NULL pointer是永远安全的。<br><figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"function\"><span class=\"keyword\">void</span> <span class=\"keyword\">operator</span> <span class=\"title\">delete</span><span class=\"params\">(<span class=\"keyword\">void</span>* memory)</span> <span class=\"title\">throw</span><span class=\"params\">()</span> </span>&#123;</div><div class=\"line\">    <span class=\"keyword\">if</span>(memory == <span class=\"number\">0</span>) <span class=\"keyword\">return</span>;</div><div class=\"line\">    <span class=\"comment\">// ...</span></div><div class=\"line\">&#125;</div></pre></td></tr></table></figure></p>\n<h2 id=\"52-写了placement-new也要写placement-delete\"><a href=\"#52-写了placement-new也要写placement-delete\" class=\"headerlink\" title=\"52 写了placement new也要写placement delete\"></a>52 写了placement new也要写placement delete</h2><p>如果<code>operator new</code>接受的参数除了一定会有的那个<code>size_t</code>之外还有其他参数，那么它就叫做placement new。一个特别有用的placement new的用法是接受一个指针指向对象该被构造之处。声明如下所示：<br><figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"function\"><span class=\"keyword\">void</span>* <span class=\"keyword\">operator</span> <span class=\"title\">new</span><span class=\"params\">(<span class=\"keyword\">size_t</span> size, <span class=\"keyword\">void</span>* memory)</span> <span class=\"title\">throw</span><span class=\"params\">()</span></span>;</div></pre></td></tr></table></figure></p>\n<p>上述placement new已经被纳入C++规范（可以在头文件<code>&lt;new&gt;</code>中找到它。）这个函数常用来在<code>vector</code>的未使用空间上构造对象。实际上这是placement的得来：特定位置上的new。有的时候，人们谈论placement new时，实际是在专指这个函数。</p>\n<p>本条款主要探讨与placement new使用不当相关的内存泄漏问题。<br>当你写一个<code>new</code>表达式时，共有两个函数被调用：</p>\n<ul>\n<li>分配内存的<code>operator new</code></li>\n<li>该类的构造函数</li>\n</ul>\n<p>假设第一个函数调用成功，第二个函数却抛出异常。这时候我们需要将第一步申请得到的内存返还并恢复旧观，否则就会造成内存泄漏。具体来说，系统会调用和刚才申请内存的<code>operator new</code>对应的delete版本。</p>\n<p>如果目前面对的是正常签名的<code>operator new delete</code>，不会有问题。不过若是当时调用的是修改过签名形式的placement new时，就可能出现问题。例如，我们有下面的placement new，它的功能是在分配内存的时候做一些logging工作。<br><figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\">// 某个类Wedget内部有自定义的placement new如下</span></div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">static</span> <span class=\"keyword\">void</span>* <span class=\"keyword\">operator</span> <span class=\"title\">new</span><span class=\"params\">(<span class=\"keyword\">size_t</span> size, ostream&amp; logStream)</span> <span class=\"title\">throw</span> <span class=\"params\">(bad_alloc)</span></span>;</div><div class=\"line\"></div><div class=\"line\">Widget* pw = <span class=\"keyword\">new</span> (<span class=\"built_in\">std</span>::<span class=\"built_in\">cerr</span>) Widget;</div></pre></td></tr></table></figure></p>\n<p>如果系统找不到相应的placement delete版本，就会什么都不做。这样，就无法归还已经申请的内存，造成内存泄漏。所以有必要声明一个placement delete，对应那个有logging功能的placement new。<br><figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"function\"><span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"keyword\">operator</span> <span class=\"title\">delete</span><span class=\"params\">(<span class=\"keyword\">void</span>* memory, ostream&amp; logStream)</span> <span class=\"title\">throw</span><span class=\"params\">()</span></span>;</div><div class=\"line\"><span class=\"comment\">// 这样，即使下式抛出异常，也能正确处理</span></div><div class=\"line\">Widget* pw = <span class=\"keyword\">new</span> (<span class=\"built_in\">std</span>::<span class=\"built_in\">cerr</span>) Widget;</div></pre></td></tr></table></figure></p>\n<p>然而，如果什么异常都没有抛出，而客户又使用了下面的表达式返还内存：<br><figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">delete</span> pw;</div></pre></td></tr></table></figure></p>\n<p>那么它调用的是正常版本的delete。所以，除了相对应的placement delete，还有必要同时提供正常版本的delete。前者为了解决构造过程中有异常抛出的情况，后者处理无异常抛出。</p>\n<p>一个比较简单的做法是，建立一个基类，其中有所有正常形式的new和delete。<br><figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div></pre></td><td class=\"code\"><pre><div class=\"line\">class StdNewDeleteForms &#123;</div><div class=\"line\">public:</div><div class=\"line\">    // 正常的new和delete</div><div class=\"line\">    static void* operator new(std::size_t size) throw std::bad_alloc) &#123;</div><div class=\"line\">        return ::operator new(size);</div><div class=\"line\">    &#125;</div><div class=\"line\"></div><div class=\"line\">    static void operator delete(void* memory) throw() &#123;</div><div class=\"line\">        ::operator delete(memory);</div><div class=\"line\">    &#125;</div><div class=\"line\">    // placement new 和 delete</div><div class=\"line\">    static void* operator new(std::size_t size, void* p) throw() &#123;</div><div class=\"line\">        ::operator new(size, p);</div><div class=\"line\">    &#125;</div><div class=\"line\">    static void operator delete(void* memory, void* p) throw() &#123;</div><div class=\"line\">        ::operator delete(memory, p);</div><div class=\"line\">    &#125;</div><div class=\"line\">    // nothrow new 和 delete</div><div class=\"line\">    static void* operator new(std::size_t size, const std::nothrow_t&amp; nt) throw() &#123;</div><div class=\"line\">        return ::operator new(size, nt);</div><div class=\"line\">    &#125;</div><div class=\"line\">    static void operator delete(void* memory, const std::nothrow_t&amp;) throw() &#123;</div><div class=\"line\">        ::operator delete(mempry);</div><div class=\"line\">    &#125;</div><div class=\"line\">&#125;;</div></pre></td></tr></table></figure></p>\n<p>上面这个类中包含了C++标准中已经规定好的三种形式的new和delete。那么，凡是想以自定义方式扩充标准形式，可利用继承机制和<code>using</code>声明（见条款39），取得标准形式。<br><figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">class</span> Widget: <span class=\"keyword\">public</span> StdNewDeleteForms &#123;</div><div class=\"line\"><span class=\"keyword\">public</span>:</div><div class=\"line\">    <span class=\"comment\">// 使用标准new 和 delete</span></div><div class=\"line\">    <span class=\"keyword\">using</span> StdNewDeleteForms::<span class=\"keyword\">operator</span> <span class=\"keyword\">new</span>;</div><div class=\"line\">    <span class=\"keyword\">using</span> StdNetDeleteForms::<span class=\"keyword\">operator</span> <span class=\"keyword\">delete</span>;</div><div class=\"line\">    <span class=\"comment\">// 添加自定义的placement new 和 delete</span></div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">static</span> <span class=\"keyword\">void</span>* <span class=\"keyword\">operator</span> <span class=\"title\">new</span><span class=\"params\">(<span class=\"built_in\">std</span>::<span class=\"keyword\">size_t</span> size,</span></span></div><div class=\"line\">        <span class=\"built_in\">std</span>::ostream&amp; logStream) <span class=\"title\">throw</span><span class=\"params\">(<span class=\"built_in\">std</span>::bad_alloc)</span>;</div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"keyword\">operator</span> <span class=\"title\">delete</span><span class=\"params\">(<span class=\"keyword\">void</span>* memory, <span class=\"built_in\">std</span>::ostream&amp; logStream)</span> <span class=\"title\">throw</span><span class=\"params\">()</span></span>;</div><div class=\"line\">&#125;;</div></pre></td></tr></table></figure></p>\n","excerpt":"<p>手动管理内存，这既是C++的优点，也是C++中很容易出问题的地方。本章主要给出分配内存和归还时候的注意事项，主角是<code>operator new</code>和<code>operator delete</code>，配角是new_handler，它在当<code>operator new</code>无法满足客户内存需求时候被调用。</p>\n<p>另外，<code>operator new</code>和<code>operator delete</code>只用于分配单一对象内存。对于数组，应使用<code>operator new[]</code>，并通过<code>operator delete[]</code>归还。除非特别指定，本章中的各项既适用于单一<code>operator new</code>，也适用于<code>operator new[]</code>。</p>\n<p>最后，STL中容器使用的堆内存是由容器拥有的分配器对象（allocator objects）来管理的。本章不讨论。<br><img src=\"/img/effectivecpp_08_memory_leak_everywherre.jpg\" alt=\"memory_leak_everywhere\"><br>","more":"</p>\n<h2 id=\"49-了解new-handler的行为\"><a href=\"#49-了解new-handler的行为\" class=\"headerlink\" title=\"49 了解new-handler的行为\"></a>49 了解new-handler的行为</h2><p>什么是new-handler？当<code>operator new</code>无法满足内存分配需求时，会抛出异常。在抛出异常之前，会先调用一个客户指定的错误处理函数，这就是所谓的new-handler，也就是一个擦屁股的角色。</p>\n<p>为了指定new-handler，必须调用位于标准库<code>&lt;new&gt;</code>的函数<code>set_new_handler</code>。其声明如下：<br><figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">namespace</span> <span class=\"built_in\">std</span> &#123;</div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">typedef</span> <span class=\"title\">void</span> <span class=\"params\">(*new_handler)</span> <span class=\"params\">()</span></span>;</div><div class=\"line\">    <span class=\"function\">new_handler <span class=\"title\">set_new_handler</span><span class=\"params\">(new_handler p)</span> <span class=\"title\">throw</span><span class=\"params\">()</span></span>;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure></p>\n<p>其中，传入参数<code>p</code>是你要指定的那个擦屁股函数的指针，返回参数是被取代的那个原始处理函数。<code>throw()</code>表示该函数不抛出任何异常。</p>\n<p>当<code>operator new</code>无法满足内存需求时，会不断调用<code>set_new_handler()</code>，直到找到足够的内存。更加具体的介绍见条款51.</p>\n<p>一个设计良好的new_handler函数可以是以下的设计策略：</p>\n<ul>\n<li>设法找到更多的内存可供使用，以便使得下一次的<code>operator new</code>成功。</li>\n<li>安装另一个new_handler函数。即在其中再次调用<code>set_new_handler</code>，找到其他的擦屁股函数接盘。</li>\n<li>卸载new_handler函数。即将<code>NULL</code>指针传进<code>set_new_handler()</code>中去。这样，<code>operator new</code>会抛出异常。</li>\n<li>抛出<code>bad_alloc</code>（或其派生类）异常。</li>\n<li>不返回（放弃治疗），直接告诉程序exit或abort。</li>\n</ul>\n<p>有的时候想为不同的类定制不同的擦屁股函数。这时候，需要为每个类提供自己的<code>set_new_handler()</code>函数和<code>operator new</code>。如下所示，由于对类的不同对象而言，擦屁股机制都是相同的，所以我们将擦屁股函数声明为类内的静态成员。</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">class</span> A &#123;</div><div class=\"line\"><span class=\"keyword\">public</span>:</div><div class=\"line\">    <span class=\"keyword\">static</span> <span class=\"built_in\">std</span>::<span class=\"function\">new_handler <span class=\"title\">set_new_handler</span><span class=\"params\">(<span class=\"built_in\">std</span>::new_handler p)</span> <span class=\"title\">throw</span><span class=\"params\">()</span></span>;</div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">static</span> <span class=\"keyword\">void</span>* <span class=\"keyword\">operator</span> <span class=\"title\">new</span><span class=\"params\">(<span class=\"built_in\">std</span>::<span class=\"keyword\">size_t</span> size)</span> <span class=\"title\">throw</span><span class=\"params\">(<span class=\"built_in\">std</span>::bad_alloc)</span></span>;</div><div class=\"line\"></div><div class=\"line\"><span class=\"keyword\">private</span>:</div><div class=\"line\">    <span class=\"keyword\">static</span> <span class=\"built_in\">std</span>::new_handler current_handler;</div><div class=\"line\">&#125;;</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">// 实现文件</span></div><div class=\"line\"><span class=\"built_in\">std</span>::new_handler A::set_new_handler(<span class=\"built_in\">std</span>::new_handler p) <span class=\"keyword\">throw</span>() &#123;</div><div class=\"line\">    <span class=\"built_in\">std</span>::new_hanlder old = current_handler;</div><div class=\"line\">    current_handler = p;</div><div class=\"line\">    <span class=\"keyword\">return</span> old;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<p>静态成员变量必须在类外进行定义（除非是<code>const</code>且为整数型），所以需要在类外定义：<br><figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\">// 实现文件</span></div><div class=\"line\"><span class=\"built_in\">std</span>::new_handler A::current_handler = <span class=\"number\">0</span>;</div></pre></td></tr></table></figure></p>\n<p>在实现自定义的<code>operator new</code>的时候，首先调用<code>set_new_handler()</code>将自己的擦屁股函数安装为默认，然后调用global的<code>operator new</code>进行内存分配，最后恢复，把原来的擦屁股函数复原回去。书中，作者使用了一个类进行包装，利用类在scope的自动构造与析构，实现了自动化处理：</p>\n<figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div></pre></td><td class=\"code\"><pre><div class=\"line\">// 这个类实现了自动安装与恢复new_handler</div><div class=\"line\">class Helper &#123;</div><div class=\"line\">public:</div><div class=\"line\">    explicit Helper(std::new_handler p): handler(p) &#123;&#125;</div><div class=\"line\">    ~Helper() &#123;std::set_new_handler(handler); &#125;</div><div class=\"line\">private:</div><div class=\"line\">    std::new_handler handler;</div><div class=\"line\">    // 禁止拷贝构造与赋值</div><div class=\"line\">    Helper(const Helper&amp;);</div><div class=\"line\">    Helper&amp; operator= (const Helper&amp;);</div><div class=\"line\">&#125;;</div><div class=\"line\">// 实现类A自定义的operator new</div><div class=\"line\">void* A::operator new(std::size_t size) throw(std::bad_alloc) &#123;</div><div class=\"line\">    // 存储了函数返回值，也就是原始的 new_handler</div><div class=\"line\">    Helper h(std::set_new_handler(current_handler));</div><div class=\"line\">    return ::operator new(size);</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<p>新的问题随之而来。如果我们想方便地复用上述代码呢？一个简单的方法是建立一个mixin风格的基类，这种基类用来让派生类继承某个唯一的能力（本例中是设定类的专属new_handler的能力）。而为了让不同的类获得不同的<code>current_handler</code>变量，我们把这个基类做成模板。</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">template</span> &lt;<span class=\"keyword\">typename</span> T&gt;</div><div class=\"line\"><span class=\"keyword\">class</span> HandlerHelper &#123;</div><div class=\"line\"><span class=\"keyword\">public</span>:</div><div class=\"line\">    <span class=\"keyword\">static</span> <span class=\"built_in\">std</span>::<span class=\"function\">new_handler <span class=\"title\">set_new_handler</span><span class=\"params\">(<span class=\"built_in\">std</span>::new_handler p)</span> <span class=\"title\">throw</span><span class=\"params\">()</span></span>;</div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">static</span> <span class=\"keyword\">void</span>* <span class=\"keyword\">operator</span> <span class=\"title\">new</span><span class=\"params\">(<span class=\"built_in\">std</span>::<span class=\"keyword\">size_t</span> size)</span> <span class=\"title\">throw</span><span class=\"params\">(<span class=\"built_in\">std</span>::bad_alloc)</span></span>;</div><div class=\"line\">    ... <span class=\"comment\">// 其他的new版本，见条款52</span></div><div class=\"line\"><span class=\"keyword\">private</span>:</div><div class=\"line\">    <span class=\"keyword\">static</span> <span class=\"built_in\">std</span>::new_handler current_handler;</div><div class=\"line\">&#125;;</div><div class=\"line\"><span class=\"comment\">// 实现部分的代码不写了，和上面的Helper和A中的对应内容基本完全一样</span></div></pre></td></tr></table></figure>\n<p>这样，我们只要让类<code>A</code>继承自<code>HandlerHelper&lt;A&gt;</code>即可（看上去很怪异。。。）：<br><figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">class</span> A: <span class=\"keyword\">public</span> HandlerHelper&lt;A&gt; &#123;</div><div class=\"line\">    ...</div><div class=\"line\">&#125;;</div></pre></td></tr></table></figure></p>\n<h2 id=\"50-了解替换new和delete的合适时机\"><a href=\"#50-了解替换new和delete的合适时机\" class=\"headerlink\" title=\"50 了解替换new和delete的合适时机\"></a>50 了解替换<code>new</code>和<code>delete</code>的合适时机</h2><p>最常见的理由（替换之后你能得到什么好处）：</p>\n<ul>\n<li>检测运用上的错误。比如缓冲区越界，我们可以在<code>delete</code>的时候进行检查。</li>\n<li>强化效能。编译器实现的<code>operator new</code>是为了普适性的功能，改成自定义版本可能提升效能。</li>\n<li>收集使用上的统计数据。为了优化程序性能，理当先收集你的软件如何使用动态内存。自定义的<code>operator new</code>和<code>delete</code>能够收集到这些信息。</li>\n</ul>\n<p>但是，写出能正常工作的<code>new</code>却不一定获得很好的性能。（各种细节上的问题，例如内存的对齐。也正因为如此，这里不再重复书上的一个具体实现）例如Boost库中的<code>Pool</code>对分配大量小型对象很有帮助。</p>\n<h2 id=\"51-编写new和delete时候需要遵守常规\"><a href=\"#51-编写new和delete时候需要遵守常规\" class=\"headerlink\" title=\"51 编写new和delete时候需要遵守常规\"></a>51 编写<code>new</code>和<code>delete</code>时候需要遵守常规</h2><p>自定义的<code>operator new</code>需要满足以下几点：</p>\n<ul>\n<li>如果有足够的内存，则返回其指针；否则，遵循条款49的约定。</li>\n<li>具体地，如果内存不足，那么应该循环调用new_handling函数（里面可能会清理出一些内存以供使用）。只有当指向new_handling的指针为<code>NULL</code>时，才抛出异常<code>bad_alloc</code>。</li>\n<li>C++规定，即使用户申请的内存大小为0，也要返回一个合法指针。这个看似诡异的行为是为了简化语言的其他部分。</li>\n<li>还要避免掩盖正常的<code>operator new</code>。</li>\n</ul>\n<p>下面就是一个自定义<code>operator new</code>的例子：<br><figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"function\"><span class=\"keyword\">void</span>* <span class=\"keyword\">operator</span> <span class=\"title\">new</span><span class=\"params\">(<span class=\"keyword\">size_t</span> size)</span> <span class=\"title\">throw</span><span class=\"params\">(bad_alloc)</span> </span>&#123;</div><div class=\"line\">    <span class=\"comment\">// 你的operator new也可能接受额外参数</span></div><div class=\"line\">    <span class=\"keyword\">using</span> <span class=\"keyword\">namespace</span> <span class=\"built_in\">std</span>;</div><div class=\"line\">    <span class=\"keyword\">if</span>(size == <span class=\"number\">0</span>) &#123;</div><div class=\"line\">        size = <span class=\"number\">1</span>; <span class=\"comment\">// 处理0byte申请</span></div><div class=\"line\">    &#125;</div><div class=\"line\">    <span class=\"keyword\">while</span>(<span class=\"literal\">true</span>) &#123;</div><div class=\"line\">        <span class=\"comment\">// ... try to alloc memory</span></div><div class=\"line\">        <span class=\"keyword\">if</span>(success) &#123;</div><div class=\"line\">            <span class=\"keyword\">return</span> the pointer;</div><div class=\"line\">        &#125;</div><div class=\"line\">        <span class=\"comment\">// 处理分配失败，找出当前的handler</span></div><div class=\"line\">        <span class=\"comment\">// 我们没有诸如get_new_handler()的方法来获取new_handler函数句柄</span></div><div class=\"line\">        <span class=\"comment\">// 所以只能用下面这种方法，利用set_new_handler的返回值获取当前处理函数</span></div><div class=\"line\">        new_handler globalHandler = set_new_handler(<span class=\"number\">0</span>);</div><div class=\"line\">        set_new_handler(globalHandler);</div><div class=\"line\"></div><div class=\"line\">        <span class=\"keyword\">if</span>(globalHandler) &#123;</div><div class=\"line\">            (*globalHandler)();</div><div class=\"line\">        &#125; <span class=\"keyword\">else</span> &#123;</div><div class=\"line\">            <span class=\"keyword\">throw</span> bad_alloc();</div><div class=\"line\">        &#125;</div><div class=\"line\">    &#125;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure></p>\n<p>在自定义<code>operator delete</code>时候，注意处理空指针的情况。C++确保delete NULL pointer是永远安全的。<br><figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"function\"><span class=\"keyword\">void</span> <span class=\"keyword\">operator</span> <span class=\"title\">delete</span><span class=\"params\">(<span class=\"keyword\">void</span>* memory)</span> <span class=\"title\">throw</span><span class=\"params\">()</span> </span>&#123;</div><div class=\"line\">    <span class=\"keyword\">if</span>(memory == <span class=\"number\">0</span>) <span class=\"keyword\">return</span>;</div><div class=\"line\">    <span class=\"comment\">// ...</span></div><div class=\"line\">&#125;</div></pre></td></tr></table></figure></p>\n<h2 id=\"52-写了placement-new也要写placement-delete\"><a href=\"#52-写了placement-new也要写placement-delete\" class=\"headerlink\" title=\"52 写了placement new也要写placement delete\"></a>52 写了placement new也要写placement delete</h2><p>如果<code>operator new</code>接受的参数除了一定会有的那个<code>size_t</code>之外还有其他参数，那么它就叫做placement new。一个特别有用的placement new的用法是接受一个指针指向对象该被构造之处。声明如下所示：<br><figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"function\"><span class=\"keyword\">void</span>* <span class=\"keyword\">operator</span> <span class=\"title\">new</span><span class=\"params\">(<span class=\"keyword\">size_t</span> size, <span class=\"keyword\">void</span>* memory)</span> <span class=\"title\">throw</span><span class=\"params\">()</span></span>;</div></pre></td></tr></table></figure></p>\n<p>上述placement new已经被纳入C++规范（可以在头文件<code>&lt;new&gt;</code>中找到它。）这个函数常用来在<code>vector</code>的未使用空间上构造对象。实际上这是placement的得来：特定位置上的new。有的时候，人们谈论placement new时，实际是在专指这个函数。</p>\n<p>本条款主要探讨与placement new使用不当相关的内存泄漏问题。<br>当你写一个<code>new</code>表达式时，共有两个函数被调用：</p>\n<ul>\n<li>分配内存的<code>operator new</code></li>\n<li>该类的构造函数</li>\n</ul>\n<p>假设第一个函数调用成功，第二个函数却抛出异常。这时候我们需要将第一步申请得到的内存返还并恢复旧观，否则就会造成内存泄漏。具体来说，系统会调用和刚才申请内存的<code>operator new</code>对应的delete版本。</p>\n<p>如果目前面对的是正常签名的<code>operator new delete</code>，不会有问题。不过若是当时调用的是修改过签名形式的placement new时，就可能出现问题。例如，我们有下面的placement new，它的功能是在分配内存的时候做一些logging工作。<br><figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\">// 某个类Wedget内部有自定义的placement new如下</span></div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">static</span> <span class=\"keyword\">void</span>* <span class=\"keyword\">operator</span> <span class=\"title\">new</span><span class=\"params\">(<span class=\"keyword\">size_t</span> size, ostream&amp; logStream)</span> <span class=\"title\">throw</span> <span class=\"params\">(bad_alloc)</span></span>;</div><div class=\"line\"></div><div class=\"line\">Widget* pw = <span class=\"keyword\">new</span> (<span class=\"built_in\">std</span>::<span class=\"built_in\">cerr</span>) Widget;</div></pre></td></tr></table></figure></p>\n<p>如果系统找不到相应的placement delete版本，就会什么都不做。这样，就无法归还已经申请的内存，造成内存泄漏。所以有必要声明一个placement delete，对应那个有logging功能的placement new。<br><figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"function\"><span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"keyword\">operator</span> <span class=\"title\">delete</span><span class=\"params\">(<span class=\"keyword\">void</span>* memory, ostream&amp; logStream)</span> <span class=\"title\">throw</span><span class=\"params\">()</span></span>;</div><div class=\"line\"><span class=\"comment\">// 这样，即使下式抛出异常，也能正确处理</span></div><div class=\"line\">Widget* pw = <span class=\"keyword\">new</span> (<span class=\"built_in\">std</span>::<span class=\"built_in\">cerr</span>) Widget;</div></pre></td></tr></table></figure></p>\n<p>然而，如果什么异常都没有抛出，而客户又使用了下面的表达式返还内存：<br><figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">delete</span> pw;</div></pre></td></tr></table></figure></p>\n<p>那么它调用的是正常版本的delete。所以，除了相对应的placement delete，还有必要同时提供正常版本的delete。前者为了解决构造过程中有异常抛出的情况，后者处理无异常抛出。</p>\n<p>一个比较简单的做法是，建立一个基类，其中有所有正常形式的new和delete。<br><figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div></pre></td><td class=\"code\"><pre><div class=\"line\">class StdNewDeleteForms &#123;</div><div class=\"line\">public:</div><div class=\"line\">    // 正常的new和delete</div><div class=\"line\">    static void* operator new(std::size_t size) throw std::bad_alloc) &#123;</div><div class=\"line\">        return ::operator new(size);</div><div class=\"line\">    &#125;</div><div class=\"line\"></div><div class=\"line\">    static void operator delete(void* memory) throw() &#123;</div><div class=\"line\">        ::operator delete(memory);</div><div class=\"line\">    &#125;</div><div class=\"line\">    // placement new 和 delete</div><div class=\"line\">    static void* operator new(std::size_t size, void* p) throw() &#123;</div><div class=\"line\">        ::operator new(size, p);</div><div class=\"line\">    &#125;</div><div class=\"line\">    static void operator delete(void* memory, void* p) throw() &#123;</div><div class=\"line\">        ::operator delete(memory, p);</div><div class=\"line\">    &#125;</div><div class=\"line\">    // nothrow new 和 delete</div><div class=\"line\">    static void* operator new(std::size_t size, const std::nothrow_t&amp; nt) throw() &#123;</div><div class=\"line\">        return ::operator new(size, nt);</div><div class=\"line\">    &#125;</div><div class=\"line\">    static void operator delete(void* memory, const std::nothrow_t&amp;) throw() &#123;</div><div class=\"line\">        ::operator delete(mempry);</div><div class=\"line\">    &#125;</div><div class=\"line\">&#125;;</div></pre></td></tr></table></figure></p>\n<p>上面这个类中包含了C++标准中已经规定好的三种形式的new和delete。那么，凡是想以自定义方式扩充标准形式，可利用继承机制和<code>using</code>声明（见条款39），取得标准形式。<br><figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">class</span> Widget: <span class=\"keyword\">public</span> StdNewDeleteForms &#123;</div><div class=\"line\"><span class=\"keyword\">public</span>:</div><div class=\"line\">    <span class=\"comment\">// 使用标准new 和 delete</span></div><div class=\"line\">    <span class=\"keyword\">using</span> StdNewDeleteForms::<span class=\"keyword\">operator</span> <span class=\"keyword\">new</span>;</div><div class=\"line\">    <span class=\"keyword\">using</span> StdNetDeleteForms::<span class=\"keyword\">operator</span> <span class=\"keyword\">delete</span>;</div><div class=\"line\">    <span class=\"comment\">// 添加自定义的placement new 和 delete</span></div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">static</span> <span class=\"keyword\">void</span>* <span class=\"keyword\">operator</span> <span class=\"title\">new</span><span class=\"params\">(<span class=\"built_in\">std</span>::<span class=\"keyword\">size_t</span> size,</div><div class=\"line\">        <span class=\"built_in\">std</span>::ostream&amp; logStream)</span> <span class=\"title\">throw</span><span class=\"params\">(<span class=\"built_in\">std</span>::bad_alloc)</span></span>;</div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"keyword\">operator</span> <span class=\"title\">delete</span><span class=\"params\">(<span class=\"keyword\">void</span>* memory, <span class=\"built_in\">std</span>::ostream&amp; logStream)</span> <span class=\"title\">throw</span><span class=\"params\">()</span></span>;</div><div class=\"line\">&#125;;</div></pre></td></tr></table></figure></p>"},{"title":"Hello World","date":"2016-12-16T11:00:00.000Z","_content":"Welcome to [Hexo](https://hexo.io/)! This is your very first post. Check [documentation](https://hexo.io/docs/) for more info. If you get any problems when using Hexo, you can find the answer in [troubleshooting](https://hexo.io/docs/troubleshooting.html) or you can ask me on [GitHub](https://github.com/hexojs/hexo/issues).\n\n![Hexo](/img/helloworld_hexo.png)\n<!-- more -->\n## Quick Start\n\n### Create a new post\n\n``` bash\n$ hexo new \"My New Post\"\n```\n\nMore info: [Writing](https://hexo.io/docs/writing.html)\n\n### Run server\n\n``` bash\n$ hexo server\n```\n\nMore info: [Server](https://hexo.io/docs/server.html)\n\n### Generate static files\n\n``` bash\n$ hexo generate\n```\n\nMore info: [Generating](https://hexo.io/docs/generating.html)\n\n### Deploy to remote sites\n\n``` bash\n$ hexo deploy\n```\n\nMore info: [Deployment](https://hexo.io/docs/deployment.html)\n\n### Code highlight\n\nHello World!\n\n``` cpp\n#include <iostream>\nint main() {\n    std::cout << \"HelloWorld\\n\";\n}\n```\n\n``` py\nprint 'HelloWorld'\n```\n\n### Latex Support by Mathjax\n\nMass-energy equation by Einstein: $E = mc^2$\n\na linear equation:\n\n$$\\mathbf{A}\\mathbf{v} = \\mathbf{y}$$\n","source":"_posts/hello-world.md","raw":"---\ntitle: Hello World\ndate: 2016-12-16 19:00:00\n---\nWelcome to [Hexo](https://hexo.io/)! This is your very first post. Check [documentation](https://hexo.io/docs/) for more info. If you get any problems when using Hexo, you can find the answer in [troubleshooting](https://hexo.io/docs/troubleshooting.html) or you can ask me on [GitHub](https://github.com/hexojs/hexo/issues).\n\n![Hexo](/img/helloworld_hexo.png)\n<!-- more -->\n## Quick Start\n\n### Create a new post\n\n``` bash\n$ hexo new \"My New Post\"\n```\n\nMore info: [Writing](https://hexo.io/docs/writing.html)\n\n### Run server\n\n``` bash\n$ hexo server\n```\n\nMore info: [Server](https://hexo.io/docs/server.html)\n\n### Generate static files\n\n``` bash\n$ hexo generate\n```\n\nMore info: [Generating](https://hexo.io/docs/generating.html)\n\n### Deploy to remote sites\n\n``` bash\n$ hexo deploy\n```\n\nMore info: [Deployment](https://hexo.io/docs/deployment.html)\n\n### Code highlight\n\nHello World!\n\n``` cpp\n#include <iostream>\nint main() {\n    std::cout << \"HelloWorld\\n\";\n}\n```\n\n``` py\nprint 'HelloWorld'\n```\n\n### Latex Support by Mathjax\n\nMass-energy equation by Einstein: $E = mc^2$\n\na linear equation:\n\n$$\\mathbf{A}\\mathbf{v} = \\mathbf{y}$$\n","slug":"hello-world","published":1,"updated":"2018-01-12T06:22:20.470Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjcu6vct7001equ46tom0lv9w","content":"<p>Welcome to <a href=\"https://hexo.io/\" target=\"_blank\" rel=\"external\">Hexo</a>! This is your very first post. Check <a href=\"https://hexo.io/docs/\" target=\"_blank\" rel=\"external\">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href=\"https://hexo.io/docs/troubleshooting.html\" target=\"_blank\" rel=\"external\">troubleshooting</a> or you can ask me on <a href=\"https://github.com/hexojs/hexo/issues\" target=\"_blank\" rel=\"external\">GitHub</a>.</p>\n<p><img src=\"/img/helloworld_hexo.png\" alt=\"Hexo\"><br><a id=\"more\"></a></p>\n<h2 id=\"Quick-Start\"><a href=\"#Quick-Start\" class=\"headerlink\" title=\"Quick Start\"></a>Quick Start</h2><h3 id=\"Create-a-new-post\"><a href=\"#Create-a-new-post\" class=\"headerlink\" title=\"Create a new post\"></a>Create a new post</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">$ hexo new <span class=\"string\">\"My New Post\"</span></div></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/writing.html\" target=\"_blank\" rel=\"external\">Writing</a></p>\n<h3 id=\"Run-server\"><a href=\"#Run-server\" class=\"headerlink\" title=\"Run server\"></a>Run server</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">$ hexo server</div></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/server.html\" target=\"_blank\" rel=\"external\">Server</a></p>\n<h3 id=\"Generate-static-files\"><a href=\"#Generate-static-files\" class=\"headerlink\" title=\"Generate static files\"></a>Generate static files</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">$ hexo generate</div></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/generating.html\" target=\"_blank\" rel=\"external\">Generating</a></p>\n<h3 id=\"Deploy-to-remote-sites\"><a href=\"#Deploy-to-remote-sites\" class=\"headerlink\" title=\"Deploy to remote sites\"></a>Deploy to remote sites</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">$ hexo deploy</div></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/deployment.html\" target=\"_blank\" rel=\"external\">Deployment</a></p>\n<h3 id=\"Code-highlight\"><a href=\"#Code-highlight\" class=\"headerlink\" title=\"Code highlight\"></a>Code highlight</h3><p>Hello World!</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">include</span> <span class=\"meta-string\">&lt;iostream&gt;</span></span></div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">int</span> <span class=\"title\">main</span><span class=\"params\">()</span> </span>&#123;</div><div class=\"line\">    <span class=\"built_in\">std</span>::<span class=\"built_in\">cout</span> &lt;&lt; <span class=\"string\">\"HelloWorld\\n\"</span>;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">print</span> <span class=\"string\">'HelloWorld'</span></div></pre></td></tr></table></figure>\n<h3 id=\"Latex-Support-by-Mathjax\"><a href=\"#Latex-Support-by-Mathjax\" class=\"headerlink\" title=\"Latex Support by Mathjax\"></a>Latex Support by Mathjax</h3><p>Mass-energy equation by Einstein: $E = mc^2$</p>\n<p>a linear equation:</p>\n<script type=\"math/tex; mode=display\">\\mathbf{A}\\mathbf{v} = \\mathbf{y}</script>","excerpt":"<p>Welcome to <a href=\"https://hexo.io/\">Hexo</a>! This is your very first post. Check <a href=\"https://hexo.io/docs/\">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href=\"https://hexo.io/docs/troubleshooting.html\">troubleshooting</a> or you can ask me on <a href=\"https://github.com/hexojs/hexo/issues\">GitHub</a>.</p>\n<p><img src=\"/img/helloworld_hexo.png\" alt=\"Hexo\"><br>","more":"</p>\n<h2 id=\"Quick-Start\"><a href=\"#Quick-Start\" class=\"headerlink\" title=\"Quick Start\"></a>Quick Start</h2><h3 id=\"Create-a-new-post\"><a href=\"#Create-a-new-post\" class=\"headerlink\" title=\"Create a new post\"></a>Create a new post</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">$ hexo new <span class=\"string\">\"My New Post\"</span></div></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/writing.html\">Writing</a></p>\n<h3 id=\"Run-server\"><a href=\"#Run-server\" class=\"headerlink\" title=\"Run server\"></a>Run server</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">$ hexo server</div></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/server.html\">Server</a></p>\n<h3 id=\"Generate-static-files\"><a href=\"#Generate-static-files\" class=\"headerlink\" title=\"Generate static files\"></a>Generate static files</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">$ hexo generate</div></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/generating.html\">Generating</a></p>\n<h3 id=\"Deploy-to-remote-sites\"><a href=\"#Deploy-to-remote-sites\" class=\"headerlink\" title=\"Deploy to remote sites\"></a>Deploy to remote sites</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">$ hexo deploy</div></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/deployment.html\">Deployment</a></p>\n<h3 id=\"Code-highlight\"><a href=\"#Code-highlight\" class=\"headerlink\" title=\"Code highlight\"></a>Code highlight</h3><p>Hello World!</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">include</span> <span class=\"meta-string\">&lt;iostream&gt;</span></span></div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">int</span> <span class=\"title\">main</span><span class=\"params\">()</span> </span>&#123;</div><div class=\"line\">    <span class=\"built_in\">std</span>::<span class=\"built_in\">cout</span> &lt;&lt; <span class=\"string\">\"HelloWorld\\n\"</span>;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">print</span> <span class=\"string\">'HelloWorld'</span></div></pre></td></tr></table></figure>\n<h3 id=\"Latex-Support-by-Mathjax\"><a href=\"#Latex-Support-by-Mathjax\" class=\"headerlink\" title=\"Latex Support by Mathjax\"></a>Latex Support by Mathjax</h3><p>Mass-energy equation by Einstein: $E = mc^2$</p>\n<p>a linear equation:</p>\n<script type=\"math/tex; mode=display\">\\mathbf{A}\\mathbf{v} = \\mathbf{y}</script>"},{"title":"过秦论","date":"2017-10-27T01:56:00.000Z","_content":"秦以耕战立国，起于西北，从边陲弱小诸侯，借以天时地利，用商鞅、李斯，举法家大旗。而后逐鹿中原，坑赵括，灌大梁，掳六国贵族，结束战乱，建立大一统帝国。然而，强秦却二世而亡。“始皇之心，自以为关中之固，金城千里，子孙帝王万世之业也。”封建王朝的皇帝总觉得自己的国祚能够绵延不绝，历史却一次次地打他们的脸。\n![just a joke](/img/just-a-joke.png)\n\n<!-- more -->\n　　秦孝公据崤函之固，拥雍州之地，君臣固守以窥周室，有席卷天下，包举宇内，囊括四海之意，并吞八荒之心。当是时也，商君佐之，内立法度，务耕织，修守战之具；外连衡而斗诸侯。于是秦人拱手而取西河之外。 \n\n　　孝公既没，惠文、武、昭襄蒙故业，因遗策，南取汉中，西举巴、蜀，东割膏腴之地，北收要害之郡。诸侯恐惧，会盟而谋弱秦，不爱珍器重宝肥饶之地，以致天下之士，合从缔交，相与为一。当此之时，齐有孟尝，赵有平原，楚有春申，魏有信陵。此四君者，皆明智而忠信，宽厚而爱人，尊贤而重士，约从离衡，兼韩、魏、燕、楚、齐、赵、宋、卫、中山之众。于是六国之士，有宁越、徐尚、苏秦、杜赫之属为之谋，齐明、周最、陈轸、召滑、楼缓、翟景、苏厉、乐毅之徒通其意，吴起、孙膑、带佗、倪良、王廖、田忌、廉颇、赵奢之伦制其兵。尝以十倍之地，百万之众，叩关而攻秦。秦人开关延敌，九国之师，逡巡而不敢进。秦无亡矢遗镞之费，而天下诸侯已困矣。于是从散约败，争割地而赂秦。秦有余力而制其弊，追亡逐北，伏尸百万，流血漂橹。因利乘便，宰割天下，分裂山河。强国请服，弱国入朝。 \n\n　　延及孝文王、庄襄王，享国之日浅，国家无事。及至始皇，奋六世之余烈，振长策而御宇内，吞二周而亡诸侯，履至尊而制六合，执敲扑而鞭笞天下，威振四海。南取百越之地，以为桂林、象郡；百越之君，俯首系颈，委命下吏。乃使蒙恬北筑长城而守藩篱，却匈奴七百余里。胡人不敢南下而牧马，士不敢弯弓而报怨。于是废先王之道，焚百家之言，以愚黔首；隳名城，杀豪杰，收天下之兵，聚之咸阳，销锋镝，铸以为金人十二，以弱天下之民。然后践华为城，因河为池，据亿丈之城，临不测之渊，以为固。良将劲弩守要害之处，信臣精卒陈利兵而谁何。天下已定，始皇之心，自以为关中之固，金城千里，子孙帝王万世之业也。\n\n　　始皇既没，余威震于殊俗。然陈涉瓮牖绳枢之子，氓隶之人，而迁徙之徒也；才能不及中人，非有仲尼、墨翟之贤，陶朱、猗顿之富；蹑足行伍之间，而倔起阡陌之中，率疲弊之卒，将数百之众，转而攻秦，斩木为兵，揭竿为旗，天下云集响应，赢粮而景从。山东豪俊遂并起而亡秦族矣。 \n\n　　且夫天下非小弱也，雍州之地，崤函之固，自若也。陈涉之位，非尊于齐、楚、燕、赵、韩、魏、宋、卫、中山之君也；锄櫌棘矜，非铦于钩戟长铩也；谪戍之众，非抗于九国之师也；深谋远虑，行军用兵之道，非及向时之士也。然而成败异变，功业相反，何也？试使山东之国与陈涉度长絜大，比权量力，则不可同年而语矣。然秦以区区之地，致万乘之势，序八州而朝同列，百有余年矣；然后以六合为家，崤函为宫；一夫作难而七庙隳，身死人手，为天下笑者，何也？仁义不施而攻守之势异也。\n","source":"_posts/fuck-gfw.md","raw":"---\ntitle: 过秦论\ndate: 2017-10-27 09:56:00\ntags:\n    - 扯淡\n---\n秦以耕战立国，起于西北，从边陲弱小诸侯，借以天时地利，用商鞅、李斯，举法家大旗。而后逐鹿中原，坑赵括，灌大梁，掳六国贵族，结束战乱，建立大一统帝国。然而，强秦却二世而亡。“始皇之心，自以为关中之固，金城千里，子孙帝王万世之业也。”封建王朝的皇帝总觉得自己的国祚能够绵延不绝，历史却一次次地打他们的脸。\n![just a joke](/img/just-a-joke.png)\n\n<!-- more -->\n　　秦孝公据崤函之固，拥雍州之地，君臣固守以窥周室，有席卷天下，包举宇内，囊括四海之意，并吞八荒之心。当是时也，商君佐之，内立法度，务耕织，修守战之具；外连衡而斗诸侯。于是秦人拱手而取西河之外。 \n\n　　孝公既没，惠文、武、昭襄蒙故业，因遗策，南取汉中，西举巴、蜀，东割膏腴之地，北收要害之郡。诸侯恐惧，会盟而谋弱秦，不爱珍器重宝肥饶之地，以致天下之士，合从缔交，相与为一。当此之时，齐有孟尝，赵有平原，楚有春申，魏有信陵。此四君者，皆明智而忠信，宽厚而爱人，尊贤而重士，约从离衡，兼韩、魏、燕、楚、齐、赵、宋、卫、中山之众。于是六国之士，有宁越、徐尚、苏秦、杜赫之属为之谋，齐明、周最、陈轸、召滑、楼缓、翟景、苏厉、乐毅之徒通其意，吴起、孙膑、带佗、倪良、王廖、田忌、廉颇、赵奢之伦制其兵。尝以十倍之地，百万之众，叩关而攻秦。秦人开关延敌，九国之师，逡巡而不敢进。秦无亡矢遗镞之费，而天下诸侯已困矣。于是从散约败，争割地而赂秦。秦有余力而制其弊，追亡逐北，伏尸百万，流血漂橹。因利乘便，宰割天下，分裂山河。强国请服，弱国入朝。 \n\n　　延及孝文王、庄襄王，享国之日浅，国家无事。及至始皇，奋六世之余烈，振长策而御宇内，吞二周而亡诸侯，履至尊而制六合，执敲扑而鞭笞天下，威振四海。南取百越之地，以为桂林、象郡；百越之君，俯首系颈，委命下吏。乃使蒙恬北筑长城而守藩篱，却匈奴七百余里。胡人不敢南下而牧马，士不敢弯弓而报怨。于是废先王之道，焚百家之言，以愚黔首；隳名城，杀豪杰，收天下之兵，聚之咸阳，销锋镝，铸以为金人十二，以弱天下之民。然后践华为城，因河为池，据亿丈之城，临不测之渊，以为固。良将劲弩守要害之处，信臣精卒陈利兵而谁何。天下已定，始皇之心，自以为关中之固，金城千里，子孙帝王万世之业也。\n\n　　始皇既没，余威震于殊俗。然陈涉瓮牖绳枢之子，氓隶之人，而迁徙之徒也；才能不及中人，非有仲尼、墨翟之贤，陶朱、猗顿之富；蹑足行伍之间，而倔起阡陌之中，率疲弊之卒，将数百之众，转而攻秦，斩木为兵，揭竿为旗，天下云集响应，赢粮而景从。山东豪俊遂并起而亡秦族矣。 \n\n　　且夫天下非小弱也，雍州之地，崤函之固，自若也。陈涉之位，非尊于齐、楚、燕、赵、韩、魏、宋、卫、中山之君也；锄櫌棘矜，非铦于钩戟长铩也；谪戍之众，非抗于九国之师也；深谋远虑，行军用兵之道，非及向时之士也。然而成败异变，功业相反，何也？试使山东之国与陈涉度长絜大，比权量力，则不可同年而语矣。然秦以区区之地，致万乘之势，序八州而朝同列，百有余年矣；然后以六合为家，崤函为宫；一夫作难而七庙隳，身死人手，为天下笑者，何也？仁义不施而攻守之势异也。\n","slug":"fuck-gfw","published":1,"updated":"2018-01-12T06:22:20.470Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjcu6vcta001gqu46nmp16g5g","content":"<p>秦以耕战立国，起于西北，从边陲弱小诸侯，借以天时地利，用商鞅、李斯，举法家大旗。而后逐鹿中原，坑赵括，灌大梁，掳六国贵族，结束战乱，建立大一统帝国。然而，强秦却二世而亡。“始皇之心，自以为关中之固，金城千里，子孙帝王万世之业也。”封建王朝的皇帝总觉得自己的国祚能够绵延不绝，历史却一次次地打他们的脸。<br><img src=\"/img/just-a-joke.png\" alt=\"just a joke\"></p>\n<a id=\"more\"></a>\n<p>　　秦孝公据崤函之固，拥雍州之地，君臣固守以窥周室，有席卷天下，包举宇内，囊括四海之意，并吞八荒之心。当是时也，商君佐之，内立法度，务耕织，修守战之具；外连衡而斗诸侯。于是秦人拱手而取西河之外。 </p>\n<p>　　孝公既没，惠文、武、昭襄蒙故业，因遗策，南取汉中，西举巴、蜀，东割膏腴之地，北收要害之郡。诸侯恐惧，会盟而谋弱秦，不爱珍器重宝肥饶之地，以致天下之士，合从缔交，相与为一。当此之时，齐有孟尝，赵有平原，楚有春申，魏有信陵。此四君者，皆明智而忠信，宽厚而爱人，尊贤而重士，约从离衡，兼韩、魏、燕、楚、齐、赵、宋、卫、中山之众。于是六国之士，有宁越、徐尚、苏秦、杜赫之属为之谋，齐明、周最、陈轸、召滑、楼缓、翟景、苏厉、乐毅之徒通其意，吴起、孙膑、带佗、倪良、王廖、田忌、廉颇、赵奢之伦制其兵。尝以十倍之地，百万之众，叩关而攻秦。秦人开关延敌，九国之师，逡巡而不敢进。秦无亡矢遗镞之费，而天下诸侯已困矣。于是从散约败，争割地而赂秦。秦有余力而制其弊，追亡逐北，伏尸百万，流血漂橹。因利乘便，宰割天下，分裂山河。强国请服，弱国入朝。 </p>\n<p>　　延及孝文王、庄襄王，享国之日浅，国家无事。及至始皇，奋六世之余烈，振长策而御宇内，吞二周而亡诸侯，履至尊而制六合，执敲扑而鞭笞天下，威振四海。南取百越之地，以为桂林、象郡；百越之君，俯首系颈，委命下吏。乃使蒙恬北筑长城而守藩篱，却匈奴七百余里。胡人不敢南下而牧马，士不敢弯弓而报怨。于是废先王之道，焚百家之言，以愚黔首；隳名城，杀豪杰，收天下之兵，聚之咸阳，销锋镝，铸以为金人十二，以弱天下之民。然后践华为城，因河为池，据亿丈之城，临不测之渊，以为固。良将劲弩守要害之处，信臣精卒陈利兵而谁何。天下已定，始皇之心，自以为关中之固，金城千里，子孙帝王万世之业也。</p>\n<p>　　始皇既没，余威震于殊俗。然陈涉瓮牖绳枢之子，氓隶之人，而迁徙之徒也；才能不及中人，非有仲尼、墨翟之贤，陶朱、猗顿之富；蹑足行伍之间，而倔起阡陌之中，率疲弊之卒，将数百之众，转而攻秦，斩木为兵，揭竿为旗，天下云集响应，赢粮而景从。山东豪俊遂并起而亡秦族矣。 </p>\n<p>　　且夫天下非小弱也，雍州之地，崤函之固，自若也。陈涉之位，非尊于齐、楚、燕、赵、韩、魏、宋、卫、中山之君也；锄櫌棘矜，非铦于钩戟长铩也；谪戍之众，非抗于九国之师也；深谋远虑，行军用兵之道，非及向时之士也。然而成败异变，功业相反，何也？试使山东之国与陈涉度长絜大，比权量力，则不可同年而语矣。然秦以区区之地，致万乘之势，序八州而朝同列，百有余年矣；然后以六合为家，崤函为宫；一夫作难而七庙隳，身死人手，为天下笑者，何也？仁义不施而攻守之势异也。</p>\n","excerpt":"<p>秦以耕战立国，起于西北，从边陲弱小诸侯，借以天时地利，用商鞅、李斯，举法家大旗。而后逐鹿中原，坑赵括，灌大梁，掳六国贵族，结束战乱，建立大一统帝国。然而，强秦却二世而亡。“始皇之心，自以为关中之固，金城千里，子孙帝王万世之业也。”封建王朝的皇帝总觉得自己的国祚能够绵延不绝，历史却一次次地打他们的脸。<br><img src=\"/img/just-a-joke.png\" alt=\"just a joke\"></p>","more":"<p>　　秦孝公据崤函之固，拥雍州之地，君臣固守以窥周室，有席卷天下，包举宇内，囊括四海之意，并吞八荒之心。当是时也，商君佐之，内立法度，务耕织，修守战之具；外连衡而斗诸侯。于是秦人拱手而取西河之外。 </p>\n<p>　　孝公既没，惠文、武、昭襄蒙故业，因遗策，南取汉中，西举巴、蜀，东割膏腴之地，北收要害之郡。诸侯恐惧，会盟而谋弱秦，不爱珍器重宝肥饶之地，以致天下之士，合从缔交，相与为一。当此之时，齐有孟尝，赵有平原，楚有春申，魏有信陵。此四君者，皆明智而忠信，宽厚而爱人，尊贤而重士，约从离衡，兼韩、魏、燕、楚、齐、赵、宋、卫、中山之众。于是六国之士，有宁越、徐尚、苏秦、杜赫之属为之谋，齐明、周最、陈轸、召滑、楼缓、翟景、苏厉、乐毅之徒通其意，吴起、孙膑、带佗、倪良、王廖、田忌、廉颇、赵奢之伦制其兵。尝以十倍之地，百万之众，叩关而攻秦。秦人开关延敌，九国之师，逡巡而不敢进。秦无亡矢遗镞之费，而天下诸侯已困矣。于是从散约败，争割地而赂秦。秦有余力而制其弊，追亡逐北，伏尸百万，流血漂橹。因利乘便，宰割天下，分裂山河。强国请服，弱国入朝。 </p>\n<p>　　延及孝文王、庄襄王，享国之日浅，国家无事。及至始皇，奋六世之余烈，振长策而御宇内，吞二周而亡诸侯，履至尊而制六合，执敲扑而鞭笞天下，威振四海。南取百越之地，以为桂林、象郡；百越之君，俯首系颈，委命下吏。乃使蒙恬北筑长城而守藩篱，却匈奴七百余里。胡人不敢南下而牧马，士不敢弯弓而报怨。于是废先王之道，焚百家之言，以愚黔首；隳名城，杀豪杰，收天下之兵，聚之咸阳，销锋镝，铸以为金人十二，以弱天下之民。然后践华为城，因河为池，据亿丈之城，临不测之渊，以为固。良将劲弩守要害之处，信臣精卒陈利兵而谁何。天下已定，始皇之心，自以为关中之固，金城千里，子孙帝王万世之业也。</p>\n<p>　　始皇既没，余威震于殊俗。然陈涉瓮牖绳枢之子，氓隶之人，而迁徙之徒也；才能不及中人，非有仲尼、墨翟之贤，陶朱、猗顿之富；蹑足行伍之间，而倔起阡陌之中，率疲弊之卒，将数百之众，转而攻秦，斩木为兵，揭竿为旗，天下云集响应，赢粮而景从。山东豪俊遂并起而亡秦族矣。 </p>\n<p>　　且夫天下非小弱也，雍州之地，崤函之固，自若也。陈涉之位，非尊于齐、楚、燕、赵、韩、魏、宋、卫、中山之君也；锄櫌棘矜，非铦于钩戟长铩也；谪戍之众，非抗于九国之师也；深谋远虑，行军用兵之道，非及向时之士也。然而成败异变，功业相反，何也？试使山东之国与陈涉度长絜大，比权量力，则不可同年而语矣。然秦以区区之地，致万乘之势，序八州而朝同列，百有余年矣；然后以六合为家，崤函为宫；一夫作难而七庙隳，身死人手，为天下笑者，何也？仁义不施而攻守之势异也。</p>"},{"title":"Neural Network for Machine Learning - Lecture 02","date":"2017-05-25T05:26:33.000Z","_content":"这是第二周的课程内容，主要介绍了几种神经网络的分类，详细地介绍了感知机这一最简单的模型。\n\n![Perceptron](/img/hinton_02_perceptron_gragh.png)\n\n<!-- more -->\n## Different neural network archs\n### Feed-forward neural network 前馈神经网络\n前馈神经网络可能是最常见的网络，主要由输入层，若干隐含层和输出层组成。一般，当隐含层数目超过$1$时，我们可以说网络是deep的。\n![前馈网络](/img/hinton_02_feed_forward_nn.png)\n\n### Recurrent network\nRNN内部的节点之间存在有向的环，这使得它能够使用内部状态来对动态过程建模。RNN能力强大，但是不易训练。\n![RNN](/img/hinton_02_recurrent_nn.png)\n\nRNN常用来对序列进行建模（modeling squence）。这里有一篇[不错的介绍](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)。\n![RNN modeling squences](/img/hinton_02_rnn_app.png)\n\n### Symmetrically connected network\n这种网络结构上很像RNN，但是它的节点之间的连接是对称的，意思是说由此到彼和由彼到此的权重相同。\n\n## Percetron\n### 训练\n最早的神经网络应用是感知机。使用感知机等统计学习方法进行模式分类的一般思路是：\n- 将原始输入向量转化为feature activation。\n- 寻找合适的权重对feature进行加权，得到某个标量。\n- 如果这个标量大于某一给定的阈值，则分类为正；否则分类为负。\n\n![通用思路](/img/hinton_02_perceptron_paradigm_for_pattern_recong.png)\n\n对于决策节点，常常使用Binary threshold neuron，将输入映射为${0,1}$。而这相当于给输入加上一个偏置项，然后和$0$比较。\n\n所以，感知机的数学模型可以描述如下：\n\n$$y=\\begin{cases}1, \\quad \\text{if} \\quad \\sum w_ix_i+b>0 \\\\ 0 \\quad \\text{otherwise}\\end{cases}$$\n\n训练时，遍历样本集中的样本点，根据真实值与预测值是否相同，有如下的更新方法：\n- 若预测值与真实值相同，则不作调整；\n- 否则，若错输出为$0$，则将输入的$x$加到权重$w$上去；\n- 若错输出为1，则从权重$w$上将输入的$x$减去。\n\n当训练集确实是线性可分的时候，这种方法能够保证找到那样的一组参数，使得样本集完全正确分类。\n\n### 原理\n下面从几何角度分析一下感知机。\n\n权重空间（Weight Space的概念），对于权重向量的每个分量，都有一个维度对应。空间中的某个点就代表一个权重的实例。\n\n让我们忽略偏置项，那么每个训练样本都可以看作是权重空间的分类超平面。这个超平面的方程可以写作$x^\\dagger w = 0$，平面的法向量就是输入样本$x$。下图中假设输入样本为正样本，黑线即为超平面。平面上方的权重都是正确的（例如绿色的那个），下方的则都会使得该样本分类错误（红色的那个）。因为黑线上方的那些权重和输入$x$的夹角小于直角，也就是说内积是大于$0$的，自然就会给出正样本的预测。反之同理。\n\n![权重空间的分类超平面](/img/hinton_02_weight_space_hyperplane.png)\n\n当输入样本为负样本时，分析同理。只不过正确的权重此时应该位于平面下方，与输入向量夹角大于直角。\n\n所以，感知机的参数调整就是要在权重空间内找到某个权重点，使其在所有的训练样本构成的这么多超平面都位于正确的位置。\n\n下面用刚才的这种思考方式证明上述训练方法的正确性。\n\n首先考虑当前的权重和最终要找的那个权重之间的距离平方为$d_a^2+d_b^2$，如图所示。\n![why training works](/img/hinton_02_why_training_works_1.png)\n\n我们希望，对于每个错分的样例，学习算法能够将当前的参数向正确的参数推进。对照上图，似乎我们只要加上蓝色的那个输入向量就可以了。然而如果输入向量长度较长，而我们离正确的权值又比较近了，有可能出现更加远离的情况。\n\n我们取一个margin，认为我们要找的那个权重可行域不仅要满足分类正确，还要保证分类面和可行域的距离大于margin。（这里不是很懂，这样来看可行域的条件更加苛刻了，如果有正确分类面却没有这样的可行域呢？有可能出现吗？感知机这里还是看李航的统计学习更清楚点。。。我还是喜欢解析而不是几何。。。）\n![带margin的可行域](/img/hinton_02_margin.png)\n\n每次做出一次错误分类，权重根据输入向量做更新，向可行域前进至少input vector的长度这么多。这样不停迭代，就能收敛。（前面还提到了这是一个凸优化，也是一脸懵逼。。。）\n\n## 感知机的局限\n感知机不能解决线性不可问题，如异或运算。通过做特征变换，选取不同的特征，可能可以解决。\n![感知机不能解决异或问题](/img/hinton_02_perceptron_xor.png)\n","source":"_posts/hinton-nnml-02.md","raw":"---\ntitle: Neural Network for Machine Learning - Lecture 02\ndate: 2017-05-25 13:26:33\ntags:\n    - 公开课\n    - deep learning\n---\n这是第二周的课程内容，主要介绍了几种神经网络的分类，详细地介绍了感知机这一最简单的模型。\n\n![Perceptron](/img/hinton_02_perceptron_gragh.png)\n\n<!-- more -->\n## Different neural network archs\n### Feed-forward neural network 前馈神经网络\n前馈神经网络可能是最常见的网络，主要由输入层，若干隐含层和输出层组成。一般，当隐含层数目超过$1$时，我们可以说网络是deep的。\n![前馈网络](/img/hinton_02_feed_forward_nn.png)\n\n### Recurrent network\nRNN内部的节点之间存在有向的环，这使得它能够使用内部状态来对动态过程建模。RNN能力强大，但是不易训练。\n![RNN](/img/hinton_02_recurrent_nn.png)\n\nRNN常用来对序列进行建模（modeling squence）。这里有一篇[不错的介绍](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)。\n![RNN modeling squences](/img/hinton_02_rnn_app.png)\n\n### Symmetrically connected network\n这种网络结构上很像RNN，但是它的节点之间的连接是对称的，意思是说由此到彼和由彼到此的权重相同。\n\n## Percetron\n### 训练\n最早的神经网络应用是感知机。使用感知机等统计学习方法进行模式分类的一般思路是：\n- 将原始输入向量转化为feature activation。\n- 寻找合适的权重对feature进行加权，得到某个标量。\n- 如果这个标量大于某一给定的阈值，则分类为正；否则分类为负。\n\n![通用思路](/img/hinton_02_perceptron_paradigm_for_pattern_recong.png)\n\n对于决策节点，常常使用Binary threshold neuron，将输入映射为${0,1}$。而这相当于给输入加上一个偏置项，然后和$0$比较。\n\n所以，感知机的数学模型可以描述如下：\n\n$$y=\\begin{cases}1, \\quad \\text{if} \\quad \\sum w_ix_i+b>0 \\\\ 0 \\quad \\text{otherwise}\\end{cases}$$\n\n训练时，遍历样本集中的样本点，根据真实值与预测值是否相同，有如下的更新方法：\n- 若预测值与真实值相同，则不作调整；\n- 否则，若错输出为$0$，则将输入的$x$加到权重$w$上去；\n- 若错输出为1，则从权重$w$上将输入的$x$减去。\n\n当训练集确实是线性可分的时候，这种方法能够保证找到那样的一组参数，使得样本集完全正确分类。\n\n### 原理\n下面从几何角度分析一下感知机。\n\n权重空间（Weight Space的概念），对于权重向量的每个分量，都有一个维度对应。空间中的某个点就代表一个权重的实例。\n\n让我们忽略偏置项，那么每个训练样本都可以看作是权重空间的分类超平面。这个超平面的方程可以写作$x^\\dagger w = 0$，平面的法向量就是输入样本$x$。下图中假设输入样本为正样本，黑线即为超平面。平面上方的权重都是正确的（例如绿色的那个），下方的则都会使得该样本分类错误（红色的那个）。因为黑线上方的那些权重和输入$x$的夹角小于直角，也就是说内积是大于$0$的，自然就会给出正样本的预测。反之同理。\n\n![权重空间的分类超平面](/img/hinton_02_weight_space_hyperplane.png)\n\n当输入样本为负样本时，分析同理。只不过正确的权重此时应该位于平面下方，与输入向量夹角大于直角。\n\n所以，感知机的参数调整就是要在权重空间内找到某个权重点，使其在所有的训练样本构成的这么多超平面都位于正确的位置。\n\n下面用刚才的这种思考方式证明上述训练方法的正确性。\n\n首先考虑当前的权重和最终要找的那个权重之间的距离平方为$d_a^2+d_b^2$，如图所示。\n![why training works](/img/hinton_02_why_training_works_1.png)\n\n我们希望，对于每个错分的样例，学习算法能够将当前的参数向正确的参数推进。对照上图，似乎我们只要加上蓝色的那个输入向量就可以了。然而如果输入向量长度较长，而我们离正确的权值又比较近了，有可能出现更加远离的情况。\n\n我们取一个margin，认为我们要找的那个权重可行域不仅要满足分类正确，还要保证分类面和可行域的距离大于margin。（这里不是很懂，这样来看可行域的条件更加苛刻了，如果有正确分类面却没有这样的可行域呢？有可能出现吗？感知机这里还是看李航的统计学习更清楚点。。。我还是喜欢解析而不是几何。。。）\n![带margin的可行域](/img/hinton_02_margin.png)\n\n每次做出一次错误分类，权重根据输入向量做更新，向可行域前进至少input vector的长度这么多。这样不停迭代，就能收敛。（前面还提到了这是一个凸优化，也是一脸懵逼。。。）\n\n## 感知机的局限\n感知机不能解决线性不可问题，如异或运算。通过做特征变换，选取不同的特征，可能可以解决。\n![感知机不能解决异或问题](/img/hinton_02_perceptron_xor.png)\n","slug":"hinton-nnml-02","published":1,"updated":"2018-01-12T06:22:20.471Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjcu6vctd001hqu46j2zpk0yh","content":"<p>这是第二周的课程内容，主要介绍了几种神经网络的分类，详细地介绍了感知机这一最简单的模型。</p>\n<p><img src=\"/img/hinton_02_perceptron_gragh.png\" alt=\"Perceptron\"></p>\n<a id=\"more\"></a>\n<h2 id=\"Different-neural-network-archs\"><a href=\"#Different-neural-network-archs\" class=\"headerlink\" title=\"Different neural network archs\"></a>Different neural network archs</h2><h3 id=\"Feed-forward-neural-network-前馈神经网络\"><a href=\"#Feed-forward-neural-network-前馈神经网络\" class=\"headerlink\" title=\"Feed-forward neural network 前馈神经网络\"></a>Feed-forward neural network 前馈神经网络</h3><p>前馈神经网络可能是最常见的网络，主要由输入层，若干隐含层和输出层组成。一般，当隐含层数目超过$1$时，我们可以说网络是deep的。<br><img src=\"/img/hinton_02_feed_forward_nn.png\" alt=\"前馈网络\"></p>\n<h3 id=\"Recurrent-network\"><a href=\"#Recurrent-network\" class=\"headerlink\" title=\"Recurrent network\"></a>Recurrent network</h3><p>RNN内部的节点之间存在有向的环，这使得它能够使用内部状态来对动态过程建模。RNN能力强大，但是不易训练。<br><img src=\"/img/hinton_02_recurrent_nn.png\" alt=\"RNN\"></p>\n<p>RNN常用来对序列进行建模（modeling squence）。这里有一篇<a href=\"http://karpathy.github.io/2015/05/21/rnn-effectiveness/\" target=\"_blank\" rel=\"external\">不错的介绍</a>。<br><img src=\"/img/hinton_02_rnn_app.png\" alt=\"RNN modeling squences\"></p>\n<h3 id=\"Symmetrically-connected-network\"><a href=\"#Symmetrically-connected-network\" class=\"headerlink\" title=\"Symmetrically connected network\"></a>Symmetrically connected network</h3><p>这种网络结构上很像RNN，但是它的节点之间的连接是对称的，意思是说由此到彼和由彼到此的权重相同。</p>\n<h2 id=\"Percetron\"><a href=\"#Percetron\" class=\"headerlink\" title=\"Percetron\"></a>Percetron</h2><h3 id=\"训练\"><a href=\"#训练\" class=\"headerlink\" title=\"训练\"></a>训练</h3><p>最早的神经网络应用是感知机。使用感知机等统计学习方法进行模式分类的一般思路是：</p>\n<ul>\n<li>将原始输入向量转化为feature activation。</li>\n<li>寻找合适的权重对feature进行加权，得到某个标量。</li>\n<li>如果这个标量大于某一给定的阈值，则分类为正；否则分类为负。</li>\n</ul>\n<p><img src=\"/img/hinton_02_perceptron_paradigm_for_pattern_recong.png\" alt=\"通用思路\"></p>\n<p>对于决策节点，常常使用Binary threshold neuron，将输入映射为${0,1}$。而这相当于给输入加上一个偏置项，然后和$0$比较。</p>\n<p>所以，感知机的数学模型可以描述如下：</p>\n<script type=\"math/tex; mode=display\">y=\\begin{cases}1, \\quad \\text{if} \\quad \\sum w_ix_i+b>0 \\\\ 0 \\quad \\text{otherwise}\\end{cases}</script><p>训练时，遍历样本集中的样本点，根据真实值与预测值是否相同，有如下的更新方法：</p>\n<ul>\n<li>若预测值与真实值相同，则不作调整；</li>\n<li>否则，若错输出为$0$，则将输入的$x$加到权重$w$上去；</li>\n<li>若错输出为1，则从权重$w$上将输入的$x$减去。</li>\n</ul>\n<p>当训练集确实是线性可分的时候，这种方法能够保证找到那样的一组参数，使得样本集完全正确分类。</p>\n<h3 id=\"原理\"><a href=\"#原理\" class=\"headerlink\" title=\"原理\"></a>原理</h3><p>下面从几何角度分析一下感知机。</p>\n<p>权重空间（Weight Space的概念），对于权重向量的每个分量，都有一个维度对应。空间中的某个点就代表一个权重的实例。</p>\n<p>让我们忽略偏置项，那么每个训练样本都可以看作是权重空间的分类超平面。这个超平面的方程可以写作$x^\\dagger w = 0$，平面的法向量就是输入样本$x$。下图中假设输入样本为正样本，黑线即为超平面。平面上方的权重都是正确的（例如绿色的那个），下方的则都会使得该样本分类错误（红色的那个）。因为黑线上方的那些权重和输入$x$的夹角小于直角，也就是说内积是大于$0$的，自然就会给出正样本的预测。反之同理。</p>\n<p><img src=\"/img/hinton_02_weight_space_hyperplane.png\" alt=\"权重空间的分类超平面\"></p>\n<p>当输入样本为负样本时，分析同理。只不过正确的权重此时应该位于平面下方，与输入向量夹角大于直角。</p>\n<p>所以，感知机的参数调整就是要在权重空间内找到某个权重点，使其在所有的训练样本构成的这么多超平面都位于正确的位置。</p>\n<p>下面用刚才的这种思考方式证明上述训练方法的正确性。</p>\n<p>首先考虑当前的权重和最终要找的那个权重之间的距离平方为$d_a^2+d_b^2$，如图所示。<br><img src=\"/img/hinton_02_why_training_works_1.png\" alt=\"why training works\"></p>\n<p>我们希望，对于每个错分的样例，学习算法能够将当前的参数向正确的参数推进。对照上图，似乎我们只要加上蓝色的那个输入向量就可以了。然而如果输入向量长度较长，而我们离正确的权值又比较近了，有可能出现更加远离的情况。</p>\n<p>我们取一个margin，认为我们要找的那个权重可行域不仅要满足分类正确，还要保证分类面和可行域的距离大于margin。（这里不是很懂，这样来看可行域的条件更加苛刻了，如果有正确分类面却没有这样的可行域呢？有可能出现吗？感知机这里还是看李航的统计学习更清楚点。。。我还是喜欢解析而不是几何。。。）<br><img src=\"/img/hinton_02_margin.png\" alt=\"带margin的可行域\"></p>\n<p>每次做出一次错误分类，权重根据输入向量做更新，向可行域前进至少input vector的长度这么多。这样不停迭代，就能收敛。（前面还提到了这是一个凸优化，也是一脸懵逼。。。）</p>\n<h2 id=\"感知机的局限\"><a href=\"#感知机的局限\" class=\"headerlink\" title=\"感知机的局限\"></a>感知机的局限</h2><p>感知机不能解决线性不可问题，如异或运算。通过做特征变换，选取不同的特征，可能可以解决。<br><img src=\"/img/hinton_02_perceptron_xor.png\" alt=\"感知机不能解决异或问题\"></p>\n","excerpt":"<p>这是第二周的课程内容，主要介绍了几种神经网络的分类，详细地介绍了感知机这一最简单的模型。</p>\n<p><img src=\"/img/hinton_02_perceptron_gragh.png\" alt=\"Perceptron\"></p>","more":"<h2 id=\"Different-neural-network-archs\"><a href=\"#Different-neural-network-archs\" class=\"headerlink\" title=\"Different neural network archs\"></a>Different neural network archs</h2><h3 id=\"Feed-forward-neural-network-前馈神经网络\"><a href=\"#Feed-forward-neural-network-前馈神经网络\" class=\"headerlink\" title=\"Feed-forward neural network 前馈神经网络\"></a>Feed-forward neural network 前馈神经网络</h3><p>前馈神经网络可能是最常见的网络，主要由输入层，若干隐含层和输出层组成。一般，当隐含层数目超过$1$时，我们可以说网络是deep的。<br><img src=\"/img/hinton_02_feed_forward_nn.png\" alt=\"前馈网络\"></p>\n<h3 id=\"Recurrent-network\"><a href=\"#Recurrent-network\" class=\"headerlink\" title=\"Recurrent network\"></a>Recurrent network</h3><p>RNN内部的节点之间存在有向的环，这使得它能够使用内部状态来对动态过程建模。RNN能力强大，但是不易训练。<br><img src=\"/img/hinton_02_recurrent_nn.png\" alt=\"RNN\"></p>\n<p>RNN常用来对序列进行建模（modeling squence）。这里有一篇<a href=\"http://karpathy.github.io/2015/05/21/rnn-effectiveness/\">不错的介绍</a>。<br><img src=\"/img/hinton_02_rnn_app.png\" alt=\"RNN modeling squences\"></p>\n<h3 id=\"Symmetrically-connected-network\"><a href=\"#Symmetrically-connected-network\" class=\"headerlink\" title=\"Symmetrically connected network\"></a>Symmetrically connected network</h3><p>这种网络结构上很像RNN，但是它的节点之间的连接是对称的，意思是说由此到彼和由彼到此的权重相同。</p>\n<h2 id=\"Percetron\"><a href=\"#Percetron\" class=\"headerlink\" title=\"Percetron\"></a>Percetron</h2><h3 id=\"训练\"><a href=\"#训练\" class=\"headerlink\" title=\"训练\"></a>训练</h3><p>最早的神经网络应用是感知机。使用感知机等统计学习方法进行模式分类的一般思路是：</p>\n<ul>\n<li>将原始输入向量转化为feature activation。</li>\n<li>寻找合适的权重对feature进行加权，得到某个标量。</li>\n<li>如果这个标量大于某一给定的阈值，则分类为正；否则分类为负。</li>\n</ul>\n<p><img src=\"/img/hinton_02_perceptron_paradigm_for_pattern_recong.png\" alt=\"通用思路\"></p>\n<p>对于决策节点，常常使用Binary threshold neuron，将输入映射为${0,1}$。而这相当于给输入加上一个偏置项，然后和$0$比较。</p>\n<p>所以，感知机的数学模型可以描述如下：</p>\n<script type=\"math/tex; mode=display\">y=\\begin{cases}1, \\quad \\text{if} \\quad \\sum w_ix_i+b>0 \\\\ 0 \\quad \\text{otherwise}\\end{cases}</script><p>训练时，遍历样本集中的样本点，根据真实值与预测值是否相同，有如下的更新方法：</p>\n<ul>\n<li>若预测值与真实值相同，则不作调整；</li>\n<li>否则，若错输出为$0$，则将输入的$x$加到权重$w$上去；</li>\n<li>若错输出为1，则从权重$w$上将输入的$x$减去。</li>\n</ul>\n<p>当训练集确实是线性可分的时候，这种方法能够保证找到那样的一组参数，使得样本集完全正确分类。</p>\n<h3 id=\"原理\"><a href=\"#原理\" class=\"headerlink\" title=\"原理\"></a>原理</h3><p>下面从几何角度分析一下感知机。</p>\n<p>权重空间（Weight Space的概念），对于权重向量的每个分量，都有一个维度对应。空间中的某个点就代表一个权重的实例。</p>\n<p>让我们忽略偏置项，那么每个训练样本都可以看作是权重空间的分类超平面。这个超平面的方程可以写作$x^\\dagger w = 0$，平面的法向量就是输入样本$x$。下图中假设输入样本为正样本，黑线即为超平面。平面上方的权重都是正确的（例如绿色的那个），下方的则都会使得该样本分类错误（红色的那个）。因为黑线上方的那些权重和输入$x$的夹角小于直角，也就是说内积是大于$0$的，自然就会给出正样本的预测。反之同理。</p>\n<p><img src=\"/img/hinton_02_weight_space_hyperplane.png\" alt=\"权重空间的分类超平面\"></p>\n<p>当输入样本为负样本时，分析同理。只不过正确的权重此时应该位于平面下方，与输入向量夹角大于直角。</p>\n<p>所以，感知机的参数调整就是要在权重空间内找到某个权重点，使其在所有的训练样本构成的这么多超平面都位于正确的位置。</p>\n<p>下面用刚才的这种思考方式证明上述训练方法的正确性。</p>\n<p>首先考虑当前的权重和最终要找的那个权重之间的距离平方为$d_a^2+d_b^2$，如图所示。<br><img src=\"/img/hinton_02_why_training_works_1.png\" alt=\"why training works\"></p>\n<p>我们希望，对于每个错分的样例，学习算法能够将当前的参数向正确的参数推进。对照上图，似乎我们只要加上蓝色的那个输入向量就可以了。然而如果输入向量长度较长，而我们离正确的权值又比较近了，有可能出现更加远离的情况。</p>\n<p>我们取一个margin，认为我们要找的那个权重可行域不仅要满足分类正确，还要保证分类面和可行域的距离大于margin。（这里不是很懂，这样来看可行域的条件更加苛刻了，如果有正确分类面却没有这样的可行域呢？有可能出现吗？感知机这里还是看李航的统计学习更清楚点。。。我还是喜欢解析而不是几何。。。）<br><img src=\"/img/hinton_02_margin.png\" alt=\"带margin的可行域\"></p>\n<p>每次做出一次错误分类，权重根据输入向量做更新，向可行域前进至少input vector的长度这么多。这样不停迭代，就能收敛。（前面还提到了这是一个凸优化，也是一脸懵逼。。。）</p>\n<h2 id=\"感知机的局限\"><a href=\"#感知机的局限\" class=\"headerlink\" title=\"感知机的局限\"></a>感知机的局限</h2><p>感知机不能解决线性不可问题，如异或运算。通过做特征变换，选取不同的特征，可能可以解决。<br><img src=\"/img/hinton_02_perceptron_xor.png\" alt=\"感知机不能解决异或问题\"></p>"},{"title":"Incremental Network Quantization 论文阅读","date":"2018-01-25T07:30:28.000Z","_content":"卷积神经网络虽然已经在很多任务上取得了很棒的效果，但是模型大小和运算量限制了它们在移动设备和嵌入式设备上的使用。模型量化压缩等问题自然引起了大家的关注。[Incremental Network Quantization](https://arxiv.org/abs/1702.03044)这篇文章关注的是如何使用更少的比特数进行模型参数量化以达到压缩模型，减少模型大小同时使得模型精度无损。如下图所示，使用5bit位数，INQ在多个模型上都取得了不逊于原始FP32模型的精度。实验结果还是很有说服力的。作者将其开源在了GitHub上，见[Incremental-Network-Quantization](https://github.com/Zhouaojun/Incremental-Network-Quantization)。\n![实验结果](/img/paper-inq-result.png)\n<!-- more -->\n\n## 量化方法\nINQ论文中，作者采用的量化方法是将权重量化为$2$的幂次或$0$。具体来说，是将权重$W_l$（表示第$l$层的参数权重）舍入到下面这个有限集合中的元素（在下面的讨论中，我们认为$n_1 > n_2$）：\n![权重集合](/img/paper-inq-quantize-set.png)\n\n假设用$b$bit表示权重，我们分出$1$位单独表示$0$。\n\nPS：这里插一句。关于为什么要单独分出$1$位表示$0$，毕竟这样浪费了($2^b$ vs $2^{b-1}+1$)。GitHub上有人发[issue](https://github.com/Zhouaojun/Incremental-Network-Quantization/issues/12)问，作者也没有正面回复这样做的原因。以我的理解，是方便判定$0$和移位。因为作者将权重都舍入到了$2$的幂次，那肯定是为了后续将乘法变成移位操作。而使用剩下的$b-1$表示，可以方便地读出移位的位数，进行操作。\n\n这样，剩下的$b-1$位用来表示$2$的幂次。我们需要决定$n_1$和$n_2$。因为它俩决定了表示范围。它们之间的关系为：\n$$(n_1-n_2 + 1) \\times 2 = 2^{b-1}$$\n\n其中，乘以$2$是考虑到正负对称的表示范围。\n\n如何确定$n_1$呢（由上式可知，有了$b$和$n_1$，$n_2$就确定了）。作者考虑了待量化权重中的最大值，我们需要设置$n_1$，使其刚好不溢出。所以有：\n$$n_1 = \\lfloor \\log_2(4s/3) \\rfloor$$\n\n其中，$s$是权重当中绝对值最大的那个，即$s = \\max \\vert W_l\\vert$。\n\n之后做最近舍入就可以了。对于小于最小分辨力$2^{n_2}$的那些权重，将其直接截断为$0$。\n\n## 训练方法\n量化完成周，网络的精度必然会下降。我们需要对其进行调整，使其精度能够恢复原始模型的水平。为此，作者提出了三个主要步骤，迭代地进行。即 weight partition（权重划分）, group-wise quantization（分组量化） 和re-training（训练）。\n\nre-training好理解，就是量化之后要继续做finetuning。前面两个名词解释如下：weight partition是指我们不是对整个权重一股脑地做量化，而是将其划分为两个不相交的集合。group-wise quantization是指对其中一个集合中的权重做量化，另一组集合中的权重不变，仍然为FP32。注意，在re-training中，我们只对没有量化的那组参数做参数更新。下面是论文中的表述。\n\n> Weight partition is to divide the weights in each layer of a pre-trained full-precision CNN model into two disjoint groups which play comple- mentary roles in our INQ. The weights in the first group are responsible for forming a low-precision base for the original model, thus they are quantized by using Equation (4). The weights in the second group adapt to compensate for the loss in model accuracy, thus they are the ones to be re-trained.\n\n训练步骤可以用下图来表示。在第一个迭代中，将所有的权重划分为黑色和白色两个部分（图$1$）。黑色部分的权重进行量化，白色部分不变（图$2$）。然后，使用SGD更新那些白色部分的权重（图$3$）。在第二次迭代中，我们扩大量化权重的范围，重复进行迭代$1$中的操作。在后面的迭代中，以此类推，只不过要不断调大量化权重的比例，最终使得所有权重都量化为止。\n![训练图解](/img/paper-inq-algorithm-demo.png)\n\n### pruning-inspired strategy\n在权重划分步骤，作者指出，随机地将权重量化，不如根据权重的幅值，优先量化那些绝对值比较大的权重。比较结果见下图。\n![两种量化方法的比较](/img/paper-inq-different-quantize.png)\n\n在代码部分，INQ基于Caffe框架，主要修改的地方集中于`blob.cpp`和`sgd_solver.cpp`中。量化部分的代码如下，首先根据要划分的比例计算出两个集合分界点处的权重大小。然后将大于该值的权重进行量化，小于该值的权重保持不变。下面的代码其实有点小问题，`data_copy`使用完之后没有释放。关于代码中`mask`的作用，下文介绍。\n\n``` cpp\n  // blob.cpp\n  // INQ  \n  if(is_quantization)\n  {\n    Dtype* data_copy=(Dtype*) malloc(count_*sizeof(Dtype));\n    caffe_copy(count_,data_vec,data_copy);\n    caffe_abs(count_,data_copy,data_copy);\n    std::sort(data_copy,data_copy+count_); //data_copy order from small to large\n    \n    //caculate the n1\n    Dtype max_data=data_copy[count_-1];\n    int n1=(int)floor(log2(max_data*4.0/3.0));\n    \n    //quantizate the top 30% of each layer, change the \"partition\" until partition=0\n    int partition=int(count_*0.7)-1;\n\n    for (int i = 0; i < (count_); ++i) {\n    \n      if(std::abs(data_vec[i])>=data_copy[partition])\n        {\n          data_vec[i] = weightCluster_zero(data_vec[i],n1);\n\t  \n          mask_vec[i]=0;\n        }\n    }\n```\n\n### 参数更新\n在re-training中，我们只对未量化的那些参数进行更新。待更新的参数，`mask`中的值都是$1$，这样和`diff`相乘仍然不变；不更新的参数，`mask`中的值都是$0$，和`diff`乘起来，相当于强制把梯度变成了$0$。\n\n```\n// sgd_solver.cpp\ncaffe_gpu_mul(net_params[param_id]->count(),net_params[param_id]->gpu_mask(),net_params[param_id]->mutable_gpu_diff(),net_params[param_id]->mutable_gpu_diff());\n```\n\n## 结语\n论文中还有一些其他的小细节，这里不再多说。本文的作者还维护了一个关于模型量化压缩相关的[repo](https://github.com/Zhouaojun/Efficient-Deep-Learning)，也可以作为参考。","source":"_posts/inq-paper.md","raw":"---\ntitle: Incremental Network Quantization 论文阅读\ndate: 2018-01-25 15:30:28\ntags:\n    - paper\n    - quantize\n---\n卷积神经网络虽然已经在很多任务上取得了很棒的效果，但是模型大小和运算量限制了它们在移动设备和嵌入式设备上的使用。模型量化压缩等问题自然引起了大家的关注。[Incremental Network Quantization](https://arxiv.org/abs/1702.03044)这篇文章关注的是如何使用更少的比特数进行模型参数量化以达到压缩模型，减少模型大小同时使得模型精度无损。如下图所示，使用5bit位数，INQ在多个模型上都取得了不逊于原始FP32模型的精度。实验结果还是很有说服力的。作者将其开源在了GitHub上，见[Incremental-Network-Quantization](https://github.com/Zhouaojun/Incremental-Network-Quantization)。\n![实验结果](/img/paper-inq-result.png)\n<!-- more -->\n\n## 量化方法\nINQ论文中，作者采用的量化方法是将权重量化为$2$的幂次或$0$。具体来说，是将权重$W_l$（表示第$l$层的参数权重）舍入到下面这个有限集合中的元素（在下面的讨论中，我们认为$n_1 > n_2$）：\n![权重集合](/img/paper-inq-quantize-set.png)\n\n假设用$b$bit表示权重，我们分出$1$位单独表示$0$。\n\nPS：这里插一句。关于为什么要单独分出$1$位表示$0$，毕竟这样浪费了($2^b$ vs $2^{b-1}+1$)。GitHub上有人发[issue](https://github.com/Zhouaojun/Incremental-Network-Quantization/issues/12)问，作者也没有正面回复这样做的原因。以我的理解，是方便判定$0$和移位。因为作者将权重都舍入到了$2$的幂次，那肯定是为了后续将乘法变成移位操作。而使用剩下的$b-1$表示，可以方便地读出移位的位数，进行操作。\n\n这样，剩下的$b-1$位用来表示$2$的幂次。我们需要决定$n_1$和$n_2$。因为它俩决定了表示范围。它们之间的关系为：\n$$(n_1-n_2 + 1) \\times 2 = 2^{b-1}$$\n\n其中，乘以$2$是考虑到正负对称的表示范围。\n\n如何确定$n_1$呢（由上式可知，有了$b$和$n_1$，$n_2$就确定了）。作者考虑了待量化权重中的最大值，我们需要设置$n_1$，使其刚好不溢出。所以有：\n$$n_1 = \\lfloor \\log_2(4s/3) \\rfloor$$\n\n其中，$s$是权重当中绝对值最大的那个，即$s = \\max \\vert W_l\\vert$。\n\n之后做最近舍入就可以了。对于小于最小分辨力$2^{n_2}$的那些权重，将其直接截断为$0$。\n\n## 训练方法\n量化完成周，网络的精度必然会下降。我们需要对其进行调整，使其精度能够恢复原始模型的水平。为此，作者提出了三个主要步骤，迭代地进行。即 weight partition（权重划分）, group-wise quantization（分组量化） 和re-training（训练）。\n\nre-training好理解，就是量化之后要继续做finetuning。前面两个名词解释如下：weight partition是指我们不是对整个权重一股脑地做量化，而是将其划分为两个不相交的集合。group-wise quantization是指对其中一个集合中的权重做量化，另一组集合中的权重不变，仍然为FP32。注意，在re-training中，我们只对没有量化的那组参数做参数更新。下面是论文中的表述。\n\n> Weight partition is to divide the weights in each layer of a pre-trained full-precision CNN model into two disjoint groups which play comple- mentary roles in our INQ. The weights in the first group are responsible for forming a low-precision base for the original model, thus they are quantized by using Equation (4). The weights in the second group adapt to compensate for the loss in model accuracy, thus they are the ones to be re-trained.\n\n训练步骤可以用下图来表示。在第一个迭代中，将所有的权重划分为黑色和白色两个部分（图$1$）。黑色部分的权重进行量化，白色部分不变（图$2$）。然后，使用SGD更新那些白色部分的权重（图$3$）。在第二次迭代中，我们扩大量化权重的范围，重复进行迭代$1$中的操作。在后面的迭代中，以此类推，只不过要不断调大量化权重的比例，最终使得所有权重都量化为止。\n![训练图解](/img/paper-inq-algorithm-demo.png)\n\n### pruning-inspired strategy\n在权重划分步骤，作者指出，随机地将权重量化，不如根据权重的幅值，优先量化那些绝对值比较大的权重。比较结果见下图。\n![两种量化方法的比较](/img/paper-inq-different-quantize.png)\n\n在代码部分，INQ基于Caffe框架，主要修改的地方集中于`blob.cpp`和`sgd_solver.cpp`中。量化部分的代码如下，首先根据要划分的比例计算出两个集合分界点处的权重大小。然后将大于该值的权重进行量化，小于该值的权重保持不变。下面的代码其实有点小问题，`data_copy`使用完之后没有释放。关于代码中`mask`的作用，下文介绍。\n\n``` cpp\n  // blob.cpp\n  // INQ  \n  if(is_quantization)\n  {\n    Dtype* data_copy=(Dtype*) malloc(count_*sizeof(Dtype));\n    caffe_copy(count_,data_vec,data_copy);\n    caffe_abs(count_,data_copy,data_copy);\n    std::sort(data_copy,data_copy+count_); //data_copy order from small to large\n    \n    //caculate the n1\n    Dtype max_data=data_copy[count_-1];\n    int n1=(int)floor(log2(max_data*4.0/3.0));\n    \n    //quantizate the top 30% of each layer, change the \"partition\" until partition=0\n    int partition=int(count_*0.7)-1;\n\n    for (int i = 0; i < (count_); ++i) {\n    \n      if(std::abs(data_vec[i])>=data_copy[partition])\n        {\n          data_vec[i] = weightCluster_zero(data_vec[i],n1);\n\t  \n          mask_vec[i]=0;\n        }\n    }\n```\n\n### 参数更新\n在re-training中，我们只对未量化的那些参数进行更新。待更新的参数，`mask`中的值都是$1$，这样和`diff`相乘仍然不变；不更新的参数，`mask`中的值都是$0$，和`diff`乘起来，相当于强制把梯度变成了$0$。\n\n```\n// sgd_solver.cpp\ncaffe_gpu_mul(net_params[param_id]->count(),net_params[param_id]->gpu_mask(),net_params[param_id]->mutable_gpu_diff(),net_params[param_id]->mutable_gpu_diff());\n```\n\n## 结语\n论文中还有一些其他的小细节，这里不再多说。本文的作者还维护了一个关于模型量化压缩相关的[repo](https://github.com/Zhouaojun/Efficient-Deep-Learning)，也可以作为参考。","slug":"inq-paper","published":1,"updated":"2018-01-25T09:04:38.894Z","_id":"cjcu6vcte001kqu46xe2v3ocm","comments":1,"layout":"post","photos":[],"link":"","content":"<p>卷积神经网络虽然已经在很多任务上取得了很棒的效果，但是模型大小和运算量限制了它们在移动设备和嵌入式设备上的使用。模型量化压缩等问题自然引起了大家的关注。<a href=\"https://arxiv.org/abs/1702.03044\" target=\"_blank\" rel=\"external\">Incremental Network Quantization</a>这篇文章关注的是如何使用更少的比特数进行模型参数量化以达到压缩模型，减少模型大小同时使得模型精度无损。如下图所示，使用5bit位数，INQ在多个模型上都取得了不逊于原始FP32模型的精度。实验结果还是很有说服力的。作者将其开源在了GitHub上，见<a href=\"https://github.com/Zhouaojun/Incremental-Network-Quantization\" target=\"_blank\" rel=\"external\">Incremental-Network-Quantization</a>。<br><img src=\"/img/paper-inq-result.png\" alt=\"实验结果\"><br><a id=\"more\"></a></p>\n<h2 id=\"量化方法\"><a href=\"#量化方法\" class=\"headerlink\" title=\"量化方法\"></a>量化方法</h2><p>INQ论文中，作者采用的量化方法是将权重量化为$2$的幂次或$0$。具体来说，是将权重$W_l$（表示第$l$层的参数权重）舍入到下面这个有限集合中的元素（在下面的讨论中，我们认为$n_1 &gt; n_2$）：<br><img src=\"/img/paper-inq-quantize-set.png\" alt=\"权重集合\"></p>\n<p>假设用$b$bit表示权重，我们分出$1$位单独表示$0$。</p>\n<p>PS：这里插一句。关于为什么要单独分出$1$位表示$0$，毕竟这样浪费了($2^b$ vs $2^{b-1}+1$)。GitHub上有人发<a href=\"https://github.com/Zhouaojun/Incremental-Network-Quantization/issues/12\" target=\"_blank\" rel=\"external\">issue</a>问，作者也没有正面回复这样做的原因。以我的理解，是方便判定$0$和移位。因为作者将权重都舍入到了$2$的幂次，那肯定是为了后续将乘法变成移位操作。而使用剩下的$b-1$表示，可以方便地读出移位的位数，进行操作。</p>\n<p>这样，剩下的$b-1$位用来表示$2$的幂次。我们需要决定$n_1$和$n_2$。因为它俩决定了表示范围。它们之间的关系为：</p>\n<script type=\"math/tex; mode=display\">(n_1-n_2 + 1) \\times 2 = 2^{b-1}</script><p>其中，乘以$2$是考虑到正负对称的表示范围。</p>\n<p>如何确定$n_1$呢（由上式可知，有了$b$和$n_1$，$n_2$就确定了）。作者考虑了待量化权重中的最大值，我们需要设置$n_1$，使其刚好不溢出。所以有：</p>\n<script type=\"math/tex; mode=display\">n_1 = \\lfloor \\log_2(4s/3) \\rfloor</script><p>其中，$s$是权重当中绝对值最大的那个，即$s = \\max \\vert W_l\\vert$。</p>\n<p>之后做最近舍入就可以了。对于小于最小分辨力$2^{n_2}$的那些权重，将其直接截断为$0$。</p>\n<h2 id=\"训练方法\"><a href=\"#训练方法\" class=\"headerlink\" title=\"训练方法\"></a>训练方法</h2><p>量化完成周，网络的精度必然会下降。我们需要对其进行调整，使其精度能够恢复原始模型的水平。为此，作者提出了三个主要步骤，迭代地进行。即 weight partition（权重划分）, group-wise quantization（分组量化） 和re-training（训练）。</p>\n<p>re-training好理解，就是量化之后要继续做finetuning。前面两个名词解释如下：weight partition是指我们不是对整个权重一股脑地做量化，而是将其划分为两个不相交的集合。group-wise quantization是指对其中一个集合中的权重做量化，另一组集合中的权重不变，仍然为FP32。注意，在re-training中，我们只对没有量化的那组参数做参数更新。下面是论文中的表述。</p>\n<blockquote>\n<p>Weight partition is to divide the weights in each layer of a pre-trained full-precision CNN model into two disjoint groups which play comple- mentary roles in our INQ. The weights in the first group are responsible for forming a low-precision base for the original model, thus they are quantized by using Equation (4). The weights in the second group adapt to compensate for the loss in model accuracy, thus they are the ones to be re-trained.</p>\n</blockquote>\n<p>训练步骤可以用下图来表示。在第一个迭代中，将所有的权重划分为黑色和白色两个部分（图$1$）。黑色部分的权重进行量化，白色部分不变（图$2$）。然后，使用SGD更新那些白色部分的权重（图$3$）。在第二次迭代中，我们扩大量化权重的范围，重复进行迭代$1$中的操作。在后面的迭代中，以此类推，只不过要不断调大量化权重的比例，最终使得所有权重都量化为止。<br><img src=\"/img/paper-inq-algorithm-demo.png\" alt=\"训练图解\"></p>\n<h3 id=\"pruning-inspired-strategy\"><a href=\"#pruning-inspired-strategy\" class=\"headerlink\" title=\"pruning-inspired strategy\"></a>pruning-inspired strategy</h3><p>在权重划分步骤，作者指出，随机地将权重量化，不如根据权重的幅值，优先量化那些绝对值比较大的权重。比较结果见下图。<br><img src=\"/img/paper-inq-different-quantize.png\" alt=\"两种量化方法的比较\"></p>\n<p>在代码部分，INQ基于Caffe框架，主要修改的地方集中于<code>blob.cpp</code>和<code>sgd_solver.cpp</code>中。量化部分的代码如下，首先根据要划分的比例计算出两个集合分界点处的权重大小。然后将大于该值的权重进行量化，小于该值的权重保持不变。下面的代码其实有点小问题，<code>data_copy</code>使用完之后没有释放。关于代码中<code>mask</code>的作用，下文介绍。</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\">// blob.cpp</span></div><div class=\"line\"><span class=\"comment\">// INQ  </span></div><div class=\"line\"><span class=\"keyword\">if</span>(is_quantization)</div><div class=\"line\">&#123;</div><div class=\"line\">  Dtype* data_copy=(Dtype*) <span class=\"built_in\">malloc</span>(count_*<span class=\"keyword\">sizeof</span>(Dtype));</div><div class=\"line\">  caffe_copy(count_,data_vec,data_copy);</div><div class=\"line\">  caffe_abs(count_,data_copy,data_copy);</div><div class=\"line\">  <span class=\"built_in\">std</span>::sort(data_copy,data_copy+count_); <span class=\"comment\">//data_copy order from small to large</span></div><div class=\"line\">  </div><div class=\"line\">  <span class=\"comment\">//caculate the n1</span></div><div class=\"line\">  Dtype max_data=data_copy[count_<span class=\"number\">-1</span>];</div><div class=\"line\">  <span class=\"keyword\">int</span> n1=(<span class=\"keyword\">int</span>)<span class=\"built_in\">floor</span>(log2(max_data*<span class=\"number\">4.0</span>/<span class=\"number\">3.0</span>));</div><div class=\"line\">  </div><div class=\"line\">  <span class=\"comment\">//quantizate the top 30% of each layer, change the \"partition\" until partition=0</span></div><div class=\"line\">  <span class=\"keyword\">int</span> partition=<span class=\"keyword\">int</span>(count_*<span class=\"number\">0.7</span>)<span class=\"number\">-1</span>;</div><div class=\"line\"></div><div class=\"line\">  <span class=\"keyword\">for</span> (<span class=\"keyword\">int</span> i = <span class=\"number\">0</span>; i &lt; (count_); ++i) &#123;</div><div class=\"line\">  </div><div class=\"line\">    <span class=\"keyword\">if</span>(<span class=\"built_in\">std</span>::<span class=\"built_in\">abs</span>(data_vec[i])&gt;=data_copy[partition])</div><div class=\"line\">      &#123;</div><div class=\"line\">        data_vec[i] = weightCluster_zero(data_vec[i],n1);</div><div class=\"line\"> </div><div class=\"line\">        mask_vec[i]=<span class=\"number\">0</span>;</div><div class=\"line\">      &#125;</div><div class=\"line\">  &#125;</div></pre></td></tr></table></figure>\n<h3 id=\"参数更新\"><a href=\"#参数更新\" class=\"headerlink\" title=\"参数更新\"></a>参数更新</h3><p>在re-training中，我们只对未量化的那些参数进行更新。待更新的参数，<code>mask</code>中的值都是$1$，这样和<code>diff</code>相乘仍然不变；不更新的参数，<code>mask</code>中的值都是$0$，和<code>diff</code>乘起来，相当于强制把梯度变成了$0$。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">// sgd_solver.cpp</div><div class=\"line\">caffe_gpu_mul(net_params[param_id]-&gt;count(),net_params[param_id]-&gt;gpu_mask(),net_params[param_id]-&gt;mutable_gpu_diff(),net_params[param_id]-&gt;mutable_gpu_diff());</div></pre></td></tr></table></figure>\n<h2 id=\"结语\"><a href=\"#结语\" class=\"headerlink\" title=\"结语\"></a>结语</h2><p>论文中还有一些其他的小细节，这里不再多说。本文的作者还维护了一个关于模型量化压缩相关的<a href=\"https://github.com/Zhouaojun/Efficient-Deep-Learning\" target=\"_blank\" rel=\"external\">repo</a>，也可以作为参考。</p>\n","excerpt":"<p>卷积神经网络虽然已经在很多任务上取得了很棒的效果，但是模型大小和运算量限制了它们在移动设备和嵌入式设备上的使用。模型量化压缩等问题自然引起了大家的关注。<a href=\"https://arxiv.org/abs/1702.03044\">Incremental Network Quantization</a>这篇文章关注的是如何使用更少的比特数进行模型参数量化以达到压缩模型，减少模型大小同时使得模型精度无损。如下图所示，使用5bit位数，INQ在多个模型上都取得了不逊于原始FP32模型的精度。实验结果还是很有说服力的。作者将其开源在了GitHub上，见<a href=\"https://github.com/Zhouaojun/Incremental-Network-Quantization\">Incremental-Network-Quantization</a>。<br><img src=\"/img/paper-inq-result.png\" alt=\"实验结果\"><br>","more":"</p>\n<h2 id=\"量化方法\"><a href=\"#量化方法\" class=\"headerlink\" title=\"量化方法\"></a>量化方法</h2><p>INQ论文中，作者采用的量化方法是将权重量化为$2$的幂次或$0$。具体来说，是将权重$W_l$（表示第$l$层的参数权重）舍入到下面这个有限集合中的元素（在下面的讨论中，我们认为$n_1 &gt; n_2$）：<br><img src=\"/img/paper-inq-quantize-set.png\" alt=\"权重集合\"></p>\n<p>假设用$b$bit表示权重，我们分出$1$位单独表示$0$。</p>\n<p>PS：这里插一句。关于为什么要单独分出$1$位表示$0$，毕竟这样浪费了($2^b$ vs $2^{b-1}+1$)。GitHub上有人发<a href=\"https://github.com/Zhouaojun/Incremental-Network-Quantization/issues/12\">issue</a>问，作者也没有正面回复这样做的原因。以我的理解，是方便判定$0$和移位。因为作者将权重都舍入到了$2$的幂次，那肯定是为了后续将乘法变成移位操作。而使用剩下的$b-1$表示，可以方便地读出移位的位数，进行操作。</p>\n<p>这样，剩下的$b-1$位用来表示$2$的幂次。我们需要决定$n_1$和$n_2$。因为它俩决定了表示范围。它们之间的关系为：</p>\n<script type=\"math/tex; mode=display\">(n_1-n_2 + 1) \\times 2 = 2^{b-1}</script><p>其中，乘以$2$是考虑到正负对称的表示范围。</p>\n<p>如何确定$n_1$呢（由上式可知，有了$b$和$n_1$，$n_2$就确定了）。作者考虑了待量化权重中的最大值，我们需要设置$n_1$，使其刚好不溢出。所以有：</p>\n<script type=\"math/tex; mode=display\">n_1 = \\lfloor \\log_2(4s/3) \\rfloor</script><p>其中，$s$是权重当中绝对值最大的那个，即$s = \\max \\vert W_l\\vert$。</p>\n<p>之后做最近舍入就可以了。对于小于最小分辨力$2^{n_2}$的那些权重，将其直接截断为$0$。</p>\n<h2 id=\"训练方法\"><a href=\"#训练方法\" class=\"headerlink\" title=\"训练方法\"></a>训练方法</h2><p>量化完成周，网络的精度必然会下降。我们需要对其进行调整，使其精度能够恢复原始模型的水平。为此，作者提出了三个主要步骤，迭代地进行。即 weight partition（权重划分）, group-wise quantization（分组量化） 和re-training（训练）。</p>\n<p>re-training好理解，就是量化之后要继续做finetuning。前面两个名词解释如下：weight partition是指我们不是对整个权重一股脑地做量化，而是将其划分为两个不相交的集合。group-wise quantization是指对其中一个集合中的权重做量化，另一组集合中的权重不变，仍然为FP32。注意，在re-training中，我们只对没有量化的那组参数做参数更新。下面是论文中的表述。</p>\n<blockquote>\n<p>Weight partition is to divide the weights in each layer of a pre-trained full-precision CNN model into two disjoint groups which play comple- mentary roles in our INQ. The weights in the first group are responsible for forming a low-precision base for the original model, thus they are quantized by using Equation (4). The weights in the second group adapt to compensate for the loss in model accuracy, thus they are the ones to be re-trained.</p>\n</blockquote>\n<p>训练步骤可以用下图来表示。在第一个迭代中，将所有的权重划分为黑色和白色两个部分（图$1$）。黑色部分的权重进行量化，白色部分不变（图$2$）。然后，使用SGD更新那些白色部分的权重（图$3$）。在第二次迭代中，我们扩大量化权重的范围，重复进行迭代$1$中的操作。在后面的迭代中，以此类推，只不过要不断调大量化权重的比例，最终使得所有权重都量化为止。<br><img src=\"/img/paper-inq-algorithm-demo.png\" alt=\"训练图解\"></p>\n<h3 id=\"pruning-inspired-strategy\"><a href=\"#pruning-inspired-strategy\" class=\"headerlink\" title=\"pruning-inspired strategy\"></a>pruning-inspired strategy</h3><p>在权重划分步骤，作者指出，随机地将权重量化，不如根据权重的幅值，优先量化那些绝对值比较大的权重。比较结果见下图。<br><img src=\"/img/paper-inq-different-quantize.png\" alt=\"两种量化方法的比较\"></p>\n<p>在代码部分，INQ基于Caffe框架，主要修改的地方集中于<code>blob.cpp</code>和<code>sgd_solver.cpp</code>中。量化部分的代码如下，首先根据要划分的比例计算出两个集合分界点处的权重大小。然后将大于该值的权重进行量化，小于该值的权重保持不变。下面的代码其实有点小问题，<code>data_copy</code>使用完之后没有释放。关于代码中<code>mask</code>的作用，下文介绍。</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\">// blob.cpp</span></div><div class=\"line\"><span class=\"comment\">// INQ  </span></div><div class=\"line\"><span class=\"keyword\">if</span>(is_quantization)</div><div class=\"line\">&#123;</div><div class=\"line\">  Dtype* data_copy=(Dtype*) <span class=\"built_in\">malloc</span>(count_*<span class=\"keyword\">sizeof</span>(Dtype));</div><div class=\"line\">  caffe_copy(count_,data_vec,data_copy);</div><div class=\"line\">  caffe_abs(count_,data_copy,data_copy);</div><div class=\"line\">  <span class=\"built_in\">std</span>::sort(data_copy,data_copy+count_); <span class=\"comment\">//data_copy order from small to large</span></div><div class=\"line\">  </div><div class=\"line\">  <span class=\"comment\">//caculate the n1</span></div><div class=\"line\">  Dtype max_data=data_copy[count_<span class=\"number\">-1</span>];</div><div class=\"line\">  <span class=\"keyword\">int</span> n1=(<span class=\"keyword\">int</span>)<span class=\"built_in\">floor</span>(log2(max_data*<span class=\"number\">4.0</span>/<span class=\"number\">3.0</span>));</div><div class=\"line\">  </div><div class=\"line\">  <span class=\"comment\">//quantizate the top 30% of each layer, change the \"partition\" until partition=0</span></div><div class=\"line\">  <span class=\"keyword\">int</span> partition=<span class=\"keyword\">int</span>(count_*<span class=\"number\">0.7</span>)<span class=\"number\">-1</span>;</div><div class=\"line\"></div><div class=\"line\">  <span class=\"keyword\">for</span> (<span class=\"keyword\">int</span> i = <span class=\"number\">0</span>; i &lt; (count_); ++i) &#123;</div><div class=\"line\">  </div><div class=\"line\">    <span class=\"keyword\">if</span>(<span class=\"built_in\">std</span>::<span class=\"built_in\">abs</span>(data_vec[i])&gt;=data_copy[partition])</div><div class=\"line\">      &#123;</div><div class=\"line\">        data_vec[i] = weightCluster_zero(data_vec[i],n1);</div><div class=\"line\"> </div><div class=\"line\">        mask_vec[i]=<span class=\"number\">0</span>;</div><div class=\"line\">      &#125;</div><div class=\"line\">  &#125;</div></pre></td></tr></table></figure>\n<h3 id=\"参数更新\"><a href=\"#参数更新\" class=\"headerlink\" title=\"参数更新\"></a>参数更新</h3><p>在re-training中，我们只对未量化的那些参数进行更新。待更新的参数，<code>mask</code>中的值都是$1$，这样和<code>diff</code>相乘仍然不变；不更新的参数，<code>mask</code>中的值都是$0$，和<code>diff</code>乘起来，相当于强制把梯度变成了$0$。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">// sgd_solver.cpp</div><div class=\"line\">caffe_gpu_mul(net_params[param_id]-&gt;count(),net_params[param_id]-&gt;gpu_mask(),net_params[param_id]-&gt;mutable_gpu_diff(),net_params[param_id]-&gt;mutable_gpu_diff());</div></pre></td></tr></table></figure>\n<h2 id=\"结语\"><a href=\"#结语\" class=\"headerlink\" title=\"结语\"></a>结语</h2><p>论文中还有一些其他的小细节，这里不再多说。本文的作者还维护了一个关于模型量化压缩相关的<a href=\"https://github.com/Zhouaojun/Efficient-Deep-Learning\">repo</a>，也可以作为参考。</p>"},{"title":"Neural Network for Machine Learning - Lecture 01","date":"2017-05-03T12:56:36.000Z","_content":"Hinton 在 Coursera 课程“Neural Network for Machine Learning”新开了一个班次。对课程内容做一总结。课程内容也许已经跟不上最近DL的发展，不过还是有很多的好东西。\n![神经元](/img/hinton_brainsimulator.jpg)\n<!-- more -->\n\n## Why do we need ML?\n从数据中学习，无需手写大量的逻辑代码。ML算法从数据中学习，给出完成任务（如人脸识别）的程序。这里的程序可能有大量的参数组成。得益于硬件的发展和大数据时代的来临，机器学习的应用越来越广泛。如下图所示，MNIST数据集中的手写数字2变化多端，很难人工设计代码逻辑找到判别一个手写数字是不是2的方法。\n![MNIST Digit 2](/img/hinton_01_mnist_example.png)\n\n## What are neural network?\n为什么要研究神经网络？\n\n- 理解人脑机理的一个途径；\n- 受到神经系统启发的并行计算\n- 新的学习算法来解决现实问题（本课所关心的）\n\n（这里总结的不是很科学，勉强概括了讲义的内容）\n神经元结构如下所示。树突（dendritic tree）和其他神经元相连作为输入，轴突（axon）发散出很多分支和其他神经元相连。轴突和树突之间通过突触（synapse）连接。轴突有足够的电荷产生兴奋。这样完成神经元到神经元的communication。\n![神经元的结构](/img/hinton_01_neuron_structure.png)\n\n神经元之间互相连接。对不同的神经元输入，有不同的权重。这些权重可以变化，使得神经元之间的连接或变得更加紧密或疏离。人类大脑的神经元多达$10^{11}$个，每一个都有多达$10^4$个连接权重。不同神经元分布式计算，带宽很大。\n![神经元的相互连接](/img/hinton_01_neuron_commucation.png)\n\n大脑中不同的神经元分工不同（局部损坏造成相应的身体功能受损），但是这些神经元长得都差不多，它们也可以在一定的环境下发育成特定功能的神经元。\n\n而人工神经网络就是根据神经元的兴奋传导机理，人工模拟的神经网络。\n\n## Simple models of different neurons\n我们简化神经元模型，用数学函数去近似描述它们的功能。这也是科学研究的通用思路，忽略次要矛盾，抓住主要矛盾。之后逐步向上加复杂度，更好地描述实验现象。下面介绍几种神经元的简化模型。\n\n### Linear neuron\n顾名思义，这种神经元用来进行线性组合的变换，不过要注意加上偏置项。如下所示：\n$$y = b+\\sum_{i=1}^{n}w_ix_i$$\n\n### Binary threshold neuron\n这种神经元用来将输入信号加权后做二元阈值化，我们可以通过两种方法来描述：\n\n$$y = \\begin{cases}1 \\quad \\text{if} \\quad z\\ge \\theta\\\\ 0\\quad \\text{otherwise}\\end{cases}$$\n\n其中，$z$是输入信号的线性组合，$z=\\sum_{i}w_ix_i$\n\n或者，\n\n$$y = \\begin{cases}1 \\quad \\text{if} \\quad z\\ge 0\\\\ 0\\quad \\text{otherwise}\\end{cases}$$\n\n其中，$z$是输入信号的线性组合并加上偏置项，$z = b+\\sum_{i}w_ix_i$\n\n### Rectified linear neuron\n和上面的二值化神经元对比，有：\n$$y = \\begin{cases}z \\quad \\text{if} \\quad z\\ge 0\\\\ 0\\quad \\text{otherwise}\\end{cases}$$\n\n### Sigmoid neuron\n这种神经元通过logistic函数将输入shrink到区间$(0, 1)$，如下所示：\n$$y = \\frac{1}{1+\\exp(-z)}$$\n![Logistic函数示意](/img/hinton_01_sigmoid_function.png)\n\n由于Logistic函数将$(-\\infty, +\\infty)$的值压缩为S型，所以得名Sigmoid。\n\n### Stochastic binary neuron\n这种函数的输出仍是二值化的，而且是将Logistic函数的输入作为输出$1$的概率。也就是：\n$$P(y=1) = \\frac{1}{1+\\exp(-z)}$$\n\n对于上面的Rectified linear neuron，也可以做类似的变形，将输出看作是泊松分布的系数。\n\n## Three types of learning\n即有监督学习，无监督学习和强化学习。\n\n- 有监督学习：给定输入向量，预测输出。\n- 无监督学习：学习一个对于输出来说的好的表示（good internal representation of input）。\n- 强化学习：学习如何决策达到最大期望奖赏。\n\n### 有监督学习\n有监督学习可以细分为分类和回归问题。有监督学习中，我们需要寻找一个model(由一个函数$f$和决定这个函数的参数$W$决定)，将输入$x$应映射为实数（回归问题）或者离散值（分类问题）。\n\n所谓的训练，就是指不断调整参数$W$，使得训练集合中的$x$在当前映射下得到的预测值与真实值之间的差异尽可能小。 在回归问题中，常常使用欧氏距离的平方作为差异的衡量。\n\n### 强化学习\n在强化学习中，算法要给出动作或者动作序列。与有监督学习不同，强化学习中没有真实值，只有不定时（occasional）出现的奖赏。\n\n强化学习的难点如下：\n- 奖赏通常是delayed的。以AlphaGo来说，你很难追究中间某一步棋的决策对最后输赢的影响。\n- 奖赏通常只是一个标量，提供不了太多的信息。我只能知道这局最后的输赢，但是对于其他信息基本都不知道。\n\n### 无监督学习\n无监督学习以前受到的关注不多，这可能和它的目的不明确有关系。其中一个目的是能够提供输入的更好的表示，以用于强化学习和有监督学习。\n","source":"_posts/hinton-nnml-01.md","raw":"---\ntitle: Neural Network for Machine Learning - Lecture 01\ndate: 2017-05-03 20:56:36\ntags:\n    - deep learning\n    - 公开课\n---\nHinton 在 Coursera 课程“Neural Network for Machine Learning”新开了一个班次。对课程内容做一总结。课程内容也许已经跟不上最近DL的发展，不过还是有很多的好东西。\n![神经元](/img/hinton_brainsimulator.jpg)\n<!-- more -->\n\n## Why do we need ML?\n从数据中学习，无需手写大量的逻辑代码。ML算法从数据中学习，给出完成任务（如人脸识别）的程序。这里的程序可能有大量的参数组成。得益于硬件的发展和大数据时代的来临，机器学习的应用越来越广泛。如下图所示，MNIST数据集中的手写数字2变化多端，很难人工设计代码逻辑找到判别一个手写数字是不是2的方法。\n![MNIST Digit 2](/img/hinton_01_mnist_example.png)\n\n## What are neural network?\n为什么要研究神经网络？\n\n- 理解人脑机理的一个途径；\n- 受到神经系统启发的并行计算\n- 新的学习算法来解决现实问题（本课所关心的）\n\n（这里总结的不是很科学，勉强概括了讲义的内容）\n神经元结构如下所示。树突（dendritic tree）和其他神经元相连作为输入，轴突（axon）发散出很多分支和其他神经元相连。轴突和树突之间通过突触（synapse）连接。轴突有足够的电荷产生兴奋。这样完成神经元到神经元的communication。\n![神经元的结构](/img/hinton_01_neuron_structure.png)\n\n神经元之间互相连接。对不同的神经元输入，有不同的权重。这些权重可以变化，使得神经元之间的连接或变得更加紧密或疏离。人类大脑的神经元多达$10^{11}$个，每一个都有多达$10^4$个连接权重。不同神经元分布式计算，带宽很大。\n![神经元的相互连接](/img/hinton_01_neuron_commucation.png)\n\n大脑中不同的神经元分工不同（局部损坏造成相应的身体功能受损），但是这些神经元长得都差不多，它们也可以在一定的环境下发育成特定功能的神经元。\n\n而人工神经网络就是根据神经元的兴奋传导机理，人工模拟的神经网络。\n\n## Simple models of different neurons\n我们简化神经元模型，用数学函数去近似描述它们的功能。这也是科学研究的通用思路，忽略次要矛盾，抓住主要矛盾。之后逐步向上加复杂度，更好地描述实验现象。下面介绍几种神经元的简化模型。\n\n### Linear neuron\n顾名思义，这种神经元用来进行线性组合的变换，不过要注意加上偏置项。如下所示：\n$$y = b+\\sum_{i=1}^{n}w_ix_i$$\n\n### Binary threshold neuron\n这种神经元用来将输入信号加权后做二元阈值化，我们可以通过两种方法来描述：\n\n$$y = \\begin{cases}1 \\quad \\text{if} \\quad z\\ge \\theta\\\\ 0\\quad \\text{otherwise}\\end{cases}$$\n\n其中，$z$是输入信号的线性组合，$z=\\sum_{i}w_ix_i$\n\n或者，\n\n$$y = \\begin{cases}1 \\quad \\text{if} \\quad z\\ge 0\\\\ 0\\quad \\text{otherwise}\\end{cases}$$\n\n其中，$z$是输入信号的线性组合并加上偏置项，$z = b+\\sum_{i}w_ix_i$\n\n### Rectified linear neuron\n和上面的二值化神经元对比，有：\n$$y = \\begin{cases}z \\quad \\text{if} \\quad z\\ge 0\\\\ 0\\quad \\text{otherwise}\\end{cases}$$\n\n### Sigmoid neuron\n这种神经元通过logistic函数将输入shrink到区间$(0, 1)$，如下所示：\n$$y = \\frac{1}{1+\\exp(-z)}$$\n![Logistic函数示意](/img/hinton_01_sigmoid_function.png)\n\n由于Logistic函数将$(-\\infty, +\\infty)$的值压缩为S型，所以得名Sigmoid。\n\n### Stochastic binary neuron\n这种函数的输出仍是二值化的，而且是将Logistic函数的输入作为输出$1$的概率。也就是：\n$$P(y=1) = \\frac{1}{1+\\exp(-z)}$$\n\n对于上面的Rectified linear neuron，也可以做类似的变形，将输出看作是泊松分布的系数。\n\n## Three types of learning\n即有监督学习，无监督学习和强化学习。\n\n- 有监督学习：给定输入向量，预测输出。\n- 无监督学习：学习一个对于输出来说的好的表示（good internal representation of input）。\n- 强化学习：学习如何决策达到最大期望奖赏。\n\n### 有监督学习\n有监督学习可以细分为分类和回归问题。有监督学习中，我们需要寻找一个model(由一个函数$f$和决定这个函数的参数$W$决定)，将输入$x$应映射为实数（回归问题）或者离散值（分类问题）。\n\n所谓的训练，就是指不断调整参数$W$，使得训练集合中的$x$在当前映射下得到的预测值与真实值之间的差异尽可能小。 在回归问题中，常常使用欧氏距离的平方作为差异的衡量。\n\n### 强化学习\n在强化学习中，算法要给出动作或者动作序列。与有监督学习不同，强化学习中没有真实值，只有不定时（occasional）出现的奖赏。\n\n强化学习的难点如下：\n- 奖赏通常是delayed的。以AlphaGo来说，你很难追究中间某一步棋的决策对最后输赢的影响。\n- 奖赏通常只是一个标量，提供不了太多的信息。我只能知道这局最后的输赢，但是对于其他信息基本都不知道。\n\n### 无监督学习\n无监督学习以前受到的关注不多，这可能和它的目的不明确有关系。其中一个目的是能够提供输入的更好的表示，以用于强化学习和有监督学习。\n","slug":"hinton-nnml-01","published":1,"updated":"2018-01-12T06:22:20.471Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjcu6vcti001mqu46h5rzsoxg","content":"<p>Hinton 在 Coursera 课程“Neural Network for Machine Learning”新开了一个班次。对课程内容做一总结。课程内容也许已经跟不上最近DL的发展，不过还是有很多的好东西。<br><img src=\"/img/hinton_brainsimulator.jpg\" alt=\"神经元\"><br><a id=\"more\"></a></p>\n<h2 id=\"Why-do-we-need-ML\"><a href=\"#Why-do-we-need-ML\" class=\"headerlink\" title=\"Why do we need ML?\"></a>Why do we need ML?</h2><p>从数据中学习，无需手写大量的逻辑代码。ML算法从数据中学习，给出完成任务（如人脸识别）的程序。这里的程序可能有大量的参数组成。得益于硬件的发展和大数据时代的来临，机器学习的应用越来越广泛。如下图所示，MNIST数据集中的手写数字2变化多端，很难人工设计代码逻辑找到判别一个手写数字是不是2的方法。<br><img src=\"/img/hinton_01_mnist_example.png\" alt=\"MNIST Digit 2\"></p>\n<h2 id=\"What-are-neural-network\"><a href=\"#What-are-neural-network\" class=\"headerlink\" title=\"What are neural network?\"></a>What are neural network?</h2><p>为什么要研究神经网络？</p>\n<ul>\n<li>理解人脑机理的一个途径；</li>\n<li>受到神经系统启发的并行计算</li>\n<li>新的学习算法来解决现实问题（本课所关心的）</li>\n</ul>\n<p>（这里总结的不是很科学，勉强概括了讲义的内容）<br>神经元结构如下所示。树突（dendritic tree）和其他神经元相连作为输入，轴突（axon）发散出很多分支和其他神经元相连。轴突和树突之间通过突触（synapse）连接。轴突有足够的电荷产生兴奋。这样完成神经元到神经元的communication。<br><img src=\"/img/hinton_01_neuron_structure.png\" alt=\"神经元的结构\"></p>\n<p>神经元之间互相连接。对不同的神经元输入，有不同的权重。这些权重可以变化，使得神经元之间的连接或变得更加紧密或疏离。人类大脑的神经元多达$10^{11}$个，每一个都有多达$10^4$个连接权重。不同神经元分布式计算，带宽很大。<br><img src=\"/img/hinton_01_neuron_commucation.png\" alt=\"神经元的相互连接\"></p>\n<p>大脑中不同的神经元分工不同（局部损坏造成相应的身体功能受损），但是这些神经元长得都差不多，它们也可以在一定的环境下发育成特定功能的神经元。</p>\n<p>而人工神经网络就是根据神经元的兴奋传导机理，人工模拟的神经网络。</p>\n<h2 id=\"Simple-models-of-different-neurons\"><a href=\"#Simple-models-of-different-neurons\" class=\"headerlink\" title=\"Simple models of different neurons\"></a>Simple models of different neurons</h2><p>我们简化神经元模型，用数学函数去近似描述它们的功能。这也是科学研究的通用思路，忽略次要矛盾，抓住主要矛盾。之后逐步向上加复杂度，更好地描述实验现象。下面介绍几种神经元的简化模型。</p>\n<h3 id=\"Linear-neuron\"><a href=\"#Linear-neuron\" class=\"headerlink\" title=\"Linear neuron\"></a>Linear neuron</h3><p>顾名思义，这种神经元用来进行线性组合的变换，不过要注意加上偏置项。如下所示：</p>\n<script type=\"math/tex; mode=display\">y = b+\\sum_{i=1}^{n}w_ix_i</script><h3 id=\"Binary-threshold-neuron\"><a href=\"#Binary-threshold-neuron\" class=\"headerlink\" title=\"Binary threshold neuron\"></a>Binary threshold neuron</h3><p>这种神经元用来将输入信号加权后做二元阈值化，我们可以通过两种方法来描述：</p>\n<script type=\"math/tex; mode=display\">y = \\begin{cases}1 \\quad \\text{if} \\quad z\\ge \\theta\\\\ 0\\quad \\text{otherwise}\\end{cases}</script><p>其中，$z$是输入信号的线性组合，$z=\\sum_{i}w_ix_i$</p>\n<p>或者，</p>\n<script type=\"math/tex; mode=display\">y = \\begin{cases}1 \\quad \\text{if} \\quad z\\ge 0\\\\ 0\\quad \\text{otherwise}\\end{cases}</script><p>其中，$z$是输入信号的线性组合并加上偏置项，$z = b+\\sum_{i}w_ix_i$</p>\n<h3 id=\"Rectified-linear-neuron\"><a href=\"#Rectified-linear-neuron\" class=\"headerlink\" title=\"Rectified linear neuron\"></a>Rectified linear neuron</h3><p>和上面的二值化神经元对比，有：</p>\n<script type=\"math/tex; mode=display\">y = \\begin{cases}z \\quad \\text{if} \\quad z\\ge 0\\\\ 0\\quad \\text{otherwise}\\end{cases}</script><h3 id=\"Sigmoid-neuron\"><a href=\"#Sigmoid-neuron\" class=\"headerlink\" title=\"Sigmoid neuron\"></a>Sigmoid neuron</h3><p>这种神经元通过logistic函数将输入shrink到区间$(0, 1)$，如下所示：</p>\n<script type=\"math/tex; mode=display\">y = \\frac{1}{1+\\exp(-z)}</script><p><img src=\"/img/hinton_01_sigmoid_function.png\" alt=\"Logistic函数示意\"></p>\n<p>由于Logistic函数将$(-\\infty, +\\infty)$的值压缩为S型，所以得名Sigmoid。</p>\n<h3 id=\"Stochastic-binary-neuron\"><a href=\"#Stochastic-binary-neuron\" class=\"headerlink\" title=\"Stochastic binary neuron\"></a>Stochastic binary neuron</h3><p>这种函数的输出仍是二值化的，而且是将Logistic函数的输入作为输出$1$的概率。也就是：</p>\n<script type=\"math/tex; mode=display\">P(y=1) = \\frac{1}{1+\\exp(-z)}</script><p>对于上面的Rectified linear neuron，也可以做类似的变形，将输出看作是泊松分布的系数。</p>\n<h2 id=\"Three-types-of-learning\"><a href=\"#Three-types-of-learning\" class=\"headerlink\" title=\"Three types of learning\"></a>Three types of learning</h2><p>即有监督学习，无监督学习和强化学习。</p>\n<ul>\n<li>有监督学习：给定输入向量，预测输出。</li>\n<li>无监督学习：学习一个对于输出来说的好的表示（good internal representation of input）。</li>\n<li>强化学习：学习如何决策达到最大期望奖赏。</li>\n</ul>\n<h3 id=\"有监督学习\"><a href=\"#有监督学习\" class=\"headerlink\" title=\"有监督学习\"></a>有监督学习</h3><p>有监督学习可以细分为分类和回归问题。有监督学习中，我们需要寻找一个model(由一个函数$f$和决定这个函数的参数$W$决定)，将输入$x$应映射为实数（回归问题）或者离散值（分类问题）。</p>\n<p>所谓的训练，就是指不断调整参数$W$，使得训练集合中的$x$在当前映射下得到的预测值与真实值之间的差异尽可能小。 在回归问题中，常常使用欧氏距离的平方作为差异的衡量。</p>\n<h3 id=\"强化学习\"><a href=\"#强化学习\" class=\"headerlink\" title=\"强化学习\"></a>强化学习</h3><p>在强化学习中，算法要给出动作或者动作序列。与有监督学习不同，强化学习中没有真实值，只有不定时（occasional）出现的奖赏。</p>\n<p>强化学习的难点如下：</p>\n<ul>\n<li>奖赏通常是delayed的。以AlphaGo来说，你很难追究中间某一步棋的决策对最后输赢的影响。</li>\n<li>奖赏通常只是一个标量，提供不了太多的信息。我只能知道这局最后的输赢，但是对于其他信息基本都不知道。</li>\n</ul>\n<h3 id=\"无监督学习\"><a href=\"#无监督学习\" class=\"headerlink\" title=\"无监督学习\"></a>无监督学习</h3><p>无监督学习以前受到的关注不多，这可能和它的目的不明确有关系。其中一个目的是能够提供输入的更好的表示，以用于强化学习和有监督学习。</p>\n","excerpt":"<p>Hinton 在 Coursera 课程“Neural Network for Machine Learning”新开了一个班次。对课程内容做一总结。课程内容也许已经跟不上最近DL的发展，不过还是有很多的好东西。<br><img src=\"/img/hinton_brainsimulator.jpg\" alt=\"神经元\"><br>","more":"</p>\n<h2 id=\"Why-do-we-need-ML\"><a href=\"#Why-do-we-need-ML\" class=\"headerlink\" title=\"Why do we need ML?\"></a>Why do we need ML?</h2><p>从数据中学习，无需手写大量的逻辑代码。ML算法从数据中学习，给出完成任务（如人脸识别）的程序。这里的程序可能有大量的参数组成。得益于硬件的发展和大数据时代的来临，机器学习的应用越来越广泛。如下图所示，MNIST数据集中的手写数字2变化多端，很难人工设计代码逻辑找到判别一个手写数字是不是2的方法。<br><img src=\"/img/hinton_01_mnist_example.png\" alt=\"MNIST Digit 2\"></p>\n<h2 id=\"What-are-neural-network\"><a href=\"#What-are-neural-network\" class=\"headerlink\" title=\"What are neural network?\"></a>What are neural network?</h2><p>为什么要研究神经网络？</p>\n<ul>\n<li>理解人脑机理的一个途径；</li>\n<li>受到神经系统启发的并行计算</li>\n<li>新的学习算法来解决现实问题（本课所关心的）</li>\n</ul>\n<p>（这里总结的不是很科学，勉强概括了讲义的内容）<br>神经元结构如下所示。树突（dendritic tree）和其他神经元相连作为输入，轴突（axon）发散出很多分支和其他神经元相连。轴突和树突之间通过突触（synapse）连接。轴突有足够的电荷产生兴奋。这样完成神经元到神经元的communication。<br><img src=\"/img/hinton_01_neuron_structure.png\" alt=\"神经元的结构\"></p>\n<p>神经元之间互相连接。对不同的神经元输入，有不同的权重。这些权重可以变化，使得神经元之间的连接或变得更加紧密或疏离。人类大脑的神经元多达$10^{11}$个，每一个都有多达$10^4$个连接权重。不同神经元分布式计算，带宽很大。<br><img src=\"/img/hinton_01_neuron_commucation.png\" alt=\"神经元的相互连接\"></p>\n<p>大脑中不同的神经元分工不同（局部损坏造成相应的身体功能受损），但是这些神经元长得都差不多，它们也可以在一定的环境下发育成特定功能的神经元。</p>\n<p>而人工神经网络就是根据神经元的兴奋传导机理，人工模拟的神经网络。</p>\n<h2 id=\"Simple-models-of-different-neurons\"><a href=\"#Simple-models-of-different-neurons\" class=\"headerlink\" title=\"Simple models of different neurons\"></a>Simple models of different neurons</h2><p>我们简化神经元模型，用数学函数去近似描述它们的功能。这也是科学研究的通用思路，忽略次要矛盾，抓住主要矛盾。之后逐步向上加复杂度，更好地描述实验现象。下面介绍几种神经元的简化模型。</p>\n<h3 id=\"Linear-neuron\"><a href=\"#Linear-neuron\" class=\"headerlink\" title=\"Linear neuron\"></a>Linear neuron</h3><p>顾名思义，这种神经元用来进行线性组合的变换，不过要注意加上偏置项。如下所示：</p>\n<script type=\"math/tex; mode=display\">y = b+\\sum_{i=1}^{n}w_ix_i</script><h3 id=\"Binary-threshold-neuron\"><a href=\"#Binary-threshold-neuron\" class=\"headerlink\" title=\"Binary threshold neuron\"></a>Binary threshold neuron</h3><p>这种神经元用来将输入信号加权后做二元阈值化，我们可以通过两种方法来描述：</p>\n<script type=\"math/tex; mode=display\">y = \\begin{cases}1 \\quad \\text{if} \\quad z\\ge \\theta\\\\ 0\\quad \\text{otherwise}\\end{cases}</script><p>其中，$z$是输入信号的线性组合，$z=\\sum_{i}w_ix_i$</p>\n<p>或者，</p>\n<script type=\"math/tex; mode=display\">y = \\begin{cases}1 \\quad \\text{if} \\quad z\\ge 0\\\\ 0\\quad \\text{otherwise}\\end{cases}</script><p>其中，$z$是输入信号的线性组合并加上偏置项，$z = b+\\sum_{i}w_ix_i$</p>\n<h3 id=\"Rectified-linear-neuron\"><a href=\"#Rectified-linear-neuron\" class=\"headerlink\" title=\"Rectified linear neuron\"></a>Rectified linear neuron</h3><p>和上面的二值化神经元对比，有：</p>\n<script type=\"math/tex; mode=display\">y = \\begin{cases}z \\quad \\text{if} \\quad z\\ge 0\\\\ 0\\quad \\text{otherwise}\\end{cases}</script><h3 id=\"Sigmoid-neuron\"><a href=\"#Sigmoid-neuron\" class=\"headerlink\" title=\"Sigmoid neuron\"></a>Sigmoid neuron</h3><p>这种神经元通过logistic函数将输入shrink到区间$(0, 1)$，如下所示：</p>\n<script type=\"math/tex; mode=display\">y = \\frac{1}{1+\\exp(-z)}</script><p><img src=\"/img/hinton_01_sigmoid_function.png\" alt=\"Logistic函数示意\"></p>\n<p>由于Logistic函数将$(-\\infty, +\\infty)$的值压缩为S型，所以得名Sigmoid。</p>\n<h3 id=\"Stochastic-binary-neuron\"><a href=\"#Stochastic-binary-neuron\" class=\"headerlink\" title=\"Stochastic binary neuron\"></a>Stochastic binary neuron</h3><p>这种函数的输出仍是二值化的，而且是将Logistic函数的输入作为输出$1$的概率。也就是：</p>\n<script type=\"math/tex; mode=display\">P(y=1) = \\frac{1}{1+\\exp(-z)}</script><p>对于上面的Rectified linear neuron，也可以做类似的变形，将输出看作是泊松分布的系数。</p>\n<h2 id=\"Three-types-of-learning\"><a href=\"#Three-types-of-learning\" class=\"headerlink\" title=\"Three types of learning\"></a>Three types of learning</h2><p>即有监督学习，无监督学习和强化学习。</p>\n<ul>\n<li>有监督学习：给定输入向量，预测输出。</li>\n<li>无监督学习：学习一个对于输出来说的好的表示（good internal representation of input）。</li>\n<li>强化学习：学习如何决策达到最大期望奖赏。</li>\n</ul>\n<h3 id=\"有监督学习\"><a href=\"#有监督学习\" class=\"headerlink\" title=\"有监督学习\"></a>有监督学习</h3><p>有监督学习可以细分为分类和回归问题。有监督学习中，我们需要寻找一个model(由一个函数$f$和决定这个函数的参数$W$决定)，将输入$x$应映射为实数（回归问题）或者离散值（分类问题）。</p>\n<p>所谓的训练，就是指不断调整参数$W$，使得训练集合中的$x$在当前映射下得到的预测值与真实值之间的差异尽可能小。 在回归问题中，常常使用欧氏距离的平方作为差异的衡量。</p>\n<h3 id=\"强化学习\"><a href=\"#强化学习\" class=\"headerlink\" title=\"强化学习\"></a>强化学习</h3><p>在强化学习中，算法要给出动作或者动作序列。与有监督学习不同，强化学习中没有真实值，只有不定时（occasional）出现的奖赏。</p>\n<p>强化学习的难点如下：</p>\n<ul>\n<li>奖赏通常是delayed的。以AlphaGo来说，你很难追究中间某一步棋的决策对最后输赢的影响。</li>\n<li>奖赏通常只是一个标量，提供不了太多的信息。我只能知道这局最后的输赢，但是对于其他信息基本都不知道。</li>\n</ul>\n<h3 id=\"无监督学习\"><a href=\"#无监督学习\" class=\"headerlink\" title=\"无监督学习\"></a>无监督学习</h3><p>无监督学习以前受到的关注不多，这可能和它的目的不明确有关系。其中一个目的是能够提供输入的更好的表示，以用于强化学习和有监督学习。</p>"},{"title":"Neural Network for Machine Learning - Lecture 06 神经网络的“调教”方法","date":"2017-06-25T05:48:31.000Z","_content":"第六周的课程主要讲解了用于神经网络训练的梯度下降方法，首先对比了SGD，full batch GD和mini batch SGD方法，然后给出了几个用于神经网络训练的trick，主要包括输入数据预处理（零均值，单位方差以及PCA解耦），学习率的自适应调节以及网络权重的初始化方法（可以参考各大框架中实现的Xavier初始化方法等）。这篇文章主要记录了后续讲解的几种GD变种方法，如何合理利用梯度信息达到更好的训练效果。由于Hinton这门课确实时间已经很久了，所以文章末尾会结合一篇不错的总结性质的[博客](http://sebastianruder.com/optimizing-gradient-descent/index.html)和对应的[论文](https://arxiv.org/abs/1609.04747)以及PyTorch中的相关代码，对目前流行的梯度下降方法做个总结。\n\n下图即来自上面的这篇博客。\n\n![几种优化方法的可视化](/img/contours_evaluation_optimizers.gif)\n<!-- more -->\n\n## Momentum\n我们可以把训练过程想象成在权重空间的一个质点（小球），移动到全局最优点的过程。不同于GD，使用梯度信息直接更新权重的位置，momentum方法是将梯度作为速度量。这样做的好处是，当梯度的方向一直不变时，速度可以加快；当梯度方向变化剧烈时，由于符号改变，所以速度减慢，起到了GD中自适应调节学习率的过程。\n\n具体来说，我们利用新得到的梯度信息，采用滑动平均的方法更新速度。式子中的$\\epsilon$为学习率，$\\alpha$为momentum系数。\n$$\\Delta w_t = v_t = \\alpha v_{t-1} - \\epsilon g_t = \\Delta w_t - \\epsilon g_t$$\n\n为了说明momentum确实对学习过程有加速作用，假设一个简单的情形，即运动轨迹是一个斜率固定的斜面。那么我们有梯度$g$固定。根据上面的递推公式可以得到通项公式（简单的待定系数法凑出等比数列）：\n$$v_t = \\alpha(v_{t-1} + \\frac{\\epsilon g}{1-\\alpha}) - \\frac{\\epsilon g}{1-\\alpha}$$\n\n由于$\\alpha < 0$，所以当$t = \\infty$时，只剩下了后面的常数项，即：\n$$v_\\infty = -\\frac{\\epsilon}{1-\\alpha}g$$\n\n也就是说，权重更新的幅度变成了原来的$\\frac{1}{1-\\alpha}$倍。若取$\\alpha=0.99$，则加速$100$倍。\n\nHinton给出的建议是由于训练开头梯度值比较大，所以momentum系数一开始不要过大，例如可以取$0.5$。当梯度值较小，训练过程被困在一个峡谷的时候，可以适当提升。\n\n一种改进方法由Nesterov提出。在上面的方法中，我们首先更新了在该处的累积梯度信息，然后向前移动。而Nesterov方法中，我们首先沿着累计梯度信息向前走，然后根据梯度信息进行更正。\n\n![Nesterov方法](/img/hinton_06_nesterov_momentum.png)\n\n## Adaptive Learning Rate\n这种方法起源于这样的观察：在网络中，不同layer之间的权重更新需要不同的学习率。因为浅层和深层的layer梯度幅值很可能不同。所以，对不同的权重乘上不同的因子是个更加合理的选择。\n\n例如，我们可以根据梯度是否发生符号变化按照下面的方式调节某个权重$w_{ij}$的增益。注意$0.95$和$0.05$的和是$1$。这样可以使得平衡点在$1$附近。\n![Different learning rate gain](/img/hinton_06_learningrate.png)\n\n下面是使用这种方法的几个trick，包括限幅，较大的batch size以及和momentum的结合。\n\n![Tricks for adaptive lr](/img/hinton_06_tricks_for_adaptive_lr.png)\n\n## RMSProp\nrprop利用梯度的符号，如果符号保持不变，则相应增大step size；否则减小。但是只能用于full batch GD。RMSProp就是一种可以结合mini batch SGD和rprop的一种方法。\n\n我们使用滑动平均方法更新梯度的mean square（即RMS中的MS得来）。\n\n$$\\text{MeanSquare}(w, t) = 0.9 \\text{MeanSquare}(w, t-1) + 0.1g_t^2$$\n\n然后，将梯度除以上面的得到的Mean Square值。\n\nRMSProp还有一些变种，列举如下：\n![Otehr RMSProp](/img/hinton_06_rmsprop_improvement.png)\n\n## 课程总结\n- 对于小数据集，使用full batch GD（LBFGS或adaptive learning rate如rprop）。\n- 对于较大数据集，使用mini batch SGD。并可以考虑加上momentmum和RMSProp。\n\n如何选择学习率是一个较为依赖经验的任务（网络结构不同，任务不同）。\n![总结](/img/hinton_06_summary.png)\n\n## “Modern” SGD\n\n从本部分开始，我将转向总结摘要中提到的那篇博客中的主要内容。首先，给出当前基于梯度的优化方法的一些问题。可以看到，之后人们提出的改进方法就是想办法解决对应问题的。由于与Hinton课程相比，这些方法提出时间（也许称之为流行时间更合适？做数学的那帮人可能很早就知道这些优化方法了吧？）较短，所以这里仿照Modern C++之称呼，就把它们统一叫做Modern SGD吧。。。\n\n- 学习率通常很难确定。学习率太大？容易扯到蛋（loss直接爆炸）；学习率太小，训练到天荒地老。。。\n- 学习率如何在训练中调整。目前常用的方法是退火，要么是固定若干次迭代之后把学习率调小，要么是观察loss到某个阈值后把学习率调小。总之，都是在训练开始前，人工预先定义好的。而这没有考虑到数据集自身的特点。\n- 学习率对每个网络参数都一样。这点在上面课程中Hinton已经提到，引出了自适应学习率的方法。\n- 高度非凸函数的优化难题。以前人们多是认为网络很容易收敛到局部极小值。后来有人提出，网络之所以难训练，更多是由于遇到了鞍点。也就是某个方向上它是极小值；而另一个方向却是极大值（高数中介绍过的，马鞍面）\n\n![马鞍面](/img/hinton_06_maanmian.jpg)\n\n### Adagrad\n[Adagrad](http://jmlr.org/papers/v12/duchi11a.html)对不同的参数采用不同的学习率，也是其Ada（Adaptive）的名字得来。我们记时间步$t$时标号为$i$的参数对应的梯度为$g_{i}$，即：\n$$g_{i} = \\bigtriangledown_{\\theta_i} J(\\theta)$$\n\nAdagrad使用一个系数来为不同的参数修正学习率，如下：\n$$\\hat{g_i} = \\frac{1}{\\sqrt{G_i+\\epsilon}}g_i$$\n\n其中，$G_i$是截止到当前时间步$t$时，参数$\\theta_i$对应梯度$g_i$的平方和。\n\n我们可以把上面的式子写成矩阵形式。其中，$\\odot$表示逐元素的矩阵相乘（element-wise product）。同时，$G_t = g_t \\odot g_t$。\n\n$$\\theta_{t+1} = \\theta_t - \\frac{\\eta}{\\sqrt{G_t+\\epsilon}}\\odot g_t$$\n\n我们再来看PyTorch中的相关实现：\n\n``` py\n# for each gradient of parameters:\n# addcmul(t, alpha, t1, t2): t = t1*t2*alpha + t\n# let epsilon = 1E-10\nstate['sum'].addcmul_(1, grad, grad)   # 计算 G\nstd = state['sum'].sqrt().add_(1e-10)  # 计算 \\sqrt(G)\np.data.addcdiv_(-clr, grad, std)       # 更新\n```\n\n由于Adagrad对不同的梯度给了不同的学习率修正值，所以使用这种方法时，我们可以不用操心学习率，只是给定一个初始值（如$0.01$）就够了。尤其是对稀疏的数据，Adagrad方法能够自适应调节其梯度更新信息，给那些不常出现（非零）的梯度对应更大的学习率。PyTorch中还为稀疏数据特别优化了更新算法。\n\nAdagrad的缺点在于由于$G_t$矩阵是平方和，所以分母会越来越大，造成训练后期学习率会变得很小。下面的Adadelta方法针对这个问题进行了改进。\n\n### Adadelta\n[Adadelta](https://arxiv.org/abs/1212.5701)给出的改进方法是不再记录所有的历史时刻的$g$的平方和，而是最近一个有限的观察窗口$w$的累积梯度平方和。在实际使用时，这种方法使用了一个参数$\\gamma$（如$0.9$）作为遗忘因子，对$E[g_t^2]$进行统计。\n\n$$E[g_t^2] = \\gamma E[g_{t-1}^2] + (1-\\gamma)g_t^2$$\n\n由于$\\sqrt{E[g_t^2]}$就是$g$的均方根RMS，所以，修正后的梯度如下。注意到，这正是Hinton在课上所讲到的RMSprop的优化方法。\n\n$$\\hat{g}_t = \\frac{1}{\\text{RMS}[g]}g_t$$\n\n作者还观察到，这样更新的话，其实$\\theta$和$\\Delta \\theta$的单位是不一样的（此时$\\Delta \\theta$是无量纲数）。所以，作者提出再乘上一个$\\text{RMS}[\\Delta \\theta]$来平衡（同时去掉了学习率$\\eta$），所以，最终的参数更新如下：\n\n$$\\theta_{t+1} = \\theta_t - \\frac{\\text{RMS}[\\Delta \\theta]}{\\text{RMS}[g]}g_t$$\n\n这种方法甚至不再需要学习率。下面是PyTorch中的实现，其中仍然保有学习率`lr`这一参数设定，默认值为$1.0$。代码注释中，我使用`MS`来指代$E[x^2]$。即，$\\text{RMS}[x] = \\sqrt{\\text{MS}[x]+\\epsilon}$。\n``` py\n# update: MS[g] = MS[g]*\\rho + g*g*(1-\\rho)\nsquare_avg.mul_(rho).addcmul_(1 - rho, grad, grad)\n# current RMS[g] = sqrt(MS[g] + \\epsilon)\nstd = square_avg.add(eps).sqrt_()\n# \\Delta \\theta = RMS[\\Delta \\theta] / RMS[g]) * g\ndelta = acc_delta.add(eps).sqrt_().div_(std).mul_(grad)\n# update parameter: \\theta -= lr * \\Delta \\theta\np.data.add_(-group['lr'], delta)\n# update MS[\\Delta \\theta] = MS[\\Delta \\theta] * \\rho + \\Delta \\theta^2 * (1-\\rho)\nacc_delta.mul_(rho).addcmul_(1 - rho, delta, delta)\n```\n\n### Adam\n[Adaptive momen Estimation（Adam，自适应矩估计）](https://arxiv.org/abs/1412.6980)，是另一种为不同参数自适应设置不同学习率的方法。Adam方法不止存储过往的梯度平方均值（二阶矩）信息，还存储过往的梯度均值信息（一阶矩）。\n$$\\begin{aligned}m_t&=\\beta_1 m_{t-1}+(1-\\beta_1)g_t\\\\v_t&=\\beta_2 v_{t-1}+(1-\\beta_2)g_t^2\\end{aligned}$$\n\n作者观察到上述估计是有偏的（biase towards $0$），所以给出如下修正：\n$$\\begin{aligned}\\hat{m} &= \\frac{m}{1-\\beta_1}\\\\ \\hat{v}&=\\frac{v}{1-\\beta_2}\\end{aligned}$$\n\n参数的更新如下：\n$$\\theta_{t+1} = \\theta_t - \\frac{\\eta}{\\sqrt{\\hat{v_t} + \\epsilon}}\\hat{m_t}$$\n\n作者给出$\\beta_1 = 0.9$，$\\beta_2=0.999$，$\\epsilon=10^{-8}$。\n\n为了更好地理解PyTorch中的实现方式，需要对上式进行变形：\n$$\\Delta \\theta = \\frac{\\sqrt{1-\\beta_2}}{1-\\beta_1}\\eta \\frac{m_t}{\\sqrt{v_t}}$$\n\n代码中令$\\text{step_size} =  \\frac{\\sqrt{1-\\beta_2}}{1-\\beta_1}\\eta$。同时，$\\beta$也要以指数规律衰减，即：$\\beta_t = \\beta_0^t$。\n\n``` py\n# exp_avg is `m`: expected average of g\nexp_avg.mul_(beta1).add_(1 - beta1, grad)\n# exp_avg_sq is `v`: expected average of g's square\nexp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n\n# \\sqrt{v_t + \\epsilon}\ndenom = exp_avg_sq.sqrt().add_(group['eps'])\n\n# 1 - \\beta_1^t\nbias_correction1 = 1 - beta1 ** state['step']\n# 1 - \\beta_2^t\nbias_correction2 = 1 - beta2 ** state['step']\n# get step_size\nstep_size = group['lr'] * math.sqrt(bias_correction2) / bias_correction1\n# delta = -step_size * m / sqrt(v)\np.data.addcdiv_(-step_size, exp_avg, denom)\n```\n\n### AdaMax\n上面Adam中，实际上我们是用梯度$g$的$2$范数（$\\sqrt{\\hat{v_t}}$）去对$g$进行Normalization。那么为什么不用其他形式的范数$p$来试试呢？然而，对于$1$范数和$2$范数，数值是稳定的。对于再大的$p$，数值不稳定。不过，当取无穷范数的时候，又是稳定的了。\n\n由于无穷范数就是求绝对值最大的分量，所以这种方法叫做[AdaMax](https://arxiv.org/abs/1412.6980)。其对应的$\\hat{v_t}$为（这里为了避免混淆，使用$u_t$指代）：\n$$u_t = \\beta_2^\\infty u_{t-1} + (1-\\beta_2^\\infty) g_t^\\infty$$\n\n我们将$u\\_t$按照时间展开，可以得到（直接摘自论文的图）。其中最后一步递推式的得来：根据$u\\_t$把$u\\_{t-1}$的展开形式也写出来，就不难发现最下面的递推形式。\n\n![Adamax中ut的推导](/img/hinton_06_adamax.png)\n\n相应的更新权重操作为：\n$$\\theta_{t+1} = \\theta_t -\\frac{\\eta}{u_t}\\hat{m}_t$$\n\n在PyTorch中的实现如下：\n``` py\n# Update biased first moment estimate, which is \\hat{m}_t\nexp_avg.mul_(beta1).add_(1 - beta1, grad)\n# 下面这种用来逐元素求取 max(A, B) 的方法可以学习一个\n# Update the exponentially weighted infinity norm.\nnorm_buf = torch.cat([\n    exp_inf.mul_(beta2).unsqueeze(0),\n    grad.abs().add_(eps).unsqueeze_(0)\n], 0)\n## 找到 exp_inf 和 g之间的较大者（只需要在刚刚聚合的这个维度上找即可~）\ntorch.max(norm_buf, 0, keepdim=False, out=(exp_inf, exp_inf.new().long()))\n\n## beta1 correction\nbias_correction = 1 - beta1 ** state['step']\nclr = group['lr'] / bias_correction\n\np.data.addcdiv_(-clr, exp_avg, exp_inf)\n```\n","source":"_posts/hinton-nnml-06.md","raw":"---\ntitle: Neural Network for Machine Learning - Lecture 06 神经网络的“调教”方法\ndate: 2017-06-25 13:48:31\ntags:\n    - 公开课\n    - deep learning\n    - pytorch\n---\n第六周的课程主要讲解了用于神经网络训练的梯度下降方法，首先对比了SGD，full batch GD和mini batch SGD方法，然后给出了几个用于神经网络训练的trick，主要包括输入数据预处理（零均值，单位方差以及PCA解耦），学习率的自适应调节以及网络权重的初始化方法（可以参考各大框架中实现的Xavier初始化方法等）。这篇文章主要记录了后续讲解的几种GD变种方法，如何合理利用梯度信息达到更好的训练效果。由于Hinton这门课确实时间已经很久了，所以文章末尾会结合一篇不错的总结性质的[博客](http://sebastianruder.com/optimizing-gradient-descent/index.html)和对应的[论文](https://arxiv.org/abs/1609.04747)以及PyTorch中的相关代码，对目前流行的梯度下降方法做个总结。\n\n下图即来自上面的这篇博客。\n\n![几种优化方法的可视化](/img/contours_evaluation_optimizers.gif)\n<!-- more -->\n\n## Momentum\n我们可以把训练过程想象成在权重空间的一个质点（小球），移动到全局最优点的过程。不同于GD，使用梯度信息直接更新权重的位置，momentum方法是将梯度作为速度量。这样做的好处是，当梯度的方向一直不变时，速度可以加快；当梯度方向变化剧烈时，由于符号改变，所以速度减慢，起到了GD中自适应调节学习率的过程。\n\n具体来说，我们利用新得到的梯度信息，采用滑动平均的方法更新速度。式子中的$\\epsilon$为学习率，$\\alpha$为momentum系数。\n$$\\Delta w_t = v_t = \\alpha v_{t-1} - \\epsilon g_t = \\Delta w_t - \\epsilon g_t$$\n\n为了说明momentum确实对学习过程有加速作用，假设一个简单的情形，即运动轨迹是一个斜率固定的斜面。那么我们有梯度$g$固定。根据上面的递推公式可以得到通项公式（简单的待定系数法凑出等比数列）：\n$$v_t = \\alpha(v_{t-1} + \\frac{\\epsilon g}{1-\\alpha}) - \\frac{\\epsilon g}{1-\\alpha}$$\n\n由于$\\alpha < 0$，所以当$t = \\infty$时，只剩下了后面的常数项，即：\n$$v_\\infty = -\\frac{\\epsilon}{1-\\alpha}g$$\n\n也就是说，权重更新的幅度变成了原来的$\\frac{1}{1-\\alpha}$倍。若取$\\alpha=0.99$，则加速$100$倍。\n\nHinton给出的建议是由于训练开头梯度值比较大，所以momentum系数一开始不要过大，例如可以取$0.5$。当梯度值较小，训练过程被困在一个峡谷的时候，可以适当提升。\n\n一种改进方法由Nesterov提出。在上面的方法中，我们首先更新了在该处的累积梯度信息，然后向前移动。而Nesterov方法中，我们首先沿着累计梯度信息向前走，然后根据梯度信息进行更正。\n\n![Nesterov方法](/img/hinton_06_nesterov_momentum.png)\n\n## Adaptive Learning Rate\n这种方法起源于这样的观察：在网络中，不同layer之间的权重更新需要不同的学习率。因为浅层和深层的layer梯度幅值很可能不同。所以，对不同的权重乘上不同的因子是个更加合理的选择。\n\n例如，我们可以根据梯度是否发生符号变化按照下面的方式调节某个权重$w_{ij}$的增益。注意$0.95$和$0.05$的和是$1$。这样可以使得平衡点在$1$附近。\n![Different learning rate gain](/img/hinton_06_learningrate.png)\n\n下面是使用这种方法的几个trick，包括限幅，较大的batch size以及和momentum的结合。\n\n![Tricks for adaptive lr](/img/hinton_06_tricks_for_adaptive_lr.png)\n\n## RMSProp\nrprop利用梯度的符号，如果符号保持不变，则相应增大step size；否则减小。但是只能用于full batch GD。RMSProp就是一种可以结合mini batch SGD和rprop的一种方法。\n\n我们使用滑动平均方法更新梯度的mean square（即RMS中的MS得来）。\n\n$$\\text{MeanSquare}(w, t) = 0.9 \\text{MeanSquare}(w, t-1) + 0.1g_t^2$$\n\n然后，将梯度除以上面的得到的Mean Square值。\n\nRMSProp还有一些变种，列举如下：\n![Otehr RMSProp](/img/hinton_06_rmsprop_improvement.png)\n\n## 课程总结\n- 对于小数据集，使用full batch GD（LBFGS或adaptive learning rate如rprop）。\n- 对于较大数据集，使用mini batch SGD。并可以考虑加上momentmum和RMSProp。\n\n如何选择学习率是一个较为依赖经验的任务（网络结构不同，任务不同）。\n![总结](/img/hinton_06_summary.png)\n\n## “Modern” SGD\n\n从本部分开始，我将转向总结摘要中提到的那篇博客中的主要内容。首先，给出当前基于梯度的优化方法的一些问题。可以看到，之后人们提出的改进方法就是想办法解决对应问题的。由于与Hinton课程相比，这些方法提出时间（也许称之为流行时间更合适？做数学的那帮人可能很早就知道这些优化方法了吧？）较短，所以这里仿照Modern C++之称呼，就把它们统一叫做Modern SGD吧。。。\n\n- 学习率通常很难确定。学习率太大？容易扯到蛋（loss直接爆炸）；学习率太小，训练到天荒地老。。。\n- 学习率如何在训练中调整。目前常用的方法是退火，要么是固定若干次迭代之后把学习率调小，要么是观察loss到某个阈值后把学习率调小。总之，都是在训练开始前，人工预先定义好的。而这没有考虑到数据集自身的特点。\n- 学习率对每个网络参数都一样。这点在上面课程中Hinton已经提到，引出了自适应学习率的方法。\n- 高度非凸函数的优化难题。以前人们多是认为网络很容易收敛到局部极小值。后来有人提出，网络之所以难训练，更多是由于遇到了鞍点。也就是某个方向上它是极小值；而另一个方向却是极大值（高数中介绍过的，马鞍面）\n\n![马鞍面](/img/hinton_06_maanmian.jpg)\n\n### Adagrad\n[Adagrad](http://jmlr.org/papers/v12/duchi11a.html)对不同的参数采用不同的学习率，也是其Ada（Adaptive）的名字得来。我们记时间步$t$时标号为$i$的参数对应的梯度为$g_{i}$，即：\n$$g_{i} = \\bigtriangledown_{\\theta_i} J(\\theta)$$\n\nAdagrad使用一个系数来为不同的参数修正学习率，如下：\n$$\\hat{g_i} = \\frac{1}{\\sqrt{G_i+\\epsilon}}g_i$$\n\n其中，$G_i$是截止到当前时间步$t$时，参数$\\theta_i$对应梯度$g_i$的平方和。\n\n我们可以把上面的式子写成矩阵形式。其中，$\\odot$表示逐元素的矩阵相乘（element-wise product）。同时，$G_t = g_t \\odot g_t$。\n\n$$\\theta_{t+1} = \\theta_t - \\frac{\\eta}{\\sqrt{G_t+\\epsilon}}\\odot g_t$$\n\n我们再来看PyTorch中的相关实现：\n\n``` py\n# for each gradient of parameters:\n# addcmul(t, alpha, t1, t2): t = t1*t2*alpha + t\n# let epsilon = 1E-10\nstate['sum'].addcmul_(1, grad, grad)   # 计算 G\nstd = state['sum'].sqrt().add_(1e-10)  # 计算 \\sqrt(G)\np.data.addcdiv_(-clr, grad, std)       # 更新\n```\n\n由于Adagrad对不同的梯度给了不同的学习率修正值，所以使用这种方法时，我们可以不用操心学习率，只是给定一个初始值（如$0.01$）就够了。尤其是对稀疏的数据，Adagrad方法能够自适应调节其梯度更新信息，给那些不常出现（非零）的梯度对应更大的学习率。PyTorch中还为稀疏数据特别优化了更新算法。\n\nAdagrad的缺点在于由于$G_t$矩阵是平方和，所以分母会越来越大，造成训练后期学习率会变得很小。下面的Adadelta方法针对这个问题进行了改进。\n\n### Adadelta\n[Adadelta](https://arxiv.org/abs/1212.5701)给出的改进方法是不再记录所有的历史时刻的$g$的平方和，而是最近一个有限的观察窗口$w$的累积梯度平方和。在实际使用时，这种方法使用了一个参数$\\gamma$（如$0.9$）作为遗忘因子，对$E[g_t^2]$进行统计。\n\n$$E[g_t^2] = \\gamma E[g_{t-1}^2] + (1-\\gamma)g_t^2$$\n\n由于$\\sqrt{E[g_t^2]}$就是$g$的均方根RMS，所以，修正后的梯度如下。注意到，这正是Hinton在课上所讲到的RMSprop的优化方法。\n\n$$\\hat{g}_t = \\frac{1}{\\text{RMS}[g]}g_t$$\n\n作者还观察到，这样更新的话，其实$\\theta$和$\\Delta \\theta$的单位是不一样的（此时$\\Delta \\theta$是无量纲数）。所以，作者提出再乘上一个$\\text{RMS}[\\Delta \\theta]$来平衡（同时去掉了学习率$\\eta$），所以，最终的参数更新如下：\n\n$$\\theta_{t+1} = \\theta_t - \\frac{\\text{RMS}[\\Delta \\theta]}{\\text{RMS}[g]}g_t$$\n\n这种方法甚至不再需要学习率。下面是PyTorch中的实现，其中仍然保有学习率`lr`这一参数设定，默认值为$1.0$。代码注释中，我使用`MS`来指代$E[x^2]$。即，$\\text{RMS}[x] = \\sqrt{\\text{MS}[x]+\\epsilon}$。\n``` py\n# update: MS[g] = MS[g]*\\rho + g*g*(1-\\rho)\nsquare_avg.mul_(rho).addcmul_(1 - rho, grad, grad)\n# current RMS[g] = sqrt(MS[g] + \\epsilon)\nstd = square_avg.add(eps).sqrt_()\n# \\Delta \\theta = RMS[\\Delta \\theta] / RMS[g]) * g\ndelta = acc_delta.add(eps).sqrt_().div_(std).mul_(grad)\n# update parameter: \\theta -= lr * \\Delta \\theta\np.data.add_(-group['lr'], delta)\n# update MS[\\Delta \\theta] = MS[\\Delta \\theta] * \\rho + \\Delta \\theta^2 * (1-\\rho)\nacc_delta.mul_(rho).addcmul_(1 - rho, delta, delta)\n```\n\n### Adam\n[Adaptive momen Estimation（Adam，自适应矩估计）](https://arxiv.org/abs/1412.6980)，是另一种为不同参数自适应设置不同学习率的方法。Adam方法不止存储过往的梯度平方均值（二阶矩）信息，还存储过往的梯度均值信息（一阶矩）。\n$$\\begin{aligned}m_t&=\\beta_1 m_{t-1}+(1-\\beta_1)g_t\\\\v_t&=\\beta_2 v_{t-1}+(1-\\beta_2)g_t^2\\end{aligned}$$\n\n作者观察到上述估计是有偏的（biase towards $0$），所以给出如下修正：\n$$\\begin{aligned}\\hat{m} &= \\frac{m}{1-\\beta_1}\\\\ \\hat{v}&=\\frac{v}{1-\\beta_2}\\end{aligned}$$\n\n参数的更新如下：\n$$\\theta_{t+1} = \\theta_t - \\frac{\\eta}{\\sqrt{\\hat{v_t} + \\epsilon}}\\hat{m_t}$$\n\n作者给出$\\beta_1 = 0.9$，$\\beta_2=0.999$，$\\epsilon=10^{-8}$。\n\n为了更好地理解PyTorch中的实现方式，需要对上式进行变形：\n$$\\Delta \\theta = \\frac{\\sqrt{1-\\beta_2}}{1-\\beta_1}\\eta \\frac{m_t}{\\sqrt{v_t}}$$\n\n代码中令$\\text{step_size} =  \\frac{\\sqrt{1-\\beta_2}}{1-\\beta_1}\\eta$。同时，$\\beta$也要以指数规律衰减，即：$\\beta_t = \\beta_0^t$。\n\n``` py\n# exp_avg is `m`: expected average of g\nexp_avg.mul_(beta1).add_(1 - beta1, grad)\n# exp_avg_sq is `v`: expected average of g's square\nexp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n\n# \\sqrt{v_t + \\epsilon}\ndenom = exp_avg_sq.sqrt().add_(group['eps'])\n\n# 1 - \\beta_1^t\nbias_correction1 = 1 - beta1 ** state['step']\n# 1 - \\beta_2^t\nbias_correction2 = 1 - beta2 ** state['step']\n# get step_size\nstep_size = group['lr'] * math.sqrt(bias_correction2) / bias_correction1\n# delta = -step_size * m / sqrt(v)\np.data.addcdiv_(-step_size, exp_avg, denom)\n```\n\n### AdaMax\n上面Adam中，实际上我们是用梯度$g$的$2$范数（$\\sqrt{\\hat{v_t}}$）去对$g$进行Normalization。那么为什么不用其他形式的范数$p$来试试呢？然而，对于$1$范数和$2$范数，数值是稳定的。对于再大的$p$，数值不稳定。不过，当取无穷范数的时候，又是稳定的了。\n\n由于无穷范数就是求绝对值最大的分量，所以这种方法叫做[AdaMax](https://arxiv.org/abs/1412.6980)。其对应的$\\hat{v_t}$为（这里为了避免混淆，使用$u_t$指代）：\n$$u_t = \\beta_2^\\infty u_{t-1} + (1-\\beta_2^\\infty) g_t^\\infty$$\n\n我们将$u\\_t$按照时间展开，可以得到（直接摘自论文的图）。其中最后一步递推式的得来：根据$u\\_t$把$u\\_{t-1}$的展开形式也写出来，就不难发现最下面的递推形式。\n\n![Adamax中ut的推导](/img/hinton_06_adamax.png)\n\n相应的更新权重操作为：\n$$\\theta_{t+1} = \\theta_t -\\frac{\\eta}{u_t}\\hat{m}_t$$\n\n在PyTorch中的实现如下：\n``` py\n# Update biased first moment estimate, which is \\hat{m}_t\nexp_avg.mul_(beta1).add_(1 - beta1, grad)\n# 下面这种用来逐元素求取 max(A, B) 的方法可以学习一个\n# Update the exponentially weighted infinity norm.\nnorm_buf = torch.cat([\n    exp_inf.mul_(beta2).unsqueeze(0),\n    grad.abs().add_(eps).unsqueeze_(0)\n], 0)\n## 找到 exp_inf 和 g之间的较大者（只需要在刚刚聚合的这个维度上找即可~）\ntorch.max(norm_buf, 0, keepdim=False, out=(exp_inf, exp_inf.new().long()))\n\n## beta1 correction\nbias_correction = 1 - beta1 ** state['step']\nclr = group['lr'] / bias_correction\n\np.data.addcdiv_(-clr, exp_avg, exp_inf)\n```\n","slug":"hinton-nnml-06","published":1,"updated":"2018-01-12T06:22:20.472Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjcu6vctp001oqu46um6gj3ge","content":"<p>第六周的课程主要讲解了用于神经网络训练的梯度下降方法，首先对比了SGD，full batch GD和mini batch SGD方法，然后给出了几个用于神经网络训练的trick，主要包括输入数据预处理（零均值，单位方差以及PCA解耦），学习率的自适应调节以及网络权重的初始化方法（可以参考各大框架中实现的Xavier初始化方法等）。这篇文章主要记录了后续讲解的几种GD变种方法，如何合理利用梯度信息达到更好的训练效果。由于Hinton这门课确实时间已经很久了，所以文章末尾会结合一篇不错的总结性质的<a href=\"http://sebastianruder.com/optimizing-gradient-descent/index.html\" target=\"_blank\" rel=\"external\">博客</a>和对应的<a href=\"https://arxiv.org/abs/1609.04747\" target=\"_blank\" rel=\"external\">论文</a>以及PyTorch中的相关代码，对目前流行的梯度下降方法做个总结。</p>\n<p>下图即来自上面的这篇博客。</p>\n<p><img src=\"/img/contours_evaluation_optimizers.gif\" alt=\"几种优化方法的可视化\"><br><a id=\"more\"></a></p>\n<h2 id=\"Momentum\"><a href=\"#Momentum\" class=\"headerlink\" title=\"Momentum\"></a>Momentum</h2><p>我们可以把训练过程想象成在权重空间的一个质点（小球），移动到全局最优点的过程。不同于GD，使用梯度信息直接更新权重的位置，momentum方法是将梯度作为速度量。这样做的好处是，当梯度的方向一直不变时，速度可以加快；当梯度方向变化剧烈时，由于符号改变，所以速度减慢，起到了GD中自适应调节学习率的过程。</p>\n<p>具体来说，我们利用新得到的梯度信息，采用滑动平均的方法更新速度。式子中的$\\epsilon$为学习率，$\\alpha$为momentum系数。</p>\n<script type=\"math/tex; mode=display\">\\Delta w_t = v_t = \\alpha v_{t-1} - \\epsilon g_t = \\Delta w_t - \\epsilon g_t</script><p>为了说明momentum确实对学习过程有加速作用，假设一个简单的情形，即运动轨迹是一个斜率固定的斜面。那么我们有梯度$g$固定。根据上面的递推公式可以得到通项公式（简单的待定系数法凑出等比数列）：</p>\n<script type=\"math/tex; mode=display\">v_t = \\alpha(v_{t-1} + \\frac{\\epsilon g}{1-\\alpha}) - \\frac{\\epsilon g}{1-\\alpha}</script><p>由于$\\alpha &lt; 0$，所以当$t = \\infty$时，只剩下了后面的常数项，即：</p>\n<script type=\"math/tex; mode=display\">v_\\infty = -\\frac{\\epsilon}{1-\\alpha}g</script><p>也就是说，权重更新的幅度变成了原来的$\\frac{1}{1-\\alpha}$倍。若取$\\alpha=0.99$，则加速$100$倍。</p>\n<p>Hinton给出的建议是由于训练开头梯度值比较大，所以momentum系数一开始不要过大，例如可以取$0.5$。当梯度值较小，训练过程被困在一个峡谷的时候，可以适当提升。</p>\n<p>一种改进方法由Nesterov提出。在上面的方法中，我们首先更新了在该处的累积梯度信息，然后向前移动。而Nesterov方法中，我们首先沿着累计梯度信息向前走，然后根据梯度信息进行更正。</p>\n<p><img src=\"/img/hinton_06_nesterov_momentum.png\" alt=\"Nesterov方法\"></p>\n<h2 id=\"Adaptive-Learning-Rate\"><a href=\"#Adaptive-Learning-Rate\" class=\"headerlink\" title=\"Adaptive Learning Rate\"></a>Adaptive Learning Rate</h2><p>这种方法起源于这样的观察：在网络中，不同layer之间的权重更新需要不同的学习率。因为浅层和深层的layer梯度幅值很可能不同。所以，对不同的权重乘上不同的因子是个更加合理的选择。</p>\n<p>例如，我们可以根据梯度是否发生符号变化按照下面的方式调节某个权重$w_{ij}$的增益。注意$0.95$和$0.05$的和是$1$。这样可以使得平衡点在$1$附近。<br><img src=\"/img/hinton_06_learningrate.png\" alt=\"Different learning rate gain\"></p>\n<p>下面是使用这种方法的几个trick，包括限幅，较大的batch size以及和momentum的结合。</p>\n<p><img src=\"/img/hinton_06_tricks_for_adaptive_lr.png\" alt=\"Tricks for adaptive lr\"></p>\n<h2 id=\"RMSProp\"><a href=\"#RMSProp\" class=\"headerlink\" title=\"RMSProp\"></a>RMSProp</h2><p>rprop利用梯度的符号，如果符号保持不变，则相应增大step size；否则减小。但是只能用于full batch GD。RMSProp就是一种可以结合mini batch SGD和rprop的一种方法。</p>\n<p>我们使用滑动平均方法更新梯度的mean square（即RMS中的MS得来）。</p>\n<script type=\"math/tex; mode=display\">\\text{MeanSquare}(w, t) = 0.9 \\text{MeanSquare}(w, t-1) + 0.1g_t^2</script><p>然后，将梯度除以上面的得到的Mean Square值。</p>\n<p>RMSProp还有一些变种，列举如下：<br><img src=\"/img/hinton_06_rmsprop_improvement.png\" alt=\"Otehr RMSProp\"></p>\n<h2 id=\"课程总结\"><a href=\"#课程总结\" class=\"headerlink\" title=\"课程总结\"></a>课程总结</h2><ul>\n<li>对于小数据集，使用full batch GD（LBFGS或adaptive learning rate如rprop）。</li>\n<li>对于较大数据集，使用mini batch SGD。并可以考虑加上momentmum和RMSProp。</li>\n</ul>\n<p>如何选择学习率是一个较为依赖经验的任务（网络结构不同，任务不同）。<br><img src=\"/img/hinton_06_summary.png\" alt=\"总结\"></p>\n<h2 id=\"“Modern”-SGD\"><a href=\"#“Modern”-SGD\" class=\"headerlink\" title=\"“Modern” SGD\"></a>“Modern” SGD</h2><p>从本部分开始，我将转向总结摘要中提到的那篇博客中的主要内容。首先，给出当前基于梯度的优化方法的一些问题。可以看到，之后人们提出的改进方法就是想办法解决对应问题的。由于与Hinton课程相比，这些方法提出时间（也许称之为流行时间更合适？做数学的那帮人可能很早就知道这些优化方法了吧？）较短，所以这里仿照Modern C++之称呼，就把它们统一叫做Modern SGD吧。。。</p>\n<ul>\n<li>学习率通常很难确定。学习率太大？容易扯到蛋（loss直接爆炸）；学习率太小，训练到天荒地老。。。</li>\n<li>学习率如何在训练中调整。目前常用的方法是退火，要么是固定若干次迭代之后把学习率调小，要么是观察loss到某个阈值后把学习率调小。总之，都是在训练开始前，人工预先定义好的。而这没有考虑到数据集自身的特点。</li>\n<li>学习率对每个网络参数都一样。这点在上面课程中Hinton已经提到，引出了自适应学习率的方法。</li>\n<li>高度非凸函数的优化难题。以前人们多是认为网络很容易收敛到局部极小值。后来有人提出，网络之所以难训练，更多是由于遇到了鞍点。也就是某个方向上它是极小值；而另一个方向却是极大值（高数中介绍过的，马鞍面）</li>\n</ul>\n<p><img src=\"/img/hinton_06_maanmian.jpg\" alt=\"马鞍面\"></p>\n<h3 id=\"Adagrad\"><a href=\"#Adagrad\" class=\"headerlink\" title=\"Adagrad\"></a>Adagrad</h3><p><a href=\"http://jmlr.org/papers/v12/duchi11a.html\" target=\"_blank\" rel=\"external\">Adagrad</a>对不同的参数采用不同的学习率，也是其Ada（Adaptive）的名字得来。我们记时间步$t$时标号为$i$的参数对应的梯度为$g_{i}$，即：</p>\n<script type=\"math/tex; mode=display\">g_{i} = \\bigtriangledown_{\\theta_i} J(\\theta)</script><p>Adagrad使用一个系数来为不同的参数修正学习率，如下：</p>\n<script type=\"math/tex; mode=display\">\\hat{g_i} = \\frac{1}{\\sqrt{G_i+\\epsilon}}g_i</script><p>其中，$G_i$是截止到当前时间步$t$时，参数$\\theta_i$对应梯度$g_i$的平方和。</p>\n<p>我们可以把上面的式子写成矩阵形式。其中，$\\odot$表示逐元素的矩阵相乘（element-wise product）。同时，$G_t = g_t \\odot g_t$。</p>\n<script type=\"math/tex; mode=display\">\\theta_{t+1} = \\theta_t - \\frac{\\eta}{\\sqrt{G_t+\\epsilon}}\\odot g_t</script><p>我们再来看PyTorch中的相关实现：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\"># for each gradient of parameters:</span></div><div class=\"line\"><span class=\"comment\"># addcmul(t, alpha, t1, t2): t = t1*t2*alpha + t</span></div><div class=\"line\"><span class=\"comment\"># let epsilon = 1E-10</span></div><div class=\"line\">state[<span class=\"string\">'sum'</span>].addcmul_(<span class=\"number\">1</span>, grad, grad)   <span class=\"comment\"># 计算 G</span></div><div class=\"line\">std = state[<span class=\"string\">'sum'</span>].sqrt().add_(<span class=\"number\">1e-10</span>)  <span class=\"comment\"># 计算 \\sqrt(G)</span></div><div class=\"line\">p.data.addcdiv_(-clr, grad, std)       <span class=\"comment\"># 更新</span></div></pre></td></tr></table></figure>\n<p>由于Adagrad对不同的梯度给了不同的学习率修正值，所以使用这种方法时，我们可以不用操心学习率，只是给定一个初始值（如$0.01$）就够了。尤其是对稀疏的数据，Adagrad方法能够自适应调节其梯度更新信息，给那些不常出现（非零）的梯度对应更大的学习率。PyTorch中还为稀疏数据特别优化了更新算法。</p>\n<p>Adagrad的缺点在于由于$G_t$矩阵是平方和，所以分母会越来越大，造成训练后期学习率会变得很小。下面的Adadelta方法针对这个问题进行了改进。</p>\n<h3 id=\"Adadelta\"><a href=\"#Adadelta\" class=\"headerlink\" title=\"Adadelta\"></a>Adadelta</h3><p><a href=\"https://arxiv.org/abs/1212.5701\" target=\"_blank\" rel=\"external\">Adadelta</a>给出的改进方法是不再记录所有的历史时刻的$g$的平方和，而是最近一个有限的观察窗口$w$的累积梯度平方和。在实际使用时，这种方法使用了一个参数$\\gamma$（如$0.9$）作为遗忘因子，对$E[g_t^2]$进行统计。</p>\n<script type=\"math/tex; mode=display\">E[g_t^2] = \\gamma E[g_{t-1}^2] + (1-\\gamma)g_t^2</script><p>由于$\\sqrt{E[g_t^2]}$就是$g$的均方根RMS，所以，修正后的梯度如下。注意到，这正是Hinton在课上所讲到的RMSprop的优化方法。</p>\n<script type=\"math/tex; mode=display\">\\hat{g}_t = \\frac{1}{\\text{RMS}[g]}g_t</script><p>作者还观察到，这样更新的话，其实$\\theta$和$\\Delta \\theta$的单位是不一样的（此时$\\Delta \\theta$是无量纲数）。所以，作者提出再乘上一个$\\text{RMS}[\\Delta \\theta]$来平衡（同时去掉了学习率$\\eta$），所以，最终的参数更新如下：</p>\n<script type=\"math/tex; mode=display\">\\theta_{t+1} = \\theta_t - \\frac{\\text{RMS}[\\Delta \\theta]}{\\text{RMS}[g]}g_t</script><p>这种方法甚至不再需要学习率。下面是PyTorch中的实现，其中仍然保有学习率<code>lr</code>这一参数设定，默认值为$1.0$。代码注释中，我使用<code>MS</code>来指代$E[x^2]$。即，$\\text{RMS}[x] = \\sqrt{\\text{MS}[x]+\\epsilon}$。<br><figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\"># update: MS[g] = MS[g]*\\rho + g*g*(1-\\rho)</span></div><div class=\"line\">square_avg.mul_(rho).addcmul_(<span class=\"number\">1</span> - rho, grad, grad)</div><div class=\"line\"><span class=\"comment\"># current RMS[g] = sqrt(MS[g] + \\epsilon)</span></div><div class=\"line\">std = square_avg.add(eps).sqrt_()</div><div class=\"line\"><span class=\"comment\"># \\Delta \\theta = RMS[\\Delta \\theta] / RMS[g]) * g</span></div><div class=\"line\">delta = acc_delta.add(eps).sqrt_().div_(std).mul_(grad)</div><div class=\"line\"><span class=\"comment\"># update parameter: \\theta -= lr * \\Delta \\theta</span></div><div class=\"line\">p.data.add_(-group[<span class=\"string\">'lr'</span>], delta)</div><div class=\"line\"><span class=\"comment\"># update MS[\\Delta \\theta] = MS[\\Delta \\theta] * \\rho + \\Delta \\theta^2 * (1-\\rho)</span></div><div class=\"line\">acc_delta.mul_(rho).addcmul_(<span class=\"number\">1</span> - rho, delta, delta)</div></pre></td></tr></table></figure></p>\n<h3 id=\"Adam\"><a href=\"#Adam\" class=\"headerlink\" title=\"Adam\"></a>Adam</h3><p><a href=\"https://arxiv.org/abs/1412.6980\" target=\"_blank\" rel=\"external\">Adaptive momen Estimation（Adam，自适应矩估计）</a>，是另一种为不同参数自适应设置不同学习率的方法。Adam方法不止存储过往的梯度平方均值（二阶矩）信息，还存储过往的梯度均值信息（一阶矩）。</p>\n<script type=\"math/tex; mode=display\">\\begin{aligned}m_t&=\\beta_1 m_{t-1}+(1-\\beta_1)g_t\\\\v_t&=\\beta_2 v_{t-1}+(1-\\beta_2)g_t^2\\end{aligned}</script><p>作者观察到上述估计是有偏的（biase towards $0$），所以给出如下修正：</p>\n<script type=\"math/tex; mode=display\">\\begin{aligned}\\hat{m} &= \\frac{m}{1-\\beta_1}\\\\ \\hat{v}&=\\frac{v}{1-\\beta_2}\\end{aligned}</script><p>参数的更新如下：</p>\n<script type=\"math/tex; mode=display\">\\theta_{t+1} = \\theta_t - \\frac{\\eta}{\\sqrt{\\hat{v_t} + \\epsilon}}\\hat{m_t}</script><p>作者给出$\\beta_1 = 0.9$，$\\beta_2=0.999$，$\\epsilon=10^{-8}$。</p>\n<p>为了更好地理解PyTorch中的实现方式，需要对上式进行变形：</p>\n<script type=\"math/tex; mode=display\">\\Delta \\theta = \\frac{\\sqrt{1-\\beta_2}}{1-\\beta_1}\\eta \\frac{m_t}{\\sqrt{v_t}}</script><p>代码中令$\\text{step_size} =  \\frac{\\sqrt{1-\\beta_2}}{1-\\beta_1}\\eta$。同时，$\\beta$也要以指数规律衰减，即：$\\beta_t = \\beta_0^t$。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\"># exp_avg is `m`: expected average of g</span></div><div class=\"line\">exp_avg.mul_(beta1).add_(<span class=\"number\">1</span> - beta1, grad)</div><div class=\"line\"><span class=\"comment\"># exp_avg_sq is `v`: expected average of g's square</span></div><div class=\"line\">exp_avg_sq.mul_(beta2).addcmul_(<span class=\"number\">1</span> - beta2, grad, grad)</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># \\sqrt&#123;v_t + \\epsilon&#125;</span></div><div class=\"line\">denom = exp_avg_sq.sqrt().add_(group[<span class=\"string\">'eps'</span>])</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># 1 - \\beta_1^t</span></div><div class=\"line\">bias_correction1 = <span class=\"number\">1</span> - beta1 ** state[<span class=\"string\">'step'</span>]</div><div class=\"line\"><span class=\"comment\"># 1 - \\beta_2^t</span></div><div class=\"line\">bias_correction2 = <span class=\"number\">1</span> - beta2 ** state[<span class=\"string\">'step'</span>]</div><div class=\"line\"><span class=\"comment\"># get step_size</span></div><div class=\"line\">step_size = group[<span class=\"string\">'lr'</span>] * math.sqrt(bias_correction2) / bias_correction1</div><div class=\"line\"><span class=\"comment\"># delta = -step_size * m / sqrt(v)</span></div><div class=\"line\">p.data.addcdiv_(-step_size, exp_avg, denom)</div></pre></td></tr></table></figure>\n<h3 id=\"AdaMax\"><a href=\"#AdaMax\" class=\"headerlink\" title=\"AdaMax\"></a>AdaMax</h3><p>上面Adam中，实际上我们是用梯度$g$的$2$范数（$\\sqrt{\\hat{v_t}}$）去对$g$进行Normalization。那么为什么不用其他形式的范数$p$来试试呢？然而，对于$1$范数和$2$范数，数值是稳定的。对于再大的$p$，数值不稳定。不过，当取无穷范数的时候，又是稳定的了。</p>\n<p>由于无穷范数就是求绝对值最大的分量，所以这种方法叫做<a href=\"https://arxiv.org/abs/1412.6980\" target=\"_blank\" rel=\"external\">AdaMax</a>。其对应的$\\hat{v_t}$为（这里为了避免混淆，使用$u_t$指代）：</p>\n<script type=\"math/tex; mode=display\">u_t = \\beta_2^\\infty u_{t-1} + (1-\\beta_2^\\infty) g_t^\\infty</script><p>我们将$u_t$按照时间展开，可以得到（直接摘自论文的图）。其中最后一步递推式的得来：根据$u_t$把$u_{t-1}$的展开形式也写出来，就不难发现最下面的递推形式。</p>\n<p><img src=\"/img/hinton_06_adamax.png\" alt=\"Adamax中ut的推导\"></p>\n<p>相应的更新权重操作为：</p>\n<script type=\"math/tex; mode=display\">\\theta_{t+1} = \\theta_t -\\frac{\\eta}{u_t}\\hat{m}_t</script><p>在PyTorch中的实现如下：<br><figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\"># Update biased first moment estimate, which is \\hat&#123;m&#125;_t</span></div><div class=\"line\">exp_avg.mul_(beta1).add_(<span class=\"number\">1</span> - beta1, grad)</div><div class=\"line\"><span class=\"comment\"># 下面这种用来逐元素求取 max(A, B) 的方法可以学习一个</span></div><div class=\"line\"><span class=\"comment\"># Update the exponentially weighted infinity norm.</span></div><div class=\"line\">norm_buf = torch.cat([</div><div class=\"line\">    exp_inf.mul_(beta2).unsqueeze(<span class=\"number\">0</span>),</div><div class=\"line\">    grad.abs().add_(eps).unsqueeze_(<span class=\"number\">0</span>)</div><div class=\"line\">], <span class=\"number\">0</span>)</div><div class=\"line\"><span class=\"comment\">## 找到 exp_inf 和 g之间的较大者（只需要在刚刚聚合的这个维度上找即可~）</span></div><div class=\"line\">torch.max(norm_buf, <span class=\"number\">0</span>, keepdim=<span class=\"keyword\">False</span>, out=(exp_inf, exp_inf.new().long()))</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">## beta1 correction</span></div><div class=\"line\">bias_correction = <span class=\"number\">1</span> - beta1 ** state[<span class=\"string\">'step'</span>]</div><div class=\"line\">clr = group[<span class=\"string\">'lr'</span>] / bias_correction</div><div class=\"line\"></div><div class=\"line\">p.data.addcdiv_(-clr, exp_avg, exp_inf)</div></pre></td></tr></table></figure></p>\n","excerpt":"<p>第六周的课程主要讲解了用于神经网络训练的梯度下降方法，首先对比了SGD，full batch GD和mini batch SGD方法，然后给出了几个用于神经网络训练的trick，主要包括输入数据预处理（零均值，单位方差以及PCA解耦），学习率的自适应调节以及网络权重的初始化方法（可以参考各大框架中实现的Xavier初始化方法等）。这篇文章主要记录了后续讲解的几种GD变种方法，如何合理利用梯度信息达到更好的训练效果。由于Hinton这门课确实时间已经很久了，所以文章末尾会结合一篇不错的总结性质的<a href=\"http://sebastianruder.com/optimizing-gradient-descent/index.html\">博客</a>和对应的<a href=\"https://arxiv.org/abs/1609.04747\">论文</a>以及PyTorch中的相关代码，对目前流行的梯度下降方法做个总结。</p>\n<p>下图即来自上面的这篇博客。</p>\n<p><img src=\"/img/contours_evaluation_optimizers.gif\" alt=\"几种优化方法的可视化\"><br>","more":"</p>\n<h2 id=\"Momentum\"><a href=\"#Momentum\" class=\"headerlink\" title=\"Momentum\"></a>Momentum</h2><p>我们可以把训练过程想象成在权重空间的一个质点（小球），移动到全局最优点的过程。不同于GD，使用梯度信息直接更新权重的位置，momentum方法是将梯度作为速度量。这样做的好处是，当梯度的方向一直不变时，速度可以加快；当梯度方向变化剧烈时，由于符号改变，所以速度减慢，起到了GD中自适应调节学习率的过程。</p>\n<p>具体来说，我们利用新得到的梯度信息，采用滑动平均的方法更新速度。式子中的$\\epsilon$为学习率，$\\alpha$为momentum系数。</p>\n<script type=\"math/tex; mode=display\">\\Delta w_t = v_t = \\alpha v_{t-1} - \\epsilon g_t = \\Delta w_t - \\epsilon g_t</script><p>为了说明momentum确实对学习过程有加速作用，假设一个简单的情形，即运动轨迹是一个斜率固定的斜面。那么我们有梯度$g$固定。根据上面的递推公式可以得到通项公式（简单的待定系数法凑出等比数列）：</p>\n<script type=\"math/tex; mode=display\">v_t = \\alpha(v_{t-1} + \\frac{\\epsilon g}{1-\\alpha}) - \\frac{\\epsilon g}{1-\\alpha}</script><p>由于$\\alpha &lt; 0$，所以当$t = \\infty$时，只剩下了后面的常数项，即：</p>\n<script type=\"math/tex; mode=display\">v_\\infty = -\\frac{\\epsilon}{1-\\alpha}g</script><p>也就是说，权重更新的幅度变成了原来的$\\frac{1}{1-\\alpha}$倍。若取$\\alpha=0.99$，则加速$100$倍。</p>\n<p>Hinton给出的建议是由于训练开头梯度值比较大，所以momentum系数一开始不要过大，例如可以取$0.5$。当梯度值较小，训练过程被困在一个峡谷的时候，可以适当提升。</p>\n<p>一种改进方法由Nesterov提出。在上面的方法中，我们首先更新了在该处的累积梯度信息，然后向前移动。而Nesterov方法中，我们首先沿着累计梯度信息向前走，然后根据梯度信息进行更正。</p>\n<p><img src=\"/img/hinton_06_nesterov_momentum.png\" alt=\"Nesterov方法\"></p>\n<h2 id=\"Adaptive-Learning-Rate\"><a href=\"#Adaptive-Learning-Rate\" class=\"headerlink\" title=\"Adaptive Learning Rate\"></a>Adaptive Learning Rate</h2><p>这种方法起源于这样的观察：在网络中，不同layer之间的权重更新需要不同的学习率。因为浅层和深层的layer梯度幅值很可能不同。所以，对不同的权重乘上不同的因子是个更加合理的选择。</p>\n<p>例如，我们可以根据梯度是否发生符号变化按照下面的方式调节某个权重$w_{ij}$的增益。注意$0.95$和$0.05$的和是$1$。这样可以使得平衡点在$1$附近。<br><img src=\"/img/hinton_06_learningrate.png\" alt=\"Different learning rate gain\"></p>\n<p>下面是使用这种方法的几个trick，包括限幅，较大的batch size以及和momentum的结合。</p>\n<p><img src=\"/img/hinton_06_tricks_for_adaptive_lr.png\" alt=\"Tricks for adaptive lr\"></p>\n<h2 id=\"RMSProp\"><a href=\"#RMSProp\" class=\"headerlink\" title=\"RMSProp\"></a>RMSProp</h2><p>rprop利用梯度的符号，如果符号保持不变，则相应增大step size；否则减小。但是只能用于full batch GD。RMSProp就是一种可以结合mini batch SGD和rprop的一种方法。</p>\n<p>我们使用滑动平均方法更新梯度的mean square（即RMS中的MS得来）。</p>\n<script type=\"math/tex; mode=display\">\\text{MeanSquare}(w, t) = 0.9 \\text{MeanSquare}(w, t-1) + 0.1g_t^2</script><p>然后，将梯度除以上面的得到的Mean Square值。</p>\n<p>RMSProp还有一些变种，列举如下：<br><img src=\"/img/hinton_06_rmsprop_improvement.png\" alt=\"Otehr RMSProp\"></p>\n<h2 id=\"课程总结\"><a href=\"#课程总结\" class=\"headerlink\" title=\"课程总结\"></a>课程总结</h2><ul>\n<li>对于小数据集，使用full batch GD（LBFGS或adaptive learning rate如rprop）。</li>\n<li>对于较大数据集，使用mini batch SGD。并可以考虑加上momentmum和RMSProp。</li>\n</ul>\n<p>如何选择学习率是一个较为依赖经验的任务（网络结构不同，任务不同）。<br><img src=\"/img/hinton_06_summary.png\" alt=\"总结\"></p>\n<h2 id=\"“Modern”-SGD\"><a href=\"#“Modern”-SGD\" class=\"headerlink\" title=\"“Modern” SGD\"></a>“Modern” SGD</h2><p>从本部分开始，我将转向总结摘要中提到的那篇博客中的主要内容。首先，给出当前基于梯度的优化方法的一些问题。可以看到，之后人们提出的改进方法就是想办法解决对应问题的。由于与Hinton课程相比，这些方法提出时间（也许称之为流行时间更合适？做数学的那帮人可能很早就知道这些优化方法了吧？）较短，所以这里仿照Modern C++之称呼，就把它们统一叫做Modern SGD吧。。。</p>\n<ul>\n<li>学习率通常很难确定。学习率太大？容易扯到蛋（loss直接爆炸）；学习率太小，训练到天荒地老。。。</li>\n<li>学习率如何在训练中调整。目前常用的方法是退火，要么是固定若干次迭代之后把学习率调小，要么是观察loss到某个阈值后把学习率调小。总之，都是在训练开始前，人工预先定义好的。而这没有考虑到数据集自身的特点。</li>\n<li>学习率对每个网络参数都一样。这点在上面课程中Hinton已经提到，引出了自适应学习率的方法。</li>\n<li>高度非凸函数的优化难题。以前人们多是认为网络很容易收敛到局部极小值。后来有人提出，网络之所以难训练，更多是由于遇到了鞍点。也就是某个方向上它是极小值；而另一个方向却是极大值（高数中介绍过的，马鞍面）</li>\n</ul>\n<p><img src=\"/img/hinton_06_maanmian.jpg\" alt=\"马鞍面\"></p>\n<h3 id=\"Adagrad\"><a href=\"#Adagrad\" class=\"headerlink\" title=\"Adagrad\"></a>Adagrad</h3><p><a href=\"http://jmlr.org/papers/v12/duchi11a.html\">Adagrad</a>对不同的参数采用不同的学习率，也是其Ada（Adaptive）的名字得来。我们记时间步$t$时标号为$i$的参数对应的梯度为$g_{i}$，即：</p>\n<script type=\"math/tex; mode=display\">g_{i} = \\bigtriangledown_{\\theta_i} J(\\theta)</script><p>Adagrad使用一个系数来为不同的参数修正学习率，如下：</p>\n<script type=\"math/tex; mode=display\">\\hat{g_i} = \\frac{1}{\\sqrt{G_i+\\epsilon}}g_i</script><p>其中，$G_i$是截止到当前时间步$t$时，参数$\\theta_i$对应梯度$g_i$的平方和。</p>\n<p>我们可以把上面的式子写成矩阵形式。其中，$\\odot$表示逐元素的矩阵相乘（element-wise product）。同时，$G_t = g_t \\odot g_t$。</p>\n<script type=\"math/tex; mode=display\">\\theta_{t+1} = \\theta_t - \\frac{\\eta}{\\sqrt{G_t+\\epsilon}}\\odot g_t</script><p>我们再来看PyTorch中的相关实现：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\"># for each gradient of parameters:</span></div><div class=\"line\"><span class=\"comment\"># addcmul(t, alpha, t1, t2): t = t1*t2*alpha + t</span></div><div class=\"line\"><span class=\"comment\"># let epsilon = 1E-10</span></div><div class=\"line\">state[<span class=\"string\">'sum'</span>].addcmul_(<span class=\"number\">1</span>, grad, grad)   <span class=\"comment\"># 计算 G</span></div><div class=\"line\">std = state[<span class=\"string\">'sum'</span>].sqrt().add_(<span class=\"number\">1e-10</span>)  <span class=\"comment\"># 计算 \\sqrt(G)</span></div><div class=\"line\">p.data.addcdiv_(-clr, grad, std)       <span class=\"comment\"># 更新</span></div></pre></td></tr></table></figure>\n<p>由于Adagrad对不同的梯度给了不同的学习率修正值，所以使用这种方法时，我们可以不用操心学习率，只是给定一个初始值（如$0.01$）就够了。尤其是对稀疏的数据，Adagrad方法能够自适应调节其梯度更新信息，给那些不常出现（非零）的梯度对应更大的学习率。PyTorch中还为稀疏数据特别优化了更新算法。</p>\n<p>Adagrad的缺点在于由于$G_t$矩阵是平方和，所以分母会越来越大，造成训练后期学习率会变得很小。下面的Adadelta方法针对这个问题进行了改进。</p>\n<h3 id=\"Adadelta\"><a href=\"#Adadelta\" class=\"headerlink\" title=\"Adadelta\"></a>Adadelta</h3><p><a href=\"https://arxiv.org/abs/1212.5701\">Adadelta</a>给出的改进方法是不再记录所有的历史时刻的$g$的平方和，而是最近一个有限的观察窗口$w$的累积梯度平方和。在实际使用时，这种方法使用了一个参数$\\gamma$（如$0.9$）作为遗忘因子，对$E[g_t^2]$进行统计。</p>\n<script type=\"math/tex; mode=display\">E[g_t^2] = \\gamma E[g_{t-1}^2] + (1-\\gamma)g_t^2</script><p>由于$\\sqrt{E[g_t^2]}$就是$g$的均方根RMS，所以，修正后的梯度如下。注意到，这正是Hinton在课上所讲到的RMSprop的优化方法。</p>\n<script type=\"math/tex; mode=display\">\\hat{g}_t = \\frac{1}{\\text{RMS}[g]}g_t</script><p>作者还观察到，这样更新的话，其实$\\theta$和$\\Delta \\theta$的单位是不一样的（此时$\\Delta \\theta$是无量纲数）。所以，作者提出再乘上一个$\\text{RMS}[\\Delta \\theta]$来平衡（同时去掉了学习率$\\eta$），所以，最终的参数更新如下：</p>\n<script type=\"math/tex; mode=display\">\\theta_{t+1} = \\theta_t - \\frac{\\text{RMS}[\\Delta \\theta]}{\\text{RMS}[g]}g_t</script><p>这种方法甚至不再需要学习率。下面是PyTorch中的实现，其中仍然保有学习率<code>lr</code>这一参数设定，默认值为$1.0$。代码注释中，我使用<code>MS</code>来指代$E[x^2]$。即，$\\text{RMS}[x] = \\sqrt{\\text{MS}[x]+\\epsilon}$。<br><figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\"># update: MS[g] = MS[g]*\\rho + g*g*(1-\\rho)</span></div><div class=\"line\">square_avg.mul_(rho).addcmul_(<span class=\"number\">1</span> - rho, grad, grad)</div><div class=\"line\"><span class=\"comment\"># current RMS[g] = sqrt(MS[g] + \\epsilon)</span></div><div class=\"line\">std = square_avg.add(eps).sqrt_()</div><div class=\"line\"><span class=\"comment\"># \\Delta \\theta = RMS[\\Delta \\theta] / RMS[g]) * g</span></div><div class=\"line\">delta = acc_delta.add(eps).sqrt_().div_(std).mul_(grad)</div><div class=\"line\"><span class=\"comment\"># update parameter: \\theta -= lr * \\Delta \\theta</span></div><div class=\"line\">p.data.add_(-group[<span class=\"string\">'lr'</span>], delta)</div><div class=\"line\"><span class=\"comment\"># update MS[\\Delta \\theta] = MS[\\Delta \\theta] * \\rho + \\Delta \\theta^2 * (1-\\rho)</span></div><div class=\"line\">acc_delta.mul_(rho).addcmul_(<span class=\"number\">1</span> - rho, delta, delta)</div></pre></td></tr></table></figure></p>\n<h3 id=\"Adam\"><a href=\"#Adam\" class=\"headerlink\" title=\"Adam\"></a>Adam</h3><p><a href=\"https://arxiv.org/abs/1412.6980\">Adaptive momen Estimation（Adam，自适应矩估计）</a>，是另一种为不同参数自适应设置不同学习率的方法。Adam方法不止存储过往的梯度平方均值（二阶矩）信息，还存储过往的梯度均值信息（一阶矩）。</p>\n<script type=\"math/tex; mode=display\">\\begin{aligned}m_t&=\\beta_1 m_{t-1}+(1-\\beta_1)g_t\\\\v_t&=\\beta_2 v_{t-1}+(1-\\beta_2)g_t^2\\end{aligned}</script><p>作者观察到上述估计是有偏的（biase towards $0$），所以给出如下修正：</p>\n<script type=\"math/tex; mode=display\">\\begin{aligned}\\hat{m} &= \\frac{m}{1-\\beta_1}\\\\ \\hat{v}&=\\frac{v}{1-\\beta_2}\\end{aligned}</script><p>参数的更新如下：</p>\n<script type=\"math/tex; mode=display\">\\theta_{t+1} = \\theta_t - \\frac{\\eta}{\\sqrt{\\hat{v_t} + \\epsilon}}\\hat{m_t}</script><p>作者给出$\\beta_1 = 0.9$，$\\beta_2=0.999$，$\\epsilon=10^{-8}$。</p>\n<p>为了更好地理解PyTorch中的实现方式，需要对上式进行变形：</p>\n<script type=\"math/tex; mode=display\">\\Delta \\theta = \\frac{\\sqrt{1-\\beta_2}}{1-\\beta_1}\\eta \\frac{m_t}{\\sqrt{v_t}}</script><p>代码中令$\\text{step_size} =  \\frac{\\sqrt{1-\\beta_2}}{1-\\beta_1}\\eta$。同时，$\\beta$也要以指数规律衰减，即：$\\beta_t = \\beta_0^t$。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\"># exp_avg is `m`: expected average of g</span></div><div class=\"line\">exp_avg.mul_(beta1).add_(<span class=\"number\">1</span> - beta1, grad)</div><div class=\"line\"><span class=\"comment\"># exp_avg_sq is `v`: expected average of g's square</span></div><div class=\"line\">exp_avg_sq.mul_(beta2).addcmul_(<span class=\"number\">1</span> - beta2, grad, grad)</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># \\sqrt&#123;v_t + \\epsilon&#125;</span></div><div class=\"line\">denom = exp_avg_sq.sqrt().add_(group[<span class=\"string\">'eps'</span>])</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># 1 - \\beta_1^t</span></div><div class=\"line\">bias_correction1 = <span class=\"number\">1</span> - beta1 ** state[<span class=\"string\">'step'</span>]</div><div class=\"line\"><span class=\"comment\"># 1 - \\beta_2^t</span></div><div class=\"line\">bias_correction2 = <span class=\"number\">1</span> - beta2 ** state[<span class=\"string\">'step'</span>]</div><div class=\"line\"><span class=\"comment\"># get step_size</span></div><div class=\"line\">step_size = group[<span class=\"string\">'lr'</span>] * math.sqrt(bias_correction2) / bias_correction1</div><div class=\"line\"><span class=\"comment\"># delta = -step_size * m / sqrt(v)</span></div><div class=\"line\">p.data.addcdiv_(-step_size, exp_avg, denom)</div></pre></td></tr></table></figure>\n<h3 id=\"AdaMax\"><a href=\"#AdaMax\" class=\"headerlink\" title=\"AdaMax\"></a>AdaMax</h3><p>上面Adam中，实际上我们是用梯度$g$的$2$范数（$\\sqrt{\\hat{v_t}}$）去对$g$进行Normalization。那么为什么不用其他形式的范数$p$来试试呢？然而，对于$1$范数和$2$范数，数值是稳定的。对于再大的$p$，数值不稳定。不过，当取无穷范数的时候，又是稳定的了。</p>\n<p>由于无穷范数就是求绝对值最大的分量，所以这种方法叫做<a href=\"https://arxiv.org/abs/1412.6980\">AdaMax</a>。其对应的$\\hat{v_t}$为（这里为了避免混淆，使用$u_t$指代）：</p>\n<script type=\"math/tex; mode=display\">u_t = \\beta_2^\\infty u_{t-1} + (1-\\beta_2^\\infty) g_t^\\infty</script><p>我们将$u_t$按照时间展开，可以得到（直接摘自论文的图）。其中最后一步递推式的得来：根据$u_t$把$u_{t-1}$的展开形式也写出来，就不难发现最下面的递推形式。</p>\n<p><img src=\"/img/hinton_06_adamax.png\" alt=\"Adamax中ut的推导\"></p>\n<p>相应的更新权重操作为：</p>\n<script type=\"math/tex; mode=display\">\\theta_{t+1} = \\theta_t -\\frac{\\eta}{u_t}\\hat{m}_t</script><p>在PyTorch中的实现如下：<br><figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\"># Update biased first moment estimate, which is \\hat&#123;m&#125;_t</span></div><div class=\"line\">exp_avg.mul_(beta1).add_(<span class=\"number\">1</span> - beta1, grad)</div><div class=\"line\"><span class=\"comment\"># 下面这种用来逐元素求取 max(A, B) 的方法可以学习一个</span></div><div class=\"line\"><span class=\"comment\"># Update the exponentially weighted infinity norm.</span></div><div class=\"line\">norm_buf = torch.cat([</div><div class=\"line\">    exp_inf.mul_(beta2).unsqueeze(<span class=\"number\">0</span>),</div><div class=\"line\">    grad.abs().add_(eps).unsqueeze_(<span class=\"number\">0</span>)</div><div class=\"line\">], <span class=\"number\">0</span>)</div><div class=\"line\"><span class=\"comment\">## 找到 exp_inf 和 g之间的较大者（只需要在刚刚聚合的这个维度上找即可~）</span></div><div class=\"line\">torch.max(norm_buf, <span class=\"number\">0</span>, keepdim=<span class=\"keyword\">False</span>, out=(exp_inf, exp_inf.new().long()))</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">## beta1 correction</span></div><div class=\"line\">bias_correction = <span class=\"number\">1</span> - beta1 ** state[<span class=\"string\">'step'</span>]</div><div class=\"line\">clr = group[<span class=\"string\">'lr'</span>] / bias_correction</div><div class=\"line\"></div><div class=\"line\">p.data.addcdiv_(-clr, exp_avg, exp_inf)</div></pre></td></tr></table></figure></p>"},{"title":"学一点PyQT","date":"2017-08-29T06:07:55.000Z","_content":"Qt是一个流行的GUI框架，支持C++/Python。这篇文章是我在这两天通过PyQT制作一个串口通信并画图的小程序的时候，阅读[PyQT5的一篇教程](http://zetcode.com/gui/pyqt5/introduction/)时候的记录。\n<!-- more -->\n\n## 主要模块\nPyQt5中的主要三个模块如下：\n- `QtCore`: 和GUI无关的核心功能：文件，时间，多线程等\n- `QtGui`：和GUI相关的的东西：事件处理，2D图形，字体和文本等\n- `QtWidget`：GUI中的相关组件，例如按钮，窗口等。\n\n其他模块还有`QtBluetooth`，`QtNetwork`等，都是比较专用的模块，用到再说。\n\n## HelloWorld\n这里首先给出一段简单的程序，可以在桌面上显示一个窗口。\n``` py\nimport sys\nfrom PyQt5.QtWidgets import QApplication, QWidget\n\nif __name__ == '__main__':\n    app = QApplication(sys.argv)\n    w = QWidget()\n    w.resize(250, 150)\n    w.move(300, 300)\n    w.setWindowTitle('Simple')\n    w.show()\n\n    sys.exit(app.exec_())\n```\n下面介绍上面代码的含义：\n```py\napp = QApplication(sys.argv)\n```\n每个Qt5应用必须首先创建一个application，后面会用到。\n``` py\nw = QWidget()\nw.resize(250, 150)\nw.move(300, 300)\nw.setWindowTitle('Simple')\nw.show()\n```\n`QtWidget`是所有组件的父类，我们创建了一个`Widget`。没有任何parent widget的Widget会作为窗口出现。接下来，调用其成员函数实现调整大小等功能。最后使用`show()`将其显示出来。\n\n``` py\nsys.exit(app.exec_())\n```\n进入application的主循环，等待事件的触发。当退出程序（也许是通过Ctrl+C实现的）或者关闭窗口（点击关闭）后，主循环退出。\n\n## 添加一个按钮\n下面，我们为窗口添加按钮，并为其添加事件响应动作。\n\n参考文档可知，按钮`QPushButton`存在这样的构造函数：\n```py\n__init__ (self, QWidget parent = None)\n```\n下面的代码在初始化`QPushButton`实例`btn`时，将`self`作为参数传入，指定了其parent。另外，在指定按钮大小的时候，使用了`sizeHint()`方法自适应调节其大小。\n\n同时，为按钮关联了点击动作。Qt中的事件响应机制通过信号和槽实现。点击事件一旦发生，信号`clicked`会被释放。然后槽相对的处理函数被调用。所谓的槽可以使PyQt提供的slot，或者是任何Python的可调用对象（函数或者实现了`__call__()`方法的对象）。\n\n我们调用了现成的处理函数，来达到关闭窗口的目的。使用`instance()`可以得到当前application实例，调用其`quit()`方法即是退出当前应用，自然窗口就被关闭了。\n```py\nimport sys\nfrom PyQt5.QtWidgets import QApplication, QWidget, QPushButton, QToolTip\nfrom PyQt5.QtCore import QCoreApplication\n\nclass MyWindow(QWidget):\n    def __init__(self):\n        super(MyWindow, self).__init__()\n        self._init_ui()\n\n    def _init_ui(self):\n        btn = QPushButton('quit', self)\n        btn.clicked.connect(QCoreApplication.instance().quit)\n        btn.setToolTip('This is a <b>QPushButton</b> widget')\n        btn.move(50, 50)\n        btn.resize(btn.sizeHint())\n\n        self.setGeometry(300, 300, 300, 200)\n        self.setWindowTitle('Window with Button')\n        self.show()\n\nif __name__ == '__main__':\n    app = QApplication(sys.argv)\n    window = MyWindow()\n    sys.exit(app.exec_())\n```\n\n## 使用Event处理事件\n除了上述的信号和槽的处理方式，也可以使用Event相关的类进行处理。下面的代码在关闭窗口时弹出对话框确认是否关闭。根据用户做出的选择，调用`event.accept()`或`ignore()`完成对事件的处理。\n\n``` py\nimport sys\nfrom PyQt5.QtWidgets import QWidget, QMessageBox, QApplication\n\nclass MyWindow(QWidget):\n    def __init__(self):\n        super(MyWindow, self).__init__()\n        self._init_ui()\n    def _init_ui(self):\n        self.setGeometry(300, 300, 300, 200)\n        self.show()\n    def closeEvent(self, ev):\n        reply = QMessageBox.question(self, 'Message', 'Are you sure?',\n                    QMessageBox.Yes | QMessageBox.No, QMessageBox.No)\n        if reply == QMessageBox.Yes:\n            ev.accept()\n        else:\n            ev.ignore()\n\nif __name__ == '__main__':\n    app = QApplication(sys.argv)\n    win = MyWindow()\n    sys.exit(app.exec_())\n```\n\n## 使用Layout组织Widget\n组织Widget的方式可以通过绝对位置调整，但是更推荐使用`Layout`组织。\n\n绝对位置是通过指定像素多少来确定widget的大小和位置。这样的话，有以下几个缺点：\n- 不同平台可能显示效果不统一；\n- 当parent resize的时候，widget大小和位置并不会自动调整\n- 编码太麻烦，牵一发而动全身\n\n下面介绍几种常见的`Layout`类。\n\n### Box Layout\n有`QVBoxLayout`和`QHBoxLayout`，用来将widget水平或者竖直排列起来。下面的代码通过这两个layout将按钮放置在窗口的右下角。关键的地方在于使用`addSkretch()`方法将一个`QSpacerItem`实例对象插入到了layout中，占据了相应位置。\n\n``` py\nimport sys\nfrom PyQt5.QtWidgets import (QWidget, QPushButton,\n    QHBoxLayout, QVBoxLayout, QApplication)\n\nclass MyWindow(QWidget):\n    def __init__(self):\n        super(MyWindow, self).__init__()\n        self._init_ui()\n\n    def _init_ui(self):\n        okButton = QPushButton(\"OK\")\n        cancelButton = QPushButton(\"Cancel\")\n\n        hbox = QHBoxLayout()\n        hbox.addStretch(1)\n        hbox.addWidget(okButton)\n        hbox.addWidget(cancelButton)\n\n        vbox = QVBoxLayout()\n        vbox.addStretch(1)\n        vbox.addLayout(hbox)\n\n        self.setLayout(vbox)    \n\n        self.setGeometry(300, 300, 300, 150)\n        self.setWindowTitle('Buttons')    \n        self.show()\n\nif __name__ == '__main__':\n    app = QApplication(sys.argv)\n    win = MyWindow()\n    sys.exit(app.exec_())\n```\n\n### Grid Layout\n`QGridLayout`将空间划分为行列的grid。在向其中添加item的时候，要指定位置。如下，将5行4列的grid设置为计算器的面板模式。\n``` py\nimport sys\nfrom PyQt5.QtWidgets import (QWidget, QGridLayout,\n    QPushButton, QApplication)\n\nclass MyWindow(QWidget):\n    def __init__(self):\n        super(MyWindow, self).__init__()\n        self._init_ui()\n\n    def _init_ui(self):\n        grid = QGridLayout()\n        self.setLayout(grid)\n        names = ['Cls', 'Bck', '', 'Close',\n                 '7', '8', '9', '/',\n                '4', '5', '6', '*',\n                 '1', '2', '3', '-',\n                '0', '.', '=', '+']\n        positions = [(i,j) for i in range(5) for j in range(4)]\n        for position, name in zip(positions, names):\n            if name == '':\n                continue\n            button = QPushButton(name)\n            grid.addWidget(button, *position)\n\n        self.move(300, 150)\n        self.setWindowTitle('Calculator')\n        self.show()\n\nif __name__ == '__main__':\n\n    app = QApplication(sys.argv)\n    win = MyWindow()\n    sys.exit(app.exec_())\n```\n\n另外，我们还可以通过`setSpacing()`方法设置每个单元格之间的间隔。如果某个widget需要占据多个单元格，可以在`addWidget()`方法中指定要扩展的行列数。\n\n## 事件驱动\nPyQt提供了两种事件驱动的处理方式：\n- 使用`event`句柄。事件可能是由于UI交互或者定时器等引起，由接收对象进行处理。\n- 信号和槽。某个widge交互时，释放相应信号，被槽对应的函数捕获进行处理。\n\n信号和槽可以见上面使用按钮关闭窗口的例子，关键在于调用信号的`connect()`函数将其绑定到某个槽上。Python中的可调用对象都可以作为槽。\n\n而使用event句柄处理时，需要重写override原来的处理函数，见上面使用其在关闭窗口时进行弹窗确认的例子。\n","source":"_posts/learn-pyqt.md","raw":"---\ntitle: 学一点PyQT\ndate: 2017-08-29 14:07:55\ntags:\n    - python\n    - qt\n---\nQt是一个流行的GUI框架，支持C++/Python。这篇文章是我在这两天通过PyQT制作一个串口通信并画图的小程序的时候，阅读[PyQT5的一篇教程](http://zetcode.com/gui/pyqt5/introduction/)时候的记录。\n<!-- more -->\n\n## 主要模块\nPyQt5中的主要三个模块如下：\n- `QtCore`: 和GUI无关的核心功能：文件，时间，多线程等\n- `QtGui`：和GUI相关的的东西：事件处理，2D图形，字体和文本等\n- `QtWidget`：GUI中的相关组件，例如按钮，窗口等。\n\n其他模块还有`QtBluetooth`，`QtNetwork`等，都是比较专用的模块，用到再说。\n\n## HelloWorld\n这里首先给出一段简单的程序，可以在桌面上显示一个窗口。\n``` py\nimport sys\nfrom PyQt5.QtWidgets import QApplication, QWidget\n\nif __name__ == '__main__':\n    app = QApplication(sys.argv)\n    w = QWidget()\n    w.resize(250, 150)\n    w.move(300, 300)\n    w.setWindowTitle('Simple')\n    w.show()\n\n    sys.exit(app.exec_())\n```\n下面介绍上面代码的含义：\n```py\napp = QApplication(sys.argv)\n```\n每个Qt5应用必须首先创建一个application，后面会用到。\n``` py\nw = QWidget()\nw.resize(250, 150)\nw.move(300, 300)\nw.setWindowTitle('Simple')\nw.show()\n```\n`QtWidget`是所有组件的父类，我们创建了一个`Widget`。没有任何parent widget的Widget会作为窗口出现。接下来，调用其成员函数实现调整大小等功能。最后使用`show()`将其显示出来。\n\n``` py\nsys.exit(app.exec_())\n```\n进入application的主循环，等待事件的触发。当退出程序（也许是通过Ctrl+C实现的）或者关闭窗口（点击关闭）后，主循环退出。\n\n## 添加一个按钮\n下面，我们为窗口添加按钮，并为其添加事件响应动作。\n\n参考文档可知，按钮`QPushButton`存在这样的构造函数：\n```py\n__init__ (self, QWidget parent = None)\n```\n下面的代码在初始化`QPushButton`实例`btn`时，将`self`作为参数传入，指定了其parent。另外，在指定按钮大小的时候，使用了`sizeHint()`方法自适应调节其大小。\n\n同时，为按钮关联了点击动作。Qt中的事件响应机制通过信号和槽实现。点击事件一旦发生，信号`clicked`会被释放。然后槽相对的处理函数被调用。所谓的槽可以使PyQt提供的slot，或者是任何Python的可调用对象（函数或者实现了`__call__()`方法的对象）。\n\n我们调用了现成的处理函数，来达到关闭窗口的目的。使用`instance()`可以得到当前application实例，调用其`quit()`方法即是退出当前应用，自然窗口就被关闭了。\n```py\nimport sys\nfrom PyQt5.QtWidgets import QApplication, QWidget, QPushButton, QToolTip\nfrom PyQt5.QtCore import QCoreApplication\n\nclass MyWindow(QWidget):\n    def __init__(self):\n        super(MyWindow, self).__init__()\n        self._init_ui()\n\n    def _init_ui(self):\n        btn = QPushButton('quit', self)\n        btn.clicked.connect(QCoreApplication.instance().quit)\n        btn.setToolTip('This is a <b>QPushButton</b> widget')\n        btn.move(50, 50)\n        btn.resize(btn.sizeHint())\n\n        self.setGeometry(300, 300, 300, 200)\n        self.setWindowTitle('Window with Button')\n        self.show()\n\nif __name__ == '__main__':\n    app = QApplication(sys.argv)\n    window = MyWindow()\n    sys.exit(app.exec_())\n```\n\n## 使用Event处理事件\n除了上述的信号和槽的处理方式，也可以使用Event相关的类进行处理。下面的代码在关闭窗口时弹出对话框确认是否关闭。根据用户做出的选择，调用`event.accept()`或`ignore()`完成对事件的处理。\n\n``` py\nimport sys\nfrom PyQt5.QtWidgets import QWidget, QMessageBox, QApplication\n\nclass MyWindow(QWidget):\n    def __init__(self):\n        super(MyWindow, self).__init__()\n        self._init_ui()\n    def _init_ui(self):\n        self.setGeometry(300, 300, 300, 200)\n        self.show()\n    def closeEvent(self, ev):\n        reply = QMessageBox.question(self, 'Message', 'Are you sure?',\n                    QMessageBox.Yes | QMessageBox.No, QMessageBox.No)\n        if reply == QMessageBox.Yes:\n            ev.accept()\n        else:\n            ev.ignore()\n\nif __name__ == '__main__':\n    app = QApplication(sys.argv)\n    win = MyWindow()\n    sys.exit(app.exec_())\n```\n\n## 使用Layout组织Widget\n组织Widget的方式可以通过绝对位置调整，但是更推荐使用`Layout`组织。\n\n绝对位置是通过指定像素多少来确定widget的大小和位置。这样的话，有以下几个缺点：\n- 不同平台可能显示效果不统一；\n- 当parent resize的时候，widget大小和位置并不会自动调整\n- 编码太麻烦，牵一发而动全身\n\n下面介绍几种常见的`Layout`类。\n\n### Box Layout\n有`QVBoxLayout`和`QHBoxLayout`，用来将widget水平或者竖直排列起来。下面的代码通过这两个layout将按钮放置在窗口的右下角。关键的地方在于使用`addSkretch()`方法将一个`QSpacerItem`实例对象插入到了layout中，占据了相应位置。\n\n``` py\nimport sys\nfrom PyQt5.QtWidgets import (QWidget, QPushButton,\n    QHBoxLayout, QVBoxLayout, QApplication)\n\nclass MyWindow(QWidget):\n    def __init__(self):\n        super(MyWindow, self).__init__()\n        self._init_ui()\n\n    def _init_ui(self):\n        okButton = QPushButton(\"OK\")\n        cancelButton = QPushButton(\"Cancel\")\n\n        hbox = QHBoxLayout()\n        hbox.addStretch(1)\n        hbox.addWidget(okButton)\n        hbox.addWidget(cancelButton)\n\n        vbox = QVBoxLayout()\n        vbox.addStretch(1)\n        vbox.addLayout(hbox)\n\n        self.setLayout(vbox)    \n\n        self.setGeometry(300, 300, 300, 150)\n        self.setWindowTitle('Buttons')    \n        self.show()\n\nif __name__ == '__main__':\n    app = QApplication(sys.argv)\n    win = MyWindow()\n    sys.exit(app.exec_())\n```\n\n### Grid Layout\n`QGridLayout`将空间划分为行列的grid。在向其中添加item的时候，要指定位置。如下，将5行4列的grid设置为计算器的面板模式。\n``` py\nimport sys\nfrom PyQt5.QtWidgets import (QWidget, QGridLayout,\n    QPushButton, QApplication)\n\nclass MyWindow(QWidget):\n    def __init__(self):\n        super(MyWindow, self).__init__()\n        self._init_ui()\n\n    def _init_ui(self):\n        grid = QGridLayout()\n        self.setLayout(grid)\n        names = ['Cls', 'Bck', '', 'Close',\n                 '7', '8', '9', '/',\n                '4', '5', '6', '*',\n                 '1', '2', '3', '-',\n                '0', '.', '=', '+']\n        positions = [(i,j) for i in range(5) for j in range(4)]\n        for position, name in zip(positions, names):\n            if name == '':\n                continue\n            button = QPushButton(name)\n            grid.addWidget(button, *position)\n\n        self.move(300, 150)\n        self.setWindowTitle('Calculator')\n        self.show()\n\nif __name__ == '__main__':\n\n    app = QApplication(sys.argv)\n    win = MyWindow()\n    sys.exit(app.exec_())\n```\n\n另外，我们还可以通过`setSpacing()`方法设置每个单元格之间的间隔。如果某个widget需要占据多个单元格，可以在`addWidget()`方法中指定要扩展的行列数。\n\n## 事件驱动\nPyQt提供了两种事件驱动的处理方式：\n- 使用`event`句柄。事件可能是由于UI交互或者定时器等引起，由接收对象进行处理。\n- 信号和槽。某个widge交互时，释放相应信号，被槽对应的函数捕获进行处理。\n\n信号和槽可以见上面使用按钮关闭窗口的例子，关键在于调用信号的`connect()`函数将其绑定到某个槽上。Python中的可调用对象都可以作为槽。\n\n而使用event句柄处理时，需要重写override原来的处理函数，见上面使用其在关闭窗口时进行弹窗确认的例子。\n","slug":"learn-pyqt","published":1,"updated":"2018-01-12T06:22:20.473Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjcu6vctr001pqu46fs6cbv33","content":"<p>Qt是一个流行的GUI框架，支持C++/Python。这篇文章是我在这两天通过PyQT制作一个串口通信并画图的小程序的时候，阅读<a href=\"http://zetcode.com/gui/pyqt5/introduction/\" target=\"_blank\" rel=\"external\">PyQT5的一篇教程</a>时候的记录。<br><a id=\"more\"></a></p>\n<h2 id=\"主要模块\"><a href=\"#主要模块\" class=\"headerlink\" title=\"主要模块\"></a>主要模块</h2><p>PyQt5中的主要三个模块如下：</p>\n<ul>\n<li><code>QtCore</code>: 和GUI无关的核心功能：文件，时间，多线程等</li>\n<li><code>QtGui</code>：和GUI相关的的东西：事件处理，2D图形，字体和文本等</li>\n<li><code>QtWidget</code>：GUI中的相关组件，例如按钮，窗口等。</li>\n</ul>\n<p>其他模块还有<code>QtBluetooth</code>，<code>QtNetwork</code>等，都是比较专用的模块，用到再说。</p>\n<h2 id=\"HelloWorld\"><a href=\"#HelloWorld\" class=\"headerlink\" title=\"HelloWorld\"></a>HelloWorld</h2><p>这里首先给出一段简单的程序，可以在桌面上显示一个窗口。<br><figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">import</span> sys</div><div class=\"line\"><span class=\"keyword\">from</span> PyQt5.QtWidgets <span class=\"keyword\">import</span> QApplication, QWidget</div><div class=\"line\"></div><div class=\"line\"><span class=\"keyword\">if</span> __name__ == <span class=\"string\">'__main__'</span>:</div><div class=\"line\">    app = QApplication(sys.argv)</div><div class=\"line\">    w = QWidget()</div><div class=\"line\">    w.resize(<span class=\"number\">250</span>, <span class=\"number\">150</span>)</div><div class=\"line\">    w.move(<span class=\"number\">300</span>, <span class=\"number\">300</span>)</div><div class=\"line\">    w.setWindowTitle(<span class=\"string\">'Simple'</span>)</div><div class=\"line\">    w.show()</div><div class=\"line\"></div><div class=\"line\">    sys.exit(app.exec_())</div></pre></td></tr></table></figure></p>\n<p>下面介绍上面代码的含义：<br><figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">app = QApplication(sys.argv)</div></pre></td></tr></table></figure></p>\n<p>每个Qt5应用必须首先创建一个application，后面会用到。<br><figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\">w = QWidget()</div><div class=\"line\">w.resize(<span class=\"number\">250</span>, <span class=\"number\">150</span>)</div><div class=\"line\">w.move(<span class=\"number\">300</span>, <span class=\"number\">300</span>)</div><div class=\"line\">w.setWindowTitle(<span class=\"string\">'Simple'</span>)</div><div class=\"line\">w.show()</div></pre></td></tr></table></figure></p>\n<p><code>QtWidget</code>是所有组件的父类，我们创建了一个<code>Widget</code>。没有任何parent widget的Widget会作为窗口出现。接下来，调用其成员函数实现调整大小等功能。最后使用<code>show()</code>将其显示出来。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">sys.exit(app.exec_())</div></pre></td></tr></table></figure>\n<p>进入application的主循环，等待事件的触发。当退出程序（也许是通过Ctrl+C实现的）或者关闭窗口（点击关闭）后，主循环退出。</p>\n<h2 id=\"添加一个按钮\"><a href=\"#添加一个按钮\" class=\"headerlink\" title=\"添加一个按钮\"></a>添加一个按钮</h2><p>下面，我们为窗口添加按钮，并为其添加事件响应动作。</p>\n<p>参考文档可知，按钮<code>QPushButton</code>存在这样的构造函数：<br><figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">__init__ (self, QWidget parent = <span class=\"keyword\">None</span>)</div></pre></td></tr></table></figure></p>\n<p>下面的代码在初始化<code>QPushButton</code>实例<code>btn</code>时，将<code>self</code>作为参数传入，指定了其parent。另外，在指定按钮大小的时候，使用了<code>sizeHint()</code>方法自适应调节其大小。</p>\n<p>同时，为按钮关联了点击动作。Qt中的事件响应机制通过信号和槽实现。点击事件一旦发生，信号<code>clicked</code>会被释放。然后槽相对的处理函数被调用。所谓的槽可以使PyQt提供的slot，或者是任何Python的可调用对象（函数或者实现了<code>__call__()</code>方法的对象）。</p>\n<p>我们调用了现成的处理函数，来达到关闭窗口的目的。使用<code>instance()</code>可以得到当前application实例，调用其<code>quit()</code>方法即是退出当前应用，自然窗口就被关闭了。<br><figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">import</span> sys</div><div class=\"line\"><span class=\"keyword\">from</span> PyQt5.QtWidgets <span class=\"keyword\">import</span> QApplication, QWidget, QPushButton, QToolTip</div><div class=\"line\"><span class=\"keyword\">from</span> PyQt5.QtCore <span class=\"keyword\">import</span> QCoreApplication</div><div class=\"line\"></div><div class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">MyWindow</span><span class=\"params\">(QWidget)</span>:</span></div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self)</span>:</span></div><div class=\"line\">        super(MyWindow, self).__init__()</div><div class=\"line\">        self._init_ui()</div><div class=\"line\"></div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">_init_ui</span><span class=\"params\">(self)</span>:</span></div><div class=\"line\">        btn = QPushButton(<span class=\"string\">'quit'</span>, self)</div><div class=\"line\">        btn.clicked.connect(QCoreApplication.instance().quit)</div><div class=\"line\">        btn.setToolTip(<span class=\"string\">'This is a &lt;b&gt;QPushButton&lt;/b&gt; widget'</span>)</div><div class=\"line\">        btn.move(<span class=\"number\">50</span>, <span class=\"number\">50</span>)</div><div class=\"line\">        btn.resize(btn.sizeHint())</div><div class=\"line\"></div><div class=\"line\">        self.setGeometry(<span class=\"number\">300</span>, <span class=\"number\">300</span>, <span class=\"number\">300</span>, <span class=\"number\">200</span>)</div><div class=\"line\">        self.setWindowTitle(<span class=\"string\">'Window with Button'</span>)</div><div class=\"line\">        self.show()</div><div class=\"line\"></div><div class=\"line\"><span class=\"keyword\">if</span> __name__ == <span class=\"string\">'__main__'</span>:</div><div class=\"line\">    app = QApplication(sys.argv)</div><div class=\"line\">    window = MyWindow()</div><div class=\"line\">    sys.exit(app.exec_())</div></pre></td></tr></table></figure></p>\n<h2 id=\"使用Event处理事件\"><a href=\"#使用Event处理事件\" class=\"headerlink\" title=\"使用Event处理事件\"></a>使用Event处理事件</h2><p>除了上述的信号和槽的处理方式，也可以使用Event相关的类进行处理。下面的代码在关闭窗口时弹出对话框确认是否关闭。根据用户做出的选择，调用<code>event.accept()</code>或<code>ignore()</code>完成对事件的处理。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">import</span> sys</div><div class=\"line\"><span class=\"keyword\">from</span> PyQt5.QtWidgets <span class=\"keyword\">import</span> QWidget, QMessageBox, QApplication</div><div class=\"line\"></div><div class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">MyWindow</span><span class=\"params\">(QWidget)</span>:</span></div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self)</span>:</span></div><div class=\"line\">        super(MyWindow, self).__init__()</div><div class=\"line\">        self._init_ui()</div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">_init_ui</span><span class=\"params\">(self)</span>:</span></div><div class=\"line\">        self.setGeometry(<span class=\"number\">300</span>, <span class=\"number\">300</span>, <span class=\"number\">300</span>, <span class=\"number\">200</span>)</div><div class=\"line\">        self.show()</div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">closeEvent</span><span class=\"params\">(self, ev)</span>:</span></div><div class=\"line\">        reply = QMessageBox.question(self, <span class=\"string\">'Message'</span>, <span class=\"string\">'Are you sure?'</span>,</div><div class=\"line\">                    QMessageBox.Yes | QMessageBox.No, QMessageBox.No)</div><div class=\"line\">        <span class=\"keyword\">if</span> reply == QMessageBox.Yes:</div><div class=\"line\">            ev.accept()</div><div class=\"line\">        <span class=\"keyword\">else</span>:</div><div class=\"line\">            ev.ignore()</div><div class=\"line\"></div><div class=\"line\"><span class=\"keyword\">if</span> __name__ == <span class=\"string\">'__main__'</span>:</div><div class=\"line\">    app = QApplication(sys.argv)</div><div class=\"line\">    win = MyWindow()</div><div class=\"line\">    sys.exit(app.exec_())</div></pre></td></tr></table></figure>\n<h2 id=\"使用Layout组织Widget\"><a href=\"#使用Layout组织Widget\" class=\"headerlink\" title=\"使用Layout组织Widget\"></a>使用Layout组织Widget</h2><p>组织Widget的方式可以通过绝对位置调整，但是更推荐使用<code>Layout</code>组织。</p>\n<p>绝对位置是通过指定像素多少来确定widget的大小和位置。这样的话，有以下几个缺点：</p>\n<ul>\n<li>不同平台可能显示效果不统一；</li>\n<li>当parent resize的时候，widget大小和位置并不会自动调整</li>\n<li>编码太麻烦，牵一发而动全身</li>\n</ul>\n<p>下面介绍几种常见的<code>Layout</code>类。</p>\n<h3 id=\"Box-Layout\"><a href=\"#Box-Layout\" class=\"headerlink\" title=\"Box Layout\"></a>Box Layout</h3><p>有<code>QVBoxLayout</code>和<code>QHBoxLayout</code>，用来将widget水平或者竖直排列起来。下面的代码通过这两个layout将按钮放置在窗口的右下角。关键的地方在于使用<code>addSkretch()</code>方法将一个<code>QSpacerItem</code>实例对象插入到了layout中，占据了相应位置。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">import</span> sys</div><div class=\"line\"><span class=\"keyword\">from</span> PyQt5.QtWidgets <span class=\"keyword\">import</span> (QWidget, QPushButton,</div><div class=\"line\">    QHBoxLayout, QVBoxLayout, QApplication)</div><div class=\"line\"></div><div class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">MyWindow</span><span class=\"params\">(QWidget)</span>:</span></div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self)</span>:</span></div><div class=\"line\">        super(MyWindow, self).__init__()</div><div class=\"line\">        self._init_ui()</div><div class=\"line\"></div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">_init_ui</span><span class=\"params\">(self)</span>:</span></div><div class=\"line\">        okButton = QPushButton(<span class=\"string\">\"OK\"</span>)</div><div class=\"line\">        cancelButton = QPushButton(<span class=\"string\">\"Cancel\"</span>)</div><div class=\"line\"></div><div class=\"line\">        hbox = QHBoxLayout()</div><div class=\"line\">        hbox.addStretch(<span class=\"number\">1</span>)</div><div class=\"line\">        hbox.addWidget(okButton)</div><div class=\"line\">        hbox.addWidget(cancelButton)</div><div class=\"line\"></div><div class=\"line\">        vbox = QVBoxLayout()</div><div class=\"line\">        vbox.addStretch(<span class=\"number\">1</span>)</div><div class=\"line\">        vbox.addLayout(hbox)</div><div class=\"line\"></div><div class=\"line\">        self.setLayout(vbox)    </div><div class=\"line\"></div><div class=\"line\">        self.setGeometry(<span class=\"number\">300</span>, <span class=\"number\">300</span>, <span class=\"number\">300</span>, <span class=\"number\">150</span>)</div><div class=\"line\">        self.setWindowTitle(<span class=\"string\">'Buttons'</span>)    </div><div class=\"line\">        self.show()</div><div class=\"line\"></div><div class=\"line\"><span class=\"keyword\">if</span> __name__ == <span class=\"string\">'__main__'</span>:</div><div class=\"line\">    app = QApplication(sys.argv)</div><div class=\"line\">    win = MyWindow()</div><div class=\"line\">    sys.exit(app.exec_())</div></pre></td></tr></table></figure>\n<h3 id=\"Grid-Layout\"><a href=\"#Grid-Layout\" class=\"headerlink\" title=\"Grid Layout\"></a>Grid Layout</h3><p><code>QGridLayout</code>将空间划分为行列的grid。在向其中添加item的时候，要指定位置。如下，将5行4列的grid设置为计算器的面板模式。<br><figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">import</span> sys</div><div class=\"line\"><span class=\"keyword\">from</span> PyQt5.QtWidgets <span class=\"keyword\">import</span> (QWidget, QGridLayout,</div><div class=\"line\">    QPushButton, QApplication)</div><div class=\"line\"></div><div class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">MyWindow</span><span class=\"params\">(QWidget)</span>:</span></div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self)</span>:</span></div><div class=\"line\">        super(MyWindow, self).__init__()</div><div class=\"line\">        self._init_ui()</div><div class=\"line\"></div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">_init_ui</span><span class=\"params\">(self)</span>:</span></div><div class=\"line\">        grid = QGridLayout()</div><div class=\"line\">        self.setLayout(grid)</div><div class=\"line\">        names = [<span class=\"string\">'Cls'</span>, <span class=\"string\">'Bck'</span>, <span class=\"string\">''</span>, <span class=\"string\">'Close'</span>,</div><div class=\"line\">                 <span class=\"string\">'7'</span>, <span class=\"string\">'8'</span>, <span class=\"string\">'9'</span>, <span class=\"string\">'/'</span>,</div><div class=\"line\">                <span class=\"string\">'4'</span>, <span class=\"string\">'5'</span>, <span class=\"string\">'6'</span>, <span class=\"string\">'*'</span>,</div><div class=\"line\">                 <span class=\"string\">'1'</span>, <span class=\"string\">'2'</span>, <span class=\"string\">'3'</span>, <span class=\"string\">'-'</span>,</div><div class=\"line\">                <span class=\"string\">'0'</span>, <span class=\"string\">'.'</span>, <span class=\"string\">'='</span>, <span class=\"string\">'+'</span>]</div><div class=\"line\">        positions = [(i,j) <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">5</span>) <span class=\"keyword\">for</span> j <span class=\"keyword\">in</span> range(<span class=\"number\">4</span>)]</div><div class=\"line\">        <span class=\"keyword\">for</span> position, name <span class=\"keyword\">in</span> zip(positions, names):</div><div class=\"line\">            <span class=\"keyword\">if</span> name == <span class=\"string\">''</span>:</div><div class=\"line\">                <span class=\"keyword\">continue</span></div><div class=\"line\">            button = QPushButton(name)</div><div class=\"line\">            grid.addWidget(button, *position)</div><div class=\"line\"></div><div class=\"line\">        self.move(<span class=\"number\">300</span>, <span class=\"number\">150</span>)</div><div class=\"line\">        self.setWindowTitle(<span class=\"string\">'Calculator'</span>)</div><div class=\"line\">        self.show()</div><div class=\"line\"></div><div class=\"line\"><span class=\"keyword\">if</span> __name__ == <span class=\"string\">'__main__'</span>:</div><div class=\"line\"></div><div class=\"line\">    app = QApplication(sys.argv)</div><div class=\"line\">    win = MyWindow()</div><div class=\"line\">    sys.exit(app.exec_())</div></pre></td></tr></table></figure></p>\n<p>另外，我们还可以通过<code>setSpacing()</code>方法设置每个单元格之间的间隔。如果某个widget需要占据多个单元格，可以在<code>addWidget()</code>方法中指定要扩展的行列数。</p>\n<h2 id=\"事件驱动\"><a href=\"#事件驱动\" class=\"headerlink\" title=\"事件驱动\"></a>事件驱动</h2><p>PyQt提供了两种事件驱动的处理方式：</p>\n<ul>\n<li>使用<code>event</code>句柄。事件可能是由于UI交互或者定时器等引起，由接收对象进行处理。</li>\n<li>信号和槽。某个widge交互时，释放相应信号，被槽对应的函数捕获进行处理。</li>\n</ul>\n<p>信号和槽可以见上面使用按钮关闭窗口的例子，关键在于调用信号的<code>connect()</code>函数将其绑定到某个槽上。Python中的可调用对象都可以作为槽。</p>\n<p>而使用event句柄处理时，需要重写override原来的处理函数，见上面使用其在关闭窗口时进行弹窗确认的例子。</p>\n","excerpt":"<p>Qt是一个流行的GUI框架，支持C++/Python。这篇文章是我在这两天通过PyQT制作一个串口通信并画图的小程序的时候，阅读<a href=\"http://zetcode.com/gui/pyqt5/introduction/\">PyQT5的一篇教程</a>时候的记录。<br>","more":"</p>\n<h2 id=\"主要模块\"><a href=\"#主要模块\" class=\"headerlink\" title=\"主要模块\"></a>主要模块</h2><p>PyQt5中的主要三个模块如下：</p>\n<ul>\n<li><code>QtCore</code>: 和GUI无关的核心功能：文件，时间，多线程等</li>\n<li><code>QtGui</code>：和GUI相关的的东西：事件处理，2D图形，字体和文本等</li>\n<li><code>QtWidget</code>：GUI中的相关组件，例如按钮，窗口等。</li>\n</ul>\n<p>其他模块还有<code>QtBluetooth</code>，<code>QtNetwork</code>等，都是比较专用的模块，用到再说。</p>\n<h2 id=\"HelloWorld\"><a href=\"#HelloWorld\" class=\"headerlink\" title=\"HelloWorld\"></a>HelloWorld</h2><p>这里首先给出一段简单的程序，可以在桌面上显示一个窗口。<br><figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">import</span> sys</div><div class=\"line\"><span class=\"keyword\">from</span> PyQt5.QtWidgets <span class=\"keyword\">import</span> QApplication, QWidget</div><div class=\"line\"></div><div class=\"line\"><span class=\"keyword\">if</span> __name__ == <span class=\"string\">'__main__'</span>:</div><div class=\"line\">    app = QApplication(sys.argv)</div><div class=\"line\">    w = QWidget()</div><div class=\"line\">    w.resize(<span class=\"number\">250</span>, <span class=\"number\">150</span>)</div><div class=\"line\">    w.move(<span class=\"number\">300</span>, <span class=\"number\">300</span>)</div><div class=\"line\">    w.setWindowTitle(<span class=\"string\">'Simple'</span>)</div><div class=\"line\">    w.show()</div><div class=\"line\"></div><div class=\"line\">    sys.exit(app.exec_())</div></pre></td></tr></table></figure></p>\n<p>下面介绍上面代码的含义：<br><figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">app = QApplication(sys.argv)</div></pre></td></tr></table></figure></p>\n<p>每个Qt5应用必须首先创建一个application，后面会用到。<br><figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\">w = QWidget()</div><div class=\"line\">w.resize(<span class=\"number\">250</span>, <span class=\"number\">150</span>)</div><div class=\"line\">w.move(<span class=\"number\">300</span>, <span class=\"number\">300</span>)</div><div class=\"line\">w.setWindowTitle(<span class=\"string\">'Simple'</span>)</div><div class=\"line\">w.show()</div></pre></td></tr></table></figure></p>\n<p><code>QtWidget</code>是所有组件的父类，我们创建了一个<code>Widget</code>。没有任何parent widget的Widget会作为窗口出现。接下来，调用其成员函数实现调整大小等功能。最后使用<code>show()</code>将其显示出来。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">sys.exit(app.exec_())</div></pre></td></tr></table></figure>\n<p>进入application的主循环，等待事件的触发。当退出程序（也许是通过Ctrl+C实现的）或者关闭窗口（点击关闭）后，主循环退出。</p>\n<h2 id=\"添加一个按钮\"><a href=\"#添加一个按钮\" class=\"headerlink\" title=\"添加一个按钮\"></a>添加一个按钮</h2><p>下面，我们为窗口添加按钮，并为其添加事件响应动作。</p>\n<p>参考文档可知，按钮<code>QPushButton</code>存在这样的构造函数：<br><figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">__init__ (self, QWidget parent = <span class=\"keyword\">None</span>)</div></pre></td></tr></table></figure></p>\n<p>下面的代码在初始化<code>QPushButton</code>实例<code>btn</code>时，将<code>self</code>作为参数传入，指定了其parent。另外，在指定按钮大小的时候，使用了<code>sizeHint()</code>方法自适应调节其大小。</p>\n<p>同时，为按钮关联了点击动作。Qt中的事件响应机制通过信号和槽实现。点击事件一旦发生，信号<code>clicked</code>会被释放。然后槽相对的处理函数被调用。所谓的槽可以使PyQt提供的slot，或者是任何Python的可调用对象（函数或者实现了<code>__call__()</code>方法的对象）。</p>\n<p>我们调用了现成的处理函数，来达到关闭窗口的目的。使用<code>instance()</code>可以得到当前application实例，调用其<code>quit()</code>方法即是退出当前应用，自然窗口就被关闭了。<br><figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">import</span> sys</div><div class=\"line\"><span class=\"keyword\">from</span> PyQt5.QtWidgets <span class=\"keyword\">import</span> QApplication, QWidget, QPushButton, QToolTip</div><div class=\"line\"><span class=\"keyword\">from</span> PyQt5.QtCore <span class=\"keyword\">import</span> QCoreApplication</div><div class=\"line\"></div><div class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">MyWindow</span><span class=\"params\">(QWidget)</span>:</span></div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self)</span>:</span></div><div class=\"line\">        super(MyWindow, self).__init__()</div><div class=\"line\">        self._init_ui()</div><div class=\"line\"></div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">_init_ui</span><span class=\"params\">(self)</span>:</span></div><div class=\"line\">        btn = QPushButton(<span class=\"string\">'quit'</span>, self)</div><div class=\"line\">        btn.clicked.connect(QCoreApplication.instance().quit)</div><div class=\"line\">        btn.setToolTip(<span class=\"string\">'This is a &lt;b&gt;QPushButton&lt;/b&gt; widget'</span>)</div><div class=\"line\">        btn.move(<span class=\"number\">50</span>, <span class=\"number\">50</span>)</div><div class=\"line\">        btn.resize(btn.sizeHint())</div><div class=\"line\"></div><div class=\"line\">        self.setGeometry(<span class=\"number\">300</span>, <span class=\"number\">300</span>, <span class=\"number\">300</span>, <span class=\"number\">200</span>)</div><div class=\"line\">        self.setWindowTitle(<span class=\"string\">'Window with Button'</span>)</div><div class=\"line\">        self.show()</div><div class=\"line\"></div><div class=\"line\"><span class=\"keyword\">if</span> __name__ == <span class=\"string\">'__main__'</span>:</div><div class=\"line\">    app = QApplication(sys.argv)</div><div class=\"line\">    window = MyWindow()</div><div class=\"line\">    sys.exit(app.exec_())</div></pre></td></tr></table></figure></p>\n<h2 id=\"使用Event处理事件\"><a href=\"#使用Event处理事件\" class=\"headerlink\" title=\"使用Event处理事件\"></a>使用Event处理事件</h2><p>除了上述的信号和槽的处理方式，也可以使用Event相关的类进行处理。下面的代码在关闭窗口时弹出对话框确认是否关闭。根据用户做出的选择，调用<code>event.accept()</code>或<code>ignore()</code>完成对事件的处理。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">import</span> sys</div><div class=\"line\"><span class=\"keyword\">from</span> PyQt5.QtWidgets <span class=\"keyword\">import</span> QWidget, QMessageBox, QApplication</div><div class=\"line\"></div><div class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">MyWindow</span><span class=\"params\">(QWidget)</span>:</span></div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self)</span>:</span></div><div class=\"line\">        super(MyWindow, self).__init__()</div><div class=\"line\">        self._init_ui()</div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">_init_ui</span><span class=\"params\">(self)</span>:</span></div><div class=\"line\">        self.setGeometry(<span class=\"number\">300</span>, <span class=\"number\">300</span>, <span class=\"number\">300</span>, <span class=\"number\">200</span>)</div><div class=\"line\">        self.show()</div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">closeEvent</span><span class=\"params\">(self, ev)</span>:</span></div><div class=\"line\">        reply = QMessageBox.question(self, <span class=\"string\">'Message'</span>, <span class=\"string\">'Are you sure?'</span>,</div><div class=\"line\">                    QMessageBox.Yes | QMessageBox.No, QMessageBox.No)</div><div class=\"line\">        <span class=\"keyword\">if</span> reply == QMessageBox.Yes:</div><div class=\"line\">            ev.accept()</div><div class=\"line\">        <span class=\"keyword\">else</span>:</div><div class=\"line\">            ev.ignore()</div><div class=\"line\"></div><div class=\"line\"><span class=\"keyword\">if</span> __name__ == <span class=\"string\">'__main__'</span>:</div><div class=\"line\">    app = QApplication(sys.argv)</div><div class=\"line\">    win = MyWindow()</div><div class=\"line\">    sys.exit(app.exec_())</div></pre></td></tr></table></figure>\n<h2 id=\"使用Layout组织Widget\"><a href=\"#使用Layout组织Widget\" class=\"headerlink\" title=\"使用Layout组织Widget\"></a>使用Layout组织Widget</h2><p>组织Widget的方式可以通过绝对位置调整，但是更推荐使用<code>Layout</code>组织。</p>\n<p>绝对位置是通过指定像素多少来确定widget的大小和位置。这样的话，有以下几个缺点：</p>\n<ul>\n<li>不同平台可能显示效果不统一；</li>\n<li>当parent resize的时候，widget大小和位置并不会自动调整</li>\n<li>编码太麻烦，牵一发而动全身</li>\n</ul>\n<p>下面介绍几种常见的<code>Layout</code>类。</p>\n<h3 id=\"Box-Layout\"><a href=\"#Box-Layout\" class=\"headerlink\" title=\"Box Layout\"></a>Box Layout</h3><p>有<code>QVBoxLayout</code>和<code>QHBoxLayout</code>，用来将widget水平或者竖直排列起来。下面的代码通过这两个layout将按钮放置在窗口的右下角。关键的地方在于使用<code>addSkretch()</code>方法将一个<code>QSpacerItem</code>实例对象插入到了layout中，占据了相应位置。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">import</span> sys</div><div class=\"line\"><span class=\"keyword\">from</span> PyQt5.QtWidgets <span class=\"keyword\">import</span> (QWidget, QPushButton,</div><div class=\"line\">    QHBoxLayout, QVBoxLayout, QApplication)</div><div class=\"line\"></div><div class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">MyWindow</span><span class=\"params\">(QWidget)</span>:</span></div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self)</span>:</span></div><div class=\"line\">        super(MyWindow, self).__init__()</div><div class=\"line\">        self._init_ui()</div><div class=\"line\"></div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">_init_ui</span><span class=\"params\">(self)</span>:</span></div><div class=\"line\">        okButton = QPushButton(<span class=\"string\">\"OK\"</span>)</div><div class=\"line\">        cancelButton = QPushButton(<span class=\"string\">\"Cancel\"</span>)</div><div class=\"line\"></div><div class=\"line\">        hbox = QHBoxLayout()</div><div class=\"line\">        hbox.addStretch(<span class=\"number\">1</span>)</div><div class=\"line\">        hbox.addWidget(okButton)</div><div class=\"line\">        hbox.addWidget(cancelButton)</div><div class=\"line\"></div><div class=\"line\">        vbox = QVBoxLayout()</div><div class=\"line\">        vbox.addStretch(<span class=\"number\">1</span>)</div><div class=\"line\">        vbox.addLayout(hbox)</div><div class=\"line\"></div><div class=\"line\">        self.setLayout(vbox)    </div><div class=\"line\"></div><div class=\"line\">        self.setGeometry(<span class=\"number\">300</span>, <span class=\"number\">300</span>, <span class=\"number\">300</span>, <span class=\"number\">150</span>)</div><div class=\"line\">        self.setWindowTitle(<span class=\"string\">'Buttons'</span>)    </div><div class=\"line\">        self.show()</div><div class=\"line\"></div><div class=\"line\"><span class=\"keyword\">if</span> __name__ == <span class=\"string\">'__main__'</span>:</div><div class=\"line\">    app = QApplication(sys.argv)</div><div class=\"line\">    win = MyWindow()</div><div class=\"line\">    sys.exit(app.exec_())</div></pre></td></tr></table></figure>\n<h3 id=\"Grid-Layout\"><a href=\"#Grid-Layout\" class=\"headerlink\" title=\"Grid Layout\"></a>Grid Layout</h3><p><code>QGridLayout</code>将空间划分为行列的grid。在向其中添加item的时候，要指定位置。如下，将5行4列的grid设置为计算器的面板模式。<br><figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">import</span> sys</div><div class=\"line\"><span class=\"keyword\">from</span> PyQt5.QtWidgets <span class=\"keyword\">import</span> (QWidget, QGridLayout,</div><div class=\"line\">    QPushButton, QApplication)</div><div class=\"line\"></div><div class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">MyWindow</span><span class=\"params\">(QWidget)</span>:</span></div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self)</span>:</span></div><div class=\"line\">        super(MyWindow, self).__init__()</div><div class=\"line\">        self._init_ui()</div><div class=\"line\"></div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">_init_ui</span><span class=\"params\">(self)</span>:</span></div><div class=\"line\">        grid = QGridLayout()</div><div class=\"line\">        self.setLayout(grid)</div><div class=\"line\">        names = [<span class=\"string\">'Cls'</span>, <span class=\"string\">'Bck'</span>, <span class=\"string\">''</span>, <span class=\"string\">'Close'</span>,</div><div class=\"line\">                 <span class=\"string\">'7'</span>, <span class=\"string\">'8'</span>, <span class=\"string\">'9'</span>, <span class=\"string\">'/'</span>,</div><div class=\"line\">                <span class=\"string\">'4'</span>, <span class=\"string\">'5'</span>, <span class=\"string\">'6'</span>, <span class=\"string\">'*'</span>,</div><div class=\"line\">                 <span class=\"string\">'1'</span>, <span class=\"string\">'2'</span>, <span class=\"string\">'3'</span>, <span class=\"string\">'-'</span>,</div><div class=\"line\">                <span class=\"string\">'0'</span>, <span class=\"string\">'.'</span>, <span class=\"string\">'='</span>, <span class=\"string\">'+'</span>]</div><div class=\"line\">        positions = [(i,j) <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">5</span>) <span class=\"keyword\">for</span> j <span class=\"keyword\">in</span> range(<span class=\"number\">4</span>)]</div><div class=\"line\">        <span class=\"keyword\">for</span> position, name <span class=\"keyword\">in</span> zip(positions, names):</div><div class=\"line\">            <span class=\"keyword\">if</span> name == <span class=\"string\">''</span>:</div><div class=\"line\">                <span class=\"keyword\">continue</span></div><div class=\"line\">            button = QPushButton(name)</div><div class=\"line\">            grid.addWidget(button, *position)</div><div class=\"line\"></div><div class=\"line\">        self.move(<span class=\"number\">300</span>, <span class=\"number\">150</span>)</div><div class=\"line\">        self.setWindowTitle(<span class=\"string\">'Calculator'</span>)</div><div class=\"line\">        self.show()</div><div class=\"line\"></div><div class=\"line\"><span class=\"keyword\">if</span> __name__ == <span class=\"string\">'__main__'</span>:</div><div class=\"line\"></div><div class=\"line\">    app = QApplication(sys.argv)</div><div class=\"line\">    win = MyWindow()</div><div class=\"line\">    sys.exit(app.exec_())</div></pre></td></tr></table></figure></p>\n<p>另外，我们还可以通过<code>setSpacing()</code>方法设置每个单元格之间的间隔。如果某个widget需要占据多个单元格，可以在<code>addWidget()</code>方法中指定要扩展的行列数。</p>\n<h2 id=\"事件驱动\"><a href=\"#事件驱动\" class=\"headerlink\" title=\"事件驱动\"></a>事件驱动</h2><p>PyQt提供了两种事件驱动的处理方式：</p>\n<ul>\n<li>使用<code>event</code>句柄。事件可能是由于UI交互或者定时器等引起，由接收对象进行处理。</li>\n<li>信号和槽。某个widge交互时，释放相应信号，被槽对应的函数捕获进行处理。</li>\n</ul>\n<p>信号和槽可以见上面使用按钮关闭窗口的例子，关键在于调用信号的<code>connect()</code>函数将其绑定到某个槽上。Python中的可调用对象都可以作为槽。</p>\n<p>而使用event句柄处理时，需要重写override原来的处理函数，见上面使用其在关闭窗口时进行弹窗确认的例子。</p>"},{"title":"DELL 游匣7559安装Ubuntu和CUDA记录","date":"2017-08-10T03:29:38.000Z","_content":"虽说开源大法好，但是在我的DELL 游匣7559笔记本上安装Ubuntu+Windows双系统可是耗费了我不少精力。这篇博客是我参考[这篇文章](https://hemenkapadia.github.io/blog/2016/11/11/Ubuntu-with-Nvidia-CUDA-Bumblebee.html)成功安装Ubuntu16.04和CUDA的记录。感谢上文作者的记录，我才能够最终解决这个问题。基本流程和上文作者相同，只不过没有安装后续的bumblee等工具，所以本文并不是原创，而更多是翻译和备份。\n![开源大法好](/img/install_ubuntu_in_dell_kaiyuandafahao.jpg)\n<!-- more -->\n## 蛋疼的过往\n之前我安装过Ubuntu14.04，但是却不支持笔记本的无线网卡，所以一直很不方便。搜索之后才发现，笔记本使用的无线网卡要到Ubuntu15.10以上才有支持，所以想要安装16.04.结果却发现安装界面都进不去。。。\n\n## 安装Ubuntu\n我使用的版本号为Ubuntu16.04.3，使用Windows中的UltraISO制作U盘启动盘。在Windows系统中，通过电池计划关闭快速启动功能，之后重启。在开机出现DELL徽标的时候，按下F12进入BIOS，关闭Security Boot选项。按F10保存并重启，选择U盘启动。\n\n选择“Install Ubuntu”选项，按`e`，找到包含有`quiet splash`的那行脚本，将`quiet splash`替换为以下内容：\n\n``` sh\nnomodeset i915.modeset=1 quiet splash\n```\n\n之后按F10重启，会进入Ubuntu的安装界面。如何安装Ubuntu这里不再详述。安装完毕之后，重启。出现Ubuntu GRUB引导界面之后，高亮Ubuntu选项（一般来说就是第一个备选项），按`e`，按照上述方法替换`quiet splash`。确定可以进入Ubuntu系统并登陆。\n\n## GRUB设置\n下面，修改GRUB设置，避免每次都手动替换。编辑相应配置文件：`sudo vi /etc/default/grub`，找到包含`GRUB_CMDLINE_LINUX_DEFAULT`的那一行，将其修改如下（就是将我们上面每次手动输入的内容直接写到了配置里面）：\n\n``` sh\nGRUB_CMDLINE_LINUX_DEFAULT=\"nomodeset i915.modeset=1 quiet splash\"\n```\n\n## 更新系统软件\n配置更新源（清华的很好用，非教育网也能轻轻松松上700K），使用如下命令更新，\n\n```\nsudo apt-get update\nsudo apt-get upgrade\nsudo apt-get dist-upgrade\nsudo apt-get autoremove\n```\n\n参考博客中指出，如果这个过程中让你选GRUB文件，要选择保持原有文件。但是我并没有遇到这个问题。可能是由于我的Ubuntu版本已经是16.04中目前最新的了？\n\n由于后续有较多的终端文件编辑操作，建议这时候顺便安装Vim。\n``` sh\nsudo apt-get install vim\n```\n\n更新完之后，重启，确认可以正常登陆系统。\n\n## 移除原有的Nvidia和Nouveau驱动\n按下ALT+CTL+F1，进入虚拟终端，首先关闭lightdm服务。这项操作之后会比较经常用到。\n``` sh\nsudo service lightdm stop\n```\n\n之后，执行卸载操作：\n``` sh\nsudo apt-get remove --purge nvidia*\nsudo apt-get remove --purge bumblebee*\nsudo apt-get --purge remove xserver-xorg-video-nouveau*\n```\n\n编辑配置文件，`/etc/modprobe.d/blacklist.conf`，将Nouveau加入到黑名单中：\n```\nblacklist nouveau\nblacklist lbm-nouveau\nalias nouveau off\nalias lbm-nouveau off\noptions nouveau modeset=0\n```\n\n编辑`/etc/init/gpu-manager.conf`文件，将其前面几行注释掉，改成下面的样子，停止gpu-manager服务：\n``` sh\n# Comment these start on settings ; GPU Manager ruins our work\n#start on (starting lightdm\n#          or starting kdm\n#          or starting xdm\n#          or starting lxdm)\ntask\nexec gpu-manager --log /var/log/gpu-manager.log\n```\n\n之后，更新initramfs并重启。\n``` sh\nsudo update-initramfs -u -k all\n```\n\n重启后，确定可以正常登陆系统。并使用下面的命令确定Nouveau被卸载掉了：\n``` sh\n# 正常情况下，下面的命令应该不产生任何输出\nlsmod | grep nouveau\n```\n\n并确定关闭了gpu-manager服务：\n``` sh\nsudo service gpu-manager stop\n```\n\n至此，Ubuntu系统算是安装完毕了。如果没有使用CUDA的需求，可以从这里开始，安安静静地做一个使用Ubuntu的美男子/小仙女了。\n![微笑中带着疲惫](/img/install_ubuntu_in_dell_weixiaodaizhepibei.jpg)\n\n## 安装CUDA\n鉴于国内坑爹的连接资本主义世界的网络环境，建议还是先去Nvidia的官网把CUDA离线安装包下载下来再安装。我使用的是CUDA-8.0-linux.deb安装包。\n\n按ALT+CTL+F1进入虚拟终端，停止lightdm服务，并安装一些可能要用到的包。\n``` sh\nsudo service lightdm stop\nsudo apt-get install linux-headers-$(uname -r)\nsudo apt-get install mesa-utils\n```\n\n安装CUDA包：\n``` sh\nsudo dpkg -i YOUR_CUDA_DEB_PATH\nsudo apt-get update\nsudo apt-get install cuda-8-0\nsudo apt-get autoremove\n```\n安装完毕之后使用`sudo reboot`重启，确定能够正常登陆系统。\n\n在这个过程中，作者提到登录界面会出现两次，再次重启之后没有这个问题了。我也遇到了相同的情况。所以，不要慌！\n\n## 测试CUDA\n我们来测试一下CUDA。首先，依照你使用shell的不同，将环境变量加入到`~/.bashrc`或者`~/.zshrc`中去（不过我相信在经历完这些安装之后应该还没有闲心去搞oh-my-zsh。。。）。\n``` sh\nexport PATH=\"$PATH:/usr/local/cuda-8.0/bin\"\nexport LD_LIBRARY_PATH=\"$LD_LIBRARY_PATH:/usr/local/cuda-8.0/lib\"\n```\n\n接下来，我们将使用CUDA自带的example进行测试：\n``` sh\n# 导入我们刚加入的环境变量\nsource ~/.bashrc\ncd /usr/local/cuda-8.0/bin\n# 将CUDA example拷贝到$HOME下\n./cuda-install-samples-8.0.sh ~\n# 进入拷贝到的那个目录 build\ncd ~/NVIDIA_CUDA-8.0_Samples\nmake -j12\n# 自己挑选几个目录进去运行编译生成的可执行文件测试吧~\n```\n\n## Last But Not Least\n安装玩CUDA之后，不要随便更新系统！！！否则可能会损坏你的Kernel和Xserver。\n![微笑就好](/img/install_ubuntu_in_dell_weixiaojiuhao.jpg)\n","source":"_posts/install-ubuntu-in-dell.md","raw":"---\ntitle: DELL 游匣7559安装Ubuntu和CUDA记录\ndate: 2017-08-10 11:29:38\ntags:\n    - ubuntu\n---\n虽说开源大法好，但是在我的DELL 游匣7559笔记本上安装Ubuntu+Windows双系统可是耗费了我不少精力。这篇博客是我参考[这篇文章](https://hemenkapadia.github.io/blog/2016/11/11/Ubuntu-with-Nvidia-CUDA-Bumblebee.html)成功安装Ubuntu16.04和CUDA的记录。感谢上文作者的记录，我才能够最终解决这个问题。基本流程和上文作者相同，只不过没有安装后续的bumblee等工具，所以本文并不是原创，而更多是翻译和备份。\n![开源大法好](/img/install_ubuntu_in_dell_kaiyuandafahao.jpg)\n<!-- more -->\n## 蛋疼的过往\n之前我安装过Ubuntu14.04，但是却不支持笔记本的无线网卡，所以一直很不方便。搜索之后才发现，笔记本使用的无线网卡要到Ubuntu15.10以上才有支持，所以想要安装16.04.结果却发现安装界面都进不去。。。\n\n## 安装Ubuntu\n我使用的版本号为Ubuntu16.04.3，使用Windows中的UltraISO制作U盘启动盘。在Windows系统中，通过电池计划关闭快速启动功能，之后重启。在开机出现DELL徽标的时候，按下F12进入BIOS，关闭Security Boot选项。按F10保存并重启，选择U盘启动。\n\n选择“Install Ubuntu”选项，按`e`，找到包含有`quiet splash`的那行脚本，将`quiet splash`替换为以下内容：\n\n``` sh\nnomodeset i915.modeset=1 quiet splash\n```\n\n之后按F10重启，会进入Ubuntu的安装界面。如何安装Ubuntu这里不再详述。安装完毕之后，重启。出现Ubuntu GRUB引导界面之后，高亮Ubuntu选项（一般来说就是第一个备选项），按`e`，按照上述方法替换`quiet splash`。确定可以进入Ubuntu系统并登陆。\n\n## GRUB设置\n下面，修改GRUB设置，避免每次都手动替换。编辑相应配置文件：`sudo vi /etc/default/grub`，找到包含`GRUB_CMDLINE_LINUX_DEFAULT`的那一行，将其修改如下（就是将我们上面每次手动输入的内容直接写到了配置里面）：\n\n``` sh\nGRUB_CMDLINE_LINUX_DEFAULT=\"nomodeset i915.modeset=1 quiet splash\"\n```\n\n## 更新系统软件\n配置更新源（清华的很好用，非教育网也能轻轻松松上700K），使用如下命令更新，\n\n```\nsudo apt-get update\nsudo apt-get upgrade\nsudo apt-get dist-upgrade\nsudo apt-get autoremove\n```\n\n参考博客中指出，如果这个过程中让你选GRUB文件，要选择保持原有文件。但是我并没有遇到这个问题。可能是由于我的Ubuntu版本已经是16.04中目前最新的了？\n\n由于后续有较多的终端文件编辑操作，建议这时候顺便安装Vim。\n``` sh\nsudo apt-get install vim\n```\n\n更新完之后，重启，确认可以正常登陆系统。\n\n## 移除原有的Nvidia和Nouveau驱动\n按下ALT+CTL+F1，进入虚拟终端，首先关闭lightdm服务。这项操作之后会比较经常用到。\n``` sh\nsudo service lightdm stop\n```\n\n之后，执行卸载操作：\n``` sh\nsudo apt-get remove --purge nvidia*\nsudo apt-get remove --purge bumblebee*\nsudo apt-get --purge remove xserver-xorg-video-nouveau*\n```\n\n编辑配置文件，`/etc/modprobe.d/blacklist.conf`，将Nouveau加入到黑名单中：\n```\nblacklist nouveau\nblacklist lbm-nouveau\nalias nouveau off\nalias lbm-nouveau off\noptions nouveau modeset=0\n```\n\n编辑`/etc/init/gpu-manager.conf`文件，将其前面几行注释掉，改成下面的样子，停止gpu-manager服务：\n``` sh\n# Comment these start on settings ; GPU Manager ruins our work\n#start on (starting lightdm\n#          or starting kdm\n#          or starting xdm\n#          or starting lxdm)\ntask\nexec gpu-manager --log /var/log/gpu-manager.log\n```\n\n之后，更新initramfs并重启。\n``` sh\nsudo update-initramfs -u -k all\n```\n\n重启后，确定可以正常登陆系统。并使用下面的命令确定Nouveau被卸载掉了：\n``` sh\n# 正常情况下，下面的命令应该不产生任何输出\nlsmod | grep nouveau\n```\n\n并确定关闭了gpu-manager服务：\n``` sh\nsudo service gpu-manager stop\n```\n\n至此，Ubuntu系统算是安装完毕了。如果没有使用CUDA的需求，可以从这里开始，安安静静地做一个使用Ubuntu的美男子/小仙女了。\n![微笑中带着疲惫](/img/install_ubuntu_in_dell_weixiaodaizhepibei.jpg)\n\n## 安装CUDA\n鉴于国内坑爹的连接资本主义世界的网络环境，建议还是先去Nvidia的官网把CUDA离线安装包下载下来再安装。我使用的是CUDA-8.0-linux.deb安装包。\n\n按ALT+CTL+F1进入虚拟终端，停止lightdm服务，并安装一些可能要用到的包。\n``` sh\nsudo service lightdm stop\nsudo apt-get install linux-headers-$(uname -r)\nsudo apt-get install mesa-utils\n```\n\n安装CUDA包：\n``` sh\nsudo dpkg -i YOUR_CUDA_DEB_PATH\nsudo apt-get update\nsudo apt-get install cuda-8-0\nsudo apt-get autoremove\n```\n安装完毕之后使用`sudo reboot`重启，确定能够正常登陆系统。\n\n在这个过程中，作者提到登录界面会出现两次，再次重启之后没有这个问题了。我也遇到了相同的情况。所以，不要慌！\n\n## 测试CUDA\n我们来测试一下CUDA。首先，依照你使用shell的不同，将环境变量加入到`~/.bashrc`或者`~/.zshrc`中去（不过我相信在经历完这些安装之后应该还没有闲心去搞oh-my-zsh。。。）。\n``` sh\nexport PATH=\"$PATH:/usr/local/cuda-8.0/bin\"\nexport LD_LIBRARY_PATH=\"$LD_LIBRARY_PATH:/usr/local/cuda-8.0/lib\"\n```\n\n接下来，我们将使用CUDA自带的example进行测试：\n``` sh\n# 导入我们刚加入的环境变量\nsource ~/.bashrc\ncd /usr/local/cuda-8.0/bin\n# 将CUDA example拷贝到$HOME下\n./cuda-install-samples-8.0.sh ~\n# 进入拷贝到的那个目录 build\ncd ~/NVIDIA_CUDA-8.0_Samples\nmake -j12\n# 自己挑选几个目录进去运行编译生成的可执行文件测试吧~\n```\n\n## Last But Not Least\n安装玩CUDA之后，不要随便更新系统！！！否则可能会损坏你的Kernel和Xserver。\n![微笑就好](/img/install_ubuntu_in_dell_weixiaojiuhao.jpg)\n","slug":"install-ubuntu-in-dell","published":1,"updated":"2018-01-12T06:22:20.472Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjcu6vcts001squ46qckj35v8","content":"<p>虽说开源大法好，但是在我的DELL 游匣7559笔记本上安装Ubuntu+Windows双系统可是耗费了我不少精力。这篇博客是我参考<a href=\"https://hemenkapadia.github.io/blog/2016/11/11/Ubuntu-with-Nvidia-CUDA-Bumblebee.html\" target=\"_blank\" rel=\"external\">这篇文章</a>成功安装Ubuntu16.04和CUDA的记录。感谢上文作者的记录，我才能够最终解决这个问题。基本流程和上文作者相同，只不过没有安装后续的bumblee等工具，所以本文并不是原创，而更多是翻译和备份。<br><img src=\"/img/install_ubuntu_in_dell_kaiyuandafahao.jpg\" alt=\"开源大法好\"><br><a id=\"more\"></a></p>\n<h2 id=\"蛋疼的过往\"><a href=\"#蛋疼的过往\" class=\"headerlink\" title=\"蛋疼的过往\"></a>蛋疼的过往</h2><p>之前我安装过Ubuntu14.04，但是却不支持笔记本的无线网卡，所以一直很不方便。搜索之后才发现，笔记本使用的无线网卡要到Ubuntu15.10以上才有支持，所以想要安装16.04.结果却发现安装界面都进不去。。。</p>\n<h2 id=\"安装Ubuntu\"><a href=\"#安装Ubuntu\" class=\"headerlink\" title=\"安装Ubuntu\"></a>安装Ubuntu</h2><p>我使用的版本号为Ubuntu16.04.3，使用Windows中的UltraISO制作U盘启动盘。在Windows系统中，通过电池计划关闭快速启动功能，之后重启。在开机出现DELL徽标的时候，按下F12进入BIOS，关闭Security Boot选项。按F10保存并重启，选择U盘启动。</p>\n<p>选择“Install Ubuntu”选项，按<code>e</code>，找到包含有<code>quiet splash</code>的那行脚本，将<code>quiet splash</code>替换为以下内容：</p>\n<figure class=\"highlight sh\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">nomodeset i915.modeset=1 quiet splash</div></pre></td></tr></table></figure>\n<p>之后按F10重启，会进入Ubuntu的安装界面。如何安装Ubuntu这里不再详述。安装完毕之后，重启。出现Ubuntu GRUB引导界面之后，高亮Ubuntu选项（一般来说就是第一个备选项），按<code>e</code>，按照上述方法替换<code>quiet splash</code>。确定可以进入Ubuntu系统并登陆。</p>\n<h2 id=\"GRUB设置\"><a href=\"#GRUB设置\" class=\"headerlink\" title=\"GRUB设置\"></a>GRUB设置</h2><p>下面，修改GRUB设置，避免每次都手动替换。编辑相应配置文件：<code>sudo vi /etc/default/grub</code>，找到包含<code>GRUB_CMDLINE_LINUX_DEFAULT</code>的那一行，将其修改如下（就是将我们上面每次手动输入的内容直接写到了配置里面）：</p>\n<figure class=\"highlight sh\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">GRUB_CMDLINE_LINUX_DEFAULT=<span class=\"string\">\"nomodeset i915.modeset=1 quiet splash\"</span></div></pre></td></tr></table></figure>\n<h2 id=\"更新系统软件\"><a href=\"#更新系统软件\" class=\"headerlink\" title=\"更新系统软件\"></a>更新系统软件</h2><p>配置更新源（清华的很好用，非教育网也能轻轻松松上700K），使用如下命令更新，</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\">sudo apt-get update</div><div class=\"line\">sudo apt-get upgrade</div><div class=\"line\">sudo apt-get dist-upgrade</div><div class=\"line\">sudo apt-get autoremove</div></pre></td></tr></table></figure>\n<p>参考博客中指出，如果这个过程中让你选GRUB文件，要选择保持原有文件。但是我并没有遇到这个问题。可能是由于我的Ubuntu版本已经是16.04中目前最新的了？</p>\n<p>由于后续有较多的终端文件编辑操作，建议这时候顺便安装Vim。<br><figure class=\"highlight sh\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">sudo apt-get install vim</div></pre></td></tr></table></figure></p>\n<p>更新完之后，重启，确认可以正常登陆系统。</p>\n<h2 id=\"移除原有的Nvidia和Nouveau驱动\"><a href=\"#移除原有的Nvidia和Nouveau驱动\" class=\"headerlink\" title=\"移除原有的Nvidia和Nouveau驱动\"></a>移除原有的Nvidia和Nouveau驱动</h2><p>按下ALT+CTL+F1，进入虚拟终端，首先关闭lightdm服务。这项操作之后会比较经常用到。<br><figure class=\"highlight sh\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">sudo service lightdm stop</div></pre></td></tr></table></figure></p>\n<p>之后，执行卸载操作：<br><figure class=\"highlight sh\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\">sudo apt-get remove --purge nvidia*</div><div class=\"line\">sudo apt-get remove --purge bumblebee*</div><div class=\"line\">sudo apt-get --purge remove xserver-xorg-video-nouveau*</div></pre></td></tr></table></figure></p>\n<p>编辑配置文件，<code>/etc/modprobe.d/blacklist.conf</code>，将Nouveau加入到黑名单中：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\">blacklist nouveau</div><div class=\"line\">blacklist lbm-nouveau</div><div class=\"line\">alias nouveau off</div><div class=\"line\">alias lbm-nouveau off</div><div class=\"line\">options nouveau modeset=0</div></pre></td></tr></table></figure></p>\n<p>编辑<code>/etc/init/gpu-manager.conf</code>文件，将其前面几行注释掉，改成下面的样子，停止gpu-manager服务：<br><figure class=\"highlight sh\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\"># Comment these start on settings ; GPU Manager ruins our work</span></div><div class=\"line\"><span class=\"comment\">#start on (starting lightdm</span></div><div class=\"line\"><span class=\"comment\">#          or starting kdm</span></div><div class=\"line\"><span class=\"comment\">#          or starting xdm</span></div><div class=\"line\"><span class=\"comment\">#          or starting lxdm)</span></div><div class=\"line\">task</div><div class=\"line\"><span class=\"built_in\">exec</span> gpu-manager --log /var/<span class=\"built_in\">log</span>/gpu-manager.log</div></pre></td></tr></table></figure></p>\n<p>之后，更新initramfs并重启。<br><figure class=\"highlight sh\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">sudo update-initramfs -u -k all</div></pre></td></tr></table></figure></p>\n<p>重启后，确定可以正常登陆系统。并使用下面的命令确定Nouveau被卸载掉了：<br><figure class=\"highlight sh\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\"># 正常情况下，下面的命令应该不产生任何输出</span></div><div class=\"line\">lsmod | grep nouveau</div></pre></td></tr></table></figure></p>\n<p>并确定关闭了gpu-manager服务：<br><figure class=\"highlight sh\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">sudo service gpu-manager stop</div></pre></td></tr></table></figure></p>\n<p>至此，Ubuntu系统算是安装完毕了。如果没有使用CUDA的需求，可以从这里开始，安安静静地做一个使用Ubuntu的美男子/小仙女了。<br><img src=\"/img/install_ubuntu_in_dell_weixiaodaizhepibei.jpg\" alt=\"微笑中带着疲惫\"></p>\n<h2 id=\"安装CUDA\"><a href=\"#安装CUDA\" class=\"headerlink\" title=\"安装CUDA\"></a>安装CUDA</h2><p>鉴于国内坑爹的连接资本主义世界的网络环境，建议还是先去Nvidia的官网把CUDA离线安装包下载下来再安装。我使用的是CUDA-8.0-linux.deb安装包。</p>\n<p>按ALT+CTL+F1进入虚拟终端，停止lightdm服务，并安装一些可能要用到的包。<br><figure class=\"highlight sh\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\">sudo service lightdm stop</div><div class=\"line\">sudo apt-get install linux-headers-$(uname -r)</div><div class=\"line\">sudo apt-get install mesa-utils</div></pre></td></tr></table></figure></p>\n<p>安装CUDA包：<br><figure class=\"highlight sh\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\">sudo dpkg -i YOUR_CUDA_DEB_PATH</div><div class=\"line\">sudo apt-get update</div><div class=\"line\">sudo apt-get install cuda-8-0</div><div class=\"line\">sudo apt-get autoremove</div></pre></td></tr></table></figure></p>\n<p>安装完毕之后使用<code>sudo reboot</code>重启，确定能够正常登陆系统。</p>\n<p>在这个过程中，作者提到登录界面会出现两次，再次重启之后没有这个问题了。我也遇到了相同的情况。所以，不要慌！</p>\n<h2 id=\"测试CUDA\"><a href=\"#测试CUDA\" class=\"headerlink\" title=\"测试CUDA\"></a>测试CUDA</h2><p>我们来测试一下CUDA。首先，依照你使用shell的不同，将环境变量加入到<code>~/.bashrc</code>或者<code>~/.zshrc</code>中去（不过我相信在经历完这些安装之后应该还没有闲心去搞oh-my-zsh。。。）。<br><figure class=\"highlight sh\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"built_in\">export</span> PATH=<span class=\"string\">\"<span class=\"variable\">$PATH</span>:/usr/local/cuda-8.0/bin\"</span></div><div class=\"line\"><span class=\"built_in\">export</span> LD_LIBRARY_PATH=<span class=\"string\">\"<span class=\"variable\">$LD_LIBRARY_PATH</span>:/usr/local/cuda-8.0/lib\"</span></div></pre></td></tr></table></figure></p>\n<p>接下来，我们将使用CUDA自带的example进行测试：<br><figure class=\"highlight sh\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\"># 导入我们刚加入的环境变量</span></div><div class=\"line\"><span class=\"built_in\">source</span> ~/.bashrc</div><div class=\"line\"><span class=\"built_in\">cd</span> /usr/<span class=\"built_in\">local</span>/cuda-8.0/bin</div><div class=\"line\"><span class=\"comment\"># 将CUDA example拷贝到$HOME下</span></div><div class=\"line\">./cuda-install-samples-8.0.sh ~</div><div class=\"line\"><span class=\"comment\"># 进入拷贝到的那个目录 build</span></div><div class=\"line\"><span class=\"built_in\">cd</span> ~/NVIDIA_CUDA-8.0_Samples</div><div class=\"line\">make -j12</div><div class=\"line\"><span class=\"comment\"># 自己挑选几个目录进去运行编译生成的可执行文件测试吧~</span></div></pre></td></tr></table></figure></p>\n<h2 id=\"Last-But-Not-Least\"><a href=\"#Last-But-Not-Least\" class=\"headerlink\" title=\"Last But Not Least\"></a>Last But Not Least</h2><p>安装玩CUDA之后，不要随便更新系统！！！否则可能会损坏你的Kernel和Xserver。<br><img src=\"/img/install_ubuntu_in_dell_weixiaojiuhao.jpg\" alt=\"微笑就好\"></p>\n","excerpt":"<p>虽说开源大法好，但是在我的DELL 游匣7559笔记本上安装Ubuntu+Windows双系统可是耗费了我不少精力。这篇博客是我参考<a href=\"https://hemenkapadia.github.io/blog/2016/11/11/Ubuntu-with-Nvidia-CUDA-Bumblebee.html\">这篇文章</a>成功安装Ubuntu16.04和CUDA的记录。感谢上文作者的记录，我才能够最终解决这个问题。基本流程和上文作者相同，只不过没有安装后续的bumblee等工具，所以本文并不是原创，而更多是翻译和备份。<br><img src=\"/img/install_ubuntu_in_dell_kaiyuandafahao.jpg\" alt=\"开源大法好\"><br>","more":"</p>\n<h2 id=\"蛋疼的过往\"><a href=\"#蛋疼的过往\" class=\"headerlink\" title=\"蛋疼的过往\"></a>蛋疼的过往</h2><p>之前我安装过Ubuntu14.04，但是却不支持笔记本的无线网卡，所以一直很不方便。搜索之后才发现，笔记本使用的无线网卡要到Ubuntu15.10以上才有支持，所以想要安装16.04.结果却发现安装界面都进不去。。。</p>\n<h2 id=\"安装Ubuntu\"><a href=\"#安装Ubuntu\" class=\"headerlink\" title=\"安装Ubuntu\"></a>安装Ubuntu</h2><p>我使用的版本号为Ubuntu16.04.3，使用Windows中的UltraISO制作U盘启动盘。在Windows系统中，通过电池计划关闭快速启动功能，之后重启。在开机出现DELL徽标的时候，按下F12进入BIOS，关闭Security Boot选项。按F10保存并重启，选择U盘启动。</p>\n<p>选择“Install Ubuntu”选项，按<code>e</code>，找到包含有<code>quiet splash</code>的那行脚本，将<code>quiet splash</code>替换为以下内容：</p>\n<figure class=\"highlight sh\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">nomodeset i915.modeset=1 quiet splash</div></pre></td></tr></table></figure>\n<p>之后按F10重启，会进入Ubuntu的安装界面。如何安装Ubuntu这里不再详述。安装完毕之后，重启。出现Ubuntu GRUB引导界面之后，高亮Ubuntu选项（一般来说就是第一个备选项），按<code>e</code>，按照上述方法替换<code>quiet splash</code>。确定可以进入Ubuntu系统并登陆。</p>\n<h2 id=\"GRUB设置\"><a href=\"#GRUB设置\" class=\"headerlink\" title=\"GRUB设置\"></a>GRUB设置</h2><p>下面，修改GRUB设置，避免每次都手动替换。编辑相应配置文件：<code>sudo vi /etc/default/grub</code>，找到包含<code>GRUB_CMDLINE_LINUX_DEFAULT</code>的那一行，将其修改如下（就是将我们上面每次手动输入的内容直接写到了配置里面）：</p>\n<figure class=\"highlight sh\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">GRUB_CMDLINE_LINUX_DEFAULT=<span class=\"string\">\"nomodeset i915.modeset=1 quiet splash\"</span></div></pre></td></tr></table></figure>\n<h2 id=\"更新系统软件\"><a href=\"#更新系统软件\" class=\"headerlink\" title=\"更新系统软件\"></a>更新系统软件</h2><p>配置更新源（清华的很好用，非教育网也能轻轻松松上700K），使用如下命令更新，</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\">sudo apt-get update</div><div class=\"line\">sudo apt-get upgrade</div><div class=\"line\">sudo apt-get dist-upgrade</div><div class=\"line\">sudo apt-get autoremove</div></pre></td></tr></table></figure>\n<p>参考博客中指出，如果这个过程中让你选GRUB文件，要选择保持原有文件。但是我并没有遇到这个问题。可能是由于我的Ubuntu版本已经是16.04中目前最新的了？</p>\n<p>由于后续有较多的终端文件编辑操作，建议这时候顺便安装Vim。<br><figure class=\"highlight sh\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">sudo apt-get install vim</div></pre></td></tr></table></figure></p>\n<p>更新完之后，重启，确认可以正常登陆系统。</p>\n<h2 id=\"移除原有的Nvidia和Nouveau驱动\"><a href=\"#移除原有的Nvidia和Nouveau驱动\" class=\"headerlink\" title=\"移除原有的Nvidia和Nouveau驱动\"></a>移除原有的Nvidia和Nouveau驱动</h2><p>按下ALT+CTL+F1，进入虚拟终端，首先关闭lightdm服务。这项操作之后会比较经常用到。<br><figure class=\"highlight sh\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">sudo service lightdm stop</div></pre></td></tr></table></figure></p>\n<p>之后，执行卸载操作：<br><figure class=\"highlight sh\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\">sudo apt-get remove --purge nvidia*</div><div class=\"line\">sudo apt-get remove --purge bumblebee*</div><div class=\"line\">sudo apt-get --purge remove xserver-xorg-video-nouveau*</div></pre></td></tr></table></figure></p>\n<p>编辑配置文件，<code>/etc/modprobe.d/blacklist.conf</code>，将Nouveau加入到黑名单中：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\">blacklist nouveau</div><div class=\"line\">blacklist lbm-nouveau</div><div class=\"line\">alias nouveau off</div><div class=\"line\">alias lbm-nouveau off</div><div class=\"line\">options nouveau modeset=0</div></pre></td></tr></table></figure></p>\n<p>编辑<code>/etc/init/gpu-manager.conf</code>文件，将其前面几行注释掉，改成下面的样子，停止gpu-manager服务：<br><figure class=\"highlight sh\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\"># Comment these start on settings ; GPU Manager ruins our work</span></div><div class=\"line\"><span class=\"comment\">#start on (starting lightdm</span></div><div class=\"line\"><span class=\"comment\">#          or starting kdm</span></div><div class=\"line\"><span class=\"comment\">#          or starting xdm</span></div><div class=\"line\"><span class=\"comment\">#          or starting lxdm)</span></div><div class=\"line\">task</div><div class=\"line\"><span class=\"built_in\">exec</span> gpu-manager --log /var/<span class=\"built_in\">log</span>/gpu-manager.log</div></pre></td></tr></table></figure></p>\n<p>之后，更新initramfs并重启。<br><figure class=\"highlight sh\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">sudo update-initramfs -u -k all</div></pre></td></tr></table></figure></p>\n<p>重启后，确定可以正常登陆系统。并使用下面的命令确定Nouveau被卸载掉了：<br><figure class=\"highlight sh\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\"># 正常情况下，下面的命令应该不产生任何输出</span></div><div class=\"line\">lsmod | grep nouveau</div></pre></td></tr></table></figure></p>\n<p>并确定关闭了gpu-manager服务：<br><figure class=\"highlight sh\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">sudo service gpu-manager stop</div></pre></td></tr></table></figure></p>\n<p>至此，Ubuntu系统算是安装完毕了。如果没有使用CUDA的需求，可以从这里开始，安安静静地做一个使用Ubuntu的美男子/小仙女了。<br><img src=\"/img/install_ubuntu_in_dell_weixiaodaizhepibei.jpg\" alt=\"微笑中带着疲惫\"></p>\n<h2 id=\"安装CUDA\"><a href=\"#安装CUDA\" class=\"headerlink\" title=\"安装CUDA\"></a>安装CUDA</h2><p>鉴于国内坑爹的连接资本主义世界的网络环境，建议还是先去Nvidia的官网把CUDA离线安装包下载下来再安装。我使用的是CUDA-8.0-linux.deb安装包。</p>\n<p>按ALT+CTL+F1进入虚拟终端，停止lightdm服务，并安装一些可能要用到的包。<br><figure class=\"highlight sh\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\">sudo service lightdm stop</div><div class=\"line\">sudo apt-get install linux-headers-$(uname -r)</div><div class=\"line\">sudo apt-get install mesa-utils</div></pre></td></tr></table></figure></p>\n<p>安装CUDA包：<br><figure class=\"highlight sh\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\">sudo dpkg -i YOUR_CUDA_DEB_PATH</div><div class=\"line\">sudo apt-get update</div><div class=\"line\">sudo apt-get install cuda-8-0</div><div class=\"line\">sudo apt-get autoremove</div></pre></td></tr></table></figure></p>\n<p>安装完毕之后使用<code>sudo reboot</code>重启，确定能够正常登陆系统。</p>\n<p>在这个过程中，作者提到登录界面会出现两次，再次重启之后没有这个问题了。我也遇到了相同的情况。所以，不要慌！</p>\n<h2 id=\"测试CUDA\"><a href=\"#测试CUDA\" class=\"headerlink\" title=\"测试CUDA\"></a>测试CUDA</h2><p>我们来测试一下CUDA。首先，依照你使用shell的不同，将环境变量加入到<code>~/.bashrc</code>或者<code>~/.zshrc</code>中去（不过我相信在经历完这些安装之后应该还没有闲心去搞oh-my-zsh。。。）。<br><figure class=\"highlight sh\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"built_in\">export</span> PATH=<span class=\"string\">\"<span class=\"variable\">$PATH</span>:/usr/local/cuda-8.0/bin\"</span></div><div class=\"line\"><span class=\"built_in\">export</span> LD_LIBRARY_PATH=<span class=\"string\">\"<span class=\"variable\">$LD_LIBRARY_PATH</span>:/usr/local/cuda-8.0/lib\"</span></div></pre></td></tr></table></figure></p>\n<p>接下来，我们将使用CUDA自带的example进行测试：<br><figure class=\"highlight sh\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\"># 导入我们刚加入的环境变量</span></div><div class=\"line\"><span class=\"built_in\">source</span> ~/.bashrc</div><div class=\"line\"><span class=\"built_in\">cd</span> /usr/<span class=\"built_in\">local</span>/cuda-8.0/bin</div><div class=\"line\"><span class=\"comment\"># 将CUDA example拷贝到$HOME下</span></div><div class=\"line\">./cuda-install-samples-8.0.sh ~</div><div class=\"line\"><span class=\"comment\"># 进入拷贝到的那个目录 build</span></div><div class=\"line\"><span class=\"built_in\">cd</span> ~/NVIDIA_CUDA-8.0_Samples</div><div class=\"line\">make -j12</div><div class=\"line\"><span class=\"comment\"># 自己挑选几个目录进去运行编译生成的可执行文件测试吧~</span></div></pre></td></tr></table></figure></p>\n<h2 id=\"Last-But-Not-Least\"><a href=\"#Last-But-Not-Least\" class=\"headerlink\" title=\"Last But Not Least\"></a>Last But Not Least</h2><p>安装玩CUDA之后，不要随便更新系统！！！否则可能会损坏你的Kernel和Xserver。<br><img src=\"/img/install_ubuntu_in_dell_weixiaojiuhao.jpg\" alt=\"微笑就好\"></p>"},{"title":"远程登录Jupyter笔记本","date":"2017-02-26T11:53:11.000Z","_content":"Jupyter Notebook可以很方便地记录代码和内容，很适合边写笔记边写示例代码进行学习总结。在本机使用时，只需在相应文件夹下使用`jupyter notebook`命令即可在浏览器中打开笔记页面，进行编辑。而本篇文章记述了如何在远端登录并使用Jupyter笔记本。这样，就可以利用服务器较强的运算能力来搞事情了。\n![jupyternotebook](/img/jupyternotebook_logo.png)\n<!-- more -->\n\n## 配置jupter notebook\n登录远程服务器后，使用如下命令生成配置文件。\n\n``` bash\njupyter notebook --generate-config\n```\n\n并对其内容进行修改。打开配置文件：\n\n``` bash\nvim ~/.jupyter/jupyter_notebook_config.py\n```\n\n主要修改两处地方：\n\n- `c.NotebookApp.ip='*'`，即不限制ip访问\n- `c.NotebookApp.password = u'hash_value'`\n\n上面的`hash_value`是由用户给定的密码生成的。可以使用`ipython`中的命令轻松搞定。\n\n``` python\nfrom notebook.auth import passwd\npasswd()\n\"\"\"\n这里会要求用户输入密码并确认，生成的hash值要填写到上面\n\"\"\"\n```\n\n当在服务器上运行jupyter-notebook时，我们常常不需要服务器上额外启动浏览器窗口。可以修改上述的配置文件，禁用服务器端的浏览器。找到下面这一行，改成`False`即可。\n\n``` bash\nc.NotebookApp.open_browser = True\n```\n\n## 启动notebook\n之后，在远程服务器上启动笔记本`jupyter notebook`。接着，在本地机器上访问`远程服务器ip:8888`（默认端口为`8888`，也可以在配置文件中修改），输入密码即可访问远程笔记本。\n\n## Jupyter Notebook的常用快捷键\n类似Vim编辑器，Jupyter Notebook有两种键盘输入模式：\n- 编辑模式，在单元中键入代码或文本，这时的单元框线是绿色的。通过`Esc`可以切换至命令模式。\n- 命令模式，键盘输入运行命令，这时的单元框线是蓝色的。通过`Enter`可以切换至编辑模式。\n\n常用的快捷键有：\n\n- h：弹出快捷键列表\n- m：将该Cell转换为Markdown输入状态\n- y：将该Cell转换为代码输入状态\n- shift+Enter：执行该Cell并将焦点置于下一个Cell（如果没有，将新建）\n- Ctrl+Enter：执行该Cell\n\n\n本篇内容参考自博客[远程访问jupyter notebook](http://blog.leanote.com/post/jevonswang/远程访问jupyter-notebook)以及[Jupyter Notebook快捷键\n](http://www.cnblogs.com/weidiao/p/7792885.html)。\n","source":"_posts/jupyternotebook-remote-useage.md","raw":"---\ntitle: 远程登录Jupyter笔记本\ndate: 2017-02-26 19:53:11\ntags:\n    - tool\n---\nJupyter Notebook可以很方便地记录代码和内容，很适合边写笔记边写示例代码进行学习总结。在本机使用时，只需在相应文件夹下使用`jupyter notebook`命令即可在浏览器中打开笔记页面，进行编辑。而本篇文章记述了如何在远端登录并使用Jupyter笔记本。这样，就可以利用服务器较强的运算能力来搞事情了。\n![jupyternotebook](/img/jupyternotebook_logo.png)\n<!-- more -->\n\n## 配置jupter notebook\n登录远程服务器后，使用如下命令生成配置文件。\n\n``` bash\njupyter notebook --generate-config\n```\n\n并对其内容进行修改。打开配置文件：\n\n``` bash\nvim ~/.jupyter/jupyter_notebook_config.py\n```\n\n主要修改两处地方：\n\n- `c.NotebookApp.ip='*'`，即不限制ip访问\n- `c.NotebookApp.password = u'hash_value'`\n\n上面的`hash_value`是由用户给定的密码生成的。可以使用`ipython`中的命令轻松搞定。\n\n``` python\nfrom notebook.auth import passwd\npasswd()\n\"\"\"\n这里会要求用户输入密码并确认，生成的hash值要填写到上面\n\"\"\"\n```\n\n当在服务器上运行jupyter-notebook时，我们常常不需要服务器上额外启动浏览器窗口。可以修改上述的配置文件，禁用服务器端的浏览器。找到下面这一行，改成`False`即可。\n\n``` bash\nc.NotebookApp.open_browser = True\n```\n\n## 启动notebook\n之后，在远程服务器上启动笔记本`jupyter notebook`。接着，在本地机器上访问`远程服务器ip:8888`（默认端口为`8888`，也可以在配置文件中修改），输入密码即可访问远程笔记本。\n\n## Jupyter Notebook的常用快捷键\n类似Vim编辑器，Jupyter Notebook有两种键盘输入模式：\n- 编辑模式，在单元中键入代码或文本，这时的单元框线是绿色的。通过`Esc`可以切换至命令模式。\n- 命令模式，键盘输入运行命令，这时的单元框线是蓝色的。通过`Enter`可以切换至编辑模式。\n\n常用的快捷键有：\n\n- h：弹出快捷键列表\n- m：将该Cell转换为Markdown输入状态\n- y：将该Cell转换为代码输入状态\n- shift+Enter：执行该Cell并将焦点置于下一个Cell（如果没有，将新建）\n- Ctrl+Enter：执行该Cell\n\n\n本篇内容参考自博客[远程访问jupyter notebook](http://blog.leanote.com/post/jevonswang/远程访问jupyter-notebook)以及[Jupyter Notebook快捷键\n](http://www.cnblogs.com/weidiao/p/7792885.html)。\n","slug":"jupyternotebook-remote-useage","published":1,"updated":"2018-01-12T06:22:20.473Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjcu6vctu001uqu46trk0ye4w","content":"<p>Jupyter Notebook可以很方便地记录代码和内容，很适合边写笔记边写示例代码进行学习总结。在本机使用时，只需在相应文件夹下使用<code>jupyter notebook</code>命令即可在浏览器中打开笔记页面，进行编辑。而本篇文章记述了如何在远端登录并使用Jupyter笔记本。这样，就可以利用服务器较强的运算能力来搞事情了。<br><img src=\"/img/jupyternotebook_logo.png\" alt=\"jupyternotebook\"><br><a id=\"more\"></a></p>\n<h2 id=\"配置jupter-notebook\"><a href=\"#配置jupter-notebook\" class=\"headerlink\" title=\"配置jupter notebook\"></a>配置jupter notebook</h2><p>登录远程服务器后，使用如下命令生成配置文件。</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">jupyter notebook --generate-config</div></pre></td></tr></table></figure>\n<p>并对其内容进行修改。打开配置文件：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">vim ~/.jupyter/jupyter_notebook_config.py</div></pre></td></tr></table></figure>\n<p>主要修改两处地方：</p>\n<ul>\n<li><code>c.NotebookApp.ip=&#39;*&#39;</code>，即不限制ip访问</li>\n<li><code>c.NotebookApp.password = u&#39;hash_value&#39;</code></li>\n</ul>\n<p>上面的<code>hash_value</code>是由用户给定的密码生成的。可以使用<code>ipython</code>中的命令轻松搞定。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">from</span> notebook.auth <span class=\"keyword\">import</span> passwd</div><div class=\"line\">passwd()</div><div class=\"line\"><span class=\"string\">\"\"\"</span></div><div class=\"line\">这里会要求用户输入密码并确认，生成的hash值要填写到上面</div><div class=\"line\">\"\"\"</div></pre></td></tr></table></figure>\n<p>当在服务器上运行jupyter-notebook时，我们常常不需要服务器上额外启动浏览器窗口。可以修改上述的配置文件，禁用服务器端的浏览器。找到下面这一行，改成<code>False</code>即可。</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">c.NotebookApp.open_browser = True</div></pre></td></tr></table></figure>\n<h2 id=\"启动notebook\"><a href=\"#启动notebook\" class=\"headerlink\" title=\"启动notebook\"></a>启动notebook</h2><p>之后，在远程服务器上启动笔记本<code>jupyter notebook</code>。接着，在本地机器上访问<code>远程服务器ip:8888</code>（默认端口为<code>8888</code>，也可以在配置文件中修改），输入密码即可访问远程笔记本。</p>\n<h2 id=\"Jupyter-Notebook的常用快捷键\"><a href=\"#Jupyter-Notebook的常用快捷键\" class=\"headerlink\" title=\"Jupyter Notebook的常用快捷键\"></a>Jupyter Notebook的常用快捷键</h2><p>类似Vim编辑器，Jupyter Notebook有两种键盘输入模式：</p>\n<ul>\n<li>编辑模式，在单元中键入代码或文本，这时的单元框线是绿色的。通过<code>Esc</code>可以切换至命令模式。</li>\n<li>命令模式，键盘输入运行命令，这时的单元框线是蓝色的。通过<code>Enter</code>可以切换至编辑模式。</li>\n</ul>\n<p>常用的快捷键有：</p>\n<ul>\n<li>h：弹出快捷键列表</li>\n<li>m：将该Cell转换为Markdown输入状态</li>\n<li>y：将该Cell转换为代码输入状态</li>\n<li>shift+Enter：执行该Cell并将焦点置于下一个Cell（如果没有，将新建）</li>\n<li>Ctrl+Enter：执行该Cell</li>\n</ul>\n<p>本篇内容参考自博客<a href=\"http://blog.leanote.com/post/jevonswang/远程访问jupyter-notebook\" target=\"_blank\" rel=\"external\">远程访问jupyter notebook</a>以及<a href=\"http://www.cnblogs.com/weidiao/p/7792885.html\" target=\"_blank\" rel=\"external\">Jupyter Notebook快捷键\n</a>。</p>\n","excerpt":"<p>Jupyter Notebook可以很方便地记录代码和内容，很适合边写笔记边写示例代码进行学习总结。在本机使用时，只需在相应文件夹下使用<code>jupyter notebook</code>命令即可在浏览器中打开笔记页面，进行编辑。而本篇文章记述了如何在远端登录并使用Jupyter笔记本。这样，就可以利用服务器较强的运算能力来搞事情了。<br><img src=\"/img/jupyternotebook_logo.png\" alt=\"jupyternotebook\"><br>","more":"</p>\n<h2 id=\"配置jupter-notebook\"><a href=\"#配置jupter-notebook\" class=\"headerlink\" title=\"配置jupter notebook\"></a>配置jupter notebook</h2><p>登录远程服务器后，使用如下命令生成配置文件。</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">jupyter notebook --generate-config</div></pre></td></tr></table></figure>\n<p>并对其内容进行修改。打开配置文件：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">vim ~/.jupyter/jupyter_notebook_config.py</div></pre></td></tr></table></figure>\n<p>主要修改两处地方：</p>\n<ul>\n<li><code>c.NotebookApp.ip=&#39;*&#39;</code>，即不限制ip访问</li>\n<li><code>c.NotebookApp.password = u&#39;hash_value&#39;</code></li>\n</ul>\n<p>上面的<code>hash_value</code>是由用户给定的密码生成的。可以使用<code>ipython</code>中的命令轻松搞定。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">from</span> notebook.auth <span class=\"keyword\">import</span> passwd</div><div class=\"line\">passwd()</div><div class=\"line\"><span class=\"string\">\"\"\"</div><div class=\"line\">这里会要求用户输入密码并确认，生成的hash值要填写到上面</div><div class=\"line\">\"\"\"</span></div></pre></td></tr></table></figure>\n<p>当在服务器上运行jupyter-notebook时，我们常常不需要服务器上额外启动浏览器窗口。可以修改上述的配置文件，禁用服务器端的浏览器。找到下面这一行，改成<code>False</code>即可。</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">c.NotebookApp.open_browser = True</div></pre></td></tr></table></figure>\n<h2 id=\"启动notebook\"><a href=\"#启动notebook\" class=\"headerlink\" title=\"启动notebook\"></a>启动notebook</h2><p>之后，在远程服务器上启动笔记本<code>jupyter notebook</code>。接着，在本地机器上访问<code>远程服务器ip:8888</code>（默认端口为<code>8888</code>，也可以在配置文件中修改），输入密码即可访问远程笔记本。</p>\n<h2 id=\"Jupyter-Notebook的常用快捷键\"><a href=\"#Jupyter-Notebook的常用快捷键\" class=\"headerlink\" title=\"Jupyter Notebook的常用快捷键\"></a>Jupyter Notebook的常用快捷键</h2><p>类似Vim编辑器，Jupyter Notebook有两种键盘输入模式：</p>\n<ul>\n<li>编辑模式，在单元中键入代码或文本，这时的单元框线是绿色的。通过<code>Esc</code>可以切换至命令模式。</li>\n<li>命令模式，键盘输入运行命令，这时的单元框线是蓝色的。通过<code>Enter</code>可以切换至编辑模式。</li>\n</ul>\n<p>常用的快捷键有：</p>\n<ul>\n<li>h：弹出快捷键列表</li>\n<li>m：将该Cell转换为Markdown输入状态</li>\n<li>y：将该Cell转换为代码输入状态</li>\n<li>shift+Enter：执行该Cell并将焦点置于下一个Cell（如果没有，将新建）</li>\n<li>Ctrl+Enter：执行该Cell</li>\n</ul>\n<p>本篇内容参考自博客<a href=\"http://blog.leanote.com/post/jevonswang/远程访问jupyter-notebook\">远程访问jupyter notebook</a>以及<a href=\"http://www.cnblogs.com/weidiao/p/7792885.html\">Jupyter Notebook快捷键\n</a>。</p>"},{"title":"Caffe中的底层数学计算函数","date":"2017-03-08T09:24:48.000Z","_content":"Caffe中使用了BLAS库作为底层矩阵运算的实现，这篇文章对[mathfunction.hpp 文件](https://github.com/BVLC/caffe/blob/master/include/caffe/util/math_functions.hpp)中的相关函数做一总结。我们在自己实现layer运算的时候，也要注意是否Caffe中已经支持了类似运算，不要从scratch开始编码，自己累点不算啥，CPU/GPU的运算能力发挥不出来，更别说自己写的代码也许还是错的，那就尴尬了。。。\n![一卡一栋楼，双卡灭地球，三卡银河系，四卡创世纪](/img/caffe_mathfunctions_gpuisnuclearweapon.jpg)\n\n<!-- more -->\n## BLAS介绍\n以下内容参考[BLAS wiki页面](https://en.wikipedia.org/wiki/Basic_Linear_Algebra_Subprograms)整理。这里不涉及BLAS的过多内容，只为介绍Caffe中的相关函数做一过渡。\n\nBLAS的全称是基础线性代数子程序库（Basic Linear Algebra Subprograms），提供了一些低层次的通用线性代数运算的实现函数，如向量的相加，数乘，点积和矩阵相乘等。BLAS的实现根绝硬件平台的不同而不同，常常利用了特定处理器的硬件特点进行加速计算（例如处理器上的向量寄存器和SIMD指令集），提供了C和Fortran语言支持。\n\n不同的厂商根据自己硬件的特点，在BLAS的统一框架下，开发了自己的加速库，比如~~AMD的ACML~~（已经不再支持），Intel的MKL，ATLAS和OpenBLAS。其中后面的三个均可以在Caffe中配置使用。\n\n在BLAS中，实现了矩阵与矩阵相乘的函数`gemm`（GEMM: General Matrix to Matrix Multiplication）和矩阵和向量相乘的函数`gemv`，这两个数学运算的高效实现，关系到整个DL 框架的运算速度。下面这张图来源于Jia Yangqing的博士论文。\n![前向计算中的典型时间分布](/img/mathfunctions_time_distribution.png)\n\n可以看到，在前向计算过程中，无论是CPU还是GPU，大量时间都花在了卷积层和全连接层上。全连接层不必多说，就是一个输入feature和权重的矩阵乘法。卷积运算也是通过矩阵相乘实现的。因为我们可以把卷积核变成一列，和相应的feature区域做相乘（如下图，这部分可以看一下Caffe中im2col部分的介绍和代码）。\n![im2col的原理](/img/mathfunctions_im2col.png)\n\n对于BLAS和GEMM等对DL的作用意义，可以参见这篇文章[Why GEMM is at the heart of deep learning](https://petewarden.com/2015/04/20/why-gemm-is-at-the-heart-of-deep-learning/)的分析。上面的图也都来源于这篇博客。\n\n## 矩阵运算函数\n矩阵运算函数在文件`math_functions.hpp`中可以找到。其中的函数多是对BLAS相应API的包装。这部分内容主要参考了参考资料[1]中的内容。谢谢原作者的整理。\n\n### 矩阵与矩阵，矩阵与向量的乘法\n\n函数`caffe_cpu_gemm()`是对BLAS中矩阵与矩阵相乘函数`gemm`的包装。与之对应的`caffe_cpu_gemv()`是对矩阵与向量相乘`gemv`函数的包装。以前者为例，其实现代码如下：\n\n``` cpp\ntemplate<>\nvoid caffe_cpu_gemm<float>(const CBLAS_TRANSPOSE TransA,\n    const CBLAS_TRANSPOSE TransB, const int M, const int N, const int K,\n    const float alpha, const float* A, const float* B, const float beta,\n    float* C) {\n  int lda = (TransA == CblasNoTrans) ? K : M;\n  int ldb = (TransB == CblasNoTrans) ? N : K;\n  cblas_sgemm(CblasRowMajor, TransA, TransB, M, N, K, alpha, A, lda, B,\n      ldb, beta, C, N);\n}\n```\n\n可以看到，这个函数是对单精度浮点数（Single Float）的模板特化，在函数内部调用了BLAS包中的`cblas_sgemm()`函数。其功能是计算`C = alpha * A * B + beta * C`。参数的具体含义可以查看BLAS的相关文档。\n\n### 矩阵/向量的加减\n下面的函数都是将`X`指针所指的数据作为`src`，将`Y`指针所指的数据为`dst`。同时，第一个参数统一是向量的长度。\n\n- `caffe_axpy(N, alpha, x, mutable y)`实现向量加法`Y = alpha * X + Y`。\n- `caffe_axpby(N, alpha, x, beta, mutable y)`实现向量加法`Y = alpha * X + beta * Y`\n\n这两个函数的用法可以参见欧氏距离loss函数中的梯度计算：\n\n``` cpp\ncaffe_cpu_axpby(\n    bottom[i]->count(),              // count\n    alpha,                              // alpha\n    diff_.cpu_data(),                   // a\n    Dtype(0),                           // beta\n    bottom[i]->mutable_cpu_diff());  // b\n}\n```\n其中，`bottom[i]->count()`给定了`blob`的大小，也就是向量的长度。`alpha`实际是由顶层`top_blob`传来的`loss_weight`，也即是`*top_blob->cpu_diff()/batch_size`。由于是直接将加权后的`diff`直接赋给`bottom_blob`的`cpu_diff`，所以，将`beta`赋值为0。\n\n### 内存相关\n和C语言中的`memset()`和`memcpy()`类似，Caffe内也提供了对内存的拷贝与置位。使用方法也和两者相似：\n- `caffe_copy(N, x, mutable y)`实现向量拷贝。源地址和目标地址服从上小节的约定。\n- `caffe_set(N, alpha, mutable x)`实现向量的置位，将向量分量填充为值`alpha`。\n\n查看其实现可以知道，这里Caffe中直接调用了`memset()`完成任务。\n``` cpp\ntemplate <typename Dtype>\nvoid caffe_set(const int N, const Dtype alpha, Dtype* Y) {\n  if (alpha == 0) {\n    memset(Y, 0, sizeof(Dtype) * N);  // NOLINT(caffe/alt_fn)\n    return;\n  }\n  for (int i = 0; i < N; ++i) {\n    Y[i] = alpha;\n  }\n}\n\n// 模板的特化\ntemplate void caffe_set<int>(const int N, const int alpha, int* Y);\ntemplate void caffe_set<float>(const int N, const float alpha, float* Y);\ntemplate void caffe_set<double>(const int N, const double alpha, double* Y);\n```\n\n而`caffe_copy()`中则是直接实现了CPU和GPU的功能。注意到下面代码中调用`cudaMemcpy()`的时候，使用了参数`cudaMemcpyDefault`。通过查阅文档，这个变量的含义是[cudaMemcpyDefault: Default based unified virtual address space](http://horacio9573.no-ip.org/cuda/group__CUDART__TYPES_g18fa99055ee694244a270e4d5101e95b.html)。通过它，我们可以无需知道源地址和目标地址是否在CPU内存或者GPU内存上而分别处理，减少了代码负担。\n\n``` cpp\ntemplate <typename Dtype>\nvoid caffe_copy(const int N, const Dtype* X, Dtype* Y) {\n  if (X != Y) {\n    if (Caffe::mode() == Caffe::GPU) {\n#ifndef CPU_ONLY\n      // NOLINT_NEXT_LINE(caffe/alt_fn)\n      CUDA_CHECK(cudaMemcpy(Y, X, sizeof(Dtype) * N, cudaMemcpyDefault));\n#else\n      NO_GPU;\n#endif\n    } else {\n      memcpy(Y, X, sizeof(Dtype) * N);  // NOLINT(caffe/alt_fn)\n    }\n  }\n}\n```\n\n所谓的unified virtual address（UVA）就是下图这个意思（见[P2P&UVA](http://on-demand.gputechconf.com/gtc-express/2011/presentations/cuda_webinars_GPUDirect_uva.pdf)）。\n![UVA图示](/img/caffe_mathfunctions_whatisuva.png)\n\n有了这个东西，可以将内存和GPU显存看做一个统一的内存空间。CUDA运行时会根据指针的值自动判断数据的实际位置。这样一来，简化了编程者的工作量，如下所示：\n![How to use UVA](/img/caffe_mathfunctions_useuva.png)\n\n其使用条件如下：\n![UVA Requirement](/img/caffe_mathfunctions_uvarequirement.png)\n\n### 向量逐元素运算\n- `caffe_add(N, a, b, y)`函数实现`Y[i] = a[i] + b[i]`。\n- `caffe_sub`, `caffe_div`, `caffe_mul`同理。\n- `caffe_exp`, `caffe_powx`, `caffe_abs`, `caffe_sqr`, `caffe_log`相似，这里只将`caffe_exp()`的实现复制如下：\n\n``` cpp\n// 又是模板特化\ntemplate <>\nvoid caffe_exp<float>(const int n, const float* a, float* y) {\n  vsExp(n, a, y);   // 返回 y[i] = exp(a[i])\n}\n```\n\n- `caffe_scal`实现向量的数乘。这个函数常常用在`loss_layer`中计算反传的梯度，常常要乘上一个标量`loss_weight`。\n- `caffe_add_scalar`实现向量每个分量与标量相加。\n\n## GPU版本\n相应地，和基于BLAS的CPU数学计算函数相似，各GPU版本的函数声明也放在了`math_functions.hpp`中，而相应的实现代码在`math_functions.cu`中。\n\n## 随机数产生器\nCaffe中还提供了若干随机数产生器，可以用来做数据（如权重矩阵）的初始化等。\n\n这里，Caffe提供饿了均匀分布（uniform），高斯分布（gaussian），伯努利分布（bernoulli）的实现。这里就不再详述，使用函数`caffe_rng_distribution_name`即可。\n\n## 参考资料\n【1】[seven-first 的博客](http://blog.csdn.net/seven_first/article/details/47378697)\n","source":"_posts/mathfunctions-in-caffe.md","raw":"---\ntitle: Caffe中的底层数学计算函数\ndate: 2017-03-08 17:24:48\ntags:\n    - caffe\n---\nCaffe中使用了BLAS库作为底层矩阵运算的实现，这篇文章对[mathfunction.hpp 文件](https://github.com/BVLC/caffe/blob/master/include/caffe/util/math_functions.hpp)中的相关函数做一总结。我们在自己实现layer运算的时候，也要注意是否Caffe中已经支持了类似运算，不要从scratch开始编码，自己累点不算啥，CPU/GPU的运算能力发挥不出来，更别说自己写的代码也许还是错的，那就尴尬了。。。\n![一卡一栋楼，双卡灭地球，三卡银河系，四卡创世纪](/img/caffe_mathfunctions_gpuisnuclearweapon.jpg)\n\n<!-- more -->\n## BLAS介绍\n以下内容参考[BLAS wiki页面](https://en.wikipedia.org/wiki/Basic_Linear_Algebra_Subprograms)整理。这里不涉及BLAS的过多内容，只为介绍Caffe中的相关函数做一过渡。\n\nBLAS的全称是基础线性代数子程序库（Basic Linear Algebra Subprograms），提供了一些低层次的通用线性代数运算的实现函数，如向量的相加，数乘，点积和矩阵相乘等。BLAS的实现根绝硬件平台的不同而不同，常常利用了特定处理器的硬件特点进行加速计算（例如处理器上的向量寄存器和SIMD指令集），提供了C和Fortran语言支持。\n\n不同的厂商根据自己硬件的特点，在BLAS的统一框架下，开发了自己的加速库，比如~~AMD的ACML~~（已经不再支持），Intel的MKL，ATLAS和OpenBLAS。其中后面的三个均可以在Caffe中配置使用。\n\n在BLAS中，实现了矩阵与矩阵相乘的函数`gemm`（GEMM: General Matrix to Matrix Multiplication）和矩阵和向量相乘的函数`gemv`，这两个数学运算的高效实现，关系到整个DL 框架的运算速度。下面这张图来源于Jia Yangqing的博士论文。\n![前向计算中的典型时间分布](/img/mathfunctions_time_distribution.png)\n\n可以看到，在前向计算过程中，无论是CPU还是GPU，大量时间都花在了卷积层和全连接层上。全连接层不必多说，就是一个输入feature和权重的矩阵乘法。卷积运算也是通过矩阵相乘实现的。因为我们可以把卷积核变成一列，和相应的feature区域做相乘（如下图，这部分可以看一下Caffe中im2col部分的介绍和代码）。\n![im2col的原理](/img/mathfunctions_im2col.png)\n\n对于BLAS和GEMM等对DL的作用意义，可以参见这篇文章[Why GEMM is at the heart of deep learning](https://petewarden.com/2015/04/20/why-gemm-is-at-the-heart-of-deep-learning/)的分析。上面的图也都来源于这篇博客。\n\n## 矩阵运算函数\n矩阵运算函数在文件`math_functions.hpp`中可以找到。其中的函数多是对BLAS相应API的包装。这部分内容主要参考了参考资料[1]中的内容。谢谢原作者的整理。\n\n### 矩阵与矩阵，矩阵与向量的乘法\n\n函数`caffe_cpu_gemm()`是对BLAS中矩阵与矩阵相乘函数`gemm`的包装。与之对应的`caffe_cpu_gemv()`是对矩阵与向量相乘`gemv`函数的包装。以前者为例，其实现代码如下：\n\n``` cpp\ntemplate<>\nvoid caffe_cpu_gemm<float>(const CBLAS_TRANSPOSE TransA,\n    const CBLAS_TRANSPOSE TransB, const int M, const int N, const int K,\n    const float alpha, const float* A, const float* B, const float beta,\n    float* C) {\n  int lda = (TransA == CblasNoTrans) ? K : M;\n  int ldb = (TransB == CblasNoTrans) ? N : K;\n  cblas_sgemm(CblasRowMajor, TransA, TransB, M, N, K, alpha, A, lda, B,\n      ldb, beta, C, N);\n}\n```\n\n可以看到，这个函数是对单精度浮点数（Single Float）的模板特化，在函数内部调用了BLAS包中的`cblas_sgemm()`函数。其功能是计算`C = alpha * A * B + beta * C`。参数的具体含义可以查看BLAS的相关文档。\n\n### 矩阵/向量的加减\n下面的函数都是将`X`指针所指的数据作为`src`，将`Y`指针所指的数据为`dst`。同时，第一个参数统一是向量的长度。\n\n- `caffe_axpy(N, alpha, x, mutable y)`实现向量加法`Y = alpha * X + Y`。\n- `caffe_axpby(N, alpha, x, beta, mutable y)`实现向量加法`Y = alpha * X + beta * Y`\n\n这两个函数的用法可以参见欧氏距离loss函数中的梯度计算：\n\n``` cpp\ncaffe_cpu_axpby(\n    bottom[i]->count(),              // count\n    alpha,                              // alpha\n    diff_.cpu_data(),                   // a\n    Dtype(0),                           // beta\n    bottom[i]->mutable_cpu_diff());  // b\n}\n```\n其中，`bottom[i]->count()`给定了`blob`的大小，也就是向量的长度。`alpha`实际是由顶层`top_blob`传来的`loss_weight`，也即是`*top_blob->cpu_diff()/batch_size`。由于是直接将加权后的`diff`直接赋给`bottom_blob`的`cpu_diff`，所以，将`beta`赋值为0。\n\n### 内存相关\n和C语言中的`memset()`和`memcpy()`类似，Caffe内也提供了对内存的拷贝与置位。使用方法也和两者相似：\n- `caffe_copy(N, x, mutable y)`实现向量拷贝。源地址和目标地址服从上小节的约定。\n- `caffe_set(N, alpha, mutable x)`实现向量的置位，将向量分量填充为值`alpha`。\n\n查看其实现可以知道，这里Caffe中直接调用了`memset()`完成任务。\n``` cpp\ntemplate <typename Dtype>\nvoid caffe_set(const int N, const Dtype alpha, Dtype* Y) {\n  if (alpha == 0) {\n    memset(Y, 0, sizeof(Dtype) * N);  // NOLINT(caffe/alt_fn)\n    return;\n  }\n  for (int i = 0; i < N; ++i) {\n    Y[i] = alpha;\n  }\n}\n\n// 模板的特化\ntemplate void caffe_set<int>(const int N, const int alpha, int* Y);\ntemplate void caffe_set<float>(const int N, const float alpha, float* Y);\ntemplate void caffe_set<double>(const int N, const double alpha, double* Y);\n```\n\n而`caffe_copy()`中则是直接实现了CPU和GPU的功能。注意到下面代码中调用`cudaMemcpy()`的时候，使用了参数`cudaMemcpyDefault`。通过查阅文档，这个变量的含义是[cudaMemcpyDefault: Default based unified virtual address space](http://horacio9573.no-ip.org/cuda/group__CUDART__TYPES_g18fa99055ee694244a270e4d5101e95b.html)。通过它，我们可以无需知道源地址和目标地址是否在CPU内存或者GPU内存上而分别处理，减少了代码负担。\n\n``` cpp\ntemplate <typename Dtype>\nvoid caffe_copy(const int N, const Dtype* X, Dtype* Y) {\n  if (X != Y) {\n    if (Caffe::mode() == Caffe::GPU) {\n#ifndef CPU_ONLY\n      // NOLINT_NEXT_LINE(caffe/alt_fn)\n      CUDA_CHECK(cudaMemcpy(Y, X, sizeof(Dtype) * N, cudaMemcpyDefault));\n#else\n      NO_GPU;\n#endif\n    } else {\n      memcpy(Y, X, sizeof(Dtype) * N);  // NOLINT(caffe/alt_fn)\n    }\n  }\n}\n```\n\n所谓的unified virtual address（UVA）就是下图这个意思（见[P2P&UVA](http://on-demand.gputechconf.com/gtc-express/2011/presentations/cuda_webinars_GPUDirect_uva.pdf)）。\n![UVA图示](/img/caffe_mathfunctions_whatisuva.png)\n\n有了这个东西，可以将内存和GPU显存看做一个统一的内存空间。CUDA运行时会根据指针的值自动判断数据的实际位置。这样一来，简化了编程者的工作量，如下所示：\n![How to use UVA](/img/caffe_mathfunctions_useuva.png)\n\n其使用条件如下：\n![UVA Requirement](/img/caffe_mathfunctions_uvarequirement.png)\n\n### 向量逐元素运算\n- `caffe_add(N, a, b, y)`函数实现`Y[i] = a[i] + b[i]`。\n- `caffe_sub`, `caffe_div`, `caffe_mul`同理。\n- `caffe_exp`, `caffe_powx`, `caffe_abs`, `caffe_sqr`, `caffe_log`相似，这里只将`caffe_exp()`的实现复制如下：\n\n``` cpp\n// 又是模板特化\ntemplate <>\nvoid caffe_exp<float>(const int n, const float* a, float* y) {\n  vsExp(n, a, y);   // 返回 y[i] = exp(a[i])\n}\n```\n\n- `caffe_scal`实现向量的数乘。这个函数常常用在`loss_layer`中计算反传的梯度，常常要乘上一个标量`loss_weight`。\n- `caffe_add_scalar`实现向量每个分量与标量相加。\n\n## GPU版本\n相应地，和基于BLAS的CPU数学计算函数相似，各GPU版本的函数声明也放在了`math_functions.hpp`中，而相应的实现代码在`math_functions.cu`中。\n\n## 随机数产生器\nCaffe中还提供了若干随机数产生器，可以用来做数据（如权重矩阵）的初始化等。\n\n这里，Caffe提供饿了均匀分布（uniform），高斯分布（gaussian），伯努利分布（bernoulli）的实现。这里就不再详述，使用函数`caffe_rng_distribution_name`即可。\n\n## 参考资料\n【1】[seven-first 的博客](http://blog.csdn.net/seven_first/article/details/47378697)\n","slug":"mathfunctions-in-caffe","published":1,"updated":"2018-01-12T06:22:20.474Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjcu6vctw001wqu46q4y48ohf","content":"<p>Caffe中使用了BLAS库作为底层矩阵运算的实现，这篇文章对<a href=\"https://github.com/BVLC/caffe/blob/master/include/caffe/util/math_functions.hpp\" target=\"_blank\" rel=\"external\">mathfunction.hpp 文件</a>中的相关函数做一总结。我们在自己实现layer运算的时候，也要注意是否Caffe中已经支持了类似运算，不要从scratch开始编码，自己累点不算啥，CPU/GPU的运算能力发挥不出来，更别说自己写的代码也许还是错的，那就尴尬了。。。<br><img src=\"/img/caffe_mathfunctions_gpuisnuclearweapon.jpg\" alt=\"一卡一栋楼，双卡灭地球，三卡银河系，四卡创世纪\"></p>\n<a id=\"more\"></a>\n<h2 id=\"BLAS介绍\"><a href=\"#BLAS介绍\" class=\"headerlink\" title=\"BLAS介绍\"></a>BLAS介绍</h2><p>以下内容参考<a href=\"https://en.wikipedia.org/wiki/Basic_Linear_Algebra_Subprograms\" target=\"_blank\" rel=\"external\">BLAS wiki页面</a>整理。这里不涉及BLAS的过多内容，只为介绍Caffe中的相关函数做一过渡。</p>\n<p>BLAS的全称是基础线性代数子程序库（Basic Linear Algebra Subprograms），提供了一些低层次的通用线性代数运算的实现函数，如向量的相加，数乘，点积和矩阵相乘等。BLAS的实现根绝硬件平台的不同而不同，常常利用了特定处理器的硬件特点进行加速计算（例如处理器上的向量寄存器和SIMD指令集），提供了C和Fortran语言支持。</p>\n<p>不同的厂商根据自己硬件的特点，在BLAS的统一框架下，开发了自己的加速库，比如<del>AMD的ACML</del>（已经不再支持），Intel的MKL，ATLAS和OpenBLAS。其中后面的三个均可以在Caffe中配置使用。</p>\n<p>在BLAS中，实现了矩阵与矩阵相乘的函数<code>gemm</code>（GEMM: General Matrix to Matrix Multiplication）和矩阵和向量相乘的函数<code>gemv</code>，这两个数学运算的高效实现，关系到整个DL 框架的运算速度。下面这张图来源于Jia Yangqing的博士论文。<br><img src=\"/img/mathfunctions_time_distribution.png\" alt=\"前向计算中的典型时间分布\"></p>\n<p>可以看到，在前向计算过程中，无论是CPU还是GPU，大量时间都花在了卷积层和全连接层上。全连接层不必多说，就是一个输入feature和权重的矩阵乘法。卷积运算也是通过矩阵相乘实现的。因为我们可以把卷积核变成一列，和相应的feature区域做相乘（如下图，这部分可以看一下Caffe中im2col部分的介绍和代码）。<br><img src=\"/img/mathfunctions_im2col.png\" alt=\"im2col的原理\"></p>\n<p>对于BLAS和GEMM等对DL的作用意义，可以参见这篇文章<a href=\"https://petewarden.com/2015/04/20/why-gemm-is-at-the-heart-of-deep-learning/\" target=\"_blank\" rel=\"external\">Why GEMM is at the heart of deep learning</a>的分析。上面的图也都来源于这篇博客。</p>\n<h2 id=\"矩阵运算函数\"><a href=\"#矩阵运算函数\" class=\"headerlink\" title=\"矩阵运算函数\"></a>矩阵运算函数</h2><p>矩阵运算函数在文件<code>math_functions.hpp</code>中可以找到。其中的函数多是对BLAS相应API的包装。这部分内容主要参考了参考资料[1]中的内容。谢谢原作者的整理。</p>\n<h3 id=\"矩阵与矩阵，矩阵与向量的乘法\"><a href=\"#矩阵与矩阵，矩阵与向量的乘法\" class=\"headerlink\" title=\"矩阵与矩阵，矩阵与向量的乘法\"></a>矩阵与矩阵，矩阵与向量的乘法</h3><p>函数<code>caffe_cpu_gemm()</code>是对BLAS中矩阵与矩阵相乘函数<code>gemm</code>的包装。与之对应的<code>caffe_cpu_gemv()</code>是对矩阵与向量相乘<code>gemv</code>函数的包装。以前者为例，其实现代码如下：</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">template</span>&lt;&gt;</div><div class=\"line\"><span class=\"keyword\">void</span> caffe_cpu_gemm&lt;<span class=\"keyword\">float</span>&gt;(<span class=\"keyword\">const</span> CBLAS_TRANSPOSE TransA,</div><div class=\"line\">    <span class=\"keyword\">const</span> CBLAS_TRANSPOSE TransB, <span class=\"keyword\">const</span> <span class=\"keyword\">int</span> M, <span class=\"keyword\">const</span> <span class=\"keyword\">int</span> N, <span class=\"keyword\">const</span> <span class=\"keyword\">int</span> K,</div><div class=\"line\">    <span class=\"keyword\">const</span> <span class=\"keyword\">float</span> alpha, <span class=\"keyword\">const</span> <span class=\"keyword\">float</span>* A, <span class=\"keyword\">const</span> <span class=\"keyword\">float</span>* B, <span class=\"keyword\">const</span> <span class=\"keyword\">float</span> beta,</div><div class=\"line\">    <span class=\"keyword\">float</span>* C) &#123;</div><div class=\"line\">  <span class=\"keyword\">int</span> lda = (TransA == CblasNoTrans) ? K : M;</div><div class=\"line\">  <span class=\"keyword\">int</span> ldb = (TransB == CblasNoTrans) ? N : K;</div><div class=\"line\">  cblas_sgemm(CblasRowMajor, TransA, TransB, M, N, K, alpha, A, lda, B,</div><div class=\"line\">      ldb, beta, C, N);</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<p>可以看到，这个函数是对单精度浮点数（Single Float）的模板特化，在函数内部调用了BLAS包中的<code>cblas_sgemm()</code>函数。其功能是计算<code>C = alpha * A * B + beta * C</code>。参数的具体含义可以查看BLAS的相关文档。</p>\n<h3 id=\"矩阵-向量的加减\"><a href=\"#矩阵-向量的加减\" class=\"headerlink\" title=\"矩阵/向量的加减\"></a>矩阵/向量的加减</h3><p>下面的函数都是将<code>X</code>指针所指的数据作为<code>src</code>，将<code>Y</code>指针所指的数据为<code>dst</code>。同时，第一个参数统一是向量的长度。</p>\n<ul>\n<li><code>caffe_axpy(N, alpha, x, mutable y)</code>实现向量加法<code>Y = alpha * X + Y</code>。</li>\n<li><code>caffe_axpby(N, alpha, x, beta, mutable y)</code>实现向量加法<code>Y = alpha * X + beta * Y</code></li>\n</ul>\n<p>这两个函数的用法可以参见欧氏距离loss函数中的梯度计算：</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div></pre></td><td class=\"code\"><pre><div class=\"line\">caffe_cpu_axpby(</div><div class=\"line\">    bottom[i]-&gt;count(),              <span class=\"comment\">// count</span></div><div class=\"line\">    alpha,                              <span class=\"comment\">// alpha</span></div><div class=\"line\">    diff_.cpu_data(),                   <span class=\"comment\">// a</span></div><div class=\"line\">    Dtype(<span class=\"number\">0</span>),                           <span class=\"comment\">// beta</span></div><div class=\"line\">    bottom[i]-&gt;mutable_cpu_diff());  <span class=\"comment\">// b</span></div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<p>其中，<code>bottom[i]-&gt;count()</code>给定了<code>blob</code>的大小，也就是向量的长度。<code>alpha</code>实际是由顶层<code>top_blob</code>传来的<code>loss_weight</code>，也即是<code>*top_blob-&gt;cpu_diff()/batch_size</code>。由于是直接将加权后的<code>diff</code>直接赋给<code>bottom_blob</code>的<code>cpu_diff</code>，所以，将<code>beta</code>赋值为0。</p>\n<h3 id=\"内存相关\"><a href=\"#内存相关\" class=\"headerlink\" title=\"内存相关\"></a>内存相关</h3><p>和C语言中的<code>memset()</code>和<code>memcpy()</code>类似，Caffe内也提供了对内存的拷贝与置位。使用方法也和两者相似：</p>\n<ul>\n<li><code>caffe_copy(N, x, mutable y)</code>实现向量拷贝。源地址和目标地址服从上小节的约定。</li>\n<li><code>caffe_set(N, alpha, mutable x)</code>实现向量的置位，将向量分量填充为值<code>alpha</code>。</li>\n</ul>\n<p>查看其实现可以知道，这里Caffe中直接调用了<code>memset()</code>完成任务。<br><figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">template</span> &lt;<span class=\"keyword\">typename</span> Dtype&gt;</div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">caffe_set</span><span class=\"params\">(<span class=\"keyword\">const</span> <span class=\"keyword\">int</span> N, <span class=\"keyword\">const</span> Dtype alpha, Dtype* Y)</span> </span>&#123;</div><div class=\"line\">  <span class=\"keyword\">if</span> (alpha == <span class=\"number\">0</span>) &#123;</div><div class=\"line\">    <span class=\"built_in\">memset</span>(Y, <span class=\"number\">0</span>, <span class=\"keyword\">sizeof</span>(Dtype) * N);  <span class=\"comment\">// NOLINT(caffe/alt_fn)</span></div><div class=\"line\">    <span class=\"keyword\">return</span>;</div><div class=\"line\">  &#125;</div><div class=\"line\">  <span class=\"keyword\">for</span> (<span class=\"keyword\">int</span> i = <span class=\"number\">0</span>; i &lt; N; ++i) &#123;</div><div class=\"line\">    Y[i] = alpha;</div><div class=\"line\">  &#125;</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">// 模板的特化</span></div><div class=\"line\"><span class=\"keyword\">template</span> <span class=\"keyword\">void</span> caffe_set&lt;<span class=\"keyword\">int</span>&gt;(<span class=\"keyword\">const</span> <span class=\"keyword\">int</span> N, <span class=\"keyword\">const</span> <span class=\"keyword\">int</span> alpha, <span class=\"keyword\">int</span>* Y);</div><div class=\"line\"><span class=\"keyword\">template</span> <span class=\"keyword\">void</span> caffe_set&lt;<span class=\"keyword\">float</span>&gt;(<span class=\"keyword\">const</span> <span class=\"keyword\">int</span> N, <span class=\"keyword\">const</span> <span class=\"keyword\">float</span> alpha, <span class=\"keyword\">float</span>* Y);</div><div class=\"line\"><span class=\"keyword\">template</span> <span class=\"keyword\">void</span> caffe_set&lt;<span class=\"keyword\">double</span>&gt;(<span class=\"keyword\">const</span> <span class=\"keyword\">int</span> N, <span class=\"keyword\">const</span> <span class=\"keyword\">double</span> alpha, <span class=\"keyword\">double</span>* Y);</div></pre></td></tr></table></figure></p>\n<p>而<code>caffe_copy()</code>中则是直接实现了CPU和GPU的功能。注意到下面代码中调用<code>cudaMemcpy()</code>的时候，使用了参数<code>cudaMemcpyDefault</code>。通过查阅文档，这个变量的含义是<a href=\"http://horacio9573.no-ip.org/cuda/group__CUDART__TYPES_g18fa99055ee694244a270e4d5101e95b.html\" target=\"_blank\" rel=\"external\">cudaMemcpyDefault: Default based unified virtual address space</a>。通过它，我们可以无需知道源地址和目标地址是否在CPU内存或者GPU内存上而分别处理，减少了代码负担。</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">template</span> &lt;<span class=\"keyword\">typename</span> Dtype&gt;</div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">caffe_copy</span><span class=\"params\">(<span class=\"keyword\">const</span> <span class=\"keyword\">int</span> N, <span class=\"keyword\">const</span> Dtype* X, Dtype* Y)</span> </span>&#123;</div><div class=\"line\">  <span class=\"keyword\">if</span> (X != Y) &#123;</div><div class=\"line\">    <span class=\"keyword\">if</span> (Caffe::mode() == Caffe::GPU) &#123;</div><div class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">ifndef</span> CPU_ONLY</span></div><div class=\"line\">      <span class=\"comment\">// NOLINT_NEXT_LINE(caffe/alt_fn)</span></div><div class=\"line\">      CUDA_CHECK(cudaMemcpy(Y, X, <span class=\"keyword\">sizeof</span>(Dtype) * N, cudaMemcpyDefault));</div><div class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">else</span></span></div><div class=\"line\">      NO_GPU;</div><div class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">endif</span></span></div><div class=\"line\">    &#125; <span class=\"keyword\">else</span> &#123;</div><div class=\"line\">      <span class=\"built_in\">memcpy</span>(Y, X, <span class=\"keyword\">sizeof</span>(Dtype) * N);  <span class=\"comment\">// NOLINT(caffe/alt_fn)</span></div><div class=\"line\">    &#125;</div><div class=\"line\">  &#125;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<p>所谓的unified virtual address（UVA）就是下图这个意思（见<a href=\"http://on-demand.gputechconf.com/gtc-express/2011/presentations/cuda_webinars_GPUDirect_uva.pdf\" target=\"_blank\" rel=\"external\">P2P&amp;UVA</a>）。<br><img src=\"/img/caffe_mathfunctions_whatisuva.png\" alt=\"UVA图示\"></p>\n<p>有了这个东西，可以将内存和GPU显存看做一个统一的内存空间。CUDA运行时会根据指针的值自动判断数据的实际位置。这样一来，简化了编程者的工作量，如下所示：<br><img src=\"/img/caffe_mathfunctions_useuva.png\" alt=\"How to use UVA\"></p>\n<p>其使用条件如下：<br><img src=\"/img/caffe_mathfunctions_uvarequirement.png\" alt=\"UVA Requirement\"></p>\n<h3 id=\"向量逐元素运算\"><a href=\"#向量逐元素运算\" class=\"headerlink\" title=\"向量逐元素运算\"></a>向量逐元素运算</h3><ul>\n<li><code>caffe_add(N, a, b, y)</code>函数实现<code>Y[i] = a[i] + b[i]</code>。</li>\n<li><code>caffe_sub</code>, <code>caffe_div</code>, <code>caffe_mul</code>同理。</li>\n<li><code>caffe_exp</code>, <code>caffe_powx</code>, <code>caffe_abs</code>, <code>caffe_sqr</code>, <code>caffe_log</code>相似，这里只将<code>caffe_exp()</code>的实现复制如下：</li>\n</ul>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\">// 又是模板特化</span></div><div class=\"line\"><span class=\"keyword\">template</span> &lt;&gt;</div><div class=\"line\"><span class=\"keyword\">void</span> caffe_exp&lt;<span class=\"keyword\">float</span>&gt;(<span class=\"keyword\">const</span> <span class=\"keyword\">int</span> n, <span class=\"keyword\">const</span> <span class=\"keyword\">float</span>* a, <span class=\"keyword\">float</span>* y) &#123;</div><div class=\"line\">  vsExp(n, a, y);   <span class=\"comment\">// 返回 y[i] = exp(a[i])</span></div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<ul>\n<li><code>caffe_scal</code>实现向量的数乘。这个函数常常用在<code>loss_layer</code>中计算反传的梯度，常常要乘上一个标量<code>loss_weight</code>。</li>\n<li><code>caffe_add_scalar</code>实现向量每个分量与标量相加。</li>\n</ul>\n<h2 id=\"GPU版本\"><a href=\"#GPU版本\" class=\"headerlink\" title=\"GPU版本\"></a>GPU版本</h2><p>相应地，和基于BLAS的CPU数学计算函数相似，各GPU版本的函数声明也放在了<code>math_functions.hpp</code>中，而相应的实现代码在<code>math_functions.cu</code>中。</p>\n<h2 id=\"随机数产生器\"><a href=\"#随机数产生器\" class=\"headerlink\" title=\"随机数产生器\"></a>随机数产生器</h2><p>Caffe中还提供了若干随机数产生器，可以用来做数据（如权重矩阵）的初始化等。</p>\n<p>这里，Caffe提供饿了均匀分布（uniform），高斯分布（gaussian），伯努利分布（bernoulli）的实现。这里就不再详述，使用函数<code>caffe_rng_distribution_name</code>即可。</p>\n<h2 id=\"参考资料\"><a href=\"#参考资料\" class=\"headerlink\" title=\"参考资料\"></a>参考资料</h2><p>【1】<a href=\"http://blog.csdn.net/seven_first/article/details/47378697\" target=\"_blank\" rel=\"external\">seven-first 的博客</a></p>\n","excerpt":"<p>Caffe中使用了BLAS库作为底层矩阵运算的实现，这篇文章对<a href=\"https://github.com/BVLC/caffe/blob/master/include/caffe/util/math_functions.hpp\">mathfunction.hpp 文件</a>中的相关函数做一总结。我们在自己实现layer运算的时候，也要注意是否Caffe中已经支持了类似运算，不要从scratch开始编码，自己累点不算啥，CPU/GPU的运算能力发挥不出来，更别说自己写的代码也许还是错的，那就尴尬了。。。<br><img src=\"/img/caffe_mathfunctions_gpuisnuclearweapon.jpg\" alt=\"一卡一栋楼，双卡灭地球，三卡银河系，四卡创世纪\"></p>","more":"<h2 id=\"BLAS介绍\"><a href=\"#BLAS介绍\" class=\"headerlink\" title=\"BLAS介绍\"></a>BLAS介绍</h2><p>以下内容参考<a href=\"https://en.wikipedia.org/wiki/Basic_Linear_Algebra_Subprograms\">BLAS wiki页面</a>整理。这里不涉及BLAS的过多内容，只为介绍Caffe中的相关函数做一过渡。</p>\n<p>BLAS的全称是基础线性代数子程序库（Basic Linear Algebra Subprograms），提供了一些低层次的通用线性代数运算的实现函数，如向量的相加，数乘，点积和矩阵相乘等。BLAS的实现根绝硬件平台的不同而不同，常常利用了特定处理器的硬件特点进行加速计算（例如处理器上的向量寄存器和SIMD指令集），提供了C和Fortran语言支持。</p>\n<p>不同的厂商根据自己硬件的特点，在BLAS的统一框架下，开发了自己的加速库，比如<del>AMD的ACML</del>（已经不再支持），Intel的MKL，ATLAS和OpenBLAS。其中后面的三个均可以在Caffe中配置使用。</p>\n<p>在BLAS中，实现了矩阵与矩阵相乘的函数<code>gemm</code>（GEMM: General Matrix to Matrix Multiplication）和矩阵和向量相乘的函数<code>gemv</code>，这两个数学运算的高效实现，关系到整个DL 框架的运算速度。下面这张图来源于Jia Yangqing的博士论文。<br><img src=\"/img/mathfunctions_time_distribution.png\" alt=\"前向计算中的典型时间分布\"></p>\n<p>可以看到，在前向计算过程中，无论是CPU还是GPU，大量时间都花在了卷积层和全连接层上。全连接层不必多说，就是一个输入feature和权重的矩阵乘法。卷积运算也是通过矩阵相乘实现的。因为我们可以把卷积核变成一列，和相应的feature区域做相乘（如下图，这部分可以看一下Caffe中im2col部分的介绍和代码）。<br><img src=\"/img/mathfunctions_im2col.png\" alt=\"im2col的原理\"></p>\n<p>对于BLAS和GEMM等对DL的作用意义，可以参见这篇文章<a href=\"https://petewarden.com/2015/04/20/why-gemm-is-at-the-heart-of-deep-learning/\">Why GEMM is at the heart of deep learning</a>的分析。上面的图也都来源于这篇博客。</p>\n<h2 id=\"矩阵运算函数\"><a href=\"#矩阵运算函数\" class=\"headerlink\" title=\"矩阵运算函数\"></a>矩阵运算函数</h2><p>矩阵运算函数在文件<code>math_functions.hpp</code>中可以找到。其中的函数多是对BLAS相应API的包装。这部分内容主要参考了参考资料[1]中的内容。谢谢原作者的整理。</p>\n<h3 id=\"矩阵与矩阵，矩阵与向量的乘法\"><a href=\"#矩阵与矩阵，矩阵与向量的乘法\" class=\"headerlink\" title=\"矩阵与矩阵，矩阵与向量的乘法\"></a>矩阵与矩阵，矩阵与向量的乘法</h3><p>函数<code>caffe_cpu_gemm()</code>是对BLAS中矩阵与矩阵相乘函数<code>gemm</code>的包装。与之对应的<code>caffe_cpu_gemv()</code>是对矩阵与向量相乘<code>gemv</code>函数的包装。以前者为例，其实现代码如下：</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">template</span>&lt;&gt;</div><div class=\"line\"><span class=\"keyword\">void</span> caffe_cpu_gemm&lt;<span class=\"keyword\">float</span>&gt;(<span class=\"keyword\">const</span> CBLAS_TRANSPOSE TransA,</div><div class=\"line\">    <span class=\"keyword\">const</span> CBLAS_TRANSPOSE TransB, <span class=\"keyword\">const</span> <span class=\"keyword\">int</span> M, <span class=\"keyword\">const</span> <span class=\"keyword\">int</span> N, <span class=\"keyword\">const</span> <span class=\"keyword\">int</span> K,</div><div class=\"line\">    <span class=\"keyword\">const</span> <span class=\"keyword\">float</span> alpha, <span class=\"keyword\">const</span> <span class=\"keyword\">float</span>* A, <span class=\"keyword\">const</span> <span class=\"keyword\">float</span>* B, <span class=\"keyword\">const</span> <span class=\"keyword\">float</span> beta,</div><div class=\"line\">    <span class=\"keyword\">float</span>* C) &#123;</div><div class=\"line\">  <span class=\"keyword\">int</span> lda = (TransA == CblasNoTrans) ? K : M;</div><div class=\"line\">  <span class=\"keyword\">int</span> ldb = (TransB == CblasNoTrans) ? N : K;</div><div class=\"line\">  cblas_sgemm(CblasRowMajor, TransA, TransB, M, N, K, alpha, A, lda, B,</div><div class=\"line\">      ldb, beta, C, N);</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<p>可以看到，这个函数是对单精度浮点数（Single Float）的模板特化，在函数内部调用了BLAS包中的<code>cblas_sgemm()</code>函数。其功能是计算<code>C = alpha * A * B + beta * C</code>。参数的具体含义可以查看BLAS的相关文档。</p>\n<h3 id=\"矩阵-向量的加减\"><a href=\"#矩阵-向量的加减\" class=\"headerlink\" title=\"矩阵/向量的加减\"></a>矩阵/向量的加减</h3><p>下面的函数都是将<code>X</code>指针所指的数据作为<code>src</code>，将<code>Y</code>指针所指的数据为<code>dst</code>。同时，第一个参数统一是向量的长度。</p>\n<ul>\n<li><code>caffe_axpy(N, alpha, x, mutable y)</code>实现向量加法<code>Y = alpha * X + Y</code>。</li>\n<li><code>caffe_axpby(N, alpha, x, beta, mutable y)</code>实现向量加法<code>Y = alpha * X + beta * Y</code></li>\n</ul>\n<p>这两个函数的用法可以参见欧氏距离loss函数中的梯度计算：</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div></pre></td><td class=\"code\"><pre><div class=\"line\">caffe_cpu_axpby(</div><div class=\"line\">    bottom[i]-&gt;count(),              <span class=\"comment\">// count</span></div><div class=\"line\">    alpha,                              <span class=\"comment\">// alpha</span></div><div class=\"line\">    diff_.cpu_data(),                   <span class=\"comment\">// a</span></div><div class=\"line\">    Dtype(<span class=\"number\">0</span>),                           <span class=\"comment\">// beta</span></div><div class=\"line\">    bottom[i]-&gt;mutable_cpu_diff());  <span class=\"comment\">// b</span></div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<p>其中，<code>bottom[i]-&gt;count()</code>给定了<code>blob</code>的大小，也就是向量的长度。<code>alpha</code>实际是由顶层<code>top_blob</code>传来的<code>loss_weight</code>，也即是<code>*top_blob-&gt;cpu_diff()/batch_size</code>。由于是直接将加权后的<code>diff</code>直接赋给<code>bottom_blob</code>的<code>cpu_diff</code>，所以，将<code>beta</code>赋值为0。</p>\n<h3 id=\"内存相关\"><a href=\"#内存相关\" class=\"headerlink\" title=\"内存相关\"></a>内存相关</h3><p>和C语言中的<code>memset()</code>和<code>memcpy()</code>类似，Caffe内也提供了对内存的拷贝与置位。使用方法也和两者相似：</p>\n<ul>\n<li><code>caffe_copy(N, x, mutable y)</code>实现向量拷贝。源地址和目标地址服从上小节的约定。</li>\n<li><code>caffe_set(N, alpha, mutable x)</code>实现向量的置位，将向量分量填充为值<code>alpha</code>。</li>\n</ul>\n<p>查看其实现可以知道，这里Caffe中直接调用了<code>memset()</code>完成任务。<br><figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">template</span> &lt;<span class=\"keyword\">typename</span> Dtype&gt;</div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">caffe_set</span><span class=\"params\">(<span class=\"keyword\">const</span> <span class=\"keyword\">int</span> N, <span class=\"keyword\">const</span> Dtype alpha, Dtype* Y)</span> </span>&#123;</div><div class=\"line\">  <span class=\"keyword\">if</span> (alpha == <span class=\"number\">0</span>) &#123;</div><div class=\"line\">    <span class=\"built_in\">memset</span>(Y, <span class=\"number\">0</span>, <span class=\"keyword\">sizeof</span>(Dtype) * N);  <span class=\"comment\">// NOLINT(caffe/alt_fn)</span></div><div class=\"line\">    <span class=\"keyword\">return</span>;</div><div class=\"line\">  &#125;</div><div class=\"line\">  <span class=\"keyword\">for</span> (<span class=\"keyword\">int</span> i = <span class=\"number\">0</span>; i &lt; N; ++i) &#123;</div><div class=\"line\">    Y[i] = alpha;</div><div class=\"line\">  &#125;</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">// 模板的特化</span></div><div class=\"line\"><span class=\"keyword\">template</span> <span class=\"keyword\">void</span> caffe_set&lt;<span class=\"keyword\">int</span>&gt;(<span class=\"keyword\">const</span> <span class=\"keyword\">int</span> N, <span class=\"keyword\">const</span> <span class=\"keyword\">int</span> alpha, <span class=\"keyword\">int</span>* Y);</div><div class=\"line\"><span class=\"keyword\">template</span> <span class=\"keyword\">void</span> caffe_set&lt;<span class=\"keyword\">float</span>&gt;(<span class=\"keyword\">const</span> <span class=\"keyword\">int</span> N, <span class=\"keyword\">const</span> <span class=\"keyword\">float</span> alpha, <span class=\"keyword\">float</span>* Y);</div><div class=\"line\"><span class=\"keyword\">template</span> <span class=\"keyword\">void</span> caffe_set&lt;<span class=\"keyword\">double</span>&gt;(<span class=\"keyword\">const</span> <span class=\"keyword\">int</span> N, <span class=\"keyword\">const</span> <span class=\"keyword\">double</span> alpha, <span class=\"keyword\">double</span>* Y);</div></pre></td></tr></table></figure></p>\n<p>而<code>caffe_copy()</code>中则是直接实现了CPU和GPU的功能。注意到下面代码中调用<code>cudaMemcpy()</code>的时候，使用了参数<code>cudaMemcpyDefault</code>。通过查阅文档，这个变量的含义是<a href=\"http://horacio9573.no-ip.org/cuda/group__CUDART__TYPES_g18fa99055ee694244a270e4d5101e95b.html\">cudaMemcpyDefault: Default based unified virtual address space</a>。通过它，我们可以无需知道源地址和目标地址是否在CPU内存或者GPU内存上而分别处理，减少了代码负担。</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">template</span> &lt;<span class=\"keyword\">typename</span> Dtype&gt;</div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">caffe_copy</span><span class=\"params\">(<span class=\"keyword\">const</span> <span class=\"keyword\">int</span> N, <span class=\"keyword\">const</span> Dtype* X, Dtype* Y)</span> </span>&#123;</div><div class=\"line\">  <span class=\"keyword\">if</span> (X != Y) &#123;</div><div class=\"line\">    <span class=\"keyword\">if</span> (Caffe::mode() == Caffe::GPU) &#123;</div><div class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">ifndef</span> CPU_ONLY</span></div><div class=\"line\">      <span class=\"comment\">// NOLINT_NEXT_LINE(caffe/alt_fn)</span></div><div class=\"line\">      CUDA_CHECK(cudaMemcpy(Y, X, <span class=\"keyword\">sizeof</span>(Dtype) * N, cudaMemcpyDefault));</div><div class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">else</span></span></div><div class=\"line\">      NO_GPU;</div><div class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">endif</span></span></div><div class=\"line\">    &#125; <span class=\"keyword\">else</span> &#123;</div><div class=\"line\">      <span class=\"built_in\">memcpy</span>(Y, X, <span class=\"keyword\">sizeof</span>(Dtype) * N);  <span class=\"comment\">// NOLINT(caffe/alt_fn)</span></div><div class=\"line\">    &#125;</div><div class=\"line\">  &#125;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<p>所谓的unified virtual address（UVA）就是下图这个意思（见<a href=\"http://on-demand.gputechconf.com/gtc-express/2011/presentations/cuda_webinars_GPUDirect_uva.pdf\">P2P&amp;UVA</a>）。<br><img src=\"/img/caffe_mathfunctions_whatisuva.png\" alt=\"UVA图示\"></p>\n<p>有了这个东西，可以将内存和GPU显存看做一个统一的内存空间。CUDA运行时会根据指针的值自动判断数据的实际位置。这样一来，简化了编程者的工作量，如下所示：<br><img src=\"/img/caffe_mathfunctions_useuva.png\" alt=\"How to use UVA\"></p>\n<p>其使用条件如下：<br><img src=\"/img/caffe_mathfunctions_uvarequirement.png\" alt=\"UVA Requirement\"></p>\n<h3 id=\"向量逐元素运算\"><a href=\"#向量逐元素运算\" class=\"headerlink\" title=\"向量逐元素运算\"></a>向量逐元素运算</h3><ul>\n<li><code>caffe_add(N, a, b, y)</code>函数实现<code>Y[i] = a[i] + b[i]</code>。</li>\n<li><code>caffe_sub</code>, <code>caffe_div</code>, <code>caffe_mul</code>同理。</li>\n<li><code>caffe_exp</code>, <code>caffe_powx</code>, <code>caffe_abs</code>, <code>caffe_sqr</code>, <code>caffe_log</code>相似，这里只将<code>caffe_exp()</code>的实现复制如下：</li>\n</ul>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\">// 又是模板特化</span></div><div class=\"line\"><span class=\"keyword\">template</span> &lt;&gt;</div><div class=\"line\"><span class=\"keyword\">void</span> caffe_exp&lt;<span class=\"keyword\">float</span>&gt;(<span class=\"keyword\">const</span> <span class=\"keyword\">int</span> n, <span class=\"keyword\">const</span> <span class=\"keyword\">float</span>* a, <span class=\"keyword\">float</span>* y) &#123;</div><div class=\"line\">  vsExp(n, a, y);   <span class=\"comment\">// 返回 y[i] = exp(a[i])</span></div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<ul>\n<li><code>caffe_scal</code>实现向量的数乘。这个函数常常用在<code>loss_layer</code>中计算反传的梯度，常常要乘上一个标量<code>loss_weight</code>。</li>\n<li><code>caffe_add_scalar</code>实现向量每个分量与标量相加。</li>\n</ul>\n<h2 id=\"GPU版本\"><a href=\"#GPU版本\" class=\"headerlink\" title=\"GPU版本\"></a>GPU版本</h2><p>相应地，和基于BLAS的CPU数学计算函数相似，各GPU版本的函数声明也放在了<code>math_functions.hpp</code>中，而相应的实现代码在<code>math_functions.cu</code>中。</p>\n<h2 id=\"随机数产生器\"><a href=\"#随机数产生器\" class=\"headerlink\" title=\"随机数产生器\"></a>随机数产生器</h2><p>Caffe中还提供了若干随机数产生器，可以用来做数据（如权重矩阵）的初始化等。</p>\n<p>这里，Caffe提供饿了均匀分布（uniform），高斯分布（gaussian），伯努利分布（bernoulli）的实现。这里就不再详述，使用函数<code>caffe_rng_distribution_name</code>即可。</p>\n<h2 id=\"参考资料\"><a href=\"#参考资料\" class=\"headerlink\" title=\"参考资料\"></a>参考资料</h2><p>【1】<a href=\"http://blog.csdn.net/seven_first/article/details/47378697\">seven-first 的博客</a></p>"},{"title":"PyTorch简介","date":"2017-02-25T11:23:39.000Z","_content":"这是一份阅读PyTorch教程的笔记，记录jupyter notebook的关键点。原地址位于[GitHub repo](https://github.com/pytorch/tutorials/blob/master/Deep%20Learning%20with%20PyTorch.ipynb)。\n![PyTorch Logo](/img/pytorch_logo.png)\n\n<!-- more -->\n## PyTorch简介\n[PyTorch](https://github.com/pytorch/pytorch)是一个较新的深度学习框架。从名字可以看出，其和Torch不同之处在于PyTorch使用了Python作为开发语言，所谓“Python first”。一方面，使用者可以将其作为加入了GPU支持的`numpy`，另一方面，PyTorch也是强大的深度学习框架。\n\n目前有很多深度学习框架，PyTorch主推的功能是动态网络模型。例如在Caffe中，使用者通过编写网络结构的`prototxt`进行定义，网络结构是不能变化的。而PyTorch中的网络结构不再只能是一成不变的。同时PyTorch实现了多达上百种op的自动求导（AutoGrad）。\n\n## Tensors\n`Tensor`，即`numpy`中的多维数组。上面已经提到过，PyTorch对其加入了GPU支持。同时，PyTorch中的`Tensor`可以与`numpy`中的`array`很方便地进行互相转换。\n\n通过`Tensor(shape)`便可以创建所需要大小的`tensor`。如下所示。\n\n``` py\nx = torch.Tensor(5, 3)  # construct a 5x3 matrix, uninitialized\n# 或者随机填充\ny = torch.rand(5, 3)    # construct a randomly initialized matrix\n# 使用size方法可以获得tensor的shape信息，torch.Size 可以看做 tuple\nx.size()                # out: torch.Size([5, 3])\n```\n\nPyTorch中已经实现了很多常用的`op`，如下所示。\n\n``` py\n# addition: syntax 1\nx + y                  # out: [torch.FloatTensor of size 5x3]\n\n# addition: syntax 2\ntorch.add(x, y)        # 或者使用torch包中的显式的op名称\n\n# addition: giving an output tensor\nresult = torch.Tensor(5, 3)  # 预先定义size\ntorch.add(x, y, out=result)  # 结果被填充到变量result\n\n# 对于加法运算，其实没必要这么复杂\nout = x + y                  # 无需预先定义size\n\n# torch包中带有下划线的op说明是就地进行的，如下所示\n# addition: in-place\ny.add_(x)              # 将x加到y上\n# 其他的例子: x.copy_(y), x.t_().\n```\n\nPyTorch中的元素索引方式和`numpy`相同。\n\n``` py\n# standard numpy-like indexing with all bells and whistles\nx[:,1]                 # out: [torch.FloatTensor of size 5]\n```\n\n对于更多的`op`，可以参见PyTorch的[文档页面](http://pytorch.org/docs/torch.html)。\n\n`Tensor`可以和`numpy`中的数组进行很方便地转换。并且转换前后并没有发生内存的复制（这里文档里面没有明说？），所以修改其中某一方的值，也会引起另一方的改变。如下所示。\n\n``` py\n# Tensor 转为 np.array\na = torch.ones(5)    # out: [torch.FloatTensor of size 5]\n# 使用 numpy方法即可实现转换\nb = a.numpy()        # out: array([ 1.,  1.,  1.,  1.,  1.], dtype=float32)\n# 注意！a的值的变化同样引起b的变化\na.add_(1)\nprint(a)\nprint(b)             # a b的值都变成2\n\n# np.array 转为Tensor\nimport numpy as np\na = np.ones(5)\n# 使用torch.from_numpy即可实现转换\nb = torch.from_numpy(a)  # out: [torch.DoubleTensor of size 5]\nnp.add(a, 1, out=a)\nprint(a)\nprint(b)            # a b的值都变为2\n```\n\nPyTorch中使用GPU计算很简单，通过调用`.cuda()`方法，很容易实现GPU支持。\n\n``` py\n# let us run this cell only if CUDA is available\nif torch.cuda.is_available():\n    print('cuda is avaliable')\n    x = x.cuda()\n    y = y.cuda()\n    x + y          # 在GPU上进行计算\n```\n\n## Neural Network\n说完了数据类型`Tensor`，下一步便是如何实现一个神经网络。首先，对[自动求导](http://pytorch.org/docs/autograd.html)做一说明。\n\n我们需要关注的是`autograd.Variable`。这个东西包装了`Tensor`。一旦你完成了计算，就可以使用`.backward()`方法自动得到（以该`Variable`为叶子节点的那个）网络中参数的梯度。`Variable`有一个名叫`data`的字段，可以通过它获得被包装起来的那个原始的`Tensor`数据。同时，使用`grad`字段，可以获取梯度（也是一个`Variable`）。\n\n`Variable`是计算图的节点，同时`Function`实现了变量之间的变换。它们互相联系，构成了用于计算的无环图。每个`Variable`有一个`creator`的字段，表明了它是由哪个`Function`创建的（除了用户自己显式创建的那些，这时候`creator`是`None`）。\n\n当进行反向传播计算梯度时，如果`Variable`是标量（比如最终的`loss`是欧氏距离或者交叉熵），那么`backward()`函数不需要参数。然而如果`Variable`有不止一个元素的时候，需要为其中的每个元素指明其（由上层传导来的）梯度（也就是一个和`Variable`shape匹配的`Tensor`）。看下面的说明代码。\n\n``` py\nfrom torch.autograd import Variable\nx = Variable(torch.ones(2, 2), requires_grad = True)\nx     # x 包装了一个2x2的Tensor\n\"\"\"\nVariable containing:\n 1  1\n 1  1\n[torch.FloatTensor of size 2x2]\n\"\"\"\n# Variable进行计算\n# y was created as a result of an operation,\n# so it has a creator\ny = x + 2\ny.creator    # out: <torch.autograd._functions.basic_ops.AddConstant at 0x7fa1cc158c08>\n\nz = y * y * 3  \nout = z.mean()   # out: Variable containing: 27 [torch.FloatTensor of size 1]\n\n# let's backprop now\nout.backward()  # 其实相当于 out.backward(torch.Tensor([1.0]))\n\n# print gradients d(out)/dx\nx.grad\n\"\"\"\nVariable containing:\n 4.5000  4.5000\n 4.5000  4.5000\n[torch.FloatTensor of size 2x2]\n\"\"\"\n```\n\n下面的代码就是结果不是标量，而是普通的`Tensor`的例子。\n``` py\n# 也可以通过Tensor显式地创建Variable\nx = torch.randn(3)\nx = Variable(x, requires_grad = True)\n# 一个更复杂的 op例子\ny = x * 2\nwhile y.data.norm() < 1000:\n    y = y * 2\n\n# 计算 dy/dx\ngradients = torch.FloatTensor([0.1, 1.0, 0.0001])\ny.backward(gradients)\nx.grad\n\"\"\"\nVariable containing:\n  204.8000\n 2048.0000\n    0.2048\n[torch.FloatTensor of size 3]\n\"\"\"\n```\n\n说完了NN的构成元素`Variable`，下面可以介绍如何使用PyTorch构建网络了。这部分主要使用了`torch.nn`包。我们自定义的网络结构是由若干的`layer`组成的，我们将其设置为 `nn.Module`的子类，只要使用方法`forward(input)`就可以返回网络的`output`。下面的代码展示了如何建立一个包含有`conv`和`max-pooling`和`fc`层的简单CNN网络。\n\n``` py\nimport torch.nn as nn                 # 以我的理解，貌似有参数的都在nn里面\nimport torch.nn.functional as F       # 没有参数的（如pooling和relu）都在functional里面？\n\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(1, 6, 5) # 1 input image channel, 6 output channels, 5x5 square convolution kernel\n        self.conv2 = nn.Conv2d(6, 16, 5)\n        # 一会可以看到，input是1x32x32大小的。经过计算，conv-pooling-conv-pooling后大小为16x5x5。\n        # 所以fc层的第一个参数是 16x5x5\n        self.fc1   = nn.Linear(16*5*5, 120) # an affine operation: y = Wx + b\n        self.fc2   = nn.Linear(120, 84)\n        self.fc3   = nn.Linear(84, 10)\n\n    def forward(self, x):\n        # 你可以发现，构建计算图的过程是在前向计算中完成的，也许这可以让你体会到所谓的动态图结构\n        # 同时，我们无需实现 backward，这是被自动求导实现的\n        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2)) # Max pooling over a (2, 2) window\n        x = F.max_pool2d(F.relu(self.conv2(x)), 2) # If the size is a square you can only specify a single number\n        x = x.view(-1, self.num_flat_features(x))  # 把它拉直\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n\n    def num_flat_features(self, x):\n        size = x.size()[1:] # all dimensions except the batch dimension\n        num_features = 1\n        for s in size:\n            num_features *= s\n        return num_features\n\n# 实例化Net对象\nnet = Net()\nnet     # 给出了网络结构\n\"\"\"\nNet (\n  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n  (fc1): Linear (400 -> 120)\n  (fc2): Linear (120 -> 84)\n  (fc3): Linear (84 -> 10)\n)\n\"\"\"\n```\n\n我们可以列出网络中的所有参数。\n\n``` py\nparams = list(net.parameters())\nprint(len(params))      # out: 10, 5个权重，5个bias\nprint(params[0].size())  # conv1's weight out: torch.Size([6, 1, 5, 5])\nprint(params[1].size())  # conv1's bias, out: torch.Size([6])\n```\n\n给出网络的输入，得到网络的输出。并进行反向传播梯度。\n\n``` py\ninput = Variable(torch.randn(1, 1, 32, 32))\nout = net(input)         # 重载了()运算符？\nnet.zero_grad()          # bp前，把所有参数的grad buffer清零\nout.backward(torch.randn(1, 10))\n```\n\n注意一点，`torch.nn`只支持mini-batch。所以如果你的输入只有一个样例的时候，使用`input.unsqueeze(0)`人为给它加上一个维度，让它变成一个4-D的`Tensor`。\n\n## 网络训练\n给定target和网络的output，就可以计算loss函数了。在`torch.nn`中已经[实现好了一些loss函数](http://pytorch.org/docs/nn.html#loss-functions)。\n\n``` py\noutput = net(input)\ntarget = Variable(torch.range(1, 10))  # a dummy target, for example\n# 使用平均平方误差，即欧几里得距离\ncriterion = nn.MSELoss()\nloss = criterion(output, target)\nloss\n\"\"\"\nVariable containing:\n 38.6049\n[torch.FloatTensor of size 1]\n\"\"\"\n```\n\n网络的整体结构如下所示。\n\n```\ninput -> conv2d -> relu -> maxpool2d -> conv2d -> relu -> maxpool2d  \n      -> view -> linear -> relu -> linear -> relu -> linear\n      -> MSELoss\n      -> loss\n```\n\n我们可以使用`previous_functions`来获得该节点前面`Function`的信息。\n\n```\n# For illustration, let us follow a few steps backward\nprint(loss.creator) # MSELoss\nprint(loss.creator.previous_functions[0][0]) # Linear\nprint(loss.creator.previous_functions[0][0].previous_functions[0][0]) # ReLU\n\"\"\"\n<torch.nn._functions.thnn.auto.MSELoss object at 0x7fa18011db40>\n<torch.nn._functions.linear.Linear object at 0x7fa18011da78>\n<torch.nn._functions.thnn.auto.Threshold object at 0x7fa18011d9b0>\n\"\"\"\n```\n\n进行反向传播后，让我们查看一下参数的变化。\n\n``` py\n# now we shall call loss.backward(), and have a look at conv1's bias gradients before and after the backward.\nnet.zero_grad() # zeroes the gradient buffers of all parameters\nprint('conv1.bias.grad before backward')\nprint(net.conv1.bias.grad)\nloss.backward()\nprint('conv1.bias.grad after backward')\nprint(net.conv1.bias.grad)\n```\n\n计算梯度后，自然需要更新参数了。简单的方法可以自己手写：\n\n``` py\nlearning_rate = 0.01\nfor f in net.parameters():\n    f.data.sub_(f.grad.data * learning_rate)\n```\n\n不过，`torch.optim`中已经提供了若干优化方法（SGD, Nesterov-SGD, Adam, RMSProp, etc）。如下所示。\n\n``` py\nimport torch.optim as optim\n# create your optimizer\noptimizer = optim.SGD(net.parameters(), lr = 0.01)\n# in your training loop:\noptimizer.zero_grad() # zero the gradient buffers\noutput = net(input)\nloss = criterion(output, target)\nloss.backward()\noptimizer.step() # Does the update\n```\n\n## 数据载入\n由于PyTorch的Python接口和`np.array`之间的方便转换，所以可以使用其他任何数据读入的方法（例如OpenCV等）。特别地，对于vision的数据，PyTorch提供了`torchvision`包，可以方便地载入常用的数据集（Imagenet, CIFAR10, MNIST, etc），同时提供了图像的各种变换方法。下面以CIFAR为例子。\n\n``` py\nimport torchvision\nimport torchvision.transforms as transforms\n\n# The output of torchvision datasets are PILImage images of range [0, 1].\n# We transform them to Tensors of normalized range [-1, 1]\n# Compose: Composes several transforms together.\n# see http://pytorch.org/docs/torchvision/transforms.html?highlight=transforms\ntransform=transforms.Compose([transforms.ToTensor(),\n                              transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n                             ])   # torchvision.transforms.Normalize(mean, std)\n# 读取CIFAR10数据集                             \ntrainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n# 使用DataLoader\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n                                          shuffle=True, num_workers=2)\n# Test集，设置train = False\ntestset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\ntestloader = torch.utils.data.DataLoader(testset, batch_size=4,\n                                          shuffle=False, num_workers=2)\nclasses = ('plane', 'car', 'bird', 'cat',\n           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n```\n\n接下来，我们对上面部分的CNN网络进行小修，设置第一个`conv`层接受3通道的输入。并使用交叉熵定义loss。\n\n``` py\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(3, 6, 5)\n        self.pool  = nn.MaxPool2d(2,2)\n        self.conv2 = nn.Conv2d(6, 16, 5)\n        self.fc1   = nn.Linear(16*5*5, 120)\n        self.fc2   = nn.Linear(120, 84)\n        self.fc3   = nn.Linear(84, 10)\n\n    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = x.view(-1, 16*5*5)\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n\nnet = Net()\n# use a Classification Cross-Entropy loss\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n```\n\n接下来，我们进行模型的训练。我们loop over整个dataset两次，对每个mini-batch进行参数的更新。并且设置每隔2000个mini-batch打印一次loss。\n\n``` py\nfor epoch in range(2): # loop over the dataset multiple times\n\n    running_loss = 0.0\n    for i, data in enumerate(trainloader, 0):\n        # get the inputs\n        inputs, labels = data\n\n        # wrap them in Variable\n        inputs, labels = Variable(inputs), Variable(labels)\n\n        # zero the parameter gradients\n        optimizer.zero_grad()\n\n        # forward + backward + optimize\n        outputs = net(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()        \n        optimizer.step()\n\n        # print statistics\n        running_loss += loss.data[0]\n        if i % 2000 == 1999: # print every 2000 mini-batches\n            print('[%d, %5d] loss: %.3f' % (epoch+1, i+1, running_loss / 2000))\n            running_loss = 0.0\nprint('Finished Training')\n```\n\n我们在测试集上选取一个mini-batch（也就是4张，见上面`testloader`的定义），进行测试。\n\n``` py\ndataiter = iter(testloader)\nimages, labels = dataiter.next()   # 得到image和对应的label\noutputs = net(Variable(images))\n\n# the outputs are energies for the 10 classes.\n# Higher the energy for a class, the more the network\n# thinks that the image is of the particular class\n# So, let's get the index of the highest energy\n_, predicted = torch.max(outputs.data, 1)   # 找出分数最高的对应的channel，即为top-1类别\n\nprint('Predicted: ', ' '.join('%5s'% classes[predicted[j][0]] for j in range(4)))\n```\n\n测试一下整个测试集合上的表现。\n\n``` py\ncorrect = 0\ntotal = 0\nfor data in testloader:     # 每一个test mini-batch\n    images, labels = data\n    outputs = net(Variable(images))\n    _, predicted = torch.max(outputs.data, 1)\n    total += labels.size(0)\n    correct += (predicted == labels).sum()\n\nprint('Accuracy of the network on the 10000 test images: %d %%' % (100 * correct / total))\n```\n\n对哪一类的预测精度更高呢？\n\n``` py\nclass_correct = list(0. for i in range(10))\nclass_total = list(0. for i in range(10))\nfor data in testloader:\n    images, labels = data\n    outputs = net(Variable(images))\n    _, predicted = torch.max(outputs.data, 1)\n    c = (predicted == labels).squeeze()\n    for i in range(4):\n        label = labels[i]\n        class_correct[label] += c[i]\n        class_total[label] += 1\n```\n\n上面这些训练和测试都是在CPU上进行的，如何迁移到GPU？很简单，同样用`.cuda()`方法就行了。\n\n``` py\nnet.cuda()\n```\n\n不过记得在每次训练测试的迭代中，`images`和`label`也要传送到GPU上才可以。\n\n``` py\ninputs, labels = Variable(inputs.cuda()), Variable(labels.cuda())\n```\n## 更多的例子和教程\n[更多的例子](https://github.com/pytorch/examples)\n[更多的教程](https://github.com/pytorch/tutorials)\n","source":"_posts/pytorch-tutor-01.md","raw":"---\ntitle: PyTorch简介\ndate: 2017-02-25 19:23:39\ntags:\n    - pytorch\n---\n这是一份阅读PyTorch教程的笔记，记录jupyter notebook的关键点。原地址位于[GitHub repo](https://github.com/pytorch/tutorials/blob/master/Deep%20Learning%20with%20PyTorch.ipynb)。\n![PyTorch Logo](/img/pytorch_logo.png)\n\n<!-- more -->\n## PyTorch简介\n[PyTorch](https://github.com/pytorch/pytorch)是一个较新的深度学习框架。从名字可以看出，其和Torch不同之处在于PyTorch使用了Python作为开发语言，所谓“Python first”。一方面，使用者可以将其作为加入了GPU支持的`numpy`，另一方面，PyTorch也是强大的深度学习框架。\n\n目前有很多深度学习框架，PyTorch主推的功能是动态网络模型。例如在Caffe中，使用者通过编写网络结构的`prototxt`进行定义，网络结构是不能变化的。而PyTorch中的网络结构不再只能是一成不变的。同时PyTorch实现了多达上百种op的自动求导（AutoGrad）。\n\n## Tensors\n`Tensor`，即`numpy`中的多维数组。上面已经提到过，PyTorch对其加入了GPU支持。同时，PyTorch中的`Tensor`可以与`numpy`中的`array`很方便地进行互相转换。\n\n通过`Tensor(shape)`便可以创建所需要大小的`tensor`。如下所示。\n\n``` py\nx = torch.Tensor(5, 3)  # construct a 5x3 matrix, uninitialized\n# 或者随机填充\ny = torch.rand(5, 3)    # construct a randomly initialized matrix\n# 使用size方法可以获得tensor的shape信息，torch.Size 可以看做 tuple\nx.size()                # out: torch.Size([5, 3])\n```\n\nPyTorch中已经实现了很多常用的`op`，如下所示。\n\n``` py\n# addition: syntax 1\nx + y                  # out: [torch.FloatTensor of size 5x3]\n\n# addition: syntax 2\ntorch.add(x, y)        # 或者使用torch包中的显式的op名称\n\n# addition: giving an output tensor\nresult = torch.Tensor(5, 3)  # 预先定义size\ntorch.add(x, y, out=result)  # 结果被填充到变量result\n\n# 对于加法运算，其实没必要这么复杂\nout = x + y                  # 无需预先定义size\n\n# torch包中带有下划线的op说明是就地进行的，如下所示\n# addition: in-place\ny.add_(x)              # 将x加到y上\n# 其他的例子: x.copy_(y), x.t_().\n```\n\nPyTorch中的元素索引方式和`numpy`相同。\n\n``` py\n# standard numpy-like indexing with all bells and whistles\nx[:,1]                 # out: [torch.FloatTensor of size 5]\n```\n\n对于更多的`op`，可以参见PyTorch的[文档页面](http://pytorch.org/docs/torch.html)。\n\n`Tensor`可以和`numpy`中的数组进行很方便地转换。并且转换前后并没有发生内存的复制（这里文档里面没有明说？），所以修改其中某一方的值，也会引起另一方的改变。如下所示。\n\n``` py\n# Tensor 转为 np.array\na = torch.ones(5)    # out: [torch.FloatTensor of size 5]\n# 使用 numpy方法即可实现转换\nb = a.numpy()        # out: array([ 1.,  1.,  1.,  1.,  1.], dtype=float32)\n# 注意！a的值的变化同样引起b的变化\na.add_(1)\nprint(a)\nprint(b)             # a b的值都变成2\n\n# np.array 转为Tensor\nimport numpy as np\na = np.ones(5)\n# 使用torch.from_numpy即可实现转换\nb = torch.from_numpy(a)  # out: [torch.DoubleTensor of size 5]\nnp.add(a, 1, out=a)\nprint(a)\nprint(b)            # a b的值都变为2\n```\n\nPyTorch中使用GPU计算很简单，通过调用`.cuda()`方法，很容易实现GPU支持。\n\n``` py\n# let us run this cell only if CUDA is available\nif torch.cuda.is_available():\n    print('cuda is avaliable')\n    x = x.cuda()\n    y = y.cuda()\n    x + y          # 在GPU上进行计算\n```\n\n## Neural Network\n说完了数据类型`Tensor`，下一步便是如何实现一个神经网络。首先，对[自动求导](http://pytorch.org/docs/autograd.html)做一说明。\n\n我们需要关注的是`autograd.Variable`。这个东西包装了`Tensor`。一旦你完成了计算，就可以使用`.backward()`方法自动得到（以该`Variable`为叶子节点的那个）网络中参数的梯度。`Variable`有一个名叫`data`的字段，可以通过它获得被包装起来的那个原始的`Tensor`数据。同时，使用`grad`字段，可以获取梯度（也是一个`Variable`）。\n\n`Variable`是计算图的节点，同时`Function`实现了变量之间的变换。它们互相联系，构成了用于计算的无环图。每个`Variable`有一个`creator`的字段，表明了它是由哪个`Function`创建的（除了用户自己显式创建的那些，这时候`creator`是`None`）。\n\n当进行反向传播计算梯度时，如果`Variable`是标量（比如最终的`loss`是欧氏距离或者交叉熵），那么`backward()`函数不需要参数。然而如果`Variable`有不止一个元素的时候，需要为其中的每个元素指明其（由上层传导来的）梯度（也就是一个和`Variable`shape匹配的`Tensor`）。看下面的说明代码。\n\n``` py\nfrom torch.autograd import Variable\nx = Variable(torch.ones(2, 2), requires_grad = True)\nx     # x 包装了一个2x2的Tensor\n\"\"\"\nVariable containing:\n 1  1\n 1  1\n[torch.FloatTensor of size 2x2]\n\"\"\"\n# Variable进行计算\n# y was created as a result of an operation,\n# so it has a creator\ny = x + 2\ny.creator    # out: <torch.autograd._functions.basic_ops.AddConstant at 0x7fa1cc158c08>\n\nz = y * y * 3  \nout = z.mean()   # out: Variable containing: 27 [torch.FloatTensor of size 1]\n\n# let's backprop now\nout.backward()  # 其实相当于 out.backward(torch.Tensor([1.0]))\n\n# print gradients d(out)/dx\nx.grad\n\"\"\"\nVariable containing:\n 4.5000  4.5000\n 4.5000  4.5000\n[torch.FloatTensor of size 2x2]\n\"\"\"\n```\n\n下面的代码就是结果不是标量，而是普通的`Tensor`的例子。\n``` py\n# 也可以通过Tensor显式地创建Variable\nx = torch.randn(3)\nx = Variable(x, requires_grad = True)\n# 一个更复杂的 op例子\ny = x * 2\nwhile y.data.norm() < 1000:\n    y = y * 2\n\n# 计算 dy/dx\ngradients = torch.FloatTensor([0.1, 1.0, 0.0001])\ny.backward(gradients)\nx.grad\n\"\"\"\nVariable containing:\n  204.8000\n 2048.0000\n    0.2048\n[torch.FloatTensor of size 3]\n\"\"\"\n```\n\n说完了NN的构成元素`Variable`，下面可以介绍如何使用PyTorch构建网络了。这部分主要使用了`torch.nn`包。我们自定义的网络结构是由若干的`layer`组成的，我们将其设置为 `nn.Module`的子类，只要使用方法`forward(input)`就可以返回网络的`output`。下面的代码展示了如何建立一个包含有`conv`和`max-pooling`和`fc`层的简单CNN网络。\n\n``` py\nimport torch.nn as nn                 # 以我的理解，貌似有参数的都在nn里面\nimport torch.nn.functional as F       # 没有参数的（如pooling和relu）都在functional里面？\n\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(1, 6, 5) # 1 input image channel, 6 output channels, 5x5 square convolution kernel\n        self.conv2 = nn.Conv2d(6, 16, 5)\n        # 一会可以看到，input是1x32x32大小的。经过计算，conv-pooling-conv-pooling后大小为16x5x5。\n        # 所以fc层的第一个参数是 16x5x5\n        self.fc1   = nn.Linear(16*5*5, 120) # an affine operation: y = Wx + b\n        self.fc2   = nn.Linear(120, 84)\n        self.fc3   = nn.Linear(84, 10)\n\n    def forward(self, x):\n        # 你可以发现，构建计算图的过程是在前向计算中完成的，也许这可以让你体会到所谓的动态图结构\n        # 同时，我们无需实现 backward，这是被自动求导实现的\n        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2)) # Max pooling over a (2, 2) window\n        x = F.max_pool2d(F.relu(self.conv2(x)), 2) # If the size is a square you can only specify a single number\n        x = x.view(-1, self.num_flat_features(x))  # 把它拉直\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n\n    def num_flat_features(self, x):\n        size = x.size()[1:] # all dimensions except the batch dimension\n        num_features = 1\n        for s in size:\n            num_features *= s\n        return num_features\n\n# 实例化Net对象\nnet = Net()\nnet     # 给出了网络结构\n\"\"\"\nNet (\n  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n  (fc1): Linear (400 -> 120)\n  (fc2): Linear (120 -> 84)\n  (fc3): Linear (84 -> 10)\n)\n\"\"\"\n```\n\n我们可以列出网络中的所有参数。\n\n``` py\nparams = list(net.parameters())\nprint(len(params))      # out: 10, 5个权重，5个bias\nprint(params[0].size())  # conv1's weight out: torch.Size([6, 1, 5, 5])\nprint(params[1].size())  # conv1's bias, out: torch.Size([6])\n```\n\n给出网络的输入，得到网络的输出。并进行反向传播梯度。\n\n``` py\ninput = Variable(torch.randn(1, 1, 32, 32))\nout = net(input)         # 重载了()运算符？\nnet.zero_grad()          # bp前，把所有参数的grad buffer清零\nout.backward(torch.randn(1, 10))\n```\n\n注意一点，`torch.nn`只支持mini-batch。所以如果你的输入只有一个样例的时候，使用`input.unsqueeze(0)`人为给它加上一个维度，让它变成一个4-D的`Tensor`。\n\n## 网络训练\n给定target和网络的output，就可以计算loss函数了。在`torch.nn`中已经[实现好了一些loss函数](http://pytorch.org/docs/nn.html#loss-functions)。\n\n``` py\noutput = net(input)\ntarget = Variable(torch.range(1, 10))  # a dummy target, for example\n# 使用平均平方误差，即欧几里得距离\ncriterion = nn.MSELoss()\nloss = criterion(output, target)\nloss\n\"\"\"\nVariable containing:\n 38.6049\n[torch.FloatTensor of size 1]\n\"\"\"\n```\n\n网络的整体结构如下所示。\n\n```\ninput -> conv2d -> relu -> maxpool2d -> conv2d -> relu -> maxpool2d  \n      -> view -> linear -> relu -> linear -> relu -> linear\n      -> MSELoss\n      -> loss\n```\n\n我们可以使用`previous_functions`来获得该节点前面`Function`的信息。\n\n```\n# For illustration, let us follow a few steps backward\nprint(loss.creator) # MSELoss\nprint(loss.creator.previous_functions[0][0]) # Linear\nprint(loss.creator.previous_functions[0][0].previous_functions[0][0]) # ReLU\n\"\"\"\n<torch.nn._functions.thnn.auto.MSELoss object at 0x7fa18011db40>\n<torch.nn._functions.linear.Linear object at 0x7fa18011da78>\n<torch.nn._functions.thnn.auto.Threshold object at 0x7fa18011d9b0>\n\"\"\"\n```\n\n进行反向传播后，让我们查看一下参数的变化。\n\n``` py\n# now we shall call loss.backward(), and have a look at conv1's bias gradients before and after the backward.\nnet.zero_grad() # zeroes the gradient buffers of all parameters\nprint('conv1.bias.grad before backward')\nprint(net.conv1.bias.grad)\nloss.backward()\nprint('conv1.bias.grad after backward')\nprint(net.conv1.bias.grad)\n```\n\n计算梯度后，自然需要更新参数了。简单的方法可以自己手写：\n\n``` py\nlearning_rate = 0.01\nfor f in net.parameters():\n    f.data.sub_(f.grad.data * learning_rate)\n```\n\n不过，`torch.optim`中已经提供了若干优化方法（SGD, Nesterov-SGD, Adam, RMSProp, etc）。如下所示。\n\n``` py\nimport torch.optim as optim\n# create your optimizer\noptimizer = optim.SGD(net.parameters(), lr = 0.01)\n# in your training loop:\noptimizer.zero_grad() # zero the gradient buffers\noutput = net(input)\nloss = criterion(output, target)\nloss.backward()\noptimizer.step() # Does the update\n```\n\n## 数据载入\n由于PyTorch的Python接口和`np.array`之间的方便转换，所以可以使用其他任何数据读入的方法（例如OpenCV等）。特别地，对于vision的数据，PyTorch提供了`torchvision`包，可以方便地载入常用的数据集（Imagenet, CIFAR10, MNIST, etc），同时提供了图像的各种变换方法。下面以CIFAR为例子。\n\n``` py\nimport torchvision\nimport torchvision.transforms as transforms\n\n# The output of torchvision datasets are PILImage images of range [0, 1].\n# We transform them to Tensors of normalized range [-1, 1]\n# Compose: Composes several transforms together.\n# see http://pytorch.org/docs/torchvision/transforms.html?highlight=transforms\ntransform=transforms.Compose([transforms.ToTensor(),\n                              transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n                             ])   # torchvision.transforms.Normalize(mean, std)\n# 读取CIFAR10数据集                             \ntrainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n# 使用DataLoader\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n                                          shuffle=True, num_workers=2)\n# Test集，设置train = False\ntestset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\ntestloader = torch.utils.data.DataLoader(testset, batch_size=4,\n                                          shuffle=False, num_workers=2)\nclasses = ('plane', 'car', 'bird', 'cat',\n           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n```\n\n接下来，我们对上面部分的CNN网络进行小修，设置第一个`conv`层接受3通道的输入。并使用交叉熵定义loss。\n\n``` py\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(3, 6, 5)\n        self.pool  = nn.MaxPool2d(2,2)\n        self.conv2 = nn.Conv2d(6, 16, 5)\n        self.fc1   = nn.Linear(16*5*5, 120)\n        self.fc2   = nn.Linear(120, 84)\n        self.fc3   = nn.Linear(84, 10)\n\n    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = x.view(-1, 16*5*5)\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n\nnet = Net()\n# use a Classification Cross-Entropy loss\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n```\n\n接下来，我们进行模型的训练。我们loop over整个dataset两次，对每个mini-batch进行参数的更新。并且设置每隔2000个mini-batch打印一次loss。\n\n``` py\nfor epoch in range(2): # loop over the dataset multiple times\n\n    running_loss = 0.0\n    for i, data in enumerate(trainloader, 0):\n        # get the inputs\n        inputs, labels = data\n\n        # wrap them in Variable\n        inputs, labels = Variable(inputs), Variable(labels)\n\n        # zero the parameter gradients\n        optimizer.zero_grad()\n\n        # forward + backward + optimize\n        outputs = net(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()        \n        optimizer.step()\n\n        # print statistics\n        running_loss += loss.data[0]\n        if i % 2000 == 1999: # print every 2000 mini-batches\n            print('[%d, %5d] loss: %.3f' % (epoch+1, i+1, running_loss / 2000))\n            running_loss = 0.0\nprint('Finished Training')\n```\n\n我们在测试集上选取一个mini-batch（也就是4张，见上面`testloader`的定义），进行测试。\n\n``` py\ndataiter = iter(testloader)\nimages, labels = dataiter.next()   # 得到image和对应的label\noutputs = net(Variable(images))\n\n# the outputs are energies for the 10 classes.\n# Higher the energy for a class, the more the network\n# thinks that the image is of the particular class\n# So, let's get the index of the highest energy\n_, predicted = torch.max(outputs.data, 1)   # 找出分数最高的对应的channel，即为top-1类别\n\nprint('Predicted: ', ' '.join('%5s'% classes[predicted[j][0]] for j in range(4)))\n```\n\n测试一下整个测试集合上的表现。\n\n``` py\ncorrect = 0\ntotal = 0\nfor data in testloader:     # 每一个test mini-batch\n    images, labels = data\n    outputs = net(Variable(images))\n    _, predicted = torch.max(outputs.data, 1)\n    total += labels.size(0)\n    correct += (predicted == labels).sum()\n\nprint('Accuracy of the network on the 10000 test images: %d %%' % (100 * correct / total))\n```\n\n对哪一类的预测精度更高呢？\n\n``` py\nclass_correct = list(0. for i in range(10))\nclass_total = list(0. for i in range(10))\nfor data in testloader:\n    images, labels = data\n    outputs = net(Variable(images))\n    _, predicted = torch.max(outputs.data, 1)\n    c = (predicted == labels).squeeze()\n    for i in range(4):\n        label = labels[i]\n        class_correct[label] += c[i]\n        class_total[label] += 1\n```\n\n上面这些训练和测试都是在CPU上进行的，如何迁移到GPU？很简单，同样用`.cuda()`方法就行了。\n\n``` py\nnet.cuda()\n```\n\n不过记得在每次训练测试的迭代中，`images`和`label`也要传送到GPU上才可以。\n\n``` py\ninputs, labels = Variable(inputs.cuda()), Variable(labels.cuda())\n```\n## 更多的例子和教程\n[更多的例子](https://github.com/pytorch/examples)\n[更多的教程](https://github.com/pytorch/tutorials)\n","slug":"pytorch-tutor-01","published":1,"updated":"2018-01-12T06:22:20.479Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjcu6vcty001yqu46hrp6t45r","content":"<p>这是一份阅读PyTorch教程的笔记，记录jupyter notebook的关键点。原地址位于<a href=\"https://github.com/pytorch/tutorials/blob/master/Deep%20Learning%20with%20PyTorch.ipynb\" target=\"_blank\" rel=\"external\">GitHub repo</a>。<br><img src=\"/img/pytorch_logo.png\" alt=\"PyTorch Logo\"></p>\n<a id=\"more\"></a>\n<h2 id=\"PyTorch简介\"><a href=\"#PyTorch简介\" class=\"headerlink\" title=\"PyTorch简介\"></a>PyTorch简介</h2><p><a href=\"https://github.com/pytorch/pytorch\" target=\"_blank\" rel=\"external\">PyTorch</a>是一个较新的深度学习框架。从名字可以看出，其和Torch不同之处在于PyTorch使用了Python作为开发语言，所谓“Python first”。一方面，使用者可以将其作为加入了GPU支持的<code>numpy</code>，另一方面，PyTorch也是强大的深度学习框架。</p>\n<p>目前有很多深度学习框架，PyTorch主推的功能是动态网络模型。例如在Caffe中，使用者通过编写网络结构的<code>prototxt</code>进行定义，网络结构是不能变化的。而PyTorch中的网络结构不再只能是一成不变的。同时PyTorch实现了多达上百种op的自动求导（AutoGrad）。</p>\n<h2 id=\"Tensors\"><a href=\"#Tensors\" class=\"headerlink\" title=\"Tensors\"></a>Tensors</h2><p><code>Tensor</code>，即<code>numpy</code>中的多维数组。上面已经提到过，PyTorch对其加入了GPU支持。同时，PyTorch中的<code>Tensor</code>可以与<code>numpy</code>中的<code>array</code>很方便地进行互相转换。</p>\n<p>通过<code>Tensor(shape)</code>便可以创建所需要大小的<code>tensor</code>。如下所示。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\">x = torch.Tensor(<span class=\"number\">5</span>, <span class=\"number\">3</span>)  <span class=\"comment\"># construct a 5x3 matrix, uninitialized</span></div><div class=\"line\"><span class=\"comment\"># 或者随机填充</span></div><div class=\"line\">y = torch.rand(<span class=\"number\">5</span>, <span class=\"number\">3</span>)    <span class=\"comment\"># construct a randomly initialized matrix</span></div><div class=\"line\"><span class=\"comment\"># 使用size方法可以获得tensor的shape信息，torch.Size 可以看做 tuple</span></div><div class=\"line\">x.size()                <span class=\"comment\"># out: torch.Size([5, 3])</span></div></pre></td></tr></table></figure>\n<p>PyTorch中已经实现了很多常用的<code>op</code>，如下所示。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\"># addition: syntax 1</span></div><div class=\"line\">x + y                  <span class=\"comment\"># out: [torch.FloatTensor of size 5x3]</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># addition: syntax 2</span></div><div class=\"line\">torch.add(x, y)        <span class=\"comment\"># 或者使用torch包中的显式的op名称</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># addition: giving an output tensor</span></div><div class=\"line\">result = torch.Tensor(<span class=\"number\">5</span>, <span class=\"number\">3</span>)  <span class=\"comment\"># 预先定义size</span></div><div class=\"line\">torch.add(x, y, out=result)  <span class=\"comment\"># 结果被填充到变量result</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># 对于加法运算，其实没必要这么复杂</span></div><div class=\"line\">out = x + y                  <span class=\"comment\"># 无需预先定义size</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># torch包中带有下划线的op说明是就地进行的，如下所示</span></div><div class=\"line\"><span class=\"comment\"># addition: in-place</span></div><div class=\"line\">y.add_(x)              <span class=\"comment\"># 将x加到y上</span></div><div class=\"line\"><span class=\"comment\"># 其他的例子: x.copy_(y), x.t_().</span></div></pre></td></tr></table></figure>\n<p>PyTorch中的元素索引方式和<code>numpy</code>相同。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\"># standard numpy-like indexing with all bells and whistles</span></div><div class=\"line\">x[:,<span class=\"number\">1</span>]                 <span class=\"comment\"># out: [torch.FloatTensor of size 5]</span></div></pre></td></tr></table></figure>\n<p>对于更多的<code>op</code>，可以参见PyTorch的<a href=\"http://pytorch.org/docs/torch.html\" target=\"_blank\" rel=\"external\">文档页面</a>。</p>\n<p><code>Tensor</code>可以和<code>numpy</code>中的数组进行很方便地转换。并且转换前后并没有发生内存的复制（这里文档里面没有明说？），所以修改其中某一方的值，也会引起另一方的改变。如下所示。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\"># Tensor 转为 np.array</span></div><div class=\"line\">a = torch.ones(<span class=\"number\">5</span>)    <span class=\"comment\"># out: [torch.FloatTensor of size 5]</span></div><div class=\"line\"><span class=\"comment\"># 使用 numpy方法即可实现转换</span></div><div class=\"line\">b = a.numpy()        <span class=\"comment\"># out: array([ 1.,  1.,  1.,  1.,  1.], dtype=float32)</span></div><div class=\"line\"><span class=\"comment\"># 注意！a的值的变化同样引起b的变化</span></div><div class=\"line\">a.add_(<span class=\"number\">1</span>)</div><div class=\"line\">print(a)</div><div class=\"line\">print(b)             <span class=\"comment\"># a b的值都变成2</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># np.array 转为Tensor</span></div><div class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</div><div class=\"line\">a = np.ones(<span class=\"number\">5</span>)</div><div class=\"line\"><span class=\"comment\"># 使用torch.from_numpy即可实现转换</span></div><div class=\"line\">b = torch.from_numpy(a)  <span class=\"comment\"># out: [torch.DoubleTensor of size 5]</span></div><div class=\"line\">np.add(a, <span class=\"number\">1</span>, out=a)</div><div class=\"line\">print(a)</div><div class=\"line\">print(b)            <span class=\"comment\"># a b的值都变为2</span></div></pre></td></tr></table></figure>\n<p>PyTorch中使用GPU计算很简单，通过调用<code>.cuda()</code>方法，很容易实现GPU支持。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\"># let us run this cell only if CUDA is available</span></div><div class=\"line\"><span class=\"keyword\">if</span> torch.cuda.is_available():</div><div class=\"line\">    print(<span class=\"string\">'cuda is avaliable'</span>)</div><div class=\"line\">    x = x.cuda()</div><div class=\"line\">    y = y.cuda()</div><div class=\"line\">    x + y          <span class=\"comment\"># 在GPU上进行计算</span></div></pre></td></tr></table></figure>\n<h2 id=\"Neural-Network\"><a href=\"#Neural-Network\" class=\"headerlink\" title=\"Neural Network\"></a>Neural Network</h2><p>说完了数据类型<code>Tensor</code>，下一步便是如何实现一个神经网络。首先，对<a href=\"http://pytorch.org/docs/autograd.html\" target=\"_blank\" rel=\"external\">自动求导</a>做一说明。</p>\n<p>我们需要关注的是<code>autograd.Variable</code>。这个东西包装了<code>Tensor</code>。一旦你完成了计算，就可以使用<code>.backward()</code>方法自动得到（以该<code>Variable</code>为叶子节点的那个）网络中参数的梯度。<code>Variable</code>有一个名叫<code>data</code>的字段，可以通过它获得被包装起来的那个原始的<code>Tensor</code>数据。同时，使用<code>grad</code>字段，可以获取梯度（也是一个<code>Variable</code>）。</p>\n<p><code>Variable</code>是计算图的节点，同时<code>Function</code>实现了变量之间的变换。它们互相联系，构成了用于计算的无环图。每个<code>Variable</code>有一个<code>creator</code>的字段，表明了它是由哪个<code>Function</code>创建的（除了用户自己显式创建的那些，这时候<code>creator</code>是<code>None</code>）。</p>\n<p>当进行反向传播计算梯度时，如果<code>Variable</code>是标量（比如最终的<code>loss</code>是欧氏距离或者交叉熵），那么<code>backward()</code>函数不需要参数。然而如果<code>Variable</code>有不止一个元素的时候，需要为其中的每个元素指明其（由上层传导来的）梯度（也就是一个和<code>Variable</code>shape匹配的<code>Tensor</code>）。看下面的说明代码。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">from</span> torch.autograd <span class=\"keyword\">import</span> Variable</div><div class=\"line\">x = Variable(torch.ones(<span class=\"number\">2</span>, <span class=\"number\">2</span>), requires_grad = <span class=\"keyword\">True</span>)</div><div class=\"line\">x     <span class=\"comment\"># x 包装了一个2x2的Tensor</span></div><div class=\"line\"><span class=\"string\">\"\"\"</span></div><div class=\"line\">Variable containing:</div><div class=\"line\"> 1  1</div><div class=\"line\"> 1  1</div><div class=\"line\">[torch.FloatTensor of size 2x2]</div><div class=\"line\">\"\"\"</div><div class=\"line\"><span class=\"comment\"># Variable进行计算</span></div><div class=\"line\"><span class=\"comment\"># y was created as a result of an operation,</span></div><div class=\"line\"><span class=\"comment\"># so it has a creator</span></div><div class=\"line\">y = x + <span class=\"number\">2</span></div><div class=\"line\">y.creator    <span class=\"comment\"># out: &lt;torch.autograd._functions.basic_ops.AddConstant at 0x7fa1cc158c08&gt;</span></div><div class=\"line\"></div><div class=\"line\">z = y * y * <span class=\"number\">3</span>  </div><div class=\"line\">out = z.mean()   <span class=\"comment\"># out: Variable containing: 27 [torch.FloatTensor of size 1]</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># let's backprop now</span></div><div class=\"line\">out.backward()  <span class=\"comment\"># 其实相当于 out.backward(torch.Tensor([1.0]))</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># print gradients d(out)/dx</span></div><div class=\"line\">x.grad</div><div class=\"line\"><span class=\"string\">\"\"\"</span></div><div class=\"line\">Variable containing:</div><div class=\"line\"> 4.5000  4.5000</div><div class=\"line\"> 4.5000  4.5000</div><div class=\"line\">[torch.FloatTensor of size 2x2]</div><div class=\"line\">\"\"\"</div></pre></td></tr></table></figure>\n<p>下面的代码就是结果不是标量，而是普通的<code>Tensor</code>的例子。<br><figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\"># 也可以通过Tensor显式地创建Variable</span></div><div class=\"line\">x = torch.randn(<span class=\"number\">3</span>)</div><div class=\"line\">x = Variable(x, requires_grad = <span class=\"keyword\">True</span>)</div><div class=\"line\"><span class=\"comment\"># 一个更复杂的 op例子</span></div><div class=\"line\">y = x * <span class=\"number\">2</span></div><div class=\"line\"><span class=\"keyword\">while</span> y.data.norm() &lt; <span class=\"number\">1000</span>:</div><div class=\"line\">    y = y * <span class=\"number\">2</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># 计算 dy/dx</span></div><div class=\"line\">gradients = torch.FloatTensor([<span class=\"number\">0.1</span>, <span class=\"number\">1.0</span>, <span class=\"number\">0.0001</span>])</div><div class=\"line\">y.backward(gradients)</div><div class=\"line\">x.grad</div><div class=\"line\"><span class=\"string\">\"\"\"</span></div><div class=\"line\">Variable containing:</div><div class=\"line\">  204.8000</div><div class=\"line\"> 2048.0000</div><div class=\"line\">    0.2048</div><div class=\"line\">[torch.FloatTensor of size 3]</div><div class=\"line\">\"\"\"</div></pre></td></tr></table></figure></p>\n<p>说完了NN的构成元素<code>Variable</code>，下面可以介绍如何使用PyTorch构建网络了。这部分主要使用了<code>torch.nn</code>包。我们自定义的网络结构是由若干的<code>layer</code>组成的，我们将其设置为 <code>nn.Module</code>的子类，只要使用方法<code>forward(input)</code>就可以返回网络的<code>output</code>。下面的代码展示了如何建立一个包含有<code>conv</code>和<code>max-pooling</code>和<code>fc</code>层的简单CNN网络。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">import</span> torch.nn <span class=\"keyword\">as</span> nn                 <span class=\"comment\"># 以我的理解，貌似有参数的都在nn里面</span></div><div class=\"line\"><span class=\"keyword\">import</span> torch.nn.functional <span class=\"keyword\">as</span> F       <span class=\"comment\"># 没有参数的（如pooling和relu）都在functional里面？</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Net</span><span class=\"params\">(nn.Module)</span>:</span></div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self)</span>:</span></div><div class=\"line\">        super(Net, self).__init__()</div><div class=\"line\">        self.conv1 = nn.Conv2d(<span class=\"number\">1</span>, <span class=\"number\">6</span>, <span class=\"number\">5</span>) <span class=\"comment\"># 1 input image channel, 6 output channels, 5x5 square convolution kernel</span></div><div class=\"line\">        self.conv2 = nn.Conv2d(<span class=\"number\">6</span>, <span class=\"number\">16</span>, <span class=\"number\">5</span>)</div><div class=\"line\">        <span class=\"comment\"># 一会可以看到，input是1x32x32大小的。经过计算，conv-pooling-conv-pooling后大小为16x5x5。</span></div><div class=\"line\">        <span class=\"comment\"># 所以fc层的第一个参数是 16x5x5</span></div><div class=\"line\">        self.fc1   = nn.Linear(<span class=\"number\">16</span>*<span class=\"number\">5</span>*<span class=\"number\">5</span>, <span class=\"number\">120</span>) <span class=\"comment\"># an affine operation: y = Wx + b</span></div><div class=\"line\">        self.fc2   = nn.Linear(<span class=\"number\">120</span>, <span class=\"number\">84</span>)</div><div class=\"line\">        self.fc3   = nn.Linear(<span class=\"number\">84</span>, <span class=\"number\">10</span>)</div><div class=\"line\"></div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">forward</span><span class=\"params\">(self, x)</span>:</span></div><div class=\"line\">        <span class=\"comment\"># 你可以发现，构建计算图的过程是在前向计算中完成的，也许这可以让你体会到所谓的动态图结构</span></div><div class=\"line\">        <span class=\"comment\"># 同时，我们无需实现 backward，这是被自动求导实现的</span></div><div class=\"line\">        x = F.max_pool2d(F.relu(self.conv1(x)), (<span class=\"number\">2</span>, <span class=\"number\">2</span>)) <span class=\"comment\"># Max pooling over a (2, 2) window</span></div><div class=\"line\">        x = F.max_pool2d(F.relu(self.conv2(x)), <span class=\"number\">2</span>) <span class=\"comment\"># If the size is a square you can only specify a single number</span></div><div class=\"line\">        x = x.view(<span class=\"number\">-1</span>, self.num_flat_features(x))  <span class=\"comment\"># 把它拉直</span></div><div class=\"line\">        x = F.relu(self.fc1(x))</div><div class=\"line\">        x = F.relu(self.fc2(x))</div><div class=\"line\">        x = self.fc3(x)</div><div class=\"line\">        <span class=\"keyword\">return</span> x</div><div class=\"line\"></div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">num_flat_features</span><span class=\"params\">(self, x)</span>:</span></div><div class=\"line\">        size = x.size()[<span class=\"number\">1</span>:] <span class=\"comment\"># all dimensions except the batch dimension</span></div><div class=\"line\">        num_features = <span class=\"number\">1</span></div><div class=\"line\">        <span class=\"keyword\">for</span> s <span class=\"keyword\">in</span> size:</div><div class=\"line\">            num_features *= s</div><div class=\"line\">        <span class=\"keyword\">return</span> num_features</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># 实例化Net对象</span></div><div class=\"line\">net = Net()</div><div class=\"line\">net     <span class=\"comment\"># 给出了网络结构</span></div><div class=\"line\"><span class=\"string\">\"\"\"</span></div><div class=\"line\">Net (</div><div class=\"line\">  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))</div><div class=\"line\">  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))</div><div class=\"line\">  (fc1): Linear (400 -&gt; 120)</div><div class=\"line\">  (fc2): Linear (120 -&gt; 84)</div><div class=\"line\">  (fc3): Linear (84 -&gt; 10)</div><div class=\"line\">)</div><div class=\"line\">\"\"\"</div></pre></td></tr></table></figure>\n<p>我们可以列出网络中的所有参数。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\">params = list(net.parameters())</div><div class=\"line\">print(len(params))      <span class=\"comment\"># out: 10, 5个权重，5个bias</span></div><div class=\"line\">print(params[<span class=\"number\">0</span>].size())  <span class=\"comment\"># conv1's weight out: torch.Size([6, 1, 5, 5])</span></div><div class=\"line\">print(params[<span class=\"number\">1</span>].size())  <span class=\"comment\"># conv1's bias, out: torch.Size([6])</span></div></pre></td></tr></table></figure>\n<p>给出网络的输入，得到网络的输出。并进行反向传播梯度。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\">input = Variable(torch.randn(<span class=\"number\">1</span>, <span class=\"number\">1</span>, <span class=\"number\">32</span>, <span class=\"number\">32</span>))</div><div class=\"line\">out = net(input)         <span class=\"comment\"># 重载了()运算符？</span></div><div class=\"line\">net.zero_grad()          <span class=\"comment\"># bp前，把所有参数的grad buffer清零</span></div><div class=\"line\">out.backward(torch.randn(<span class=\"number\">1</span>, <span class=\"number\">10</span>))</div></pre></td></tr></table></figure>\n<p>注意一点，<code>torch.nn</code>只支持mini-batch。所以如果你的输入只有一个样例的时候，使用<code>input.unsqueeze(0)</code>人为给它加上一个维度，让它变成一个4-D的<code>Tensor</code>。</p>\n<h2 id=\"网络训练\"><a href=\"#网络训练\" class=\"headerlink\" title=\"网络训练\"></a>网络训练</h2><p>给定target和网络的output，就可以计算loss函数了。在<code>torch.nn</code>中已经<a href=\"http://pytorch.org/docs/nn.html#loss-functions\" target=\"_blank\" rel=\"external\">实现好了一些loss函数</a>。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div></pre></td><td class=\"code\"><pre><div class=\"line\">output = net(input)</div><div class=\"line\">target = Variable(torch.range(<span class=\"number\">1</span>, <span class=\"number\">10</span>))  <span class=\"comment\"># a dummy target, for example</span></div><div class=\"line\"><span class=\"comment\"># 使用平均平方误差，即欧几里得距离</span></div><div class=\"line\">criterion = nn.MSELoss()</div><div class=\"line\">loss = criterion(output, target)</div><div class=\"line\">loss</div><div class=\"line\"><span class=\"string\">\"\"\"</span></div><div class=\"line\">Variable containing:</div><div class=\"line\"> 38.6049</div><div class=\"line\">[torch.FloatTensor of size 1]</div><div class=\"line\">\"\"\"</div></pre></td></tr></table></figure>\n<p>网络的整体结构如下所示。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\">input -&gt; conv2d -&gt; relu -&gt; maxpool2d -&gt; conv2d -&gt; relu -&gt; maxpool2d  </div><div class=\"line\">      -&gt; view -&gt; linear -&gt; relu -&gt; linear -&gt; relu -&gt; linear</div><div class=\"line\">      -&gt; MSELoss</div><div class=\"line\">      -&gt; loss</div></pre></td></tr></table></figure>\n<p>我们可以使用<code>previous_functions</code>来获得该节点前面<code>Function</code>的信息。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div></pre></td><td class=\"code\"><pre><div class=\"line\"># For illustration, let us follow a few steps backward</div><div class=\"line\">print(loss.creator) # MSELoss</div><div class=\"line\">print(loss.creator.previous_functions[0][0]) # Linear</div><div class=\"line\">print(loss.creator.previous_functions[0][0].previous_functions[0][0]) # ReLU</div><div class=\"line\">&quot;&quot;&quot;</div><div class=\"line\">&lt;torch.nn._functions.thnn.auto.MSELoss object at 0x7fa18011db40&gt;</div><div class=\"line\">&lt;torch.nn._functions.linear.Linear object at 0x7fa18011da78&gt;</div><div class=\"line\">&lt;torch.nn._functions.thnn.auto.Threshold object at 0x7fa18011d9b0&gt;</div><div class=\"line\">&quot;&quot;&quot;</div></pre></td></tr></table></figure>\n<p>进行反向传播后，让我们查看一下参数的变化。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\"># now we shall call loss.backward(), and have a look at conv1's bias gradients before and after the backward.</span></div><div class=\"line\">net.zero_grad() <span class=\"comment\"># zeroes the gradient buffers of all parameters</span></div><div class=\"line\">print(<span class=\"string\">'conv1.bias.grad before backward'</span>)</div><div class=\"line\">print(net.conv1.bias.grad)</div><div class=\"line\">loss.backward()</div><div class=\"line\">print(<span class=\"string\">'conv1.bias.grad after backward'</span>)</div><div class=\"line\">print(net.conv1.bias.grad)</div></pre></td></tr></table></figure>\n<p>计算梯度后，自然需要更新参数了。简单的方法可以自己手写：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\">learning_rate = <span class=\"number\">0.01</span></div><div class=\"line\"><span class=\"keyword\">for</span> f <span class=\"keyword\">in</span> net.parameters():</div><div class=\"line\">    f.data.sub_(f.grad.data * learning_rate)</div></pre></td></tr></table></figure>\n<p>不过，<code>torch.optim</code>中已经提供了若干优化方法（SGD, Nesterov-SGD, Adam, RMSProp, etc）。如下所示。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">import</span> torch.optim <span class=\"keyword\">as</span> optim</div><div class=\"line\"><span class=\"comment\"># create your optimizer</span></div><div class=\"line\">optimizer = optim.SGD(net.parameters(), lr = <span class=\"number\">0.01</span>)</div><div class=\"line\"><span class=\"comment\"># in your training loop:</span></div><div class=\"line\">optimizer.zero_grad() <span class=\"comment\"># zero the gradient buffers</span></div><div class=\"line\">output = net(input)</div><div class=\"line\">loss = criterion(output, target)</div><div class=\"line\">loss.backward()</div><div class=\"line\">optimizer.step() <span class=\"comment\"># Does the update</span></div></pre></td></tr></table></figure>\n<h2 id=\"数据载入\"><a href=\"#数据载入\" class=\"headerlink\" title=\"数据载入\"></a>数据载入</h2><p>由于PyTorch的Python接口和<code>np.array</code>之间的方便转换，所以可以使用其他任何数据读入的方法（例如OpenCV等）。特别地，对于vision的数据，PyTorch提供了<code>torchvision</code>包，可以方便地载入常用的数据集（Imagenet, CIFAR10, MNIST, etc），同时提供了图像的各种变换方法。下面以CIFAR为例子。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">import</span> torchvision</div><div class=\"line\"><span class=\"keyword\">import</span> torchvision.transforms <span class=\"keyword\">as</span> transforms</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># The output of torchvision datasets are PILImage images of range [0, 1].</span></div><div class=\"line\"><span class=\"comment\"># We transform them to Tensors of normalized range [-1, 1]</span></div><div class=\"line\"><span class=\"comment\"># Compose: Composes several transforms together.</span></div><div class=\"line\"><span class=\"comment\"># see http://pytorch.org/docs/torchvision/transforms.html?highlight=transforms</span></div><div class=\"line\">transform=transforms.Compose([transforms.ToTensor(),</div><div class=\"line\">                              transforms.Normalize((<span class=\"number\">0.5</span>, <span class=\"number\">0.5</span>, <span class=\"number\">0.5</span>), (<span class=\"number\">0.5</span>, <span class=\"number\">0.5</span>, <span class=\"number\">0.5</span>)),</div><div class=\"line\">                             ])   <span class=\"comment\"># torchvision.transforms.Normalize(mean, std)</span></div><div class=\"line\"><span class=\"comment\"># 读取CIFAR10数据集                             </span></div><div class=\"line\">trainset = torchvision.datasets.CIFAR10(root=<span class=\"string\">'./data'</span>, train=<span class=\"keyword\">True</span>, download=<span class=\"keyword\">True</span>, transform=transform)</div><div class=\"line\"><span class=\"comment\"># 使用DataLoader</span></div><div class=\"line\">trainloader = torch.utils.data.DataLoader(trainset, batch_size=<span class=\"number\">4</span>,</div><div class=\"line\">                                          shuffle=<span class=\"keyword\">True</span>, num_workers=<span class=\"number\">2</span>)</div><div class=\"line\"><span class=\"comment\"># Test集，设置train = False</span></div><div class=\"line\">testset = torchvision.datasets.CIFAR10(root=<span class=\"string\">'./data'</span>, train=<span class=\"keyword\">False</span>, download=<span class=\"keyword\">True</span>, transform=transform)</div><div class=\"line\">testloader = torch.utils.data.DataLoader(testset, batch_size=<span class=\"number\">4</span>,</div><div class=\"line\">                                          shuffle=<span class=\"keyword\">False</span>, num_workers=<span class=\"number\">2</span>)</div><div class=\"line\">classes = (<span class=\"string\">'plane'</span>, <span class=\"string\">'car'</span>, <span class=\"string\">'bird'</span>, <span class=\"string\">'cat'</span>,</div><div class=\"line\">           <span class=\"string\">'deer'</span>, <span class=\"string\">'dog'</span>, <span class=\"string\">'frog'</span>, <span class=\"string\">'horse'</span>, <span class=\"string\">'ship'</span>, <span class=\"string\">'truck'</span>)</div></pre></td></tr></table></figure>\n<p>接下来，我们对上面部分的CNN网络进行小修，设置第一个<code>conv</code>层接受3通道的输入。并使用交叉熵定义loss。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Net</span><span class=\"params\">(nn.Module)</span>:</span></div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self)</span>:</span></div><div class=\"line\">        super(Net, self).__init__()</div><div class=\"line\">        self.conv1 = nn.Conv2d(<span class=\"number\">3</span>, <span class=\"number\">6</span>, <span class=\"number\">5</span>)</div><div class=\"line\">        self.pool  = nn.MaxPool2d(<span class=\"number\">2</span>,<span class=\"number\">2</span>)</div><div class=\"line\">        self.conv2 = nn.Conv2d(<span class=\"number\">6</span>, <span class=\"number\">16</span>, <span class=\"number\">5</span>)</div><div class=\"line\">        self.fc1   = nn.Linear(<span class=\"number\">16</span>*<span class=\"number\">5</span>*<span class=\"number\">5</span>, <span class=\"number\">120</span>)</div><div class=\"line\">        self.fc2   = nn.Linear(<span class=\"number\">120</span>, <span class=\"number\">84</span>)</div><div class=\"line\">        self.fc3   = nn.Linear(<span class=\"number\">84</span>, <span class=\"number\">10</span>)</div><div class=\"line\"></div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">forward</span><span class=\"params\">(self, x)</span>:</span></div><div class=\"line\">        x = self.pool(F.relu(self.conv1(x)))</div><div class=\"line\">        x = self.pool(F.relu(self.conv2(x)))</div><div class=\"line\">        x = x.view(<span class=\"number\">-1</span>, <span class=\"number\">16</span>*<span class=\"number\">5</span>*<span class=\"number\">5</span>)</div><div class=\"line\">        x = F.relu(self.fc1(x))</div><div class=\"line\">        x = F.relu(self.fc2(x))</div><div class=\"line\">        x = self.fc3(x)</div><div class=\"line\">        <span class=\"keyword\">return</span> x</div><div class=\"line\"></div><div class=\"line\">net = Net()</div><div class=\"line\"><span class=\"comment\"># use a Classification Cross-Entropy loss</span></div><div class=\"line\">criterion = nn.CrossEntropyLoss()</div><div class=\"line\">optimizer = optim.SGD(net.parameters(), lr=<span class=\"number\">0.001</span>, momentum=<span class=\"number\">0.9</span>)</div></pre></td></tr></table></figure>\n<p>接下来，我们进行模型的训练。我们loop over整个dataset两次，对每个mini-batch进行参数的更新。并且设置每隔2000个mini-batch打印一次loss。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">for</span> epoch <span class=\"keyword\">in</span> range(<span class=\"number\">2</span>): <span class=\"comment\"># loop over the dataset multiple times</span></div><div class=\"line\"></div><div class=\"line\">    running_loss = <span class=\"number\">0.0</span></div><div class=\"line\">    <span class=\"keyword\">for</span> i, data <span class=\"keyword\">in</span> enumerate(trainloader, <span class=\"number\">0</span>):</div><div class=\"line\">        <span class=\"comment\"># get the inputs</span></div><div class=\"line\">        inputs, labels = data</div><div class=\"line\"></div><div class=\"line\">        <span class=\"comment\"># wrap them in Variable</span></div><div class=\"line\">        inputs, labels = Variable(inputs), Variable(labels)</div><div class=\"line\"></div><div class=\"line\">        <span class=\"comment\"># zero the parameter gradients</span></div><div class=\"line\">        optimizer.zero_grad()</div><div class=\"line\"></div><div class=\"line\">        <span class=\"comment\"># forward + backward + optimize</span></div><div class=\"line\">        outputs = net(inputs)</div><div class=\"line\">        loss = criterion(outputs, labels)</div><div class=\"line\">        loss.backward()        </div><div class=\"line\">        optimizer.step()</div><div class=\"line\"></div><div class=\"line\">        <span class=\"comment\"># print statistics</span></div><div class=\"line\">        running_loss += loss.data[<span class=\"number\">0</span>]</div><div class=\"line\">        <span class=\"keyword\">if</span> i % <span class=\"number\">2000</span> == <span class=\"number\">1999</span>: <span class=\"comment\"># print every 2000 mini-batches</span></div><div class=\"line\">            print(<span class=\"string\">'[%d, %5d] loss: %.3f'</span> % (epoch+<span class=\"number\">1</span>, i+<span class=\"number\">1</span>, running_loss / <span class=\"number\">2000</span>))</div><div class=\"line\">            running_loss = <span class=\"number\">0.0</span></div><div class=\"line\">print(<span class=\"string\">'Finished Training'</span>)</div></pre></td></tr></table></figure>\n<p>我们在测试集上选取一个mini-batch（也就是4张，见上面<code>testloader</code>的定义），进行测试。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div></pre></td><td class=\"code\"><pre><div class=\"line\">dataiter = iter(testloader)</div><div class=\"line\">images, labels = dataiter.next()   <span class=\"comment\"># 得到image和对应的label</span></div><div class=\"line\">outputs = net(Variable(images))</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># the outputs are energies for the 10 classes.</span></div><div class=\"line\"><span class=\"comment\"># Higher the energy for a class, the more the network</span></div><div class=\"line\"><span class=\"comment\"># thinks that the image is of the particular class</span></div><div class=\"line\"><span class=\"comment\"># So, let's get the index of the highest energy</span></div><div class=\"line\">_, predicted = torch.max(outputs.data, <span class=\"number\">1</span>)   <span class=\"comment\"># 找出分数最高的对应的channel，即为top-1类别</span></div><div class=\"line\"></div><div class=\"line\">print(<span class=\"string\">'Predicted: '</span>, <span class=\"string\">' '</span>.join(<span class=\"string\">'%5s'</span>% classes[predicted[j][<span class=\"number\">0</span>]] <span class=\"keyword\">for</span> j <span class=\"keyword\">in</span> range(<span class=\"number\">4</span>)))</div></pre></td></tr></table></figure>\n<p>测试一下整个测试集合上的表现。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div></pre></td><td class=\"code\"><pre><div class=\"line\">correct = <span class=\"number\">0</span></div><div class=\"line\">total = <span class=\"number\">0</span></div><div class=\"line\"><span class=\"keyword\">for</span> data <span class=\"keyword\">in</span> testloader:     <span class=\"comment\"># 每一个test mini-batch</span></div><div class=\"line\">    images, labels = data</div><div class=\"line\">    outputs = net(Variable(images))</div><div class=\"line\">    _, predicted = torch.max(outputs.data, <span class=\"number\">1</span>)</div><div class=\"line\">    total += labels.size(<span class=\"number\">0</span>)</div><div class=\"line\">    correct += (predicted == labels).sum()</div><div class=\"line\"></div><div class=\"line\">print(<span class=\"string\">'Accuracy of the network on the 10000 test images: %d %%'</span> % (<span class=\"number\">100</span> * correct / total))</div></pre></td></tr></table></figure>\n<p>对哪一类的预测精度更高呢？</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div></pre></td><td class=\"code\"><pre><div class=\"line\">class_correct = list(<span class=\"number\">0.</span> <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">10</span>))</div><div class=\"line\">class_total = list(<span class=\"number\">0.</span> <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">10</span>))</div><div class=\"line\"><span class=\"keyword\">for</span> data <span class=\"keyword\">in</span> testloader:</div><div class=\"line\">    images, labels = data</div><div class=\"line\">    outputs = net(Variable(images))</div><div class=\"line\">    _, predicted = torch.max(outputs.data, <span class=\"number\">1</span>)</div><div class=\"line\">    c = (predicted == labels).squeeze()</div><div class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">4</span>):</div><div class=\"line\">        label = labels[i]</div><div class=\"line\">        class_correct[label] += c[i]</div><div class=\"line\">        class_total[label] += <span class=\"number\">1</span></div></pre></td></tr></table></figure>\n<p>上面这些训练和测试都是在CPU上进行的，如何迁移到GPU？很简单，同样用<code>.cuda()</code>方法就行了。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">net.cuda()</div></pre></td></tr></table></figure>\n<p>不过记得在每次训练测试的迭代中，<code>images</code>和<code>label</code>也要传送到GPU上才可以。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">inputs, labels = Variable(inputs.cuda()), Variable(labels.cuda())</div></pre></td></tr></table></figure>\n<h2 id=\"更多的例子和教程\"><a href=\"#更多的例子和教程\" class=\"headerlink\" title=\"更多的例子和教程\"></a>更多的例子和教程</h2><p><a href=\"https://github.com/pytorch/examples\" target=\"_blank\" rel=\"external\">更多的例子</a><br><a href=\"https://github.com/pytorch/tutorials\" target=\"_blank\" rel=\"external\">更多的教程</a></p>\n","excerpt":"<p>这是一份阅读PyTorch教程的笔记，记录jupyter notebook的关键点。原地址位于<a href=\"https://github.com/pytorch/tutorials/blob/master/Deep%20Learning%20with%20PyTorch.ipynb\">GitHub repo</a>。<br><img src=\"/img/pytorch_logo.png\" alt=\"PyTorch Logo\"></p>","more":"<h2 id=\"PyTorch简介\"><a href=\"#PyTorch简介\" class=\"headerlink\" title=\"PyTorch简介\"></a>PyTorch简介</h2><p><a href=\"https://github.com/pytorch/pytorch\">PyTorch</a>是一个较新的深度学习框架。从名字可以看出，其和Torch不同之处在于PyTorch使用了Python作为开发语言，所谓“Python first”。一方面，使用者可以将其作为加入了GPU支持的<code>numpy</code>，另一方面，PyTorch也是强大的深度学习框架。</p>\n<p>目前有很多深度学习框架，PyTorch主推的功能是动态网络模型。例如在Caffe中，使用者通过编写网络结构的<code>prototxt</code>进行定义，网络结构是不能变化的。而PyTorch中的网络结构不再只能是一成不变的。同时PyTorch实现了多达上百种op的自动求导（AutoGrad）。</p>\n<h2 id=\"Tensors\"><a href=\"#Tensors\" class=\"headerlink\" title=\"Tensors\"></a>Tensors</h2><p><code>Tensor</code>，即<code>numpy</code>中的多维数组。上面已经提到过，PyTorch对其加入了GPU支持。同时，PyTorch中的<code>Tensor</code>可以与<code>numpy</code>中的<code>array</code>很方便地进行互相转换。</p>\n<p>通过<code>Tensor(shape)</code>便可以创建所需要大小的<code>tensor</code>。如下所示。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\">x = torch.Tensor(<span class=\"number\">5</span>, <span class=\"number\">3</span>)  <span class=\"comment\"># construct a 5x3 matrix, uninitialized</span></div><div class=\"line\"><span class=\"comment\"># 或者随机填充</span></div><div class=\"line\">y = torch.rand(<span class=\"number\">5</span>, <span class=\"number\">3</span>)    <span class=\"comment\"># construct a randomly initialized matrix</span></div><div class=\"line\"><span class=\"comment\"># 使用size方法可以获得tensor的shape信息，torch.Size 可以看做 tuple</span></div><div class=\"line\">x.size()                <span class=\"comment\"># out: torch.Size([5, 3])</span></div></pre></td></tr></table></figure>\n<p>PyTorch中已经实现了很多常用的<code>op</code>，如下所示。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\"># addition: syntax 1</span></div><div class=\"line\">x + y                  <span class=\"comment\"># out: [torch.FloatTensor of size 5x3]</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># addition: syntax 2</span></div><div class=\"line\">torch.add(x, y)        <span class=\"comment\"># 或者使用torch包中的显式的op名称</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># addition: giving an output tensor</span></div><div class=\"line\">result = torch.Tensor(<span class=\"number\">5</span>, <span class=\"number\">3</span>)  <span class=\"comment\"># 预先定义size</span></div><div class=\"line\">torch.add(x, y, out=result)  <span class=\"comment\"># 结果被填充到变量result</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># 对于加法运算，其实没必要这么复杂</span></div><div class=\"line\">out = x + y                  <span class=\"comment\"># 无需预先定义size</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># torch包中带有下划线的op说明是就地进行的，如下所示</span></div><div class=\"line\"><span class=\"comment\"># addition: in-place</span></div><div class=\"line\">y.add_(x)              <span class=\"comment\"># 将x加到y上</span></div><div class=\"line\"><span class=\"comment\"># 其他的例子: x.copy_(y), x.t_().</span></div></pre></td></tr></table></figure>\n<p>PyTorch中的元素索引方式和<code>numpy</code>相同。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\"># standard numpy-like indexing with all bells and whistles</span></div><div class=\"line\">x[:,<span class=\"number\">1</span>]                 <span class=\"comment\"># out: [torch.FloatTensor of size 5]</span></div></pre></td></tr></table></figure>\n<p>对于更多的<code>op</code>，可以参见PyTorch的<a href=\"http://pytorch.org/docs/torch.html\">文档页面</a>。</p>\n<p><code>Tensor</code>可以和<code>numpy</code>中的数组进行很方便地转换。并且转换前后并没有发生内存的复制（这里文档里面没有明说？），所以修改其中某一方的值，也会引起另一方的改变。如下所示。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\"># Tensor 转为 np.array</span></div><div class=\"line\">a = torch.ones(<span class=\"number\">5</span>)    <span class=\"comment\"># out: [torch.FloatTensor of size 5]</span></div><div class=\"line\"><span class=\"comment\"># 使用 numpy方法即可实现转换</span></div><div class=\"line\">b = a.numpy()        <span class=\"comment\"># out: array([ 1.,  1.,  1.,  1.,  1.], dtype=float32)</span></div><div class=\"line\"><span class=\"comment\"># 注意！a的值的变化同样引起b的变化</span></div><div class=\"line\">a.add_(<span class=\"number\">1</span>)</div><div class=\"line\">print(a)</div><div class=\"line\">print(b)             <span class=\"comment\"># a b的值都变成2</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># np.array 转为Tensor</span></div><div class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</div><div class=\"line\">a = np.ones(<span class=\"number\">5</span>)</div><div class=\"line\"><span class=\"comment\"># 使用torch.from_numpy即可实现转换</span></div><div class=\"line\">b = torch.from_numpy(a)  <span class=\"comment\"># out: [torch.DoubleTensor of size 5]</span></div><div class=\"line\">np.add(a, <span class=\"number\">1</span>, out=a)</div><div class=\"line\">print(a)</div><div class=\"line\">print(b)            <span class=\"comment\"># a b的值都变为2</span></div></pre></td></tr></table></figure>\n<p>PyTorch中使用GPU计算很简单，通过调用<code>.cuda()</code>方法，很容易实现GPU支持。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\"># let us run this cell only if CUDA is available</span></div><div class=\"line\"><span class=\"keyword\">if</span> torch.cuda.is_available():</div><div class=\"line\">    print(<span class=\"string\">'cuda is avaliable'</span>)</div><div class=\"line\">    x = x.cuda()</div><div class=\"line\">    y = y.cuda()</div><div class=\"line\">    x + y          <span class=\"comment\"># 在GPU上进行计算</span></div></pre></td></tr></table></figure>\n<h2 id=\"Neural-Network\"><a href=\"#Neural-Network\" class=\"headerlink\" title=\"Neural Network\"></a>Neural Network</h2><p>说完了数据类型<code>Tensor</code>，下一步便是如何实现一个神经网络。首先，对<a href=\"http://pytorch.org/docs/autograd.html\">自动求导</a>做一说明。</p>\n<p>我们需要关注的是<code>autograd.Variable</code>。这个东西包装了<code>Tensor</code>。一旦你完成了计算，就可以使用<code>.backward()</code>方法自动得到（以该<code>Variable</code>为叶子节点的那个）网络中参数的梯度。<code>Variable</code>有一个名叫<code>data</code>的字段，可以通过它获得被包装起来的那个原始的<code>Tensor</code>数据。同时，使用<code>grad</code>字段，可以获取梯度（也是一个<code>Variable</code>）。</p>\n<p><code>Variable</code>是计算图的节点，同时<code>Function</code>实现了变量之间的变换。它们互相联系，构成了用于计算的无环图。每个<code>Variable</code>有一个<code>creator</code>的字段，表明了它是由哪个<code>Function</code>创建的（除了用户自己显式创建的那些，这时候<code>creator</code>是<code>None</code>）。</p>\n<p>当进行反向传播计算梯度时，如果<code>Variable</code>是标量（比如最终的<code>loss</code>是欧氏距离或者交叉熵），那么<code>backward()</code>函数不需要参数。然而如果<code>Variable</code>有不止一个元素的时候，需要为其中的每个元素指明其（由上层传导来的）梯度（也就是一个和<code>Variable</code>shape匹配的<code>Tensor</code>）。看下面的说明代码。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">from</span> torch.autograd <span class=\"keyword\">import</span> Variable</div><div class=\"line\">x = Variable(torch.ones(<span class=\"number\">2</span>, <span class=\"number\">2</span>), requires_grad = <span class=\"keyword\">True</span>)</div><div class=\"line\">x     <span class=\"comment\"># x 包装了一个2x2的Tensor</span></div><div class=\"line\"><span class=\"string\">\"\"\"</div><div class=\"line\">Variable containing:</div><div class=\"line\"> 1  1</div><div class=\"line\"> 1  1</div><div class=\"line\">[torch.FloatTensor of size 2x2]</div><div class=\"line\">\"\"\"</span></div><div class=\"line\"><span class=\"comment\"># Variable进行计算</span></div><div class=\"line\"><span class=\"comment\"># y was created as a result of an operation,</span></div><div class=\"line\"><span class=\"comment\"># so it has a creator</span></div><div class=\"line\">y = x + <span class=\"number\">2</span></div><div class=\"line\">y.creator    <span class=\"comment\"># out: &lt;torch.autograd._functions.basic_ops.AddConstant at 0x7fa1cc158c08&gt;</span></div><div class=\"line\"></div><div class=\"line\">z = y * y * <span class=\"number\">3</span>  </div><div class=\"line\">out = z.mean()   <span class=\"comment\"># out: Variable containing: 27 [torch.FloatTensor of size 1]</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># let's backprop now</span></div><div class=\"line\">out.backward()  <span class=\"comment\"># 其实相当于 out.backward(torch.Tensor([1.0]))</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># print gradients d(out)/dx</span></div><div class=\"line\">x.grad</div><div class=\"line\"><span class=\"string\">\"\"\"</div><div class=\"line\">Variable containing:</div><div class=\"line\"> 4.5000  4.5000</div><div class=\"line\"> 4.5000  4.5000</div><div class=\"line\">[torch.FloatTensor of size 2x2]</div><div class=\"line\">\"\"\"</span></div></pre></td></tr></table></figure>\n<p>下面的代码就是结果不是标量，而是普通的<code>Tensor</code>的例子。<br><figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\"># 也可以通过Tensor显式地创建Variable</span></div><div class=\"line\">x = torch.randn(<span class=\"number\">3</span>)</div><div class=\"line\">x = Variable(x, requires_grad = <span class=\"keyword\">True</span>)</div><div class=\"line\"><span class=\"comment\"># 一个更复杂的 op例子</span></div><div class=\"line\">y = x * <span class=\"number\">2</span></div><div class=\"line\"><span class=\"keyword\">while</span> y.data.norm() &lt; <span class=\"number\">1000</span>:</div><div class=\"line\">    y = y * <span class=\"number\">2</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># 计算 dy/dx</span></div><div class=\"line\">gradients = torch.FloatTensor([<span class=\"number\">0.1</span>, <span class=\"number\">1.0</span>, <span class=\"number\">0.0001</span>])</div><div class=\"line\">y.backward(gradients)</div><div class=\"line\">x.grad</div><div class=\"line\"><span class=\"string\">\"\"\"</div><div class=\"line\">Variable containing:</div><div class=\"line\">  204.8000</div><div class=\"line\"> 2048.0000</div><div class=\"line\">    0.2048</div><div class=\"line\">[torch.FloatTensor of size 3]</div><div class=\"line\">\"\"\"</span></div></pre></td></tr></table></figure></p>\n<p>说完了NN的构成元素<code>Variable</code>，下面可以介绍如何使用PyTorch构建网络了。这部分主要使用了<code>torch.nn</code>包。我们自定义的网络结构是由若干的<code>layer</code>组成的，我们将其设置为 <code>nn.Module</code>的子类，只要使用方法<code>forward(input)</code>就可以返回网络的<code>output</code>。下面的代码展示了如何建立一个包含有<code>conv</code>和<code>max-pooling</code>和<code>fc</code>层的简单CNN网络。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">import</span> torch.nn <span class=\"keyword\">as</span> nn                 <span class=\"comment\"># 以我的理解，貌似有参数的都在nn里面</span></div><div class=\"line\"><span class=\"keyword\">import</span> torch.nn.functional <span class=\"keyword\">as</span> F       <span class=\"comment\"># 没有参数的（如pooling和relu）都在functional里面？</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Net</span><span class=\"params\">(nn.Module)</span>:</span></div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self)</span>:</span></div><div class=\"line\">        super(Net, self).__init__()</div><div class=\"line\">        self.conv1 = nn.Conv2d(<span class=\"number\">1</span>, <span class=\"number\">6</span>, <span class=\"number\">5</span>) <span class=\"comment\"># 1 input image channel, 6 output channels, 5x5 square convolution kernel</span></div><div class=\"line\">        self.conv2 = nn.Conv2d(<span class=\"number\">6</span>, <span class=\"number\">16</span>, <span class=\"number\">5</span>)</div><div class=\"line\">        <span class=\"comment\"># 一会可以看到，input是1x32x32大小的。经过计算，conv-pooling-conv-pooling后大小为16x5x5。</span></div><div class=\"line\">        <span class=\"comment\"># 所以fc层的第一个参数是 16x5x5</span></div><div class=\"line\">        self.fc1   = nn.Linear(<span class=\"number\">16</span>*<span class=\"number\">5</span>*<span class=\"number\">5</span>, <span class=\"number\">120</span>) <span class=\"comment\"># an affine operation: y = Wx + b</span></div><div class=\"line\">        self.fc2   = nn.Linear(<span class=\"number\">120</span>, <span class=\"number\">84</span>)</div><div class=\"line\">        self.fc3   = nn.Linear(<span class=\"number\">84</span>, <span class=\"number\">10</span>)</div><div class=\"line\"></div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">forward</span><span class=\"params\">(self, x)</span>:</span></div><div class=\"line\">        <span class=\"comment\"># 你可以发现，构建计算图的过程是在前向计算中完成的，也许这可以让你体会到所谓的动态图结构</span></div><div class=\"line\">        <span class=\"comment\"># 同时，我们无需实现 backward，这是被自动求导实现的</span></div><div class=\"line\">        x = F.max_pool2d(F.relu(self.conv1(x)), (<span class=\"number\">2</span>, <span class=\"number\">2</span>)) <span class=\"comment\"># Max pooling over a (2, 2) window</span></div><div class=\"line\">        x = F.max_pool2d(F.relu(self.conv2(x)), <span class=\"number\">2</span>) <span class=\"comment\"># If the size is a square you can only specify a single number</span></div><div class=\"line\">        x = x.view(<span class=\"number\">-1</span>, self.num_flat_features(x))  <span class=\"comment\"># 把它拉直</span></div><div class=\"line\">        x = F.relu(self.fc1(x))</div><div class=\"line\">        x = F.relu(self.fc2(x))</div><div class=\"line\">        x = self.fc3(x)</div><div class=\"line\">        <span class=\"keyword\">return</span> x</div><div class=\"line\"></div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">num_flat_features</span><span class=\"params\">(self, x)</span>:</span></div><div class=\"line\">        size = x.size()[<span class=\"number\">1</span>:] <span class=\"comment\"># all dimensions except the batch dimension</span></div><div class=\"line\">        num_features = <span class=\"number\">1</span></div><div class=\"line\">        <span class=\"keyword\">for</span> s <span class=\"keyword\">in</span> size:</div><div class=\"line\">            num_features *= s</div><div class=\"line\">        <span class=\"keyword\">return</span> num_features</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># 实例化Net对象</span></div><div class=\"line\">net = Net()</div><div class=\"line\">net     <span class=\"comment\"># 给出了网络结构</span></div><div class=\"line\"><span class=\"string\">\"\"\"</div><div class=\"line\">Net (</div><div class=\"line\">  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))</div><div class=\"line\">  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))</div><div class=\"line\">  (fc1): Linear (400 -&gt; 120)</div><div class=\"line\">  (fc2): Linear (120 -&gt; 84)</div><div class=\"line\">  (fc3): Linear (84 -&gt; 10)</div><div class=\"line\">)</div><div class=\"line\">\"\"\"</span></div></pre></td></tr></table></figure>\n<p>我们可以列出网络中的所有参数。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\">params = list(net.parameters())</div><div class=\"line\">print(len(params))      <span class=\"comment\"># out: 10, 5个权重，5个bias</span></div><div class=\"line\">print(params[<span class=\"number\">0</span>].size())  <span class=\"comment\"># conv1's weight out: torch.Size([6, 1, 5, 5])</span></div><div class=\"line\">print(params[<span class=\"number\">1</span>].size())  <span class=\"comment\"># conv1's bias, out: torch.Size([6])</span></div></pre></td></tr></table></figure>\n<p>给出网络的输入，得到网络的输出。并进行反向传播梯度。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\">input = Variable(torch.randn(<span class=\"number\">1</span>, <span class=\"number\">1</span>, <span class=\"number\">32</span>, <span class=\"number\">32</span>))</div><div class=\"line\">out = net(input)         <span class=\"comment\"># 重载了()运算符？</span></div><div class=\"line\">net.zero_grad()          <span class=\"comment\"># bp前，把所有参数的grad buffer清零</span></div><div class=\"line\">out.backward(torch.randn(<span class=\"number\">1</span>, <span class=\"number\">10</span>))</div></pre></td></tr></table></figure>\n<p>注意一点，<code>torch.nn</code>只支持mini-batch。所以如果你的输入只有一个样例的时候，使用<code>input.unsqueeze(0)</code>人为给它加上一个维度，让它变成一个4-D的<code>Tensor</code>。</p>\n<h2 id=\"网络训练\"><a href=\"#网络训练\" class=\"headerlink\" title=\"网络训练\"></a>网络训练</h2><p>给定target和网络的output，就可以计算loss函数了。在<code>torch.nn</code>中已经<a href=\"http://pytorch.org/docs/nn.html#loss-functions\">实现好了一些loss函数</a>。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div></pre></td><td class=\"code\"><pre><div class=\"line\">output = net(input)</div><div class=\"line\">target = Variable(torch.range(<span class=\"number\">1</span>, <span class=\"number\">10</span>))  <span class=\"comment\"># a dummy target, for example</span></div><div class=\"line\"><span class=\"comment\"># 使用平均平方误差，即欧几里得距离</span></div><div class=\"line\">criterion = nn.MSELoss()</div><div class=\"line\">loss = criterion(output, target)</div><div class=\"line\">loss</div><div class=\"line\"><span class=\"string\">\"\"\"</div><div class=\"line\">Variable containing:</div><div class=\"line\"> 38.6049</div><div class=\"line\">[torch.FloatTensor of size 1]</div><div class=\"line\">\"\"\"</span></div></pre></td></tr></table></figure>\n<p>网络的整体结构如下所示。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\">input -&gt; conv2d -&gt; relu -&gt; maxpool2d -&gt; conv2d -&gt; relu -&gt; maxpool2d  </div><div class=\"line\">      -&gt; view -&gt; linear -&gt; relu -&gt; linear -&gt; relu -&gt; linear</div><div class=\"line\">      -&gt; MSELoss</div><div class=\"line\">      -&gt; loss</div></pre></td></tr></table></figure>\n<p>我们可以使用<code>previous_functions</code>来获得该节点前面<code>Function</code>的信息。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div></pre></td><td class=\"code\"><pre><div class=\"line\"># For illustration, let us follow a few steps backward</div><div class=\"line\">print(loss.creator) # MSELoss</div><div class=\"line\">print(loss.creator.previous_functions[0][0]) # Linear</div><div class=\"line\">print(loss.creator.previous_functions[0][0].previous_functions[0][0]) # ReLU</div><div class=\"line\">&quot;&quot;&quot;</div><div class=\"line\">&lt;torch.nn._functions.thnn.auto.MSELoss object at 0x7fa18011db40&gt;</div><div class=\"line\">&lt;torch.nn._functions.linear.Linear object at 0x7fa18011da78&gt;</div><div class=\"line\">&lt;torch.nn._functions.thnn.auto.Threshold object at 0x7fa18011d9b0&gt;</div><div class=\"line\">&quot;&quot;&quot;</div></pre></td></tr></table></figure>\n<p>进行反向传播后，让我们查看一下参数的变化。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\"># now we shall call loss.backward(), and have a look at conv1's bias gradients before and after the backward.</span></div><div class=\"line\">net.zero_grad() <span class=\"comment\"># zeroes the gradient buffers of all parameters</span></div><div class=\"line\">print(<span class=\"string\">'conv1.bias.grad before backward'</span>)</div><div class=\"line\">print(net.conv1.bias.grad)</div><div class=\"line\">loss.backward()</div><div class=\"line\">print(<span class=\"string\">'conv1.bias.grad after backward'</span>)</div><div class=\"line\">print(net.conv1.bias.grad)</div></pre></td></tr></table></figure>\n<p>计算梯度后，自然需要更新参数了。简单的方法可以自己手写：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\">learning_rate = <span class=\"number\">0.01</span></div><div class=\"line\"><span class=\"keyword\">for</span> f <span class=\"keyword\">in</span> net.parameters():</div><div class=\"line\">    f.data.sub_(f.grad.data * learning_rate)</div></pre></td></tr></table></figure>\n<p>不过，<code>torch.optim</code>中已经提供了若干优化方法（SGD, Nesterov-SGD, Adam, RMSProp, etc）。如下所示。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">import</span> torch.optim <span class=\"keyword\">as</span> optim</div><div class=\"line\"><span class=\"comment\"># create your optimizer</span></div><div class=\"line\">optimizer = optim.SGD(net.parameters(), lr = <span class=\"number\">0.01</span>)</div><div class=\"line\"><span class=\"comment\"># in your training loop:</span></div><div class=\"line\">optimizer.zero_grad() <span class=\"comment\"># zero the gradient buffers</span></div><div class=\"line\">output = net(input)</div><div class=\"line\">loss = criterion(output, target)</div><div class=\"line\">loss.backward()</div><div class=\"line\">optimizer.step() <span class=\"comment\"># Does the update</span></div></pre></td></tr></table></figure>\n<h2 id=\"数据载入\"><a href=\"#数据载入\" class=\"headerlink\" title=\"数据载入\"></a>数据载入</h2><p>由于PyTorch的Python接口和<code>np.array</code>之间的方便转换，所以可以使用其他任何数据读入的方法（例如OpenCV等）。特别地，对于vision的数据，PyTorch提供了<code>torchvision</code>包，可以方便地载入常用的数据集（Imagenet, CIFAR10, MNIST, etc），同时提供了图像的各种变换方法。下面以CIFAR为例子。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">import</span> torchvision</div><div class=\"line\"><span class=\"keyword\">import</span> torchvision.transforms <span class=\"keyword\">as</span> transforms</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># The output of torchvision datasets are PILImage images of range [0, 1].</span></div><div class=\"line\"><span class=\"comment\"># We transform them to Tensors of normalized range [-1, 1]</span></div><div class=\"line\"><span class=\"comment\"># Compose: Composes several transforms together.</span></div><div class=\"line\"><span class=\"comment\"># see http://pytorch.org/docs/torchvision/transforms.html?highlight=transforms</span></div><div class=\"line\">transform=transforms.Compose([transforms.ToTensor(),</div><div class=\"line\">                              transforms.Normalize((<span class=\"number\">0.5</span>, <span class=\"number\">0.5</span>, <span class=\"number\">0.5</span>), (<span class=\"number\">0.5</span>, <span class=\"number\">0.5</span>, <span class=\"number\">0.5</span>)),</div><div class=\"line\">                             ])   <span class=\"comment\"># torchvision.transforms.Normalize(mean, std)</span></div><div class=\"line\"><span class=\"comment\"># 读取CIFAR10数据集                             </span></div><div class=\"line\">trainset = torchvision.datasets.CIFAR10(root=<span class=\"string\">'./data'</span>, train=<span class=\"keyword\">True</span>, download=<span class=\"keyword\">True</span>, transform=transform)</div><div class=\"line\"><span class=\"comment\"># 使用DataLoader</span></div><div class=\"line\">trainloader = torch.utils.data.DataLoader(trainset, batch_size=<span class=\"number\">4</span>,</div><div class=\"line\">                                          shuffle=<span class=\"keyword\">True</span>, num_workers=<span class=\"number\">2</span>)</div><div class=\"line\"><span class=\"comment\"># Test集，设置train = False</span></div><div class=\"line\">testset = torchvision.datasets.CIFAR10(root=<span class=\"string\">'./data'</span>, train=<span class=\"keyword\">False</span>, download=<span class=\"keyword\">True</span>, transform=transform)</div><div class=\"line\">testloader = torch.utils.data.DataLoader(testset, batch_size=<span class=\"number\">4</span>,</div><div class=\"line\">                                          shuffle=<span class=\"keyword\">False</span>, num_workers=<span class=\"number\">2</span>)</div><div class=\"line\">classes = (<span class=\"string\">'plane'</span>, <span class=\"string\">'car'</span>, <span class=\"string\">'bird'</span>, <span class=\"string\">'cat'</span>,</div><div class=\"line\">           <span class=\"string\">'deer'</span>, <span class=\"string\">'dog'</span>, <span class=\"string\">'frog'</span>, <span class=\"string\">'horse'</span>, <span class=\"string\">'ship'</span>, <span class=\"string\">'truck'</span>)</div></pre></td></tr></table></figure>\n<p>接下来，我们对上面部分的CNN网络进行小修，设置第一个<code>conv</code>层接受3通道的输入。并使用交叉熵定义loss。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Net</span><span class=\"params\">(nn.Module)</span>:</span></div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self)</span>:</span></div><div class=\"line\">        super(Net, self).__init__()</div><div class=\"line\">        self.conv1 = nn.Conv2d(<span class=\"number\">3</span>, <span class=\"number\">6</span>, <span class=\"number\">5</span>)</div><div class=\"line\">        self.pool  = nn.MaxPool2d(<span class=\"number\">2</span>,<span class=\"number\">2</span>)</div><div class=\"line\">        self.conv2 = nn.Conv2d(<span class=\"number\">6</span>, <span class=\"number\">16</span>, <span class=\"number\">5</span>)</div><div class=\"line\">        self.fc1   = nn.Linear(<span class=\"number\">16</span>*<span class=\"number\">5</span>*<span class=\"number\">5</span>, <span class=\"number\">120</span>)</div><div class=\"line\">        self.fc2   = nn.Linear(<span class=\"number\">120</span>, <span class=\"number\">84</span>)</div><div class=\"line\">        self.fc3   = nn.Linear(<span class=\"number\">84</span>, <span class=\"number\">10</span>)</div><div class=\"line\"></div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">forward</span><span class=\"params\">(self, x)</span>:</span></div><div class=\"line\">        x = self.pool(F.relu(self.conv1(x)))</div><div class=\"line\">        x = self.pool(F.relu(self.conv2(x)))</div><div class=\"line\">        x = x.view(<span class=\"number\">-1</span>, <span class=\"number\">16</span>*<span class=\"number\">5</span>*<span class=\"number\">5</span>)</div><div class=\"line\">        x = F.relu(self.fc1(x))</div><div class=\"line\">        x = F.relu(self.fc2(x))</div><div class=\"line\">        x = self.fc3(x)</div><div class=\"line\">        <span class=\"keyword\">return</span> x</div><div class=\"line\"></div><div class=\"line\">net = Net()</div><div class=\"line\"><span class=\"comment\"># use a Classification Cross-Entropy loss</span></div><div class=\"line\">criterion = nn.CrossEntropyLoss()</div><div class=\"line\">optimizer = optim.SGD(net.parameters(), lr=<span class=\"number\">0.001</span>, momentum=<span class=\"number\">0.9</span>)</div></pre></td></tr></table></figure>\n<p>接下来，我们进行模型的训练。我们loop over整个dataset两次，对每个mini-batch进行参数的更新。并且设置每隔2000个mini-batch打印一次loss。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">for</span> epoch <span class=\"keyword\">in</span> range(<span class=\"number\">2</span>): <span class=\"comment\"># loop over the dataset multiple times</span></div><div class=\"line\"></div><div class=\"line\">    running_loss = <span class=\"number\">0.0</span></div><div class=\"line\">    <span class=\"keyword\">for</span> i, data <span class=\"keyword\">in</span> enumerate(trainloader, <span class=\"number\">0</span>):</div><div class=\"line\">        <span class=\"comment\"># get the inputs</span></div><div class=\"line\">        inputs, labels = data</div><div class=\"line\"></div><div class=\"line\">        <span class=\"comment\"># wrap them in Variable</span></div><div class=\"line\">        inputs, labels = Variable(inputs), Variable(labels)</div><div class=\"line\"></div><div class=\"line\">        <span class=\"comment\"># zero the parameter gradients</span></div><div class=\"line\">        optimizer.zero_grad()</div><div class=\"line\"></div><div class=\"line\">        <span class=\"comment\"># forward + backward + optimize</span></div><div class=\"line\">        outputs = net(inputs)</div><div class=\"line\">        loss = criterion(outputs, labels)</div><div class=\"line\">        loss.backward()        </div><div class=\"line\">        optimizer.step()</div><div class=\"line\"></div><div class=\"line\">        <span class=\"comment\"># print statistics</span></div><div class=\"line\">        running_loss += loss.data[<span class=\"number\">0</span>]</div><div class=\"line\">        <span class=\"keyword\">if</span> i % <span class=\"number\">2000</span> == <span class=\"number\">1999</span>: <span class=\"comment\"># print every 2000 mini-batches</span></div><div class=\"line\">            print(<span class=\"string\">'[%d, %5d] loss: %.3f'</span> % (epoch+<span class=\"number\">1</span>, i+<span class=\"number\">1</span>, running_loss / <span class=\"number\">2000</span>))</div><div class=\"line\">            running_loss = <span class=\"number\">0.0</span></div><div class=\"line\">print(<span class=\"string\">'Finished Training'</span>)</div></pre></td></tr></table></figure>\n<p>我们在测试集上选取一个mini-batch（也就是4张，见上面<code>testloader</code>的定义），进行测试。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div></pre></td><td class=\"code\"><pre><div class=\"line\">dataiter = iter(testloader)</div><div class=\"line\">images, labels = dataiter.next()   <span class=\"comment\"># 得到image和对应的label</span></div><div class=\"line\">outputs = net(Variable(images))</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># the outputs are energies for the 10 classes.</span></div><div class=\"line\"><span class=\"comment\"># Higher the energy for a class, the more the network</span></div><div class=\"line\"><span class=\"comment\"># thinks that the image is of the particular class</span></div><div class=\"line\"><span class=\"comment\"># So, let's get the index of the highest energy</span></div><div class=\"line\">_, predicted = torch.max(outputs.data, <span class=\"number\">1</span>)   <span class=\"comment\"># 找出分数最高的对应的channel，即为top-1类别</span></div><div class=\"line\"></div><div class=\"line\">print(<span class=\"string\">'Predicted: '</span>, <span class=\"string\">' '</span>.join(<span class=\"string\">'%5s'</span>% classes[predicted[j][<span class=\"number\">0</span>]] <span class=\"keyword\">for</span> j <span class=\"keyword\">in</span> range(<span class=\"number\">4</span>)))</div></pre></td></tr></table></figure>\n<p>测试一下整个测试集合上的表现。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div></pre></td><td class=\"code\"><pre><div class=\"line\">correct = <span class=\"number\">0</span></div><div class=\"line\">total = <span class=\"number\">0</span></div><div class=\"line\"><span class=\"keyword\">for</span> data <span class=\"keyword\">in</span> testloader:     <span class=\"comment\"># 每一个test mini-batch</span></div><div class=\"line\">    images, labels = data</div><div class=\"line\">    outputs = net(Variable(images))</div><div class=\"line\">    _, predicted = torch.max(outputs.data, <span class=\"number\">1</span>)</div><div class=\"line\">    total += labels.size(<span class=\"number\">0</span>)</div><div class=\"line\">    correct += (predicted == labels).sum()</div><div class=\"line\"></div><div class=\"line\">print(<span class=\"string\">'Accuracy of the network on the 10000 test images: %d %%'</span> % (<span class=\"number\">100</span> * correct / total))</div></pre></td></tr></table></figure>\n<p>对哪一类的预测精度更高呢？</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div></pre></td><td class=\"code\"><pre><div class=\"line\">class_correct = list(<span class=\"number\">0.</span> <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">10</span>))</div><div class=\"line\">class_total = list(<span class=\"number\">0.</span> <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">10</span>))</div><div class=\"line\"><span class=\"keyword\">for</span> data <span class=\"keyword\">in</span> testloader:</div><div class=\"line\">    images, labels = data</div><div class=\"line\">    outputs = net(Variable(images))</div><div class=\"line\">    _, predicted = torch.max(outputs.data, <span class=\"number\">1</span>)</div><div class=\"line\">    c = (predicted == labels).squeeze()</div><div class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">4</span>):</div><div class=\"line\">        label = labels[i]</div><div class=\"line\">        class_correct[label] += c[i]</div><div class=\"line\">        class_total[label] += <span class=\"number\">1</span></div></pre></td></tr></table></figure>\n<p>上面这些训练和测试都是在CPU上进行的，如何迁移到GPU？很简单，同样用<code>.cuda()</code>方法就行了。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">net.cuda()</div></pre></td></tr></table></figure>\n<p>不过记得在每次训练测试的迭代中，<code>images</code>和<code>label</code>也要传送到GPU上才可以。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">inputs, labels = Variable(inputs.cuda()), Variable(labels.cuda())</div></pre></td></tr></table></figure>\n<h2 id=\"更多的例子和教程\"><a href=\"#更多的例子和教程\" class=\"headerlink\" title=\"更多的例子和教程\"></a>更多的例子和教程</h2><p><a href=\"https://github.com/pytorch/examples\">更多的例子</a><br><a href=\"https://github.com/pytorch/tutorials\">更多的教程</a></p>"},{"title":"Python Regular Expressions （Python 正则表达式)","date":"2014-07-17T11:00:00.000Z","_content":"\n本文来自于Google Developers中对于Python的介绍。[https://developers.google.com/edu/python/regular-expressions](https://developers.google.com/edu/python/regular-expressions \"Google Python Class, Regular Expression\")。\n\n![regex](/img/regex_picture.jpg)\n<!-- more -->\n\n## 认识正则表达式\n\nPython的正则表达式是使用 **re 模块**的。\n\n``` py    \n    match = re.search(pattern,str)\n    if match:\n    \tprint 'found',match.group()\n    else:\n        print 'NOT Found!'\n```\n\n## 正则表达式的规则\n\n### 基本规则\n- a, x, 9 都是普通字符 (ordinary characters)\n- . (一个点)可以匹配任何单个字符（除了'\\n'）\n- \\w（小写的w）可以匹配一个单词里面的字母，数字或下划线 [a-zA-Z0-9_];\\W （大写的W）可以匹配非单词里的这些元素\n- \\b 匹配单词与非单词的分界\n- \\s（小写的s）匹配一个 whitespace character，包括 space，newline，return，tab，form(\\n\\r\\t\\f)；\\S（大写的S）匹配一个非 whitespace character\n- \\d 匹配十进制数字 [0-9]\n- ^=start，$=end 用来匹配字符串的开始和结束\n- \\ 是转义字符，用 \\. 来匹配串里的'.'，等\n### 一些基本的例子\n\n``` py\n    ## 在字符串'piiig'中查找'iii'\n    match = re.search(r'iii', 'piiig')  # found, match.group() == \"iii\"\n    match = re.search(r'igs', 'piiig')  #  not found, match == None\n\n    ## . 匹配除了\\n的任意字符\n    match = re.search(r'..g', 'piiig')  #  found, match.group() == \"iig\"\n\n    ## \\d 匹配0-9的数字字符, \\w 匹配单词里的字符\n    match = re.search(r'\\d\\d\\d', 'p123g') #  found, match.group() == \"123\"\n    match = re.search(r'\\w\\w\\w', '@@abcd!!') #  found, match.group() == \"abc\"   \n```\n\n### 重复\n可以用'+' '*' '?'来匹配0个，1个或多个重复字符。\n\n- '+' 用来匹配1个或者多个字符\n- '*' 用来匹配0个或者多个字符\n- '?' 用来匹配0个或1个字符\n\n注意，'+'和'*'会匹配尽可能多的字符。\n\n### 一些重复字符的例子\n\n``` py\n    ## i+  匹配1个或者多个'i'\n    match = re.search(r'pi+', 'piiig') #  found, match.group() == \"piii\"\n\n    ## 找到字符串中最左边尽可能长的模式。\n    ## 注意，并没有匹配到第二个 'i+'\n    match = re.search(r'i+', 'piigiiii')  #  found, match.group() == \"ii\"\n\n    ## \\s*  匹配0个或1个空白字符 whitespace\n    match = re.search(r'\\d\\s*\\d\\s*\\d', 'xx1 2   3xx')  #  found, match.group() == \"1 2   3\"\n    match = re.search(r'\\d\\s*\\d\\s*\\d', 'xx12  3xx')    #  found, match.group() == \"12  3\"\n    match = re.search(r'\\d\\s*\\d\\s*\\d', 'xx123xx')      # found, match.group() == \"123\"\n\n    ## ^ 匹配字符串的第一个字符\n    match = re.search(r'^b\\w+', 'foobar')  # not found, match == None\n    ## 与上例对比\n    match = re.search(r'b\\w+', 'foobar')   # found, match.group() == \"bar\"\n```\n\n### Email\n考虑一个典型的Email地址：`someone@host.com`，可以用如下的方式匹配：\n\n```py\n    match = re.search(r'\\w+@\\w+',str)\n```\n\n但是，对于这种Email地址 `xyz alice-b@google.com purple monkey`则不能奏效。\n\n### 使用方括号\n方括号里面的字符表示一个字符集合。[abc]可以被用来匹配'a'或者'b'或者'c'。\\w \\s等都可以用在方括号里，除了'.'以外，它只能用来表示字面意义上的‘点’。所以上面的Email规则可以扩充如下：\n\n``` py\n    match = re.search('r[\\w.-]+@[\\w.-]+',str)\n```\n\n你还可以使用'-'来指定范围，如[a-z]指示的是所有小写字母的集合。所以如果你想构造的字符集合中有'-'，请把它放到末尾[ab-]。另外，前方加上'^'，用来表示取集合的补集，例如[^ab]表示除了'a'和'b'之外的其他字符。\n\n## 操作\n以Email地址为例，如果我们想要分别提取该地址的用户名'someone'和主机名'host.com'该怎么办呢？\n可以在模式中用圆括号指定。\n\n``` py\n    str = 'purple alice-b@google.com monkey dishwasher'\n    match = re.search('([\\w.-]+)@([\\w.-]+)', str)   #用圆括号指定分割\n    if match:\n        print match.group()   ## 'alice-b@google.com' (the whole match)\n        print match.group(1)  ## 'alice-b' (the username, group 1)\n      \tprint match.group(2)  ## 'google.com' (the host, group 2)\n```\n\n### findall 函数\n与group函数只找到最左端的一个匹配不同，findall函数找到字符串中所有与模式匹配的串。\n\n``` py\n    str = 'purple alice@google.com, blah monkey bob@abc.com blah dishwasher'\n    ## findall返回一个包含所有匹配结果的 list\n    emails = re.findall(r'[\\w\\.-]+@[\\w\\.-]+', str) ## ['alice@google.com', 'bob@abc.com']\n    for email in emails:\n        print email\n```\n\n### 在文件中使用findall\n当然可以读入文件的每一行，然后对每一行的内容调用findall，但是为什么不让这一切自动发生呢？\n\n``` py\n\tf = open(filename.txt,'r')\n\tmatches = re.findall(pattern,f.read())\n```\n\n### findall 和分组\n和group的用法相似，也可以指定分组。\n\n``` py\n    str = 'purple alice@google.com, blah monkey bob@abc.com blah dishwasher'\n    ##　返回了一个list\n    tuples = re.findall(r'([\\w\\.-]+)@([\\w\\.-]+)', str)\n    print tuples  ## [('alice', 'google.com'), ('bob', 'abc.com')]\n    ##　list中的元素是tuple\n    for tuple in tuples:\n      print tuple[0]  ## username\n      print tuple[1]  ## host\n```\n\n## 调试\n\n正则表达式异常强大，使用简单的几条规则就可以演变出很多的模式组合。在确定你的模式之前，可能需要很多的调试工作。在一个小的测试集合上测试正则表达式。\n\n## 其他选项\n\n正则表达式还可以设置“选项”。\n\n``` py\n    match = re.search(pat,str,opt)\n```\n\n这些可选项如下：\n\n- IGNORECASE  忽视大小写\n- DOTALL  允许'.'匹配'\\n'\n- MULTILINE  在一个由许多行组成的字符串中，允许'^'和'$'匹配每一行的开始和结束\n","source":"_posts/python-reg-exp.md","raw":"---\ntitle: Python Regular Expressions （Python 正则表达式)\ndate: 2014-07-17 19:00:00\ntags:\n    - python\n---\n\n本文来自于Google Developers中对于Python的介绍。[https://developers.google.com/edu/python/regular-expressions](https://developers.google.com/edu/python/regular-expressions \"Google Python Class, Regular Expression\")。\n\n![regex](/img/regex_picture.jpg)\n<!-- more -->\n\n## 认识正则表达式\n\nPython的正则表达式是使用 **re 模块**的。\n\n``` py    \n    match = re.search(pattern,str)\n    if match:\n    \tprint 'found',match.group()\n    else:\n        print 'NOT Found!'\n```\n\n## 正则表达式的规则\n\n### 基本规则\n- a, x, 9 都是普通字符 (ordinary characters)\n- . (一个点)可以匹配任何单个字符（除了'\\n'）\n- \\w（小写的w）可以匹配一个单词里面的字母，数字或下划线 [a-zA-Z0-9_];\\W （大写的W）可以匹配非单词里的这些元素\n- \\b 匹配单词与非单词的分界\n- \\s（小写的s）匹配一个 whitespace character，包括 space，newline，return，tab，form(\\n\\r\\t\\f)；\\S（大写的S）匹配一个非 whitespace character\n- \\d 匹配十进制数字 [0-9]\n- ^=start，$=end 用来匹配字符串的开始和结束\n- \\ 是转义字符，用 \\. 来匹配串里的'.'，等\n### 一些基本的例子\n\n``` py\n    ## 在字符串'piiig'中查找'iii'\n    match = re.search(r'iii', 'piiig')  # found, match.group() == \"iii\"\n    match = re.search(r'igs', 'piiig')  #  not found, match == None\n\n    ## . 匹配除了\\n的任意字符\n    match = re.search(r'..g', 'piiig')  #  found, match.group() == \"iig\"\n\n    ## \\d 匹配0-9的数字字符, \\w 匹配单词里的字符\n    match = re.search(r'\\d\\d\\d', 'p123g') #  found, match.group() == \"123\"\n    match = re.search(r'\\w\\w\\w', '@@abcd!!') #  found, match.group() == \"abc\"   \n```\n\n### 重复\n可以用'+' '*' '?'来匹配0个，1个或多个重复字符。\n\n- '+' 用来匹配1个或者多个字符\n- '*' 用来匹配0个或者多个字符\n- '?' 用来匹配0个或1个字符\n\n注意，'+'和'*'会匹配尽可能多的字符。\n\n### 一些重复字符的例子\n\n``` py\n    ## i+  匹配1个或者多个'i'\n    match = re.search(r'pi+', 'piiig') #  found, match.group() == \"piii\"\n\n    ## 找到字符串中最左边尽可能长的模式。\n    ## 注意，并没有匹配到第二个 'i+'\n    match = re.search(r'i+', 'piigiiii')  #  found, match.group() == \"ii\"\n\n    ## \\s*  匹配0个或1个空白字符 whitespace\n    match = re.search(r'\\d\\s*\\d\\s*\\d', 'xx1 2   3xx')  #  found, match.group() == \"1 2   3\"\n    match = re.search(r'\\d\\s*\\d\\s*\\d', 'xx12  3xx')    #  found, match.group() == \"12  3\"\n    match = re.search(r'\\d\\s*\\d\\s*\\d', 'xx123xx')      # found, match.group() == \"123\"\n\n    ## ^ 匹配字符串的第一个字符\n    match = re.search(r'^b\\w+', 'foobar')  # not found, match == None\n    ## 与上例对比\n    match = re.search(r'b\\w+', 'foobar')   # found, match.group() == \"bar\"\n```\n\n### Email\n考虑一个典型的Email地址：`someone@host.com`，可以用如下的方式匹配：\n\n```py\n    match = re.search(r'\\w+@\\w+',str)\n```\n\n但是，对于这种Email地址 `xyz alice-b@google.com purple monkey`则不能奏效。\n\n### 使用方括号\n方括号里面的字符表示一个字符集合。[abc]可以被用来匹配'a'或者'b'或者'c'。\\w \\s等都可以用在方括号里，除了'.'以外，它只能用来表示字面意义上的‘点’。所以上面的Email规则可以扩充如下：\n\n``` py\n    match = re.search('r[\\w.-]+@[\\w.-]+',str)\n```\n\n你还可以使用'-'来指定范围，如[a-z]指示的是所有小写字母的集合。所以如果你想构造的字符集合中有'-'，请把它放到末尾[ab-]。另外，前方加上'^'，用来表示取集合的补集，例如[^ab]表示除了'a'和'b'之外的其他字符。\n\n## 操作\n以Email地址为例，如果我们想要分别提取该地址的用户名'someone'和主机名'host.com'该怎么办呢？\n可以在模式中用圆括号指定。\n\n``` py\n    str = 'purple alice-b@google.com monkey dishwasher'\n    match = re.search('([\\w.-]+)@([\\w.-]+)', str)   #用圆括号指定分割\n    if match:\n        print match.group()   ## 'alice-b@google.com' (the whole match)\n        print match.group(1)  ## 'alice-b' (the username, group 1)\n      \tprint match.group(2)  ## 'google.com' (the host, group 2)\n```\n\n### findall 函数\n与group函数只找到最左端的一个匹配不同，findall函数找到字符串中所有与模式匹配的串。\n\n``` py\n    str = 'purple alice@google.com, blah monkey bob@abc.com blah dishwasher'\n    ## findall返回一个包含所有匹配结果的 list\n    emails = re.findall(r'[\\w\\.-]+@[\\w\\.-]+', str) ## ['alice@google.com', 'bob@abc.com']\n    for email in emails:\n        print email\n```\n\n### 在文件中使用findall\n当然可以读入文件的每一行，然后对每一行的内容调用findall，但是为什么不让这一切自动发生呢？\n\n``` py\n\tf = open(filename.txt,'r')\n\tmatches = re.findall(pattern,f.read())\n```\n\n### findall 和分组\n和group的用法相似，也可以指定分组。\n\n``` py\n    str = 'purple alice@google.com, blah monkey bob@abc.com blah dishwasher'\n    ##　返回了一个list\n    tuples = re.findall(r'([\\w\\.-]+)@([\\w\\.-]+)', str)\n    print tuples  ## [('alice', 'google.com'), ('bob', 'abc.com')]\n    ##　list中的元素是tuple\n    for tuple in tuples:\n      print tuple[0]  ## username\n      print tuple[1]  ## host\n```\n\n## 调试\n\n正则表达式异常强大，使用简单的几条规则就可以演变出很多的模式组合。在确定你的模式之前，可能需要很多的调试工作。在一个小的测试集合上测试正则表达式。\n\n## 其他选项\n\n正则表达式还可以设置“选项”。\n\n``` py\n    match = re.search(pat,str,opt)\n```\n\n这些可选项如下：\n\n- IGNORECASE  忽视大小写\n- DOTALL  允许'.'匹配'\\n'\n- MULTILINE  在一个由许多行组成的字符串中，允许'^'和'$'匹配每一行的开始和结束\n","slug":"python-reg-exp","published":1,"updated":"2018-01-12T06:22:20.476Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjcu6vcu00021qu463t4rn4zu","content":"<p>本文来自于Google Developers中对于Python的介绍。<a href=\"https://developers.google.com/edu/python/regular-expressions\" title=\"Google Python Class, Regular Expression\" target=\"_blank\" rel=\"external\">https://developers.google.com/edu/python/regular-expressions</a>。</p>\n<p><img src=\"/img/regex_picture.jpg\" alt=\"regex\"><br><a id=\"more\"></a></p>\n<h2 id=\"认识正则表达式\"><a href=\"#认识正则表达式\" class=\"headerlink\" title=\"认识正则表达式\"></a>认识正则表达式</h2><p>Python的正则表达式是使用 <strong>re 模块</strong>的。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\">match = re.search(pattern,str)</div><div class=\"line\"><span class=\"keyword\">if</span> match:</div><div class=\"line\">\t<span class=\"keyword\">print</span> <span class=\"string\">'found'</span>,match.group()</div><div class=\"line\"><span class=\"keyword\">else</span>:</div><div class=\"line\">    <span class=\"keyword\">print</span> <span class=\"string\">'NOT Found!'</span></div></pre></td></tr></table></figure>\n<h2 id=\"正则表达式的规则\"><a href=\"#正则表达式的规则\" class=\"headerlink\" title=\"正则表达式的规则\"></a>正则表达式的规则</h2><h3 id=\"基本规则\"><a href=\"#基本规则\" class=\"headerlink\" title=\"基本规则\"></a>基本规则</h3><ul>\n<li>a, x, 9 都是普通字符 (ordinary characters)</li>\n<li>. (一个点)可以匹配任何单个字符（除了’\\n’）</li>\n<li>\\w（小写的w）可以匹配一个单词里面的字母，数字或下划线 [a-zA-Z0-9_];\\W （大写的W）可以匹配非单词里的这些元素</li>\n<li>\\b 匹配单词与非单词的分界</li>\n<li>\\s（小写的s）匹配一个 whitespace character，包括 space，newline，return，tab，form(\\n\\r\\t\\f)；\\S（大写的S）匹配一个非 whitespace character</li>\n<li>\\d 匹配十进制数字 [0-9]</li>\n<li>^=start，$=end 用来匹配字符串的开始和结束</li>\n<li>\\ 是转义字符，用 . 来匹配串里的’.’，等<h3 id=\"一些基本的例子\"><a href=\"#一些基本的例子\" class=\"headerlink\" title=\"一些基本的例子\"></a>一些基本的例子</h3></li>\n</ul>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\">## 在字符串'piiig'中查找'iii'</span></div><div class=\"line\">match = re.search(<span class=\"string\">r'iii'</span>, <span class=\"string\">'piiig'</span>)  <span class=\"comment\"># found, match.group() == \"iii\"</span></div><div class=\"line\">match = re.search(<span class=\"string\">r'igs'</span>, <span class=\"string\">'piiig'</span>)  <span class=\"comment\">#  not found, match == None</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">## . 匹配除了\\n的任意字符</span></div><div class=\"line\">match = re.search(<span class=\"string\">r'..g'</span>, <span class=\"string\">'piiig'</span>)  <span class=\"comment\">#  found, match.group() == \"iig\"</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">## \\d 匹配0-9的数字字符, \\w 匹配单词里的字符</span></div><div class=\"line\">match = re.search(<span class=\"string\">r'\\d\\d\\d'</span>, <span class=\"string\">'p123g'</span>) <span class=\"comment\">#  found, match.group() == \"123\"</span></div><div class=\"line\">match = re.search(<span class=\"string\">r'\\w\\w\\w'</span>, <span class=\"string\">'@@abcd!!'</span>) <span class=\"comment\">#  found, match.group() == \"abc\"</span></div></pre></td></tr></table></figure>\n<h3 id=\"重复\"><a href=\"#重复\" class=\"headerlink\" title=\"重复\"></a>重复</h3><p>可以用’+’ ‘*’ ‘?’来匹配0个，1个或多个重复字符。</p>\n<ul>\n<li>‘+’ 用来匹配1个或者多个字符</li>\n<li>‘*’ 用来匹配0个或者多个字符</li>\n<li>‘?’ 用来匹配0个或1个字符</li>\n</ul>\n<p>注意，’+’和’*’会匹配尽可能多的字符。</p>\n<h3 id=\"一些重复字符的例子\"><a href=\"#一些重复字符的例子\" class=\"headerlink\" title=\"一些重复字符的例子\"></a>一些重复字符的例子</h3><figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\">## i+  匹配1个或者多个'i'</span></div><div class=\"line\">match = re.search(<span class=\"string\">r'pi+'</span>, <span class=\"string\">'piiig'</span>) <span class=\"comment\">#  found, match.group() == \"piii\"</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">## 找到字符串中最左边尽可能长的模式。</span></div><div class=\"line\"><span class=\"comment\">## 注意，并没有匹配到第二个 'i+'</span></div><div class=\"line\">match = re.search(<span class=\"string\">r'i+'</span>, <span class=\"string\">'piigiiii'</span>)  <span class=\"comment\">#  found, match.group() == \"ii\"</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">## \\s*  匹配0个或1个空白字符 whitespace</span></div><div class=\"line\">match = re.search(<span class=\"string\">r'\\d\\s*\\d\\s*\\d'</span>, <span class=\"string\">'xx1 2   3xx'</span>)  <span class=\"comment\">#  found, match.group() == \"1 2   3\"</span></div><div class=\"line\">match = re.search(<span class=\"string\">r'\\d\\s*\\d\\s*\\d'</span>, <span class=\"string\">'xx12  3xx'</span>)    <span class=\"comment\">#  found, match.group() == \"12  3\"</span></div><div class=\"line\">match = re.search(<span class=\"string\">r'\\d\\s*\\d\\s*\\d'</span>, <span class=\"string\">'xx123xx'</span>)      <span class=\"comment\"># found, match.group() == \"123\"</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">## ^ 匹配字符串的第一个字符</span></div><div class=\"line\">match = re.search(<span class=\"string\">r'^b\\w+'</span>, <span class=\"string\">'foobar'</span>)  <span class=\"comment\"># not found, match == None</span></div><div class=\"line\"><span class=\"comment\">## 与上例对比</span></div><div class=\"line\">match = re.search(<span class=\"string\">r'b\\w+'</span>, <span class=\"string\">'foobar'</span>)   <span class=\"comment\"># found, match.group() == \"bar\"</span></div></pre></td></tr></table></figure>\n<h3 id=\"Email\"><a href=\"#Email\" class=\"headerlink\" title=\"Email\"></a>Email</h3><p>考虑一个典型的Email地址：<code>someone@host.com</code>，可以用如下的方式匹配：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">match = re.search(<span class=\"string\">r'\\w+@\\w+'</span>,str)</div></pre></td></tr></table></figure>\n<p>但是，对于这种Email地址 <code>xyz alice-b@google.com purple monkey</code>则不能奏效。</p>\n<h3 id=\"使用方括号\"><a href=\"#使用方括号\" class=\"headerlink\" title=\"使用方括号\"></a>使用方括号</h3><p>方括号里面的字符表示一个字符集合。[abc]可以被用来匹配’a’或者’b’或者’c’。\\w \\s等都可以用在方括号里，除了’.’以外，它只能用来表示字面意义上的‘点’。所以上面的Email规则可以扩充如下：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">match = re.search(<span class=\"string\">'r[\\w.-]+@[\\w.-]+'</span>,str)</div></pre></td></tr></table></figure>\n<p>你还可以使用’-‘来指定范围，如[a-z]指示的是所有小写字母的集合。所以如果你想构造的字符集合中有’-‘，请把它放到末尾[ab-]。另外，前方加上’^’，用来表示取集合的补集，例如<sup><a href=\"#fn_ab\" id=\"reffn_ab\">ab</a></sup>表示除了’a’和’b’之外的其他字符。</p>\n<h2 id=\"操作\"><a href=\"#操作\" class=\"headerlink\" title=\"操作\"></a>操作</h2><p>以Email地址为例，如果我们想要分别提取该地址的用户名’someone’和主机名’host.com’该怎么办呢？<br>可以在模式中用圆括号指定。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div></pre></td><td class=\"code\"><pre><div class=\"line\">str = <span class=\"string\">'purple alice-b@google.com monkey dishwasher'</span></div><div class=\"line\">match = re.search(<span class=\"string\">'([\\w.-]+)@([\\w.-]+)'</span>, str)   <span class=\"comment\">#用圆括号指定分割</span></div><div class=\"line\"><span class=\"keyword\">if</span> match:</div><div class=\"line\">    <span class=\"keyword\">print</span> match.group()   <span class=\"comment\">## 'alice-b@google.com' (the whole match)</span></div><div class=\"line\">    <span class=\"keyword\">print</span> match.group(<span class=\"number\">1</span>)  <span class=\"comment\">## 'alice-b' (the username, group 1)</span></div><div class=\"line\">  \t<span class=\"keyword\">print</span> match.group(<span class=\"number\">2</span>)  <span class=\"comment\">## 'google.com' (the host, group 2)</span></div></pre></td></tr></table></figure>\n<h3 id=\"findall-函数\"><a href=\"#findall-函数\" class=\"headerlink\" title=\"findall 函数\"></a>findall 函数</h3><p>与group函数只找到最左端的一个匹配不同，findall函数找到字符串中所有与模式匹配的串。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\">str = <span class=\"string\">'purple alice@google.com, blah monkey bob@abc.com blah dishwasher'</span></div><div class=\"line\"><span class=\"comment\">## findall返回一个包含所有匹配结果的 list</span></div><div class=\"line\">emails = re.findall(<span class=\"string\">r'[\\w\\.-]+@[\\w\\.-]+'</span>, str) <span class=\"comment\">## ['alice@google.com', 'bob@abc.com']</span></div><div class=\"line\"><span class=\"keyword\">for</span> email <span class=\"keyword\">in</span> emails:</div><div class=\"line\">    <span class=\"keyword\">print</span> email</div></pre></td></tr></table></figure>\n<h3 id=\"在文件中使用findall\"><a href=\"#在文件中使用findall\" class=\"headerlink\" title=\"在文件中使用findall\"></a>在文件中使用findall</h3><p>当然可以读入文件的每一行，然后对每一行的内容调用findall，但是为什么不让这一切自动发生呢？</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">f = open(filename.txt,<span class=\"string\">'r'</span>)</div><div class=\"line\">matches = re.findall(pattern,f.read())</div></pre></td></tr></table></figure>\n<h3 id=\"findall-和分组\"><a href=\"#findall-和分组\" class=\"headerlink\" title=\"findall 和分组\"></a>findall 和分组</h3><p>和group的用法相似，也可以指定分组。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div></pre></td><td class=\"code\"><pre><div class=\"line\">str = <span class=\"string\">'purple alice@google.com, blah monkey bob@abc.com blah dishwasher'</span></div><div class=\"line\"><span class=\"comment\">##　返回了一个list</span></div><div class=\"line\">tuples = re.findall(<span class=\"string\">r'([\\w\\.-]+)@([\\w\\.-]+)'</span>, str)</div><div class=\"line\"><span class=\"keyword\">print</span> tuples  <span class=\"comment\">## [('alice', 'google.com'), ('bob', 'abc.com')]</span></div><div class=\"line\"><span class=\"comment\">##　list中的元素是tuple</span></div><div class=\"line\"><span class=\"keyword\">for</span> tuple <span class=\"keyword\">in</span> tuples:</div><div class=\"line\">  <span class=\"keyword\">print</span> tuple[<span class=\"number\">0</span>]  <span class=\"comment\">## username</span></div><div class=\"line\">  <span class=\"keyword\">print</span> tuple[<span class=\"number\">1</span>]  <span class=\"comment\">## host</span></div></pre></td></tr></table></figure>\n<h2 id=\"调试\"><a href=\"#调试\" class=\"headerlink\" title=\"调试\"></a>调试</h2><p>正则表达式异常强大，使用简单的几条规则就可以演变出很多的模式组合。在确定你的模式之前，可能需要很多的调试工作。在一个小的测试集合上测试正则表达式。</p>\n<h2 id=\"其他选项\"><a href=\"#其他选项\" class=\"headerlink\" title=\"其他选项\"></a>其他选项</h2><p>正则表达式还可以设置“选项”。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">match = re.search(pat,str,opt)</div></pre></td></tr></table></figure>\n<p>这些可选项如下：</p>\n<ul>\n<li>IGNORECASE  忽视大小写</li>\n<li>DOTALL  允许’.’匹配’\\n’</li>\n<li>MULTILINE  在一个由许多行组成的字符串中，允许’^’和’$’匹配每一行的开始和结束</li>\n</ul>\n","excerpt":"<p>本文来自于Google Developers中对于Python的介绍。<a href=\"https://developers.google.com/edu/python/regular-expressions\" title=\"Google Python Class, Regular Expression\">https://developers.google.com/edu/python/regular-expressions</a>。</p>\n<p><img src=\"/img/regex_picture.jpg\" alt=\"regex\"><br>","more":"</p>\n<h2 id=\"认识正则表达式\"><a href=\"#认识正则表达式\" class=\"headerlink\" title=\"认识正则表达式\"></a>认识正则表达式</h2><p>Python的正则表达式是使用 <strong>re 模块</strong>的。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\">match = re.search(pattern,str)</div><div class=\"line\"><span class=\"keyword\">if</span> match:</div><div class=\"line\">\t<span class=\"keyword\">print</span> <span class=\"string\">'found'</span>,match.group()</div><div class=\"line\"><span class=\"keyword\">else</span>:</div><div class=\"line\">    <span class=\"keyword\">print</span> <span class=\"string\">'NOT Found!'</span></div></pre></td></tr></table></figure>\n<h2 id=\"正则表达式的规则\"><a href=\"#正则表达式的规则\" class=\"headerlink\" title=\"正则表达式的规则\"></a>正则表达式的规则</h2><h3 id=\"基本规则\"><a href=\"#基本规则\" class=\"headerlink\" title=\"基本规则\"></a>基本规则</h3><ul>\n<li>a, x, 9 都是普通字符 (ordinary characters)</li>\n<li>. (一个点)可以匹配任何单个字符（除了’\\n’）</li>\n<li>\\w（小写的w）可以匹配一个单词里面的字母，数字或下划线 [a-zA-Z0-9_];\\W （大写的W）可以匹配非单词里的这些元素</li>\n<li>\\b 匹配单词与非单词的分界</li>\n<li>\\s（小写的s）匹配一个 whitespace character，包括 space，newline，return，tab，form(\\n\\r\\t\\f)；\\S（大写的S）匹配一个非 whitespace character</li>\n<li>\\d 匹配十进制数字 [0-9]</li>\n<li>^=start，$=end 用来匹配字符串的开始和结束</li>\n<li>\\ 是转义字符，用 . 来匹配串里的’.’，等<h3 id=\"一些基本的例子\"><a href=\"#一些基本的例子\" class=\"headerlink\" title=\"一些基本的例子\"></a>一些基本的例子</h3></li>\n</ul>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\">## 在字符串'piiig'中查找'iii'</span></div><div class=\"line\">match = re.search(<span class=\"string\">r'iii'</span>, <span class=\"string\">'piiig'</span>)  <span class=\"comment\"># found, match.group() == \"iii\"</span></div><div class=\"line\">match = re.search(<span class=\"string\">r'igs'</span>, <span class=\"string\">'piiig'</span>)  <span class=\"comment\">#  not found, match == None</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">## . 匹配除了\\n的任意字符</span></div><div class=\"line\">match = re.search(<span class=\"string\">r'..g'</span>, <span class=\"string\">'piiig'</span>)  <span class=\"comment\">#  found, match.group() == \"iig\"</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">## \\d 匹配0-9的数字字符, \\w 匹配单词里的字符</span></div><div class=\"line\">match = re.search(<span class=\"string\">r'\\d\\d\\d'</span>, <span class=\"string\">'p123g'</span>) <span class=\"comment\">#  found, match.group() == \"123\"</span></div><div class=\"line\">match = re.search(<span class=\"string\">r'\\w\\w\\w'</span>, <span class=\"string\">'@@abcd!!'</span>) <span class=\"comment\">#  found, match.group() == \"abc\"</span></div></pre></td></tr></table></figure>\n<h3 id=\"重复\"><a href=\"#重复\" class=\"headerlink\" title=\"重复\"></a>重复</h3><p>可以用’+’ ‘*’ ‘?’来匹配0个，1个或多个重复字符。</p>\n<ul>\n<li>‘+’ 用来匹配1个或者多个字符</li>\n<li>‘*’ 用来匹配0个或者多个字符</li>\n<li>‘?’ 用来匹配0个或1个字符</li>\n</ul>\n<p>注意，’+’和’*’会匹配尽可能多的字符。</p>\n<h3 id=\"一些重复字符的例子\"><a href=\"#一些重复字符的例子\" class=\"headerlink\" title=\"一些重复字符的例子\"></a>一些重复字符的例子</h3><figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\">## i+  匹配1个或者多个'i'</span></div><div class=\"line\">match = re.search(<span class=\"string\">r'pi+'</span>, <span class=\"string\">'piiig'</span>) <span class=\"comment\">#  found, match.group() == \"piii\"</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">## 找到字符串中最左边尽可能长的模式。</span></div><div class=\"line\"><span class=\"comment\">## 注意，并没有匹配到第二个 'i+'</span></div><div class=\"line\">match = re.search(<span class=\"string\">r'i+'</span>, <span class=\"string\">'piigiiii'</span>)  <span class=\"comment\">#  found, match.group() == \"ii\"</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">## \\s*  匹配0个或1个空白字符 whitespace</span></div><div class=\"line\">match = re.search(<span class=\"string\">r'\\d\\s*\\d\\s*\\d'</span>, <span class=\"string\">'xx1 2   3xx'</span>)  <span class=\"comment\">#  found, match.group() == \"1 2   3\"</span></div><div class=\"line\">match = re.search(<span class=\"string\">r'\\d\\s*\\d\\s*\\d'</span>, <span class=\"string\">'xx12  3xx'</span>)    <span class=\"comment\">#  found, match.group() == \"12  3\"</span></div><div class=\"line\">match = re.search(<span class=\"string\">r'\\d\\s*\\d\\s*\\d'</span>, <span class=\"string\">'xx123xx'</span>)      <span class=\"comment\"># found, match.group() == \"123\"</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">## ^ 匹配字符串的第一个字符</span></div><div class=\"line\">match = re.search(<span class=\"string\">r'^b\\w+'</span>, <span class=\"string\">'foobar'</span>)  <span class=\"comment\"># not found, match == None</span></div><div class=\"line\"><span class=\"comment\">## 与上例对比</span></div><div class=\"line\">match = re.search(<span class=\"string\">r'b\\w+'</span>, <span class=\"string\">'foobar'</span>)   <span class=\"comment\"># found, match.group() == \"bar\"</span></div></pre></td></tr></table></figure>\n<h3 id=\"Email\"><a href=\"#Email\" class=\"headerlink\" title=\"Email\"></a>Email</h3><p>考虑一个典型的Email地址：<code>someone@host.com</code>，可以用如下的方式匹配：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">match = re.search(<span class=\"string\">r'\\w+@\\w+'</span>,str)</div></pre></td></tr></table></figure>\n<p>但是，对于这种Email地址 <code>xyz alice-b@google.com purple monkey</code>则不能奏效。</p>\n<h3 id=\"使用方括号\"><a href=\"#使用方括号\" class=\"headerlink\" title=\"使用方括号\"></a>使用方括号</h3><p>方括号里面的字符表示一个字符集合。[abc]可以被用来匹配’a’或者’b’或者’c’。\\w \\s等都可以用在方括号里，除了’.’以外，它只能用来表示字面意义上的‘点’。所以上面的Email规则可以扩充如下：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">match = re.search(<span class=\"string\">'r[\\w.-]+@[\\w.-]+'</span>,str)</div></pre></td></tr></table></figure>\n<p>你还可以使用’-‘来指定范围，如[a-z]指示的是所有小写字母的集合。所以如果你想构造的字符集合中有’-‘，请把它放到末尾[ab-]。另外，前方加上’^’，用来表示取集合的补集，例如<sup><a href=\"#fn_ab\" id=\"reffn_ab\">ab</a></sup>表示除了’a’和’b’之外的其他字符。</p>\n<h2 id=\"操作\"><a href=\"#操作\" class=\"headerlink\" title=\"操作\"></a>操作</h2><p>以Email地址为例，如果我们想要分别提取该地址的用户名’someone’和主机名’host.com’该怎么办呢？<br>可以在模式中用圆括号指定。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div></pre></td><td class=\"code\"><pre><div class=\"line\">str = <span class=\"string\">'purple alice-b@google.com monkey dishwasher'</span></div><div class=\"line\">match = re.search(<span class=\"string\">'([\\w.-]+)@([\\w.-]+)'</span>, str)   <span class=\"comment\">#用圆括号指定分割</span></div><div class=\"line\"><span class=\"keyword\">if</span> match:</div><div class=\"line\">    <span class=\"keyword\">print</span> match.group()   <span class=\"comment\">## 'alice-b@google.com' (the whole match)</span></div><div class=\"line\">    <span class=\"keyword\">print</span> match.group(<span class=\"number\">1</span>)  <span class=\"comment\">## 'alice-b' (the username, group 1)</span></div><div class=\"line\">  \t<span class=\"keyword\">print</span> match.group(<span class=\"number\">2</span>)  <span class=\"comment\">## 'google.com' (the host, group 2)</span></div></pre></td></tr></table></figure>\n<h3 id=\"findall-函数\"><a href=\"#findall-函数\" class=\"headerlink\" title=\"findall 函数\"></a>findall 函数</h3><p>与group函数只找到最左端的一个匹配不同，findall函数找到字符串中所有与模式匹配的串。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\">str = <span class=\"string\">'purple alice@google.com, blah monkey bob@abc.com blah dishwasher'</span></div><div class=\"line\"><span class=\"comment\">## findall返回一个包含所有匹配结果的 list</span></div><div class=\"line\">emails = re.findall(<span class=\"string\">r'[\\w\\.-]+@[\\w\\.-]+'</span>, str) <span class=\"comment\">## ['alice@google.com', 'bob@abc.com']</span></div><div class=\"line\"><span class=\"keyword\">for</span> email <span class=\"keyword\">in</span> emails:</div><div class=\"line\">    <span class=\"keyword\">print</span> email</div></pre></td></tr></table></figure>\n<h3 id=\"在文件中使用findall\"><a href=\"#在文件中使用findall\" class=\"headerlink\" title=\"在文件中使用findall\"></a>在文件中使用findall</h3><p>当然可以读入文件的每一行，然后对每一行的内容调用findall，但是为什么不让这一切自动发生呢？</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">f = open(filename.txt,<span class=\"string\">'r'</span>)</div><div class=\"line\">matches = re.findall(pattern,f.read())</div></pre></td></tr></table></figure>\n<h3 id=\"findall-和分组\"><a href=\"#findall-和分组\" class=\"headerlink\" title=\"findall 和分组\"></a>findall 和分组</h3><p>和group的用法相似，也可以指定分组。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div></pre></td><td class=\"code\"><pre><div class=\"line\">str = <span class=\"string\">'purple alice@google.com, blah monkey bob@abc.com blah dishwasher'</span></div><div class=\"line\"><span class=\"comment\">##　返回了一个list</span></div><div class=\"line\">tuples = re.findall(<span class=\"string\">r'([\\w\\.-]+)@([\\w\\.-]+)'</span>, str)</div><div class=\"line\"><span class=\"keyword\">print</span> tuples  <span class=\"comment\">## [('alice', 'google.com'), ('bob', 'abc.com')]</span></div><div class=\"line\"><span class=\"comment\">##　list中的元素是tuple</span></div><div class=\"line\"><span class=\"keyword\">for</span> tuple <span class=\"keyword\">in</span> tuples:</div><div class=\"line\">  <span class=\"keyword\">print</span> tuple[<span class=\"number\">0</span>]  <span class=\"comment\">## username</span></div><div class=\"line\">  <span class=\"keyword\">print</span> tuple[<span class=\"number\">1</span>]  <span class=\"comment\">## host</span></div></pre></td></tr></table></figure>\n<h2 id=\"调试\"><a href=\"#调试\" class=\"headerlink\" title=\"调试\"></a>调试</h2><p>正则表达式异常强大，使用简单的几条规则就可以演变出很多的模式组合。在确定你的模式之前，可能需要很多的调试工作。在一个小的测试集合上测试正则表达式。</p>\n<h2 id=\"其他选项\"><a href=\"#其他选项\" class=\"headerlink\" title=\"其他选项\"></a>其他选项</h2><p>正则表达式还可以设置“选项”。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">match = re.search(pat,str,opt)</div></pre></td></tr></table></figure>\n<p>这些可选项如下：</p>\n<ul>\n<li>IGNORECASE  忽视大小写</li>\n<li>DOTALL  允许’.’匹配’\\n’</li>\n<li>MULTILINE  在一个由许多行组成的字符串中，允许’^’和’$’匹配每一行的开始和结束</li>\n</ul>"},{"title":"toy demo - PyTorch + MNIST","date":"2017-03-04T14:37:44.000Z","_content":"本篇文章介绍了使用PyTorch在MNIST数据集上训练MLP和CNN，并记录自己实现过程中的若干问题。\n![MNIST](/img/mnist_example.png)\n<!-- more -->\n\n## 加载MNIST数据集\nPyTorch中提供了MNIST，CIFAR，COCO等常用数据集的加载方法。`MNIST`是`torchvision.datasets`包中的一个类，负责根据传入的参数加载数据集。如果自己之前没有下载过该数据集，可以将`download`参数设置为`True`，会自动下载数据集并解包。如果之前已经下载好了，只需将其路径通过`root`传入即可。\n\n在加载图像后，我们常常需要对图像进行若干预处理。比如减去RGB通道的均值，或者裁剪或翻转图像实现augmentation等，这些操作可以在`torchvision.transforms`包中找到对应的操作。在下面的代码中，通过使用`transforms.Compose()`，我们构造了对数据进行预处理的复合操作序列，`ToTensor`负责将PIL图像转换为Tensor数据（RGB通道从`[0, 255]`范围变为`[0, 1]`）， `Normalize`负责对图像进行规范化。这里需要注意，虽然MNIST中图像都是灰度图像，通道数均为1，但是仍要传入`tuple`。\n\n之后，我们通过`DataLoader`返回一个数据集上的可迭代对象。一会我们通过`for`循环，就可以遍历数据集了。\n\n``` py\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nimport torchvision.datasets as dset\nimport torchvision.transforms as transforms\nimport torch.nn.functional as F\nimport torch.optim as optim\n\ntrans = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (1.0,))])\n\ntrain_set = dset.MNIST(root=root, train=True, transform=trans, download=download)\ntest_set = dset.MNIST(root=root, train=False, transform=trans)\n\nbatch_size = 128\nkwargs = {'num_workers': 1, 'pin_memory': True}\ntrain_loader = torch.utils.data.DataLoader(\n                 dataset=train_set,\n                 batch_size=batch_size,\n                 shuffle=True, **kwargs)\ntest_loader = torch.utils.data.DataLoader(\n                dataset=test_set,\n                batch_size=batch_size,\n                shuffle=False, **kwargs)\n\n```\n\n## 网络构建\n在进行网络构建时，主要通过`torch.nn`包中的已经实现好的卷积层、池化层等进行搭建。例如下面的代码展示了一个具有一个隐含层的MLP网络。`nn.Linear`负责构建全连接层，需要提供输入和输出的通道数，也就是`y = wx+b`中`x`和`y`的维度。\n\n``` py\nclass MLPNet(nn.Module):\n    def __init__(self):\n        super(MLPNet, self).__init__()\n        self.fc1 = nn.Linear(28*28, 500)\n        self.fc2 = nn.Linear(500, 256)\n        self.fc3 = nn.Linear(256, 10)\n        self.ceriation = nn.CrossEntropyLoss()\n    def forward(self, x, target):\n        x = x.view(-1, 28*28)\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        loss = self.ceriation(x, target)\n        return x, loss\n```\n由于PyTorch可以实现自动求导，所以我们只需实现`forward`过程即可。这里由于池化层和非线性变换都没有参数，所以使用了`nn.functionals`中的对应操作实现。通过看文档，可以发现，一般`nn`里面的各种层，都会在`nn.functionals`里面有其对应。例如卷积层的对应实现，如下所示，需要传入卷积核的权重。\n\n``` py\n# With square kernels and equal stride\nfilters = autograd.Variable(torch.randn(8,4,3,3))\ninputs = autograd.Variable(torch.randn(1,4,5,5))\nF.conv2d(inputs, filters, padding=1)\n```\n\n同样地，我们可以实现LeNet的结构如下。\n\n``` py\n\nclass LeNet(nn.Module):\n    def __init__(self):\n        super(LeNet, self).__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5, 1)\n        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n        self.fc1 = nn.Linear(4*4*50, 500)\n        self.fc2 = nn.Linear(500, 10)\n        self.ceriation = nn.CrossEntropyLoss()\n    def forward(self, x, target):\n        x = F.relu(self.conv1(x))\n        x = F.max_pool2d(x, 2, 2)\n        x = F.relu(self.conv2(x))\n        x = F.max_pool2d(x, 2, 2)\n        x = x.view(-1, 4*4*50)\n        x = F.relu(self.fc1(x))\n        x = self.fc2(x)\n        loss = self.ceriation(x, target)\n        return x, loss\n```\n\n## 训练与测试\n\n在训练时，我们首先应确定优化方法。这里我们使用带动量的`SGD`方法。下面代码中的`optim.SGD`初始化需要接受网络中待优化的`Parameter`列表（或是迭代器），以及学习率`lr`，动量`momentum`。\n\n``` py\noptimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n```\n\n接下来，我们只需要遍历数据集，同时在每次迭代中清空待优化参数的梯度，前向计算，反向传播以及优化器的迭代求解即可。\n\n``` py\nmodel = MLPNet().cuda()   # 以MLP为例\nfor epoch in xrange(10):\n    # trainning\n    for batch_idx, (x, target) in enumerate(train_loader):\n        optimizer.zero_grad()     #每次都要清空上一步中参数的grad，否则会出错的~\n        x, target = Variable(x.cuda()), Variable(target.cuda())\n        _, loss = model(x, target)   #得到loss\n        loss.backward()              #bp\n        optimizer.step()             #优化器迭代\n        if batch_idx % 100 == 0:\n            print '==>>> epoch: {}, batch index: {}, train loss: {:.6f}'.format(epoch, batch_idx, loss.data[0])\n    # testing\n    correct_cnt, ave_loss = 0, 0\n    for batch_idx, (x, target) in enumerate(test_loader):\n        x, target = Variable(x.cuda(), volatile=True), Variable(target.cuda(), volatile=True)\n        score, loss = model(x, target)\n        _, pred_label = torch.max(score.data, 1)\n        correct_cnt += (pred_label == target.data).sum()\n        ave_loss += loss.data[0]\n    accuracy = correct_cnt*1.0/len(test_loader)/batch_size\n    ave_loss /= len(test_loader)\n\n```\n\n当优化完毕后，需要保存模型。这里[官方文档](http://pytorch.org/docs/notes/serialization.html#recommend-saving-models)给出了推荐的方法，如下所示：\n``` py\ntorch.save(model.state_dict(), PATH)   #保存网络参数\nthe_model = TheModelClass(*args, **kwargs)\nthe_model.load_state_dict(torch.load(PATH))  #读取网络参数\n```\n\n该博客的完整代码可以见：[PyTorch MNIST demo](https://gist.github.com/xmfbit/b27cdbff68870418bdb8cefa86a2d558)。\n","source":"_posts/pytorch-mnist-example.md","raw":"---\ntitle: toy demo - PyTorch + MNIST\ndate: 2017-03-04 22:37:44\ntags:\n     - pytorch\n---\n本篇文章介绍了使用PyTorch在MNIST数据集上训练MLP和CNN，并记录自己实现过程中的若干问题。\n![MNIST](/img/mnist_example.png)\n<!-- more -->\n\n## 加载MNIST数据集\nPyTorch中提供了MNIST，CIFAR，COCO等常用数据集的加载方法。`MNIST`是`torchvision.datasets`包中的一个类，负责根据传入的参数加载数据集。如果自己之前没有下载过该数据集，可以将`download`参数设置为`True`，会自动下载数据集并解包。如果之前已经下载好了，只需将其路径通过`root`传入即可。\n\n在加载图像后，我们常常需要对图像进行若干预处理。比如减去RGB通道的均值，或者裁剪或翻转图像实现augmentation等，这些操作可以在`torchvision.transforms`包中找到对应的操作。在下面的代码中，通过使用`transforms.Compose()`，我们构造了对数据进行预处理的复合操作序列，`ToTensor`负责将PIL图像转换为Tensor数据（RGB通道从`[0, 255]`范围变为`[0, 1]`）， `Normalize`负责对图像进行规范化。这里需要注意，虽然MNIST中图像都是灰度图像，通道数均为1，但是仍要传入`tuple`。\n\n之后，我们通过`DataLoader`返回一个数据集上的可迭代对象。一会我们通过`for`循环，就可以遍历数据集了。\n\n``` py\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nimport torchvision.datasets as dset\nimport torchvision.transforms as transforms\nimport torch.nn.functional as F\nimport torch.optim as optim\n\ntrans = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (1.0,))])\n\ntrain_set = dset.MNIST(root=root, train=True, transform=trans, download=download)\ntest_set = dset.MNIST(root=root, train=False, transform=trans)\n\nbatch_size = 128\nkwargs = {'num_workers': 1, 'pin_memory': True}\ntrain_loader = torch.utils.data.DataLoader(\n                 dataset=train_set,\n                 batch_size=batch_size,\n                 shuffle=True, **kwargs)\ntest_loader = torch.utils.data.DataLoader(\n                dataset=test_set,\n                batch_size=batch_size,\n                shuffle=False, **kwargs)\n\n```\n\n## 网络构建\n在进行网络构建时，主要通过`torch.nn`包中的已经实现好的卷积层、池化层等进行搭建。例如下面的代码展示了一个具有一个隐含层的MLP网络。`nn.Linear`负责构建全连接层，需要提供输入和输出的通道数，也就是`y = wx+b`中`x`和`y`的维度。\n\n``` py\nclass MLPNet(nn.Module):\n    def __init__(self):\n        super(MLPNet, self).__init__()\n        self.fc1 = nn.Linear(28*28, 500)\n        self.fc2 = nn.Linear(500, 256)\n        self.fc3 = nn.Linear(256, 10)\n        self.ceriation = nn.CrossEntropyLoss()\n    def forward(self, x, target):\n        x = x.view(-1, 28*28)\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        loss = self.ceriation(x, target)\n        return x, loss\n```\n由于PyTorch可以实现自动求导，所以我们只需实现`forward`过程即可。这里由于池化层和非线性变换都没有参数，所以使用了`nn.functionals`中的对应操作实现。通过看文档，可以发现，一般`nn`里面的各种层，都会在`nn.functionals`里面有其对应。例如卷积层的对应实现，如下所示，需要传入卷积核的权重。\n\n``` py\n# With square kernels and equal stride\nfilters = autograd.Variable(torch.randn(8,4,3,3))\ninputs = autograd.Variable(torch.randn(1,4,5,5))\nF.conv2d(inputs, filters, padding=1)\n```\n\n同样地，我们可以实现LeNet的结构如下。\n\n``` py\n\nclass LeNet(nn.Module):\n    def __init__(self):\n        super(LeNet, self).__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5, 1)\n        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n        self.fc1 = nn.Linear(4*4*50, 500)\n        self.fc2 = nn.Linear(500, 10)\n        self.ceriation = nn.CrossEntropyLoss()\n    def forward(self, x, target):\n        x = F.relu(self.conv1(x))\n        x = F.max_pool2d(x, 2, 2)\n        x = F.relu(self.conv2(x))\n        x = F.max_pool2d(x, 2, 2)\n        x = x.view(-1, 4*4*50)\n        x = F.relu(self.fc1(x))\n        x = self.fc2(x)\n        loss = self.ceriation(x, target)\n        return x, loss\n```\n\n## 训练与测试\n\n在训练时，我们首先应确定优化方法。这里我们使用带动量的`SGD`方法。下面代码中的`optim.SGD`初始化需要接受网络中待优化的`Parameter`列表（或是迭代器），以及学习率`lr`，动量`momentum`。\n\n``` py\noptimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n```\n\n接下来，我们只需要遍历数据集，同时在每次迭代中清空待优化参数的梯度，前向计算，反向传播以及优化器的迭代求解即可。\n\n``` py\nmodel = MLPNet().cuda()   # 以MLP为例\nfor epoch in xrange(10):\n    # trainning\n    for batch_idx, (x, target) in enumerate(train_loader):\n        optimizer.zero_grad()     #每次都要清空上一步中参数的grad，否则会出错的~\n        x, target = Variable(x.cuda()), Variable(target.cuda())\n        _, loss = model(x, target)   #得到loss\n        loss.backward()              #bp\n        optimizer.step()             #优化器迭代\n        if batch_idx % 100 == 0:\n            print '==>>> epoch: {}, batch index: {}, train loss: {:.6f}'.format(epoch, batch_idx, loss.data[0])\n    # testing\n    correct_cnt, ave_loss = 0, 0\n    for batch_idx, (x, target) in enumerate(test_loader):\n        x, target = Variable(x.cuda(), volatile=True), Variable(target.cuda(), volatile=True)\n        score, loss = model(x, target)\n        _, pred_label = torch.max(score.data, 1)\n        correct_cnt += (pred_label == target.data).sum()\n        ave_loss += loss.data[0]\n    accuracy = correct_cnt*1.0/len(test_loader)/batch_size\n    ave_loss /= len(test_loader)\n\n```\n\n当优化完毕后，需要保存模型。这里[官方文档](http://pytorch.org/docs/notes/serialization.html#recommend-saving-models)给出了推荐的方法，如下所示：\n``` py\ntorch.save(model.state_dict(), PATH)   #保存网络参数\nthe_model = TheModelClass(*args, **kwargs)\nthe_model.load_state_dict(torch.load(PATH))  #读取网络参数\n```\n\n该博客的完整代码可以见：[PyTorch MNIST demo](https://gist.github.com/xmfbit/b27cdbff68870418bdb8cefa86a2d558)。\n","slug":"pytorch-mnist-example","published":1,"updated":"2018-01-12T06:22:20.478Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjcu6vcu10023qu46b6oai86m","content":"<p>本篇文章介绍了使用PyTorch在MNIST数据集上训练MLP和CNN，并记录自己实现过程中的若干问题。<br><img src=\"/img/mnist_example.png\" alt=\"MNIST\"><br><a id=\"more\"></a></p>\n<h2 id=\"加载MNIST数据集\"><a href=\"#加载MNIST数据集\" class=\"headerlink\" title=\"加载MNIST数据集\"></a>加载MNIST数据集</h2><p>PyTorch中提供了MNIST，CIFAR，COCO等常用数据集的加载方法。<code>MNIST</code>是<code>torchvision.datasets</code>包中的一个类，负责根据传入的参数加载数据集。如果自己之前没有下载过该数据集，可以将<code>download</code>参数设置为<code>True</code>，会自动下载数据集并解包。如果之前已经下载好了，只需将其路径通过<code>root</code>传入即可。</p>\n<p>在加载图像后，我们常常需要对图像进行若干预处理。比如减去RGB通道的均值，或者裁剪或翻转图像实现augmentation等，这些操作可以在<code>torchvision.transforms</code>包中找到对应的操作。在下面的代码中，通过使用<code>transforms.Compose()</code>，我们构造了对数据进行预处理的复合操作序列，<code>ToTensor</code>负责将PIL图像转换为Tensor数据（RGB通道从<code>[0, 255]</code>范围变为<code>[0, 1]</code>）， <code>Normalize</code>负责对图像进行规范化。这里需要注意，虽然MNIST中图像都是灰度图像，通道数均为1，但是仍要传入<code>tuple</code>。</p>\n<p>之后，我们通过<code>DataLoader</code>返回一个数据集上的可迭代对象。一会我们通过<code>for</code>循环，就可以遍历数据集了。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">import</span> torch</div><div class=\"line\"><span class=\"keyword\">import</span> torch.nn <span class=\"keyword\">as</span> nn</div><div class=\"line\"><span class=\"keyword\">from</span> torch.autograd <span class=\"keyword\">import</span> Variable</div><div class=\"line\"><span class=\"keyword\">import</span> torchvision.datasets <span class=\"keyword\">as</span> dset</div><div class=\"line\"><span class=\"keyword\">import</span> torchvision.transforms <span class=\"keyword\">as</span> transforms</div><div class=\"line\"><span class=\"keyword\">import</span> torch.nn.functional <span class=\"keyword\">as</span> F</div><div class=\"line\"><span class=\"keyword\">import</span> torch.optim <span class=\"keyword\">as</span> optim</div><div class=\"line\"></div><div class=\"line\">trans = transforms.Compose([transforms.ToTensor(), transforms.Normalize((<span class=\"number\">0.5</span>,), (<span class=\"number\">1.0</span>,))])</div><div class=\"line\"></div><div class=\"line\">train_set = dset.MNIST(root=root, train=<span class=\"keyword\">True</span>, transform=trans, download=download)</div><div class=\"line\">test_set = dset.MNIST(root=root, train=<span class=\"keyword\">False</span>, transform=trans)</div><div class=\"line\"></div><div class=\"line\">batch_size = <span class=\"number\">128</span></div><div class=\"line\">kwargs = &#123;<span class=\"string\">'num_workers'</span>: <span class=\"number\">1</span>, <span class=\"string\">'pin_memory'</span>: <span class=\"keyword\">True</span>&#125;</div><div class=\"line\">train_loader = torch.utils.data.DataLoader(</div><div class=\"line\">                 dataset=train_set,</div><div class=\"line\">                 batch_size=batch_size,</div><div class=\"line\">                 shuffle=<span class=\"keyword\">True</span>, **kwargs)</div><div class=\"line\">test_loader = torch.utils.data.DataLoader(</div><div class=\"line\">                dataset=test_set,</div><div class=\"line\">                batch_size=batch_size,</div><div class=\"line\">                shuffle=<span class=\"keyword\">False</span>, **kwargs)</div></pre></td></tr></table></figure>\n<h2 id=\"网络构建\"><a href=\"#网络构建\" class=\"headerlink\" title=\"网络构建\"></a>网络构建</h2><p>在进行网络构建时，主要通过<code>torch.nn</code>包中的已经实现好的卷积层、池化层等进行搭建。例如下面的代码展示了一个具有一个隐含层的MLP网络。<code>nn.Linear</code>负责构建全连接层，需要提供输入和输出的通道数，也就是<code>y = wx+b</code>中<code>x</code>和<code>y</code>的维度。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">MLPNet</span><span class=\"params\">(nn.Module)</span>:</span></div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self)</span>:</span></div><div class=\"line\">        super(MLPNet, self).__init__()</div><div class=\"line\">        self.fc1 = nn.Linear(<span class=\"number\">28</span>*<span class=\"number\">28</span>, <span class=\"number\">500</span>)</div><div class=\"line\">        self.fc2 = nn.Linear(<span class=\"number\">500</span>, <span class=\"number\">256</span>)</div><div class=\"line\">        self.fc3 = nn.Linear(<span class=\"number\">256</span>, <span class=\"number\">10</span>)</div><div class=\"line\">        self.ceriation = nn.CrossEntropyLoss()</div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">forward</span><span class=\"params\">(self, x, target)</span>:</span></div><div class=\"line\">        x = x.view(<span class=\"number\">-1</span>, <span class=\"number\">28</span>*<span class=\"number\">28</span>)</div><div class=\"line\">        x = F.relu(self.fc1(x))</div><div class=\"line\">        x = F.relu(self.fc2(x))</div><div class=\"line\">        x = self.fc3(x)</div><div class=\"line\">        loss = self.ceriation(x, target)</div><div class=\"line\">        <span class=\"keyword\">return</span> x, loss</div></pre></td></tr></table></figure>\n<p>由于PyTorch可以实现自动求导，所以我们只需实现<code>forward</code>过程即可。这里由于池化层和非线性变换都没有参数，所以使用了<code>nn.functionals</code>中的对应操作实现。通过看文档，可以发现，一般<code>nn</code>里面的各种层，都会在<code>nn.functionals</code>里面有其对应。例如卷积层的对应实现，如下所示，需要传入卷积核的权重。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\"># With square kernels and equal stride</span></div><div class=\"line\">filters = autograd.Variable(torch.randn(<span class=\"number\">8</span>,<span class=\"number\">4</span>,<span class=\"number\">3</span>,<span class=\"number\">3</span>))</div><div class=\"line\">inputs = autograd.Variable(torch.randn(<span class=\"number\">1</span>,<span class=\"number\">4</span>,<span class=\"number\">5</span>,<span class=\"number\">5</span>))</div><div class=\"line\">F.conv2d(inputs, filters, padding=<span class=\"number\">1</span>)</div></pre></td></tr></table></figure>\n<p>同样地，我们可以实现LeNet的结构如下。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div></pre></td><td class=\"code\"><pre><div class=\"line\"></div><div class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">LeNet</span><span class=\"params\">(nn.Module)</span>:</span></div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self)</span>:</span></div><div class=\"line\">        super(LeNet, self).__init__()</div><div class=\"line\">        self.conv1 = nn.Conv2d(<span class=\"number\">1</span>, <span class=\"number\">20</span>, <span class=\"number\">5</span>, <span class=\"number\">1</span>)</div><div class=\"line\">        self.conv2 = nn.Conv2d(<span class=\"number\">20</span>, <span class=\"number\">50</span>, <span class=\"number\">5</span>, <span class=\"number\">1</span>)</div><div class=\"line\">        self.fc1 = nn.Linear(<span class=\"number\">4</span>*<span class=\"number\">4</span>*<span class=\"number\">50</span>, <span class=\"number\">500</span>)</div><div class=\"line\">        self.fc2 = nn.Linear(<span class=\"number\">500</span>, <span class=\"number\">10</span>)</div><div class=\"line\">        self.ceriation = nn.CrossEntropyLoss()</div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">forward</span><span class=\"params\">(self, x, target)</span>:</span></div><div class=\"line\">        x = F.relu(self.conv1(x))</div><div class=\"line\">        x = F.max_pool2d(x, <span class=\"number\">2</span>, <span class=\"number\">2</span>)</div><div class=\"line\">        x = F.relu(self.conv2(x))</div><div class=\"line\">        x = F.max_pool2d(x, <span class=\"number\">2</span>, <span class=\"number\">2</span>)</div><div class=\"line\">        x = x.view(<span class=\"number\">-1</span>, <span class=\"number\">4</span>*<span class=\"number\">4</span>*<span class=\"number\">50</span>)</div><div class=\"line\">        x = F.relu(self.fc1(x))</div><div class=\"line\">        x = self.fc2(x)</div><div class=\"line\">        loss = self.ceriation(x, target)</div><div class=\"line\">        <span class=\"keyword\">return</span> x, loss</div></pre></td></tr></table></figure>\n<h2 id=\"训练与测试\"><a href=\"#训练与测试\" class=\"headerlink\" title=\"训练与测试\"></a>训练与测试</h2><p>在训练时，我们首先应确定优化方法。这里我们使用带动量的<code>SGD</code>方法。下面代码中的<code>optim.SGD</code>初始化需要接受网络中待优化的<code>Parameter</code>列表（或是迭代器），以及学习率<code>lr</code>，动量<code>momentum</code>。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">optimizer = optim.SGD(model.parameters(), lr=<span class=\"number\">0.01</span>, momentum=<span class=\"number\">0.9</span>)</div></pre></td></tr></table></figure>\n<p>接下来，我们只需要遍历数据集，同时在每次迭代中清空待优化参数的梯度，前向计算，反向传播以及优化器的迭代求解即可。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div></pre></td><td class=\"code\"><pre><div class=\"line\">model = MLPNet().cuda()   <span class=\"comment\"># 以MLP为例</span></div><div class=\"line\"><span class=\"keyword\">for</span> epoch <span class=\"keyword\">in</span> xrange(<span class=\"number\">10</span>):</div><div class=\"line\">    <span class=\"comment\"># trainning</span></div><div class=\"line\">    <span class=\"keyword\">for</span> batch_idx, (x, target) <span class=\"keyword\">in</span> enumerate(train_loader):</div><div class=\"line\">        optimizer.zero_grad()     <span class=\"comment\">#每次都要清空上一步中参数的grad，否则会出错的~</span></div><div class=\"line\">        x, target = Variable(x.cuda()), Variable(target.cuda())</div><div class=\"line\">        _, loss = model(x, target)   <span class=\"comment\">#得到loss</span></div><div class=\"line\">        loss.backward()              <span class=\"comment\">#bp</span></div><div class=\"line\">        optimizer.step()             <span class=\"comment\">#优化器迭代</span></div><div class=\"line\">        <span class=\"keyword\">if</span> batch_idx % <span class=\"number\">100</span> == <span class=\"number\">0</span>:</div><div class=\"line\">            <span class=\"keyword\">print</span> <span class=\"string\">'==&gt;&gt;&gt; epoch: &#123;&#125;, batch index: &#123;&#125;, train loss: &#123;:.6f&#125;'</span>.format(epoch, batch_idx, loss.data[<span class=\"number\">0</span>])</div><div class=\"line\">    <span class=\"comment\"># testing</span></div><div class=\"line\">    correct_cnt, ave_loss = <span class=\"number\">0</span>, <span class=\"number\">0</span></div><div class=\"line\">    <span class=\"keyword\">for</span> batch_idx, (x, target) <span class=\"keyword\">in</span> enumerate(test_loader):</div><div class=\"line\">        x, target = Variable(x.cuda(), volatile=<span class=\"keyword\">True</span>), Variable(target.cuda(), volatile=<span class=\"keyword\">True</span>)</div><div class=\"line\">        score, loss = model(x, target)</div><div class=\"line\">        _, pred_label = torch.max(score.data, <span class=\"number\">1</span>)</div><div class=\"line\">        correct_cnt += (pred_label == target.data).sum()</div><div class=\"line\">        ave_loss += loss.data[<span class=\"number\">0</span>]</div><div class=\"line\">    accuracy = correct_cnt*<span class=\"number\">1.0</span>/len(test_loader)/batch_size</div><div class=\"line\">    ave_loss /= len(test_loader)</div></pre></td></tr></table></figure>\n<p>当优化完毕后，需要保存模型。这里<a href=\"http://pytorch.org/docs/notes/serialization.html#recommend-saving-models\" target=\"_blank\" rel=\"external\">官方文档</a>给出了推荐的方法，如下所示：<br><figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\">torch.save(model.state_dict(), PATH)   <span class=\"comment\">#保存网络参数</span></div><div class=\"line\">the_model = TheModelClass(*args, **kwargs)</div><div class=\"line\">the_model.load_state_dict(torch.load(PATH))  <span class=\"comment\">#读取网络参数</span></div></pre></td></tr></table></figure></p>\n<p>该博客的完整代码可以见：<a href=\"https://gist.github.com/xmfbit/b27cdbff68870418bdb8cefa86a2d558\" target=\"_blank\" rel=\"external\">PyTorch MNIST demo</a>。</p>\n","excerpt":"<p>本篇文章介绍了使用PyTorch在MNIST数据集上训练MLP和CNN，并记录自己实现过程中的若干问题。<br><img src=\"/img/mnist_example.png\" alt=\"MNIST\"><br>","more":"</p>\n<h2 id=\"加载MNIST数据集\"><a href=\"#加载MNIST数据集\" class=\"headerlink\" title=\"加载MNIST数据集\"></a>加载MNIST数据集</h2><p>PyTorch中提供了MNIST，CIFAR，COCO等常用数据集的加载方法。<code>MNIST</code>是<code>torchvision.datasets</code>包中的一个类，负责根据传入的参数加载数据集。如果自己之前没有下载过该数据集，可以将<code>download</code>参数设置为<code>True</code>，会自动下载数据集并解包。如果之前已经下载好了，只需将其路径通过<code>root</code>传入即可。</p>\n<p>在加载图像后，我们常常需要对图像进行若干预处理。比如减去RGB通道的均值，或者裁剪或翻转图像实现augmentation等，这些操作可以在<code>torchvision.transforms</code>包中找到对应的操作。在下面的代码中，通过使用<code>transforms.Compose()</code>，我们构造了对数据进行预处理的复合操作序列，<code>ToTensor</code>负责将PIL图像转换为Tensor数据（RGB通道从<code>[0, 255]</code>范围变为<code>[0, 1]</code>）， <code>Normalize</code>负责对图像进行规范化。这里需要注意，虽然MNIST中图像都是灰度图像，通道数均为1，但是仍要传入<code>tuple</code>。</p>\n<p>之后，我们通过<code>DataLoader</code>返回一个数据集上的可迭代对象。一会我们通过<code>for</code>循环，就可以遍历数据集了。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">import</span> torch</div><div class=\"line\"><span class=\"keyword\">import</span> torch.nn <span class=\"keyword\">as</span> nn</div><div class=\"line\"><span class=\"keyword\">from</span> torch.autograd <span class=\"keyword\">import</span> Variable</div><div class=\"line\"><span class=\"keyword\">import</span> torchvision.datasets <span class=\"keyword\">as</span> dset</div><div class=\"line\"><span class=\"keyword\">import</span> torchvision.transforms <span class=\"keyword\">as</span> transforms</div><div class=\"line\"><span class=\"keyword\">import</span> torch.nn.functional <span class=\"keyword\">as</span> F</div><div class=\"line\"><span class=\"keyword\">import</span> torch.optim <span class=\"keyword\">as</span> optim</div><div class=\"line\"></div><div class=\"line\">trans = transforms.Compose([transforms.ToTensor(), transforms.Normalize((<span class=\"number\">0.5</span>,), (<span class=\"number\">1.0</span>,))])</div><div class=\"line\"></div><div class=\"line\">train_set = dset.MNIST(root=root, train=<span class=\"keyword\">True</span>, transform=trans, download=download)</div><div class=\"line\">test_set = dset.MNIST(root=root, train=<span class=\"keyword\">False</span>, transform=trans)</div><div class=\"line\"></div><div class=\"line\">batch_size = <span class=\"number\">128</span></div><div class=\"line\">kwargs = &#123;<span class=\"string\">'num_workers'</span>: <span class=\"number\">1</span>, <span class=\"string\">'pin_memory'</span>: <span class=\"keyword\">True</span>&#125;</div><div class=\"line\">train_loader = torch.utils.data.DataLoader(</div><div class=\"line\">                 dataset=train_set,</div><div class=\"line\">                 batch_size=batch_size,</div><div class=\"line\">                 shuffle=<span class=\"keyword\">True</span>, **kwargs)</div><div class=\"line\">test_loader = torch.utils.data.DataLoader(</div><div class=\"line\">                dataset=test_set,</div><div class=\"line\">                batch_size=batch_size,</div><div class=\"line\">                shuffle=<span class=\"keyword\">False</span>, **kwargs)</div></pre></td></tr></table></figure>\n<h2 id=\"网络构建\"><a href=\"#网络构建\" class=\"headerlink\" title=\"网络构建\"></a>网络构建</h2><p>在进行网络构建时，主要通过<code>torch.nn</code>包中的已经实现好的卷积层、池化层等进行搭建。例如下面的代码展示了一个具有一个隐含层的MLP网络。<code>nn.Linear</code>负责构建全连接层，需要提供输入和输出的通道数，也就是<code>y = wx+b</code>中<code>x</code>和<code>y</code>的维度。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">MLPNet</span><span class=\"params\">(nn.Module)</span>:</span></div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self)</span>:</span></div><div class=\"line\">        super(MLPNet, self).__init__()</div><div class=\"line\">        self.fc1 = nn.Linear(<span class=\"number\">28</span>*<span class=\"number\">28</span>, <span class=\"number\">500</span>)</div><div class=\"line\">        self.fc2 = nn.Linear(<span class=\"number\">500</span>, <span class=\"number\">256</span>)</div><div class=\"line\">        self.fc3 = nn.Linear(<span class=\"number\">256</span>, <span class=\"number\">10</span>)</div><div class=\"line\">        self.ceriation = nn.CrossEntropyLoss()</div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">forward</span><span class=\"params\">(self, x, target)</span>:</span></div><div class=\"line\">        x = x.view(<span class=\"number\">-1</span>, <span class=\"number\">28</span>*<span class=\"number\">28</span>)</div><div class=\"line\">        x = F.relu(self.fc1(x))</div><div class=\"line\">        x = F.relu(self.fc2(x))</div><div class=\"line\">        x = self.fc3(x)</div><div class=\"line\">        loss = self.ceriation(x, target)</div><div class=\"line\">        <span class=\"keyword\">return</span> x, loss</div></pre></td></tr></table></figure>\n<p>由于PyTorch可以实现自动求导，所以我们只需实现<code>forward</code>过程即可。这里由于池化层和非线性变换都没有参数，所以使用了<code>nn.functionals</code>中的对应操作实现。通过看文档，可以发现，一般<code>nn</code>里面的各种层，都会在<code>nn.functionals</code>里面有其对应。例如卷积层的对应实现，如下所示，需要传入卷积核的权重。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\"># With square kernels and equal stride</span></div><div class=\"line\">filters = autograd.Variable(torch.randn(<span class=\"number\">8</span>,<span class=\"number\">4</span>,<span class=\"number\">3</span>,<span class=\"number\">3</span>))</div><div class=\"line\">inputs = autograd.Variable(torch.randn(<span class=\"number\">1</span>,<span class=\"number\">4</span>,<span class=\"number\">5</span>,<span class=\"number\">5</span>))</div><div class=\"line\">F.conv2d(inputs, filters, padding=<span class=\"number\">1</span>)</div></pre></td></tr></table></figure>\n<p>同样地，我们可以实现LeNet的结构如下。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div></pre></td><td class=\"code\"><pre><div class=\"line\"></div><div class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">LeNet</span><span class=\"params\">(nn.Module)</span>:</span></div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self)</span>:</span></div><div class=\"line\">        super(LeNet, self).__init__()</div><div class=\"line\">        self.conv1 = nn.Conv2d(<span class=\"number\">1</span>, <span class=\"number\">20</span>, <span class=\"number\">5</span>, <span class=\"number\">1</span>)</div><div class=\"line\">        self.conv2 = nn.Conv2d(<span class=\"number\">20</span>, <span class=\"number\">50</span>, <span class=\"number\">5</span>, <span class=\"number\">1</span>)</div><div class=\"line\">        self.fc1 = nn.Linear(<span class=\"number\">4</span>*<span class=\"number\">4</span>*<span class=\"number\">50</span>, <span class=\"number\">500</span>)</div><div class=\"line\">        self.fc2 = nn.Linear(<span class=\"number\">500</span>, <span class=\"number\">10</span>)</div><div class=\"line\">        self.ceriation = nn.CrossEntropyLoss()</div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">forward</span><span class=\"params\">(self, x, target)</span>:</span></div><div class=\"line\">        x = F.relu(self.conv1(x))</div><div class=\"line\">        x = F.max_pool2d(x, <span class=\"number\">2</span>, <span class=\"number\">2</span>)</div><div class=\"line\">        x = F.relu(self.conv2(x))</div><div class=\"line\">        x = F.max_pool2d(x, <span class=\"number\">2</span>, <span class=\"number\">2</span>)</div><div class=\"line\">        x = x.view(<span class=\"number\">-1</span>, <span class=\"number\">4</span>*<span class=\"number\">4</span>*<span class=\"number\">50</span>)</div><div class=\"line\">        x = F.relu(self.fc1(x))</div><div class=\"line\">        x = self.fc2(x)</div><div class=\"line\">        loss = self.ceriation(x, target)</div><div class=\"line\">        <span class=\"keyword\">return</span> x, loss</div></pre></td></tr></table></figure>\n<h2 id=\"训练与测试\"><a href=\"#训练与测试\" class=\"headerlink\" title=\"训练与测试\"></a>训练与测试</h2><p>在训练时，我们首先应确定优化方法。这里我们使用带动量的<code>SGD</code>方法。下面代码中的<code>optim.SGD</code>初始化需要接受网络中待优化的<code>Parameter</code>列表（或是迭代器），以及学习率<code>lr</code>，动量<code>momentum</code>。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">optimizer = optim.SGD(model.parameters(), lr=<span class=\"number\">0.01</span>, momentum=<span class=\"number\">0.9</span>)</div></pre></td></tr></table></figure>\n<p>接下来，我们只需要遍历数据集，同时在每次迭代中清空待优化参数的梯度，前向计算，反向传播以及优化器的迭代求解即可。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div></pre></td><td class=\"code\"><pre><div class=\"line\">model = MLPNet().cuda()   <span class=\"comment\"># 以MLP为例</span></div><div class=\"line\"><span class=\"keyword\">for</span> epoch <span class=\"keyword\">in</span> xrange(<span class=\"number\">10</span>):</div><div class=\"line\">    <span class=\"comment\"># trainning</span></div><div class=\"line\">    <span class=\"keyword\">for</span> batch_idx, (x, target) <span class=\"keyword\">in</span> enumerate(train_loader):</div><div class=\"line\">        optimizer.zero_grad()     <span class=\"comment\">#每次都要清空上一步中参数的grad，否则会出错的~</span></div><div class=\"line\">        x, target = Variable(x.cuda()), Variable(target.cuda())</div><div class=\"line\">        _, loss = model(x, target)   <span class=\"comment\">#得到loss</span></div><div class=\"line\">        loss.backward()              <span class=\"comment\">#bp</span></div><div class=\"line\">        optimizer.step()             <span class=\"comment\">#优化器迭代</span></div><div class=\"line\">        <span class=\"keyword\">if</span> batch_idx % <span class=\"number\">100</span> == <span class=\"number\">0</span>:</div><div class=\"line\">            <span class=\"keyword\">print</span> <span class=\"string\">'==&gt;&gt;&gt; epoch: &#123;&#125;, batch index: &#123;&#125;, train loss: &#123;:.6f&#125;'</span>.format(epoch, batch_idx, loss.data[<span class=\"number\">0</span>])</div><div class=\"line\">    <span class=\"comment\"># testing</span></div><div class=\"line\">    correct_cnt, ave_loss = <span class=\"number\">0</span>, <span class=\"number\">0</span></div><div class=\"line\">    <span class=\"keyword\">for</span> batch_idx, (x, target) <span class=\"keyword\">in</span> enumerate(test_loader):</div><div class=\"line\">        x, target = Variable(x.cuda(), volatile=<span class=\"keyword\">True</span>), Variable(target.cuda(), volatile=<span class=\"keyword\">True</span>)</div><div class=\"line\">        score, loss = model(x, target)</div><div class=\"line\">        _, pred_label = torch.max(score.data, <span class=\"number\">1</span>)</div><div class=\"line\">        correct_cnt += (pred_label == target.data).sum()</div><div class=\"line\">        ave_loss += loss.data[<span class=\"number\">0</span>]</div><div class=\"line\">    accuracy = correct_cnt*<span class=\"number\">1.0</span>/len(test_loader)/batch_size</div><div class=\"line\">    ave_loss /= len(test_loader)</div></pre></td></tr></table></figure>\n<p>当优化完毕后，需要保存模型。这里<a href=\"http://pytorch.org/docs/notes/serialization.html#recommend-saving-models\">官方文档</a>给出了推荐的方法，如下所示：<br><figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\">torch.save(model.state_dict(), PATH)   <span class=\"comment\">#保存网络参数</span></div><div class=\"line\">the_model = TheModelClass(*args, **kwargs)</div><div class=\"line\">the_model.load_state_dict(torch.load(PATH))  <span class=\"comment\">#读取网络参数</span></div></pre></td></tr></table></figure></p>\n<p>该博客的完整代码可以见：<a href=\"https://gist.github.com/xmfbit/b27cdbff68870418bdb8cefa86a2d558\">PyTorch MNIST demo</a>。</p>"},{"title":"Python中的迭代器和生成器","date":"2017-04-21T07:53:23.000Z","_content":"在STL中，迭代器可以剥离算法和具体数据类型之间的耦合，使得库的维护者只需要为特定的迭代器（如前向迭代器，反向迭代器和随机迭代器）等实现算法即可，而不用关心具体的数据结构。在Python中，迭代器更是无处不在。这篇博客简要介绍Python中的迭代器和生成器，它们背后的原理以及如何实现一个自定义的迭代器/生成器，主要参考了教程[Iterators & Generators](http://anandology.com/python-practice-book/iterators.html)。\n\n<!-- more -->\n\n## 迭代器\n使用`for`循环时，常常遇到迭代器。如下所示，可能是最常用的一种方式。\n\n``` py\nfor i in range(100):\n    # do something 100 times\n```\n\n在Python中，凡是可迭代的对象（Iterable Object），都可以用上面的方式进行迭代循环。例如，当被迭代对象是字符串时，每次得到的是字符串中的单个字符；当被迭代对象是文本文件时，每次得到的是文件中每一行的字符串；当被迭代对象是字典时，每次得到的是字典的`key`。\n\n同样，也有很多函数接受的参数为可迭代对象。例如`list()`和`tuple()`，当传入的参数为刻碟带对象时，返回的是由迭代返回值组成的列表或者元组。例如\n\n``` py\nlist({'x':1, 'y':2})  # => ['x', 'y']\n```\n\n为什么`list`或者`str`这样的可迭代对象能够被迭代呢？或者，自定义的类满足什么条件，就可以用`for x in XXX`这种方法来遍历了呢？\n\n在Python中，有内建的函数`iter()`和`next()`。一般用法时，`iter()`方法接受一个可迭代对象，会调用这个对象的`__iter__()`方法，返回作用在这个可迭代对象的迭代器。而作为一个迭代器，必须有“迭代器的自我修养”，也就是实现`next()`方法（Python3中改为了`__next__()`方法）。\n\n如下面的例子，`yrange_iter`是`yrange`的一个迭代器。`yrange`实现了`__iter__()`方法，是一个可迭代对象。调用`iter(yrange object)`的结果就是返回一个`yrange_iter`的对象实例。\n\n``` py\n# Version 1.0 使用迭代器类\nclass yrange_iter(object):\n    def __init__(self, yrange):\n        self.n = yrange.n\n        self.i = 0\n    def next(self):\n        v = self.i\n        self.i += 1\n        return v\n\nclass yrange(object):\n    def __init__(self, n):\n        self.n = n\n    def __iter__(self):\n        return yrange_iter(self)\n\nprint type(iter(yrange(5))) # <class '__main__.yrange_iter'>\n```\n\n而不停地调用迭代器的`next()`方法，就能够不断输出迭代序列。如下所示：\n\n``` py\nIn [3]: yiter = iter(yrange(5))\n\nIn [4]: yiter.next()\nOut[4]: 0\n\nIn [5]: yiter.next()\nOut[5]: 1\n\nIn [6]: yiter.next()\nOut[6]: 2\n```\n\n其实，上面的代码略显复杂。在代码量很小，不是很在意代码可复用性时，我们完全可以去掉`yrange_iter`，直接让`yrange.__iter__()`方法返回其自身实例。这样，我们只需要在`yrange`类中实现`__iter__()`方法和`next()`方法即可。如下所示：\n\n``` py\n# Version2.0 简化版，迭代器是本身\nclass yrange(object):\n    def __init__(self, n):\n        self.n = n\n    def __iter__(self):\n        self.i = 0\n        return self\n    def next(self):\n        v = self.i\n        self.i += 1\n        return v\n\nIn [8]: yiter = iter(yrange(5))\n\nIn [9]: yiter.next()\nOut[9]: 0\n\nIn [10]: yiter.next()\nOut[10]: 1\n\nIn [11]: yiter.next()\nOut[11]: 2\n```\n\n然而，上述的代码仍然存在问题，我们无法指定迭代器生成序列的长度，也就是`self.n`实际上并没有用到。如果我只想产生0到10以内的序列呢？\n\n我们只需要加入判断条件，当超出序列边界时，抛出Python内建的`StopIteration`异常即可。\n\n``` py\n# Version3.0 加入边界判断，生成有限长度序列\nclass yrange(object):\n    def __init__(self, n):\n        self.n = n\n    def __iter__(self):\n        self.i = 0\n        return self\n    def next(self):\n        if self.i == self.n:\n            raise StopIteration\n        v = self.i\n        self.i += 1\n        return v\n\nfor i in yrange(5):\n    print i\n```\n\n### Problem 1\nWrite an iterator class `reverse_iter`, that takes a `list` and iterates it from the reverse direction.\n\n``` py\nclass reverse_iter(object):\n    def __init__(self, alist):\n        self.container = alist\n        self.i = len(alist)\n\n    def next(self):\n        if self.i == 0:\n            raise StopIteration\n        self.i -= 1\n        return self.container[self.i]\nit = reverse_iter([1, 2, 3, 4])\n```\n\n## 生成器\n生成器是一种方法，他指定了如何生成序列中的元素，生成器内部包含特殊的`yield`语句。此外，生成器函数是懒惰求值，只有当调用`next()`方法时，生成器才开始顺序执行，直到遇到`yield`语句。`yield`语句就像`return`，但是并未退出，而是打上断电，等待下一次`next()`方法的调用，再从上一次的断点处开始执行。我直接贴出教程中的代码示例。\n\n``` py\n>>> def foo():\n        print \"begin\"\n        for i in range(3):\n            print \"before yield\", i\n            yield i\n            print \"after yield\", i\n        print \"end\"\n\n>>> f = foo()\n>>> f.next()\nbegin\nbefore yield 0\n0\n>>> f.next()\nafter yield 0\nbefore yield 1\n1\n>>> f.next()\nafter yield 1\nbefore yield 2\n2\n>>> f.next()\nafter yield 2\nend\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nStopIteration\n>>>\n```\n\n### 生成器表达式\n生成器表达式和列表相似，将`[]`换为`()`即可。如下所示：\n\n``` py\nfor i in (x**2 for x in [1,2,3,4]):\n    print i\n# print 1 4 9 16\n```\n\n生成器的好处在于惰性求值，这样一来，我们还可以生成无限长的序列。因为生成器本来就是说明了序列的生成方式，而并没有真的生成那个序列。\n\n下面的代码使用生成器得到前10组勾股数。通过在调用`take()`方法时修改传入实参`n`的大小，该代码可以很方便地转换为求取任意多得勾股数。生成器的重要作用体现在斜边`x`的取值为$[0, \\infty]$。如果不使用生成器，恐怕就需要写出好几行的循环语句加上`break`配合才可以达到相同的效果。\n\n``` py\ndef integer(start, end=None):\n    \"\"\"Generate integer sequence [start, end)\n       If `end` is not given, then [start, \\infty]\n    \"\"\"\n    i = start\n    while True:\n        if end is not None and i == end:\n            raise StopIteration\n        yield i\n        i += 1\n\ndef take(n, g):\n    i = 0\n    while True:\n        if i < n:\n            yield g.next()\n            i += 1\n        else:\n            raise StopIteration\n\n# 假定 x>y>z，以消除两直角边互换的情况，如10, 6, 8和10, 8, 6\ntup = ((x,y,z) for x in integer(0) for y in integer(0, x) for z in integer(0, y) if x*x==y*y+z*z)\nlist(take(10, tup))\n```\n\n### Problem 2\nWrite a program that takes one or more filenames as arguments and prints all the lines which are longer than 40 characters.\n\n``` py\ndef readfiles(filenames):\n    for f in filenames:\n        for line in open(f):\n            yield line\n\ndef grep(lines):\n    return (line for line in lines if len(line)>40)\n\ndef printlines(lines):\n    for line in lines:\n        print line,\n\ndef main(filenames):\n    lines = readfiles(filenames)\n    lines = grep(lines)\n    printlines(lines)\n```\n\n### Problem 3\nWrite a function `findfiles` that recursively descends the directory tree for the specified directory and generates paths of all the files in the tree.\n\n注意`get_all_file()`方法中递归中生成器的写法，见SO的[这个帖子](http://stackoverflow.com/questions/248830/python-using-a-recursive-algorithm-as-a-generator\n)。\n\n``` py\nimport os\n\ndef generate_all_file(root):\n    for item in os.listdir(root):\n        item = os.path.join(root, item)\n        if os.path.isfile(item):\n            yield os.path.abspath(item)\n        else:\n            for item in generate_all_file(item):\n                yield item\n\ndef findfiles(root):\n    for item in generate_all_file(root):\n        print item\n```\n\n### Problem 4\nWrite a function to compute the number of python files (.py extension) in a specified directory recursively.\n\n``` py\ndef generate_all_py_file(root):\n    return (file for file in generate_all_file(root) if os.path.splitext(file)[-1] == '.py')\n\nprint len(list(generate_all_py_file('./')))\n```\n\n### Problem 5\nWrite a function to compute the total number of lines of code in all python files in the specified directory recursively.\n\n``` py\ndef generate_all_line(root):\n    return (line for f in generate_all_py_file(root) for line in open(f))\nprint len(list(generate_all_line('./')))\n```\n\n### Problem 6\nWrite a function to compute the total number of lines of code, ignoring empty and comment lines, in all python files in the specified directory recursively.\n\n``` py\ndef generate_all_no_empty_and_comment_line(root):\n    return (line for line in generate_all_line(root) if not (line=='' or line.startswith('#')))\n\nprint len(list(generate_all_no_empty_and_comment_line('./')))\n```\n\n### Problem 7\nWrite a program `split.py`, that takes an integer `n` and a `filename` as command line arguments and splits the `file` into multiple small files with each having `n` lines.\n\n``` py\ndef get_numbered_line(filename):\n    i = 0\n    for line in open(filename):\n        yield i, line\n        i += 1\n\ndef split(file_name, n):\n    i = 0\n    f = open('output-%d.txt' %i, 'w')\n    for idx, line in get_numbered_line(file_name):\n        f.write(line)\n        if (idx+1) % n == 0:\n            f.close()\n            i += 1\n            f = open('output-%d.txt' %i, 'w')\n\n    f.close()\n```\n\n### Problem 9\nThe built-in function `enumerate` takes an `iteratable` and returns an `iterator` over pairs ``(index, value)`` for each value in the source.\n\nWrite a function `my_enumerate` that works like `enumerate`.\n\n``` py\ndef my_enumerate(iterable):\n    i = 0\n    seq = iter(iterable)\n    while True:\n        val = seq.next()\n        yield i, val\n        i += 1\n```\n","source":"_posts/python-iter-generator.md","raw":"---\ntitle: Python中的迭代器和生成器\ndate: 2017-04-21 15:53:23\ntags:\n     - python\n---\n在STL中，迭代器可以剥离算法和具体数据类型之间的耦合，使得库的维护者只需要为特定的迭代器（如前向迭代器，反向迭代器和随机迭代器）等实现算法即可，而不用关心具体的数据结构。在Python中，迭代器更是无处不在。这篇博客简要介绍Python中的迭代器和生成器，它们背后的原理以及如何实现一个自定义的迭代器/生成器，主要参考了教程[Iterators & Generators](http://anandology.com/python-practice-book/iterators.html)。\n\n<!-- more -->\n\n## 迭代器\n使用`for`循环时，常常遇到迭代器。如下所示，可能是最常用的一种方式。\n\n``` py\nfor i in range(100):\n    # do something 100 times\n```\n\n在Python中，凡是可迭代的对象（Iterable Object），都可以用上面的方式进行迭代循环。例如，当被迭代对象是字符串时，每次得到的是字符串中的单个字符；当被迭代对象是文本文件时，每次得到的是文件中每一行的字符串；当被迭代对象是字典时，每次得到的是字典的`key`。\n\n同样，也有很多函数接受的参数为可迭代对象。例如`list()`和`tuple()`，当传入的参数为刻碟带对象时，返回的是由迭代返回值组成的列表或者元组。例如\n\n``` py\nlist({'x':1, 'y':2})  # => ['x', 'y']\n```\n\n为什么`list`或者`str`这样的可迭代对象能够被迭代呢？或者，自定义的类满足什么条件，就可以用`for x in XXX`这种方法来遍历了呢？\n\n在Python中，有内建的函数`iter()`和`next()`。一般用法时，`iter()`方法接受一个可迭代对象，会调用这个对象的`__iter__()`方法，返回作用在这个可迭代对象的迭代器。而作为一个迭代器，必须有“迭代器的自我修养”，也就是实现`next()`方法（Python3中改为了`__next__()`方法）。\n\n如下面的例子，`yrange_iter`是`yrange`的一个迭代器。`yrange`实现了`__iter__()`方法，是一个可迭代对象。调用`iter(yrange object)`的结果就是返回一个`yrange_iter`的对象实例。\n\n``` py\n# Version 1.0 使用迭代器类\nclass yrange_iter(object):\n    def __init__(self, yrange):\n        self.n = yrange.n\n        self.i = 0\n    def next(self):\n        v = self.i\n        self.i += 1\n        return v\n\nclass yrange(object):\n    def __init__(self, n):\n        self.n = n\n    def __iter__(self):\n        return yrange_iter(self)\n\nprint type(iter(yrange(5))) # <class '__main__.yrange_iter'>\n```\n\n而不停地调用迭代器的`next()`方法，就能够不断输出迭代序列。如下所示：\n\n``` py\nIn [3]: yiter = iter(yrange(5))\n\nIn [4]: yiter.next()\nOut[4]: 0\n\nIn [5]: yiter.next()\nOut[5]: 1\n\nIn [6]: yiter.next()\nOut[6]: 2\n```\n\n其实，上面的代码略显复杂。在代码量很小，不是很在意代码可复用性时，我们完全可以去掉`yrange_iter`，直接让`yrange.__iter__()`方法返回其自身实例。这样，我们只需要在`yrange`类中实现`__iter__()`方法和`next()`方法即可。如下所示：\n\n``` py\n# Version2.0 简化版，迭代器是本身\nclass yrange(object):\n    def __init__(self, n):\n        self.n = n\n    def __iter__(self):\n        self.i = 0\n        return self\n    def next(self):\n        v = self.i\n        self.i += 1\n        return v\n\nIn [8]: yiter = iter(yrange(5))\n\nIn [9]: yiter.next()\nOut[9]: 0\n\nIn [10]: yiter.next()\nOut[10]: 1\n\nIn [11]: yiter.next()\nOut[11]: 2\n```\n\n然而，上述的代码仍然存在问题，我们无法指定迭代器生成序列的长度，也就是`self.n`实际上并没有用到。如果我只想产生0到10以内的序列呢？\n\n我们只需要加入判断条件，当超出序列边界时，抛出Python内建的`StopIteration`异常即可。\n\n``` py\n# Version3.0 加入边界判断，生成有限长度序列\nclass yrange(object):\n    def __init__(self, n):\n        self.n = n\n    def __iter__(self):\n        self.i = 0\n        return self\n    def next(self):\n        if self.i == self.n:\n            raise StopIteration\n        v = self.i\n        self.i += 1\n        return v\n\nfor i in yrange(5):\n    print i\n```\n\n### Problem 1\nWrite an iterator class `reverse_iter`, that takes a `list` and iterates it from the reverse direction.\n\n``` py\nclass reverse_iter(object):\n    def __init__(self, alist):\n        self.container = alist\n        self.i = len(alist)\n\n    def next(self):\n        if self.i == 0:\n            raise StopIteration\n        self.i -= 1\n        return self.container[self.i]\nit = reverse_iter([1, 2, 3, 4])\n```\n\n## 生成器\n生成器是一种方法，他指定了如何生成序列中的元素，生成器内部包含特殊的`yield`语句。此外，生成器函数是懒惰求值，只有当调用`next()`方法时，生成器才开始顺序执行，直到遇到`yield`语句。`yield`语句就像`return`，但是并未退出，而是打上断电，等待下一次`next()`方法的调用，再从上一次的断点处开始执行。我直接贴出教程中的代码示例。\n\n``` py\n>>> def foo():\n        print \"begin\"\n        for i in range(3):\n            print \"before yield\", i\n            yield i\n            print \"after yield\", i\n        print \"end\"\n\n>>> f = foo()\n>>> f.next()\nbegin\nbefore yield 0\n0\n>>> f.next()\nafter yield 0\nbefore yield 1\n1\n>>> f.next()\nafter yield 1\nbefore yield 2\n2\n>>> f.next()\nafter yield 2\nend\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nStopIteration\n>>>\n```\n\n### 生成器表达式\n生成器表达式和列表相似，将`[]`换为`()`即可。如下所示：\n\n``` py\nfor i in (x**2 for x in [1,2,3,4]):\n    print i\n# print 1 4 9 16\n```\n\n生成器的好处在于惰性求值，这样一来，我们还可以生成无限长的序列。因为生成器本来就是说明了序列的生成方式，而并没有真的生成那个序列。\n\n下面的代码使用生成器得到前10组勾股数。通过在调用`take()`方法时修改传入实参`n`的大小，该代码可以很方便地转换为求取任意多得勾股数。生成器的重要作用体现在斜边`x`的取值为$[0, \\infty]$。如果不使用生成器，恐怕就需要写出好几行的循环语句加上`break`配合才可以达到相同的效果。\n\n``` py\ndef integer(start, end=None):\n    \"\"\"Generate integer sequence [start, end)\n       If `end` is not given, then [start, \\infty]\n    \"\"\"\n    i = start\n    while True:\n        if end is not None and i == end:\n            raise StopIteration\n        yield i\n        i += 1\n\ndef take(n, g):\n    i = 0\n    while True:\n        if i < n:\n            yield g.next()\n            i += 1\n        else:\n            raise StopIteration\n\n# 假定 x>y>z，以消除两直角边互换的情况，如10, 6, 8和10, 8, 6\ntup = ((x,y,z) for x in integer(0) for y in integer(0, x) for z in integer(0, y) if x*x==y*y+z*z)\nlist(take(10, tup))\n```\n\n### Problem 2\nWrite a program that takes one or more filenames as arguments and prints all the lines which are longer than 40 characters.\n\n``` py\ndef readfiles(filenames):\n    for f in filenames:\n        for line in open(f):\n            yield line\n\ndef grep(lines):\n    return (line for line in lines if len(line)>40)\n\ndef printlines(lines):\n    for line in lines:\n        print line,\n\ndef main(filenames):\n    lines = readfiles(filenames)\n    lines = grep(lines)\n    printlines(lines)\n```\n\n### Problem 3\nWrite a function `findfiles` that recursively descends the directory tree for the specified directory and generates paths of all the files in the tree.\n\n注意`get_all_file()`方法中递归中生成器的写法，见SO的[这个帖子](http://stackoverflow.com/questions/248830/python-using-a-recursive-algorithm-as-a-generator\n)。\n\n``` py\nimport os\n\ndef generate_all_file(root):\n    for item in os.listdir(root):\n        item = os.path.join(root, item)\n        if os.path.isfile(item):\n            yield os.path.abspath(item)\n        else:\n            for item in generate_all_file(item):\n                yield item\n\ndef findfiles(root):\n    for item in generate_all_file(root):\n        print item\n```\n\n### Problem 4\nWrite a function to compute the number of python files (.py extension) in a specified directory recursively.\n\n``` py\ndef generate_all_py_file(root):\n    return (file for file in generate_all_file(root) if os.path.splitext(file)[-1] == '.py')\n\nprint len(list(generate_all_py_file('./')))\n```\n\n### Problem 5\nWrite a function to compute the total number of lines of code in all python files in the specified directory recursively.\n\n``` py\ndef generate_all_line(root):\n    return (line for f in generate_all_py_file(root) for line in open(f))\nprint len(list(generate_all_line('./')))\n```\n\n### Problem 6\nWrite a function to compute the total number of lines of code, ignoring empty and comment lines, in all python files in the specified directory recursively.\n\n``` py\ndef generate_all_no_empty_and_comment_line(root):\n    return (line for line in generate_all_line(root) if not (line=='' or line.startswith('#')))\n\nprint len(list(generate_all_no_empty_and_comment_line('./')))\n```\n\n### Problem 7\nWrite a program `split.py`, that takes an integer `n` and a `filename` as command line arguments and splits the `file` into multiple small files with each having `n` lines.\n\n``` py\ndef get_numbered_line(filename):\n    i = 0\n    for line in open(filename):\n        yield i, line\n        i += 1\n\ndef split(file_name, n):\n    i = 0\n    f = open('output-%d.txt' %i, 'w')\n    for idx, line in get_numbered_line(file_name):\n        f.write(line)\n        if (idx+1) % n == 0:\n            f.close()\n            i += 1\n            f = open('output-%d.txt' %i, 'w')\n\n    f.close()\n```\n\n### Problem 9\nThe built-in function `enumerate` takes an `iteratable` and returns an `iterator` over pairs ``(index, value)`` for each value in the source.\n\nWrite a function `my_enumerate` that works like `enumerate`.\n\n``` py\ndef my_enumerate(iterable):\n    i = 0\n    seq = iter(iterable)\n    while True:\n        val = seq.next()\n        yield i, val\n        i += 1\n```\n","slug":"python-iter-generator","published":1,"updated":"2018-01-12T06:22:20.474Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjcu6vcu30026qu46zrc8yvpd","content":"<p>在STL中，迭代器可以剥离算法和具体数据类型之间的耦合，使得库的维护者只需要为特定的迭代器（如前向迭代器，反向迭代器和随机迭代器）等实现算法即可，而不用关心具体的数据结构。在Python中，迭代器更是无处不在。这篇博客简要介绍Python中的迭代器和生成器，它们背后的原理以及如何实现一个自定义的迭代器/生成器，主要参考了教程<a href=\"http://anandology.com/python-practice-book/iterators.html\" target=\"_blank\" rel=\"external\">Iterators &amp; Generators</a>。</p>\n<a id=\"more\"></a>\n<h2 id=\"迭代器\"><a href=\"#迭代器\" class=\"headerlink\" title=\"迭代器\"></a>迭代器</h2><p>使用<code>for</code>循环时，常常遇到迭代器。如下所示，可能是最常用的一种方式。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">100</span>):</div><div class=\"line\">    <span class=\"comment\"># do something 100 times</span></div></pre></td></tr></table></figure>\n<p>在Python中，凡是可迭代的对象（Iterable Object），都可以用上面的方式进行迭代循环。例如，当被迭代对象是字符串时，每次得到的是字符串中的单个字符；当被迭代对象是文本文件时，每次得到的是文件中每一行的字符串；当被迭代对象是字典时，每次得到的是字典的<code>key</code>。</p>\n<p>同样，也有很多函数接受的参数为可迭代对象。例如<code>list()</code>和<code>tuple()</code>，当传入的参数为刻碟带对象时，返回的是由迭代返回值组成的列表或者元组。例如</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">list(&#123;<span class=\"string\">'x'</span>:<span class=\"number\">1</span>, <span class=\"string\">'y'</span>:<span class=\"number\">2</span>&#125;)  <span class=\"comment\"># =&gt; ['x', 'y']</span></div></pre></td></tr></table></figure>\n<p>为什么<code>list</code>或者<code>str</code>这样的可迭代对象能够被迭代呢？或者，自定义的类满足什么条件，就可以用<code>for x in XXX</code>这种方法来遍历了呢？</p>\n<p>在Python中，有内建的函数<code>iter()</code>和<code>next()</code>。一般用法时，<code>iter()</code>方法接受一个可迭代对象，会调用这个对象的<code>__iter__()</code>方法，返回作用在这个可迭代对象的迭代器。而作为一个迭代器，必须有“迭代器的自我修养”，也就是实现<code>next()</code>方法（Python3中改为了<code>__next__()</code>方法）。</p>\n<p>如下面的例子，<code>yrange_iter</code>是<code>yrange</code>的一个迭代器。<code>yrange</code>实现了<code>__iter__()</code>方法，是一个可迭代对象。调用<code>iter(yrange object)</code>的结果就是返回一个<code>yrange_iter</code>的对象实例。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\"># Version 1.0 使用迭代器类</span></div><div class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">yrange_iter</span><span class=\"params\">(object)</span>:</span></div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self, yrange)</span>:</span></div><div class=\"line\">        self.n = yrange.n</div><div class=\"line\">        self.i = <span class=\"number\">0</span></div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">next</span><span class=\"params\">(self)</span>:</span></div><div class=\"line\">        v = self.i</div><div class=\"line\">        self.i += <span class=\"number\">1</span></div><div class=\"line\">        <span class=\"keyword\">return</span> v</div><div class=\"line\"></div><div class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">yrange</span><span class=\"params\">(object)</span>:</span></div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self, n)</span>:</span></div><div class=\"line\">        self.n = n</div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__iter__</span><span class=\"params\">(self)</span>:</span></div><div class=\"line\">        <span class=\"keyword\">return</span> yrange_iter(self)</div><div class=\"line\"></div><div class=\"line\"><span class=\"keyword\">print</span> type(iter(yrange(<span class=\"number\">5</span>))) <span class=\"comment\"># &lt;class '__main__.yrange_iter'&gt;</span></div></pre></td></tr></table></figure>\n<p>而不停地调用迭代器的<code>next()</code>方法，就能够不断输出迭代序列。如下所示：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div></pre></td><td class=\"code\"><pre><div class=\"line\">In [<span class=\"number\">3</span>]: yiter = iter(yrange(<span class=\"number\">5</span>))</div><div class=\"line\"></div><div class=\"line\">In [<span class=\"number\">4</span>]: yiter.next()</div><div class=\"line\">Out[<span class=\"number\">4</span>]: <span class=\"number\">0</span></div><div class=\"line\"></div><div class=\"line\">In [<span class=\"number\">5</span>]: yiter.next()</div><div class=\"line\">Out[<span class=\"number\">5</span>]: <span class=\"number\">1</span></div><div class=\"line\"></div><div class=\"line\">In [<span class=\"number\">6</span>]: yiter.next()</div><div class=\"line\">Out[<span class=\"number\">6</span>]: <span class=\"number\">2</span></div></pre></td></tr></table></figure>\n<p>其实，上面的代码略显复杂。在代码量很小，不是很在意代码可复用性时，我们完全可以去掉<code>yrange_iter</code>，直接让<code>yrange.__iter__()</code>方法返回其自身实例。这样，我们只需要在<code>yrange</code>类中实现<code>__iter__()</code>方法和<code>next()</code>方法即可。如下所示：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\"># Version2.0 简化版，迭代器是本身</span></div><div class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">yrange</span><span class=\"params\">(object)</span>:</span></div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self, n)</span>:</span></div><div class=\"line\">        self.n = n</div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__iter__</span><span class=\"params\">(self)</span>:</span></div><div class=\"line\">        self.i = <span class=\"number\">0</span></div><div class=\"line\">        <span class=\"keyword\">return</span> self</div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">next</span><span class=\"params\">(self)</span>:</span></div><div class=\"line\">        v = self.i</div><div class=\"line\">        self.i += <span class=\"number\">1</span></div><div class=\"line\">        <span class=\"keyword\">return</span> v</div><div class=\"line\"></div><div class=\"line\">In [<span class=\"number\">8</span>]: yiter = iter(yrange(<span class=\"number\">5</span>))</div><div class=\"line\"></div><div class=\"line\">In [<span class=\"number\">9</span>]: yiter.next()</div><div class=\"line\">Out[<span class=\"number\">9</span>]: <span class=\"number\">0</span></div><div class=\"line\"></div><div class=\"line\">In [<span class=\"number\">10</span>]: yiter.next()</div><div class=\"line\">Out[<span class=\"number\">10</span>]: <span class=\"number\">1</span></div><div class=\"line\"></div><div class=\"line\">In [<span class=\"number\">11</span>]: yiter.next()</div><div class=\"line\">Out[<span class=\"number\">11</span>]: <span class=\"number\">2</span></div></pre></td></tr></table></figure>\n<p>然而，上述的代码仍然存在问题，我们无法指定迭代器生成序列的长度，也就是<code>self.n</code>实际上并没有用到。如果我只想产生0到10以内的序列呢？</p>\n<p>我们只需要加入判断条件，当超出序列边界时，抛出Python内建的<code>StopIteration</code>异常即可。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\"># Version3.0 加入边界判断，生成有限长度序列</span></div><div class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">yrange</span><span class=\"params\">(object)</span>:</span></div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self, n)</span>:</span></div><div class=\"line\">        self.n = n</div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__iter__</span><span class=\"params\">(self)</span>:</span></div><div class=\"line\">        self.i = <span class=\"number\">0</span></div><div class=\"line\">        <span class=\"keyword\">return</span> self</div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">next</span><span class=\"params\">(self)</span>:</span></div><div class=\"line\">        <span class=\"keyword\">if</span> self.i == self.n:</div><div class=\"line\">            <span class=\"keyword\">raise</span> StopIteration</div><div class=\"line\">        v = self.i</div><div class=\"line\">        self.i += <span class=\"number\">1</span></div><div class=\"line\">        <span class=\"keyword\">return</span> v</div><div class=\"line\"></div><div class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> yrange(<span class=\"number\">5</span>):</div><div class=\"line\">    <span class=\"keyword\">print</span> i</div></pre></td></tr></table></figure>\n<h3 id=\"Problem-1\"><a href=\"#Problem-1\" class=\"headerlink\" title=\"Problem 1\"></a>Problem 1</h3><p>Write an iterator class <code>reverse_iter</code>, that takes a <code>list</code> and iterates it from the reverse direction.</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">reverse_iter</span><span class=\"params\">(object)</span>:</span></div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self, alist)</span>:</span></div><div class=\"line\">        self.container = alist</div><div class=\"line\">        self.i = len(alist)</div><div class=\"line\"></div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">next</span><span class=\"params\">(self)</span>:</span></div><div class=\"line\">        <span class=\"keyword\">if</span> self.i == <span class=\"number\">0</span>:</div><div class=\"line\">            <span class=\"keyword\">raise</span> StopIteration</div><div class=\"line\">        self.i -= <span class=\"number\">1</span></div><div class=\"line\">        <span class=\"keyword\">return</span> self.container[self.i]</div><div class=\"line\">it = reverse_iter([<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">4</span>])</div></pre></td></tr></table></figure>\n<h2 id=\"生成器\"><a href=\"#生成器\" class=\"headerlink\" title=\"生成器\"></a>生成器</h2><p>生成器是一种方法，他指定了如何生成序列中的元素，生成器内部包含特殊的<code>yield</code>语句。此外，生成器函数是懒惰求值，只有当调用<code>next()</code>方法时，生成器才开始顺序执行，直到遇到<code>yield</code>语句。<code>yield</code>语句就像<code>return</code>，但是并未退出，而是打上断电，等待下一次<code>next()</code>方法的调用，再从上一次的断点处开始执行。我直接贴出教程中的代码示例。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">foo</span><span class=\"params\">()</span>:</span></div><div class=\"line\">        <span class=\"keyword\">print</span> <span class=\"string\">\"begin\"</span></div><div class=\"line\">        <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">3</span>):</div><div class=\"line\">            <span class=\"keyword\">print</span> <span class=\"string\">\"before yield\"</span>, i</div><div class=\"line\">            <span class=\"keyword\">yield</span> i</div><div class=\"line\">            <span class=\"keyword\">print</span> <span class=\"string\">\"after yield\"</span>, i</div><div class=\"line\">        <span class=\"keyword\">print</span> <span class=\"string\">\"end\"</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>f = foo()</div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>f.next()</div><div class=\"line\">begin</div><div class=\"line\">before <span class=\"keyword\">yield</span> <span class=\"number\">0</span></div><div class=\"line\"><span class=\"number\">0</span></div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>f.next()</div><div class=\"line\">after <span class=\"keyword\">yield</span> <span class=\"number\">0</span></div><div class=\"line\">before <span class=\"keyword\">yield</span> <span class=\"number\">1</span></div><div class=\"line\"><span class=\"number\">1</span></div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>f.next()</div><div class=\"line\">after <span class=\"keyword\">yield</span> <span class=\"number\">1</span></div><div class=\"line\">before <span class=\"keyword\">yield</span> <span class=\"number\">2</span></div><div class=\"line\"><span class=\"number\">2</span></div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>f.next()</div><div class=\"line\">after <span class=\"keyword\">yield</span> <span class=\"number\">2</span></div><div class=\"line\">end</div><div class=\"line\">Traceback (most recent call last):</div><div class=\"line\">  File <span class=\"string\">\"&lt;stdin&gt;\"</span>, line <span class=\"number\">1</span>, <span class=\"keyword\">in</span> &lt;module&gt;</div><div class=\"line\">StopIteration</div><div class=\"line\">&gt;&gt;&gt;</div></pre></td></tr></table></figure>\n<h3 id=\"生成器表达式\"><a href=\"#生成器表达式\" class=\"headerlink\" title=\"生成器表达式\"></a>生成器表达式</h3><p>生成器表达式和列表相似，将<code>[]</code>换为<code>()</code>即可。如下所示：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> (x**<span class=\"number\">2</span> <span class=\"keyword\">for</span> x <span class=\"keyword\">in</span> [<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">3</span>,<span class=\"number\">4</span>]):</div><div class=\"line\">    <span class=\"keyword\">print</span> i</div><div class=\"line\"><span class=\"comment\"># print 1 4 9 16</span></div></pre></td></tr></table></figure>\n<p>生成器的好处在于惰性求值，这样一来，我们还可以生成无限长的序列。因为生成器本来就是说明了序列的生成方式，而并没有真的生成那个序列。</p>\n<p>下面的代码使用生成器得到前10组勾股数。通过在调用<code>take()</code>方法时修改传入实参<code>n</code>的大小，该代码可以很方便地转换为求取任意多得勾股数。生成器的重要作用体现在斜边<code>x</code>的取值为$[0, \\infty]$。如果不使用生成器，恐怕就需要写出好几行的循环语句加上<code>break</code>配合才可以达到相同的效果。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">integer</span><span class=\"params\">(start, end=None)</span>:</span></div><div class=\"line\">    <span class=\"string\">\"\"\"Generate integer sequence [start, end)</span></div><div class=\"line\">       If `end` is not given, then [start, \\infty]</div><div class=\"line\">    \"\"\"</div><div class=\"line\">    i = start</div><div class=\"line\">    <span class=\"keyword\">while</span> <span class=\"keyword\">True</span>:</div><div class=\"line\">        <span class=\"keyword\">if</span> end <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"keyword\">None</span> <span class=\"keyword\">and</span> i == end:</div><div class=\"line\">            <span class=\"keyword\">raise</span> StopIteration</div><div class=\"line\">        <span class=\"keyword\">yield</span> i</div><div class=\"line\">        i += <span class=\"number\">1</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">take</span><span class=\"params\">(n, g)</span>:</span></div><div class=\"line\">    i = <span class=\"number\">0</span></div><div class=\"line\">    <span class=\"keyword\">while</span> <span class=\"keyword\">True</span>:</div><div class=\"line\">        <span class=\"keyword\">if</span> i &lt; n:</div><div class=\"line\">            <span class=\"keyword\">yield</span> g.next()</div><div class=\"line\">            i += <span class=\"number\">1</span></div><div class=\"line\">        <span class=\"keyword\">else</span>:</div><div class=\"line\">            <span class=\"keyword\">raise</span> StopIteration</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># 假定 x&gt;y&gt;z，以消除两直角边互换的情况，如10, 6, 8和10, 8, 6</span></div><div class=\"line\">tup = ((x,y,z) <span class=\"keyword\">for</span> x <span class=\"keyword\">in</span> integer(<span class=\"number\">0</span>) <span class=\"keyword\">for</span> y <span class=\"keyword\">in</span> integer(<span class=\"number\">0</span>, x) <span class=\"keyword\">for</span> z <span class=\"keyword\">in</span> integer(<span class=\"number\">0</span>, y) <span class=\"keyword\">if</span> x*x==y*y+z*z)</div><div class=\"line\">list(take(<span class=\"number\">10</span>, tup))</div></pre></td></tr></table></figure>\n<h3 id=\"Problem-2\"><a href=\"#Problem-2\" class=\"headerlink\" title=\"Problem 2\"></a>Problem 2</h3><p>Write a program that takes one or more filenames as arguments and prints all the lines which are longer than 40 characters.</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">readfiles</span><span class=\"params\">(filenames)</span>:</span></div><div class=\"line\">    <span class=\"keyword\">for</span> f <span class=\"keyword\">in</span> filenames:</div><div class=\"line\">        <span class=\"keyword\">for</span> line <span class=\"keyword\">in</span> open(f):</div><div class=\"line\">            <span class=\"keyword\">yield</span> line</div><div class=\"line\"></div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">grep</span><span class=\"params\">(lines)</span>:</span></div><div class=\"line\">    <span class=\"keyword\">return</span> (line <span class=\"keyword\">for</span> line <span class=\"keyword\">in</span> lines <span class=\"keyword\">if</span> len(line)&gt;<span class=\"number\">40</span>)</div><div class=\"line\"></div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">printlines</span><span class=\"params\">(lines)</span>:</span></div><div class=\"line\">    <span class=\"keyword\">for</span> line <span class=\"keyword\">in</span> lines:</div><div class=\"line\">        <span class=\"keyword\">print</span> line,</div><div class=\"line\"></div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">main</span><span class=\"params\">(filenames)</span>:</span></div><div class=\"line\">    lines = readfiles(filenames)</div><div class=\"line\">    lines = grep(lines)</div><div class=\"line\">    printlines(lines)</div></pre></td></tr></table></figure>\n<h3 id=\"Problem-3\"><a href=\"#Problem-3\" class=\"headerlink\" title=\"Problem 3\"></a>Problem 3</h3><p>Write a function <code>findfiles</code> that recursively descends the directory tree for the specified directory and generates paths of all the files in the tree.</p>\n<p>注意<code>get_all_file()</code>方法中递归中生成器的写法，见SO的<a href=\"http://stackoverflow.com/questions/248830/python-using-a-recursive-algorithm-as-a-generator\" target=\"_blank\" rel=\"external\">这个帖子</a>。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">import</span> os</div><div class=\"line\"></div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">generate_all_file</span><span class=\"params\">(root)</span>:</span></div><div class=\"line\">    <span class=\"keyword\">for</span> item <span class=\"keyword\">in</span> os.listdir(root):</div><div class=\"line\">        item = os.path.join(root, item)</div><div class=\"line\">        <span class=\"keyword\">if</span> os.path.isfile(item):</div><div class=\"line\">            <span class=\"keyword\">yield</span> os.path.abspath(item)</div><div class=\"line\">        <span class=\"keyword\">else</span>:</div><div class=\"line\">            <span class=\"keyword\">for</span> item <span class=\"keyword\">in</span> generate_all_file(item):</div><div class=\"line\">                <span class=\"keyword\">yield</span> item</div><div class=\"line\"></div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">findfiles</span><span class=\"params\">(root)</span>:</span></div><div class=\"line\">    <span class=\"keyword\">for</span> item <span class=\"keyword\">in</span> generate_all_file(root):</div><div class=\"line\">        <span class=\"keyword\">print</span> item</div></pre></td></tr></table></figure>\n<h3 id=\"Problem-4\"><a href=\"#Problem-4\" class=\"headerlink\" title=\"Problem 4\"></a>Problem 4</h3><p>Write a function to compute the number of python files (.py extension) in a specified directory recursively.</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">generate_all_py_file</span><span class=\"params\">(root)</span>:</span></div><div class=\"line\">    <span class=\"keyword\">return</span> (file <span class=\"keyword\">for</span> file <span class=\"keyword\">in</span> generate_all_file(root) <span class=\"keyword\">if</span> os.path.splitext(file)[<span class=\"number\">-1</span>] == <span class=\"string\">'.py'</span>)</div><div class=\"line\"></div><div class=\"line\"><span class=\"keyword\">print</span> len(list(generate_all_py_file(<span class=\"string\">'./'</span>)))</div></pre></td></tr></table></figure>\n<h3 id=\"Problem-5\"><a href=\"#Problem-5\" class=\"headerlink\" title=\"Problem 5\"></a>Problem 5</h3><p>Write a function to compute the total number of lines of code in all python files in the specified directory recursively.</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">generate_all_line</span><span class=\"params\">(root)</span>:</span></div><div class=\"line\">    <span class=\"keyword\">return</span> (line <span class=\"keyword\">for</span> f <span class=\"keyword\">in</span> generate_all_py_file(root) <span class=\"keyword\">for</span> line <span class=\"keyword\">in</span> open(f))</div><div class=\"line\"><span class=\"keyword\">print</span> len(list(generate_all_line(<span class=\"string\">'./'</span>)))</div></pre></td></tr></table></figure>\n<h3 id=\"Problem-6\"><a href=\"#Problem-6\" class=\"headerlink\" title=\"Problem 6\"></a>Problem 6</h3><p>Write a function to compute the total number of lines of code, ignoring empty and comment lines, in all python files in the specified directory recursively.</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">generate_all_no_empty_and_comment_line</span><span class=\"params\">(root)</span>:</span></div><div class=\"line\">    <span class=\"keyword\">return</span> (line <span class=\"keyword\">for</span> line <span class=\"keyword\">in</span> generate_all_line(root) <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> (line==<span class=\"string\">''</span> <span class=\"keyword\">or</span> line.startswith(<span class=\"string\">'#'</span>)))</div><div class=\"line\"></div><div class=\"line\"><span class=\"keyword\">print</span> len(list(generate_all_no_empty_and_comment_line(<span class=\"string\">'./'</span>)))</div></pre></td></tr></table></figure>\n<h3 id=\"Problem-7\"><a href=\"#Problem-7\" class=\"headerlink\" title=\"Problem 7\"></a>Problem 7</h3><p>Write a program <code>split.py</code>, that takes an integer <code>n</code> and a <code>filename</code> as command line arguments and splits the <code>file</code> into multiple small files with each having <code>n</code> lines.</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">get_numbered_line</span><span class=\"params\">(filename)</span>:</span></div><div class=\"line\">    i = <span class=\"number\">0</span></div><div class=\"line\">    <span class=\"keyword\">for</span> line <span class=\"keyword\">in</span> open(filename):</div><div class=\"line\">        <span class=\"keyword\">yield</span> i, line</div><div class=\"line\">        i += <span class=\"number\">1</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">split</span><span class=\"params\">(file_name, n)</span>:</span></div><div class=\"line\">    i = <span class=\"number\">0</span></div><div class=\"line\">    f = open(<span class=\"string\">'output-%d.txt'</span> %i, <span class=\"string\">'w'</span>)</div><div class=\"line\">    <span class=\"keyword\">for</span> idx, line <span class=\"keyword\">in</span> get_numbered_line(file_name):</div><div class=\"line\">        f.write(line)</div><div class=\"line\">        <span class=\"keyword\">if</span> (idx+<span class=\"number\">1</span>) % n == <span class=\"number\">0</span>:</div><div class=\"line\">            f.close()</div><div class=\"line\">            i += <span class=\"number\">1</span></div><div class=\"line\">            f = open(<span class=\"string\">'output-%d.txt'</span> %i, <span class=\"string\">'w'</span>)</div><div class=\"line\"></div><div class=\"line\">    f.close()</div></pre></td></tr></table></figure>\n<h3 id=\"Problem-9\"><a href=\"#Problem-9\" class=\"headerlink\" title=\"Problem 9\"></a>Problem 9</h3><p>The built-in function <code>enumerate</code> takes an <code>iteratable</code> and returns an <code>iterator</code> over pairs <code>(index, value)</code> for each value in the source.</p>\n<p>Write a function <code>my_enumerate</code> that works like <code>enumerate</code>.</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">my_enumerate</span><span class=\"params\">(iterable)</span>:</span></div><div class=\"line\">    i = <span class=\"number\">0</span></div><div class=\"line\">    seq = iter(iterable)</div><div class=\"line\">    <span class=\"keyword\">while</span> <span class=\"keyword\">True</span>:</div><div class=\"line\">        val = seq.next()</div><div class=\"line\">        <span class=\"keyword\">yield</span> i, val</div><div class=\"line\">        i += <span class=\"number\">1</span></div></pre></td></tr></table></figure>\n","excerpt":"<p>在STL中，迭代器可以剥离算法和具体数据类型之间的耦合，使得库的维护者只需要为特定的迭代器（如前向迭代器，反向迭代器和随机迭代器）等实现算法即可，而不用关心具体的数据结构。在Python中，迭代器更是无处不在。这篇博客简要介绍Python中的迭代器和生成器，它们背后的原理以及如何实现一个自定义的迭代器/生成器，主要参考了教程<a href=\"http://anandology.com/python-practice-book/iterators.html\">Iterators &amp; Generators</a>。</p>","more":"<h2 id=\"迭代器\"><a href=\"#迭代器\" class=\"headerlink\" title=\"迭代器\"></a>迭代器</h2><p>使用<code>for</code>循环时，常常遇到迭代器。如下所示，可能是最常用的一种方式。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">100</span>):</div><div class=\"line\">    <span class=\"comment\"># do something 100 times</span></div></pre></td></tr></table></figure>\n<p>在Python中，凡是可迭代的对象（Iterable Object），都可以用上面的方式进行迭代循环。例如，当被迭代对象是字符串时，每次得到的是字符串中的单个字符；当被迭代对象是文本文件时，每次得到的是文件中每一行的字符串；当被迭代对象是字典时，每次得到的是字典的<code>key</code>。</p>\n<p>同样，也有很多函数接受的参数为可迭代对象。例如<code>list()</code>和<code>tuple()</code>，当传入的参数为刻碟带对象时，返回的是由迭代返回值组成的列表或者元组。例如</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">list(&#123;<span class=\"string\">'x'</span>:<span class=\"number\">1</span>, <span class=\"string\">'y'</span>:<span class=\"number\">2</span>&#125;)  <span class=\"comment\"># =&gt; ['x', 'y']</span></div></pre></td></tr></table></figure>\n<p>为什么<code>list</code>或者<code>str</code>这样的可迭代对象能够被迭代呢？或者，自定义的类满足什么条件，就可以用<code>for x in XXX</code>这种方法来遍历了呢？</p>\n<p>在Python中，有内建的函数<code>iter()</code>和<code>next()</code>。一般用法时，<code>iter()</code>方法接受一个可迭代对象，会调用这个对象的<code>__iter__()</code>方法，返回作用在这个可迭代对象的迭代器。而作为一个迭代器，必须有“迭代器的自我修养”，也就是实现<code>next()</code>方法（Python3中改为了<code>__next__()</code>方法）。</p>\n<p>如下面的例子，<code>yrange_iter</code>是<code>yrange</code>的一个迭代器。<code>yrange</code>实现了<code>__iter__()</code>方法，是一个可迭代对象。调用<code>iter(yrange object)</code>的结果就是返回一个<code>yrange_iter</code>的对象实例。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\"># Version 1.0 使用迭代器类</span></div><div class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">yrange_iter</span><span class=\"params\">(object)</span>:</span></div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self, yrange)</span>:</span></div><div class=\"line\">        self.n = yrange.n</div><div class=\"line\">        self.i = <span class=\"number\">0</span></div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">next</span><span class=\"params\">(self)</span>:</span></div><div class=\"line\">        v = self.i</div><div class=\"line\">        self.i += <span class=\"number\">1</span></div><div class=\"line\">        <span class=\"keyword\">return</span> v</div><div class=\"line\"></div><div class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">yrange</span><span class=\"params\">(object)</span>:</span></div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self, n)</span>:</span></div><div class=\"line\">        self.n = n</div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__iter__</span><span class=\"params\">(self)</span>:</span></div><div class=\"line\">        <span class=\"keyword\">return</span> yrange_iter(self)</div><div class=\"line\"></div><div class=\"line\"><span class=\"keyword\">print</span> type(iter(yrange(<span class=\"number\">5</span>))) <span class=\"comment\"># &lt;class '__main__.yrange_iter'&gt;</span></div></pre></td></tr></table></figure>\n<p>而不停地调用迭代器的<code>next()</code>方法，就能够不断输出迭代序列。如下所示：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div></pre></td><td class=\"code\"><pre><div class=\"line\">In [<span class=\"number\">3</span>]: yiter = iter(yrange(<span class=\"number\">5</span>))</div><div class=\"line\"></div><div class=\"line\">In [<span class=\"number\">4</span>]: yiter.next()</div><div class=\"line\">Out[<span class=\"number\">4</span>]: <span class=\"number\">0</span></div><div class=\"line\"></div><div class=\"line\">In [<span class=\"number\">5</span>]: yiter.next()</div><div class=\"line\">Out[<span class=\"number\">5</span>]: <span class=\"number\">1</span></div><div class=\"line\"></div><div class=\"line\">In [<span class=\"number\">6</span>]: yiter.next()</div><div class=\"line\">Out[<span class=\"number\">6</span>]: <span class=\"number\">2</span></div></pre></td></tr></table></figure>\n<p>其实，上面的代码略显复杂。在代码量很小，不是很在意代码可复用性时，我们完全可以去掉<code>yrange_iter</code>，直接让<code>yrange.__iter__()</code>方法返回其自身实例。这样，我们只需要在<code>yrange</code>类中实现<code>__iter__()</code>方法和<code>next()</code>方法即可。如下所示：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\"># Version2.0 简化版，迭代器是本身</span></div><div class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">yrange</span><span class=\"params\">(object)</span>:</span></div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self, n)</span>:</span></div><div class=\"line\">        self.n = n</div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__iter__</span><span class=\"params\">(self)</span>:</span></div><div class=\"line\">        self.i = <span class=\"number\">0</span></div><div class=\"line\">        <span class=\"keyword\">return</span> self</div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">next</span><span class=\"params\">(self)</span>:</span></div><div class=\"line\">        v = self.i</div><div class=\"line\">        self.i += <span class=\"number\">1</span></div><div class=\"line\">        <span class=\"keyword\">return</span> v</div><div class=\"line\"></div><div class=\"line\">In [<span class=\"number\">8</span>]: yiter = iter(yrange(<span class=\"number\">5</span>))</div><div class=\"line\"></div><div class=\"line\">In [<span class=\"number\">9</span>]: yiter.next()</div><div class=\"line\">Out[<span class=\"number\">9</span>]: <span class=\"number\">0</span></div><div class=\"line\"></div><div class=\"line\">In [<span class=\"number\">10</span>]: yiter.next()</div><div class=\"line\">Out[<span class=\"number\">10</span>]: <span class=\"number\">1</span></div><div class=\"line\"></div><div class=\"line\">In [<span class=\"number\">11</span>]: yiter.next()</div><div class=\"line\">Out[<span class=\"number\">11</span>]: <span class=\"number\">2</span></div></pre></td></tr></table></figure>\n<p>然而，上述的代码仍然存在问题，我们无法指定迭代器生成序列的长度，也就是<code>self.n</code>实际上并没有用到。如果我只想产生0到10以内的序列呢？</p>\n<p>我们只需要加入判断条件，当超出序列边界时，抛出Python内建的<code>StopIteration</code>异常即可。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\"># Version3.0 加入边界判断，生成有限长度序列</span></div><div class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">yrange</span><span class=\"params\">(object)</span>:</span></div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self, n)</span>:</span></div><div class=\"line\">        self.n = n</div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__iter__</span><span class=\"params\">(self)</span>:</span></div><div class=\"line\">        self.i = <span class=\"number\">0</span></div><div class=\"line\">        <span class=\"keyword\">return</span> self</div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">next</span><span class=\"params\">(self)</span>:</span></div><div class=\"line\">        <span class=\"keyword\">if</span> self.i == self.n:</div><div class=\"line\">            <span class=\"keyword\">raise</span> StopIteration</div><div class=\"line\">        v = self.i</div><div class=\"line\">        self.i += <span class=\"number\">1</span></div><div class=\"line\">        <span class=\"keyword\">return</span> v</div><div class=\"line\"></div><div class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> yrange(<span class=\"number\">5</span>):</div><div class=\"line\">    <span class=\"keyword\">print</span> i</div></pre></td></tr></table></figure>\n<h3 id=\"Problem-1\"><a href=\"#Problem-1\" class=\"headerlink\" title=\"Problem 1\"></a>Problem 1</h3><p>Write an iterator class <code>reverse_iter</code>, that takes a <code>list</code> and iterates it from the reverse direction.</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">reverse_iter</span><span class=\"params\">(object)</span>:</span></div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self, alist)</span>:</span></div><div class=\"line\">        self.container = alist</div><div class=\"line\">        self.i = len(alist)</div><div class=\"line\"></div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">next</span><span class=\"params\">(self)</span>:</span></div><div class=\"line\">        <span class=\"keyword\">if</span> self.i == <span class=\"number\">0</span>:</div><div class=\"line\">            <span class=\"keyword\">raise</span> StopIteration</div><div class=\"line\">        self.i -= <span class=\"number\">1</span></div><div class=\"line\">        <span class=\"keyword\">return</span> self.container[self.i]</div><div class=\"line\">it = reverse_iter([<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">4</span>])</div></pre></td></tr></table></figure>\n<h2 id=\"生成器\"><a href=\"#生成器\" class=\"headerlink\" title=\"生成器\"></a>生成器</h2><p>生成器是一种方法，他指定了如何生成序列中的元素，生成器内部包含特殊的<code>yield</code>语句。此外，生成器函数是懒惰求值，只有当调用<code>next()</code>方法时，生成器才开始顺序执行，直到遇到<code>yield</code>语句。<code>yield</code>语句就像<code>return</code>，但是并未退出，而是打上断电，等待下一次<code>next()</code>方法的调用，再从上一次的断点处开始执行。我直接贴出教程中的代码示例。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">foo</span><span class=\"params\">()</span>:</span></div><div class=\"line\">        <span class=\"keyword\">print</span> <span class=\"string\">\"begin\"</span></div><div class=\"line\">        <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">3</span>):</div><div class=\"line\">            <span class=\"keyword\">print</span> <span class=\"string\">\"before yield\"</span>, i</div><div class=\"line\">            <span class=\"keyword\">yield</span> i</div><div class=\"line\">            <span class=\"keyword\">print</span> <span class=\"string\">\"after yield\"</span>, i</div><div class=\"line\">        <span class=\"keyword\">print</span> <span class=\"string\">\"end\"</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>f = foo()</div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>f.next()</div><div class=\"line\">begin</div><div class=\"line\">before <span class=\"keyword\">yield</span> <span class=\"number\">0</span></div><div class=\"line\"><span class=\"number\">0</span></div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>f.next()</div><div class=\"line\">after <span class=\"keyword\">yield</span> <span class=\"number\">0</span></div><div class=\"line\">before <span class=\"keyword\">yield</span> <span class=\"number\">1</span></div><div class=\"line\"><span class=\"number\">1</span></div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>f.next()</div><div class=\"line\">after <span class=\"keyword\">yield</span> <span class=\"number\">1</span></div><div class=\"line\">before <span class=\"keyword\">yield</span> <span class=\"number\">2</span></div><div class=\"line\"><span class=\"number\">2</span></div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>f.next()</div><div class=\"line\">after <span class=\"keyword\">yield</span> <span class=\"number\">2</span></div><div class=\"line\">end</div><div class=\"line\">Traceback (most recent call last):</div><div class=\"line\">  File <span class=\"string\">\"&lt;stdin&gt;\"</span>, line <span class=\"number\">1</span>, <span class=\"keyword\">in</span> &lt;module&gt;</div><div class=\"line\">StopIteration</div><div class=\"line\">&gt;&gt;&gt;</div></pre></td></tr></table></figure>\n<h3 id=\"生成器表达式\"><a href=\"#生成器表达式\" class=\"headerlink\" title=\"生成器表达式\"></a>生成器表达式</h3><p>生成器表达式和列表相似，将<code>[]</code>换为<code>()</code>即可。如下所示：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> (x**<span class=\"number\">2</span> <span class=\"keyword\">for</span> x <span class=\"keyword\">in</span> [<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">3</span>,<span class=\"number\">4</span>]):</div><div class=\"line\">    <span class=\"keyword\">print</span> i</div><div class=\"line\"><span class=\"comment\"># print 1 4 9 16</span></div></pre></td></tr></table></figure>\n<p>生成器的好处在于惰性求值，这样一来，我们还可以生成无限长的序列。因为生成器本来就是说明了序列的生成方式，而并没有真的生成那个序列。</p>\n<p>下面的代码使用生成器得到前10组勾股数。通过在调用<code>take()</code>方法时修改传入实参<code>n</code>的大小，该代码可以很方便地转换为求取任意多得勾股数。生成器的重要作用体现在斜边<code>x</code>的取值为$[0, \\infty]$。如果不使用生成器，恐怕就需要写出好几行的循环语句加上<code>break</code>配合才可以达到相同的效果。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">integer</span><span class=\"params\">(start, end=None)</span>:</span></div><div class=\"line\">    <span class=\"string\">\"\"\"Generate integer sequence [start, end)</div><div class=\"line\">       If `end` is not given, then [start, \\infty]</div><div class=\"line\">    \"\"\"</span></div><div class=\"line\">    i = start</div><div class=\"line\">    <span class=\"keyword\">while</span> <span class=\"keyword\">True</span>:</div><div class=\"line\">        <span class=\"keyword\">if</span> end <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"keyword\">None</span> <span class=\"keyword\">and</span> i == end:</div><div class=\"line\">            <span class=\"keyword\">raise</span> StopIteration</div><div class=\"line\">        <span class=\"keyword\">yield</span> i</div><div class=\"line\">        i += <span class=\"number\">1</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">take</span><span class=\"params\">(n, g)</span>:</span></div><div class=\"line\">    i = <span class=\"number\">0</span></div><div class=\"line\">    <span class=\"keyword\">while</span> <span class=\"keyword\">True</span>:</div><div class=\"line\">        <span class=\"keyword\">if</span> i &lt; n:</div><div class=\"line\">            <span class=\"keyword\">yield</span> g.next()</div><div class=\"line\">            i += <span class=\"number\">1</span></div><div class=\"line\">        <span class=\"keyword\">else</span>:</div><div class=\"line\">            <span class=\"keyword\">raise</span> StopIteration</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># 假定 x&gt;y&gt;z，以消除两直角边互换的情况，如10, 6, 8和10, 8, 6</span></div><div class=\"line\">tup = ((x,y,z) <span class=\"keyword\">for</span> x <span class=\"keyword\">in</span> integer(<span class=\"number\">0</span>) <span class=\"keyword\">for</span> y <span class=\"keyword\">in</span> integer(<span class=\"number\">0</span>, x) <span class=\"keyword\">for</span> z <span class=\"keyword\">in</span> integer(<span class=\"number\">0</span>, y) <span class=\"keyword\">if</span> x*x==y*y+z*z)</div><div class=\"line\">list(take(<span class=\"number\">10</span>, tup))</div></pre></td></tr></table></figure>\n<h3 id=\"Problem-2\"><a href=\"#Problem-2\" class=\"headerlink\" title=\"Problem 2\"></a>Problem 2</h3><p>Write a program that takes one or more filenames as arguments and prints all the lines which are longer than 40 characters.</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">readfiles</span><span class=\"params\">(filenames)</span>:</span></div><div class=\"line\">    <span class=\"keyword\">for</span> f <span class=\"keyword\">in</span> filenames:</div><div class=\"line\">        <span class=\"keyword\">for</span> line <span class=\"keyword\">in</span> open(f):</div><div class=\"line\">            <span class=\"keyword\">yield</span> line</div><div class=\"line\"></div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">grep</span><span class=\"params\">(lines)</span>:</span></div><div class=\"line\">    <span class=\"keyword\">return</span> (line <span class=\"keyword\">for</span> line <span class=\"keyword\">in</span> lines <span class=\"keyword\">if</span> len(line)&gt;<span class=\"number\">40</span>)</div><div class=\"line\"></div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">printlines</span><span class=\"params\">(lines)</span>:</span></div><div class=\"line\">    <span class=\"keyword\">for</span> line <span class=\"keyword\">in</span> lines:</div><div class=\"line\">        <span class=\"keyword\">print</span> line,</div><div class=\"line\"></div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">main</span><span class=\"params\">(filenames)</span>:</span></div><div class=\"line\">    lines = readfiles(filenames)</div><div class=\"line\">    lines = grep(lines)</div><div class=\"line\">    printlines(lines)</div></pre></td></tr></table></figure>\n<h3 id=\"Problem-3\"><a href=\"#Problem-3\" class=\"headerlink\" title=\"Problem 3\"></a>Problem 3</h3><p>Write a function <code>findfiles</code> that recursively descends the directory tree for the specified directory and generates paths of all the files in the tree.</p>\n<p>注意<code>get_all_file()</code>方法中递归中生成器的写法，见SO的<a href=\"http://stackoverflow.com/questions/248830/python-using-a-recursive-algorithm-as-a-generator\">这个帖子</a>。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">import</span> os</div><div class=\"line\"></div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">generate_all_file</span><span class=\"params\">(root)</span>:</span></div><div class=\"line\">    <span class=\"keyword\">for</span> item <span class=\"keyword\">in</span> os.listdir(root):</div><div class=\"line\">        item = os.path.join(root, item)</div><div class=\"line\">        <span class=\"keyword\">if</span> os.path.isfile(item):</div><div class=\"line\">            <span class=\"keyword\">yield</span> os.path.abspath(item)</div><div class=\"line\">        <span class=\"keyword\">else</span>:</div><div class=\"line\">            <span class=\"keyword\">for</span> item <span class=\"keyword\">in</span> generate_all_file(item):</div><div class=\"line\">                <span class=\"keyword\">yield</span> item</div><div class=\"line\"></div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">findfiles</span><span class=\"params\">(root)</span>:</span></div><div class=\"line\">    <span class=\"keyword\">for</span> item <span class=\"keyword\">in</span> generate_all_file(root):</div><div class=\"line\">        <span class=\"keyword\">print</span> item</div></pre></td></tr></table></figure>\n<h3 id=\"Problem-4\"><a href=\"#Problem-4\" class=\"headerlink\" title=\"Problem 4\"></a>Problem 4</h3><p>Write a function to compute the number of python files (.py extension) in a specified directory recursively.</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">generate_all_py_file</span><span class=\"params\">(root)</span>:</span></div><div class=\"line\">    <span class=\"keyword\">return</span> (file <span class=\"keyword\">for</span> file <span class=\"keyword\">in</span> generate_all_file(root) <span class=\"keyword\">if</span> os.path.splitext(file)[<span class=\"number\">-1</span>] == <span class=\"string\">'.py'</span>)</div><div class=\"line\"></div><div class=\"line\"><span class=\"keyword\">print</span> len(list(generate_all_py_file(<span class=\"string\">'./'</span>)))</div></pre></td></tr></table></figure>\n<h3 id=\"Problem-5\"><a href=\"#Problem-5\" class=\"headerlink\" title=\"Problem 5\"></a>Problem 5</h3><p>Write a function to compute the total number of lines of code in all python files in the specified directory recursively.</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">generate_all_line</span><span class=\"params\">(root)</span>:</span></div><div class=\"line\">    <span class=\"keyword\">return</span> (line <span class=\"keyword\">for</span> f <span class=\"keyword\">in</span> generate_all_py_file(root) <span class=\"keyword\">for</span> line <span class=\"keyword\">in</span> open(f))</div><div class=\"line\"><span class=\"keyword\">print</span> len(list(generate_all_line(<span class=\"string\">'./'</span>)))</div></pre></td></tr></table></figure>\n<h3 id=\"Problem-6\"><a href=\"#Problem-6\" class=\"headerlink\" title=\"Problem 6\"></a>Problem 6</h3><p>Write a function to compute the total number of lines of code, ignoring empty and comment lines, in all python files in the specified directory recursively.</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">generate_all_no_empty_and_comment_line</span><span class=\"params\">(root)</span>:</span></div><div class=\"line\">    <span class=\"keyword\">return</span> (line <span class=\"keyword\">for</span> line <span class=\"keyword\">in</span> generate_all_line(root) <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> (line==<span class=\"string\">''</span> <span class=\"keyword\">or</span> line.startswith(<span class=\"string\">'#'</span>)))</div><div class=\"line\"></div><div class=\"line\"><span class=\"keyword\">print</span> len(list(generate_all_no_empty_and_comment_line(<span class=\"string\">'./'</span>)))</div></pre></td></tr></table></figure>\n<h3 id=\"Problem-7\"><a href=\"#Problem-7\" class=\"headerlink\" title=\"Problem 7\"></a>Problem 7</h3><p>Write a program <code>split.py</code>, that takes an integer <code>n</code> and a <code>filename</code> as command line arguments and splits the <code>file</code> into multiple small files with each having <code>n</code> lines.</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">get_numbered_line</span><span class=\"params\">(filename)</span>:</span></div><div class=\"line\">    i = <span class=\"number\">0</span></div><div class=\"line\">    <span class=\"keyword\">for</span> line <span class=\"keyword\">in</span> open(filename):</div><div class=\"line\">        <span class=\"keyword\">yield</span> i, line</div><div class=\"line\">        i += <span class=\"number\">1</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">split</span><span class=\"params\">(file_name, n)</span>:</span></div><div class=\"line\">    i = <span class=\"number\">0</span></div><div class=\"line\">    f = open(<span class=\"string\">'output-%d.txt'</span> %i, <span class=\"string\">'w'</span>)</div><div class=\"line\">    <span class=\"keyword\">for</span> idx, line <span class=\"keyword\">in</span> get_numbered_line(file_name):</div><div class=\"line\">        f.write(line)</div><div class=\"line\">        <span class=\"keyword\">if</span> (idx+<span class=\"number\">1</span>) % n == <span class=\"number\">0</span>:</div><div class=\"line\">            f.close()</div><div class=\"line\">            i += <span class=\"number\">1</span></div><div class=\"line\">            f = open(<span class=\"string\">'output-%d.txt'</span> %i, <span class=\"string\">'w'</span>)</div><div class=\"line\"></div><div class=\"line\">    f.close()</div></pre></td></tr></table></figure>\n<h3 id=\"Problem-9\"><a href=\"#Problem-9\" class=\"headerlink\" title=\"Problem 9\"></a>Problem 9</h3><p>The built-in function <code>enumerate</code> takes an <code>iteratable</code> and returns an <code>iterator</code> over pairs <code>(index, value)</code> for each value in the source.</p>\n<p>Write a function <code>my_enumerate</code> that works like <code>enumerate</code>.</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">my_enumerate</span><span class=\"params\">(iterable)</span>:</span></div><div class=\"line\">    i = <span class=\"number\">0</span></div><div class=\"line\">    seq = iter(iterable)</div><div class=\"line\">    <span class=\"keyword\">while</span> <span class=\"keyword\">True</span>:</div><div class=\"line\">        val = seq.next()</div><div class=\"line\">        <span class=\"keyword\">yield</span> i, val</div><div class=\"line\">        i += <span class=\"number\">1</span></div></pre></td></tr></table></figure>"},{"title":"Residual Net论文阅读 - Deep Residual Learning for Image Recongnition","date":"2017-03-05T14:12:21.000Z","_content":"Residuel Net是MSRA HeKaiming组的作品，斩获了ImageNet挑战赛的所有项目的第一，并荣获CVPR的best paper，成为state of the ar的网络结构。这篇文章记录了阅读最初论文“Deep Residual Learning for Image Recongnition”的重点。\n![ResidualNet Unit](/img/residualnet_unit.png)\n<!-- more -->\n\n## 更深的网络 -> 更好的性能\n在ImageNet等比赛上，大家已经发现了一个现象，就是更深的网络往往能够获得更好的成绩。从LeNet到AlexNet再到VGG Net和GoogLeNet，网络层次越来越深，然而增加网络深度在实际中遇到了很多的问题。\n\n在序言部分，这篇论文也是首先提出了一个问题：我们只需要不断在现有结构基础上堆叠更多的layer就可以获得更好的网络吗？\n>  Is learning better networks as easy as stacking more layers?\n\n很明显，答案是否定的。一个问题就是梯度的消失（或爆炸）。在bp过程中，过深的网络结构会导致传导到底层的梯度变的很小（或飞升），导致训练失败。这一问题在BN层提出之后得到了一定的解决。\n\n另一个问题是在实验过程中观测到的。通过实验，作者发现，当不断增加网络深度的时候，网络的性能会不再提升。如果再继续添加深度，网络的性能甚至会下降！下面是作者在CIFAR-10上做的实验，使用56层的网络比20层的网络，无论在训练集还是测试集上都落于下风。\n![更深的网络表现反而不好](/img/residualnet_deepnet_problem.png)\n\n个人觉得，这个现象看上去意料之外，情理之中。并不是说56层的网络的学习能力不如20层，而是训练不同深度的神经网络的难度是不同的。作者联想到（这里的想法很好！），如果我们已经有了一个较浅的网络（shadow net），然后我们在其后面接上若干的等同映射（Identity Mapping），那么新得到的更深的网络应该是和前者有相同的表现的。这个思想实验，巧妙地说明了并不是更深的网络变坏了，而是我们现有的方法不能很好地训练更深的网络。\n\n\n## 残差单元\n也是受上面这个思想实验中的Identity Mapping的启发，作者设计了一种残差网络结构，以它为基本单元构建更深的网络，以期解决第二个问题。\n![残差结构单元](/img/residualnet_unit.png)\n\n使用残差单元时，我们不再让网络去直接学映射$\\mathcal{H}(x)$，而是学习映射$\\mathcal{F}(x) = \\mathcal{H}(x) - x$。作者也简单说了为何使用这种残差结构。这种方法给要学的映射加上了一个等同映射作为参考。同时考虑极端情况，如果最优的结构真的是等同映射的话，那么学习到的$\\mathcal{F}(x)$为$0$就好了。这个网络的性能起码是不输于那个浅层网络的。\n\n应用这种残差结构就可以很容易地搭建深层网络了。使用这种技术，作者构建了多达$1000$多层的网络，同时在多项比赛中狂揽桂冠，在实践中证明了它的威力。\n\n## 残差学习\n从上面的介绍看出，使用残差结构后，网络不再直接学习最终的映射$\\mathcal{H}$，而是这个映射和输入的残差$\\mathcal{F}$。这里叫做残差学习（Residual Learning）。\n\n神经网络可以近似任意复杂的函数（作者指出此处存疑，还是作为假设）所以，它不仅可以逼近$\\mathcal{H}$，当然也可以逼近残差。这两者虽然都可以通过网络近似，但是训练难度是不同的。\n\n上面图中的残差单元结构可以写成下面的式子，其中的$W_i$就是决定残差映射$\\mathcal{F}$的参数，也是训练中要优化的东西。这里为了书写简单，省略了偏置项。\n\n$$y = \\mathcal{F(x,\\lbrace W_i\\rbrace)}+x$$\n\n上面的式子要求$\\mathcal{F}(x)$与$x$有相同的维度，如果维度不同的话，可以给输入$x$乘上一个权重参数矩阵$W_s$，做一下维度匹配。当然，即使维度相同，我们也可以乘上一个方阵$W_s$，但是这样一来，一是给网络引入了更多的参数（这样，我们的残差结构打脸效果不就打折扣了？），同时在论文中的实验部分也证明了加入这个矩阵对提升性能没用（Identity Mapping已经够用了）。\n\n同时，在设计残差结构的时候，也不必非要像上面的图那样设计两层，完全可以设计更多（比如下面的bottleneck结构，只是别减少得只剩一层了，那样的话$\\mathcal{F}$只剩一个线性映射可以学了。。。）。\n\n## ImageNet实验和比较\n由此，我们可以构建残差网络。这里开始，作者通过一系列实验，来证明残差结构的优越性：更少的参数，更深的层数，更优秀的性能。\n\n这里着重介绍作者在ImageNet上的实验结果。\n\n作者首先对比了18层和34层plain网络和残差网络的表现，进一步验证了序言中的结论。采用普通的结构，更深的网络（34层）表现反而不如较浅的网络，而使用残差结构则没有这个问题。从下图左右的对比可以很清楚地看出这个现象。作者同时指出这一现象不大可能是由于梯度消失造成的。\n\n![残差网络和普通网络不同深度的比较](/img/residualnet_comparison_with_plainnet.png)\n\n另外一个从实验中观察到的现象指出，对于18层这种较浅的网络，使用残差结构能够加快收敛速度，使得训练更加容易。\n\n同时，对于上面提到的维度不匹配的问题，作者提出了三个解决方案并进行了对比。\n- 方案A使用zero-padding的方法\n- 方案B使用乘上权重矩阵的方法\n- 方案C不止在维度不匹配时乘权重矩阵，而且所有的Identity Mapping都换成这种形式\n\n实验结果表明，模型表现A<B<C。但是性能差距较小。由于C引入了很多额外的参数，所以并不使用这种方法（聚焦主要矛盾）。\n\n## Bottleneck结构\n为了节省训练时间，作者提出了一种新的变形——Bottleneck结构。见下图右侧。首先将两层结构扩展为三层，最前面和最后面都是$1\\times 1$的卷积核，来进行channel的变形。通过前面的$1\\times 1$卷积核，将channel降下来。和$3\\times 3$卷积核作用后，再用最后的$1\\times 1$卷积核升上去。\n\n![Bottleneck单元结构](/img/residualnet_bottleneck_unit.png)\n\n使用这一单元结构，作者构建了50层，101层和152层的深层网络，并最终取得了很好的成绩。\n\n## 附录\n在附录中，作者描述了在Pascal VOC和COCO目标检测和定位任务中使用Residual Net的情况。对于目标检测这个任务，后续可以参见MSRA的R-FCN那篇文章。\n","source":"_posts/residualnet-paper.md","raw":"---\ntitle: Residual Net论文阅读 - Deep Residual Learning for Image Recongnition\ndate: 2017-03-05 22:12:21\ntags:\n    - paper\n    - deep learning\n---\nResiduel Net是MSRA HeKaiming组的作品，斩获了ImageNet挑战赛的所有项目的第一，并荣获CVPR的best paper，成为state of the ar的网络结构。这篇文章记录了阅读最初论文“Deep Residual Learning for Image Recongnition”的重点。\n![ResidualNet Unit](/img/residualnet_unit.png)\n<!-- more -->\n\n## 更深的网络 -> 更好的性能\n在ImageNet等比赛上，大家已经发现了一个现象，就是更深的网络往往能够获得更好的成绩。从LeNet到AlexNet再到VGG Net和GoogLeNet，网络层次越来越深，然而增加网络深度在实际中遇到了很多的问题。\n\n在序言部分，这篇论文也是首先提出了一个问题：我们只需要不断在现有结构基础上堆叠更多的layer就可以获得更好的网络吗？\n>  Is learning better networks as easy as stacking more layers?\n\n很明显，答案是否定的。一个问题就是梯度的消失（或爆炸）。在bp过程中，过深的网络结构会导致传导到底层的梯度变的很小（或飞升），导致训练失败。这一问题在BN层提出之后得到了一定的解决。\n\n另一个问题是在实验过程中观测到的。通过实验，作者发现，当不断增加网络深度的时候，网络的性能会不再提升。如果再继续添加深度，网络的性能甚至会下降！下面是作者在CIFAR-10上做的实验，使用56层的网络比20层的网络，无论在训练集还是测试集上都落于下风。\n![更深的网络表现反而不好](/img/residualnet_deepnet_problem.png)\n\n个人觉得，这个现象看上去意料之外，情理之中。并不是说56层的网络的学习能力不如20层，而是训练不同深度的神经网络的难度是不同的。作者联想到（这里的想法很好！），如果我们已经有了一个较浅的网络（shadow net），然后我们在其后面接上若干的等同映射（Identity Mapping），那么新得到的更深的网络应该是和前者有相同的表现的。这个思想实验，巧妙地说明了并不是更深的网络变坏了，而是我们现有的方法不能很好地训练更深的网络。\n\n\n## 残差单元\n也是受上面这个思想实验中的Identity Mapping的启发，作者设计了一种残差网络结构，以它为基本单元构建更深的网络，以期解决第二个问题。\n![残差结构单元](/img/residualnet_unit.png)\n\n使用残差单元时，我们不再让网络去直接学映射$\\mathcal{H}(x)$，而是学习映射$\\mathcal{F}(x) = \\mathcal{H}(x) - x$。作者也简单说了为何使用这种残差结构。这种方法给要学的映射加上了一个等同映射作为参考。同时考虑极端情况，如果最优的结构真的是等同映射的话，那么学习到的$\\mathcal{F}(x)$为$0$就好了。这个网络的性能起码是不输于那个浅层网络的。\n\n应用这种残差结构就可以很容易地搭建深层网络了。使用这种技术，作者构建了多达$1000$多层的网络，同时在多项比赛中狂揽桂冠，在实践中证明了它的威力。\n\n## 残差学习\n从上面的介绍看出，使用残差结构后，网络不再直接学习最终的映射$\\mathcal{H}$，而是这个映射和输入的残差$\\mathcal{F}$。这里叫做残差学习（Residual Learning）。\n\n神经网络可以近似任意复杂的函数（作者指出此处存疑，还是作为假设）所以，它不仅可以逼近$\\mathcal{H}$，当然也可以逼近残差。这两者虽然都可以通过网络近似，但是训练难度是不同的。\n\n上面图中的残差单元结构可以写成下面的式子，其中的$W_i$就是决定残差映射$\\mathcal{F}$的参数，也是训练中要优化的东西。这里为了书写简单，省略了偏置项。\n\n$$y = \\mathcal{F(x,\\lbrace W_i\\rbrace)}+x$$\n\n上面的式子要求$\\mathcal{F}(x)$与$x$有相同的维度，如果维度不同的话，可以给输入$x$乘上一个权重参数矩阵$W_s$，做一下维度匹配。当然，即使维度相同，我们也可以乘上一个方阵$W_s$，但是这样一来，一是给网络引入了更多的参数（这样，我们的残差结构打脸效果不就打折扣了？），同时在论文中的实验部分也证明了加入这个矩阵对提升性能没用（Identity Mapping已经够用了）。\n\n同时，在设计残差结构的时候，也不必非要像上面的图那样设计两层，完全可以设计更多（比如下面的bottleneck结构，只是别减少得只剩一层了，那样的话$\\mathcal{F}$只剩一个线性映射可以学了。。。）。\n\n## ImageNet实验和比较\n由此，我们可以构建残差网络。这里开始，作者通过一系列实验，来证明残差结构的优越性：更少的参数，更深的层数，更优秀的性能。\n\n这里着重介绍作者在ImageNet上的实验结果。\n\n作者首先对比了18层和34层plain网络和残差网络的表现，进一步验证了序言中的结论。采用普通的结构，更深的网络（34层）表现反而不如较浅的网络，而使用残差结构则没有这个问题。从下图左右的对比可以很清楚地看出这个现象。作者同时指出这一现象不大可能是由于梯度消失造成的。\n\n![残差网络和普通网络不同深度的比较](/img/residualnet_comparison_with_plainnet.png)\n\n另外一个从实验中观察到的现象指出，对于18层这种较浅的网络，使用残差结构能够加快收敛速度，使得训练更加容易。\n\n同时，对于上面提到的维度不匹配的问题，作者提出了三个解决方案并进行了对比。\n- 方案A使用zero-padding的方法\n- 方案B使用乘上权重矩阵的方法\n- 方案C不止在维度不匹配时乘权重矩阵，而且所有的Identity Mapping都换成这种形式\n\n实验结果表明，模型表现A<B<C。但是性能差距较小。由于C引入了很多额外的参数，所以并不使用这种方法（聚焦主要矛盾）。\n\n## Bottleneck结构\n为了节省训练时间，作者提出了一种新的变形——Bottleneck结构。见下图右侧。首先将两层结构扩展为三层，最前面和最后面都是$1\\times 1$的卷积核，来进行channel的变形。通过前面的$1\\times 1$卷积核，将channel降下来。和$3\\times 3$卷积核作用后，再用最后的$1\\times 1$卷积核升上去。\n\n![Bottleneck单元结构](/img/residualnet_bottleneck_unit.png)\n\n使用这一单元结构，作者构建了50层，101层和152层的深层网络，并最终取得了很好的成绩。\n\n## 附录\n在附录中，作者描述了在Pascal VOC和COCO目标检测和定位任务中使用Residual Net的情况。对于目标检测这个任务，后续可以参见MSRA的R-FCN那篇文章。\n","slug":"residualnet-paper","published":1,"updated":"2018-01-12T06:22:20.479Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjcu6vcu30027qu460bvgzw5q","content":"<p>Residuel Net是MSRA HeKaiming组的作品，斩获了ImageNet挑战赛的所有项目的第一，并荣获CVPR的best paper，成为state of the ar的网络结构。这篇文章记录了阅读最初论文“Deep Residual Learning for Image Recongnition”的重点。<br><img src=\"/img/residualnet_unit.png\" alt=\"ResidualNet Unit\"><br><a id=\"more\"></a></p>\n<h2 id=\"更深的网络-gt-更好的性能\"><a href=\"#更深的网络-gt-更好的性能\" class=\"headerlink\" title=\"更深的网络 -&gt; 更好的性能\"></a>更深的网络 -&gt; 更好的性能</h2><p>在ImageNet等比赛上，大家已经发现了一个现象，就是更深的网络往往能够获得更好的成绩。从LeNet到AlexNet再到VGG Net和GoogLeNet，网络层次越来越深，然而增加网络深度在实际中遇到了很多的问题。</p>\n<p>在序言部分，这篇论文也是首先提出了一个问题：我们只需要不断在现有结构基础上堆叠更多的layer就可以获得更好的网络吗？</p>\n<blockquote>\n<p> Is learning better networks as easy as stacking more layers?</p>\n</blockquote>\n<p>很明显，答案是否定的。一个问题就是梯度的消失（或爆炸）。在bp过程中，过深的网络结构会导致传导到底层的梯度变的很小（或飞升），导致训练失败。这一问题在BN层提出之后得到了一定的解决。</p>\n<p>另一个问题是在实验过程中观测到的。通过实验，作者发现，当不断增加网络深度的时候，网络的性能会不再提升。如果再继续添加深度，网络的性能甚至会下降！下面是作者在CIFAR-10上做的实验，使用56层的网络比20层的网络，无论在训练集还是测试集上都落于下风。<br><img src=\"/img/residualnet_deepnet_problem.png\" alt=\"更深的网络表现反而不好\"></p>\n<p>个人觉得，这个现象看上去意料之外，情理之中。并不是说56层的网络的学习能力不如20层，而是训练不同深度的神经网络的难度是不同的。作者联想到（这里的想法很好！），如果我们已经有了一个较浅的网络（shadow net），然后我们在其后面接上若干的等同映射（Identity Mapping），那么新得到的更深的网络应该是和前者有相同的表现的。这个思想实验，巧妙地说明了并不是更深的网络变坏了，而是我们现有的方法不能很好地训练更深的网络。</p>\n<h2 id=\"残差单元\"><a href=\"#残差单元\" class=\"headerlink\" title=\"残差单元\"></a>残差单元</h2><p>也是受上面这个思想实验中的Identity Mapping的启发，作者设计了一种残差网络结构，以它为基本单元构建更深的网络，以期解决第二个问题。<br><img src=\"/img/residualnet_unit.png\" alt=\"残差结构单元\"></p>\n<p>使用残差单元时，我们不再让网络去直接学映射$\\mathcal{H}(x)$，而是学习映射$\\mathcal{F}(x) = \\mathcal{H}(x) - x$。作者也简单说了为何使用这种残差结构。这种方法给要学的映射加上了一个等同映射作为参考。同时考虑极端情况，如果最优的结构真的是等同映射的话，那么学习到的$\\mathcal{F}(x)$为$0$就好了。这个网络的性能起码是不输于那个浅层网络的。</p>\n<p>应用这种残差结构就可以很容易地搭建深层网络了。使用这种技术，作者构建了多达$1000$多层的网络，同时在多项比赛中狂揽桂冠，在实践中证明了它的威力。</p>\n<h2 id=\"残差学习\"><a href=\"#残差学习\" class=\"headerlink\" title=\"残差学习\"></a>残差学习</h2><p>从上面的介绍看出，使用残差结构后，网络不再直接学习最终的映射$\\mathcal{H}$，而是这个映射和输入的残差$\\mathcal{F}$。这里叫做残差学习（Residual Learning）。</p>\n<p>神经网络可以近似任意复杂的函数（作者指出此处存疑，还是作为假设）所以，它不仅可以逼近$\\mathcal{H}$，当然也可以逼近残差。这两者虽然都可以通过网络近似，但是训练难度是不同的。</p>\n<p>上面图中的残差单元结构可以写成下面的式子，其中的$W_i$就是决定残差映射$\\mathcal{F}$的参数，也是训练中要优化的东西。这里为了书写简单，省略了偏置项。</p>\n<script type=\"math/tex; mode=display\">y = \\mathcal{F(x,\\lbrace W_i\\rbrace)}+x</script><p>上面的式子要求$\\mathcal{F}(x)$与$x$有相同的维度，如果维度不同的话，可以给输入$x$乘上一个权重参数矩阵$W_s$，做一下维度匹配。当然，即使维度相同，我们也可以乘上一个方阵$W_s$，但是这样一来，一是给网络引入了更多的参数（这样，我们的残差结构打脸效果不就打折扣了？），同时在论文中的实验部分也证明了加入这个矩阵对提升性能没用（Identity Mapping已经够用了）。</p>\n<p>同时，在设计残差结构的时候，也不必非要像上面的图那样设计两层，完全可以设计更多（比如下面的bottleneck结构，只是别减少得只剩一层了，那样的话$\\mathcal{F}$只剩一个线性映射可以学了。。。）。</p>\n<h2 id=\"ImageNet实验和比较\"><a href=\"#ImageNet实验和比较\" class=\"headerlink\" title=\"ImageNet实验和比较\"></a>ImageNet实验和比较</h2><p>由此，我们可以构建残差网络。这里开始，作者通过一系列实验，来证明残差结构的优越性：更少的参数，更深的层数，更优秀的性能。</p>\n<p>这里着重介绍作者在ImageNet上的实验结果。</p>\n<p>作者首先对比了18层和34层plain网络和残差网络的表现，进一步验证了序言中的结论。采用普通的结构，更深的网络（34层）表现反而不如较浅的网络，而使用残差结构则没有这个问题。从下图左右的对比可以很清楚地看出这个现象。作者同时指出这一现象不大可能是由于梯度消失造成的。</p>\n<p><img src=\"/img/residualnet_comparison_with_plainnet.png\" alt=\"残差网络和普通网络不同深度的比较\"></p>\n<p>另外一个从实验中观察到的现象指出，对于18层这种较浅的网络，使用残差结构能够加快收敛速度，使得训练更加容易。</p>\n<p>同时，对于上面提到的维度不匹配的问题，作者提出了三个解决方案并进行了对比。</p>\n<ul>\n<li>方案A使用zero-padding的方法</li>\n<li>方案B使用乘上权重矩阵的方法</li>\n<li>方案C不止在维度不匹配时乘权重矩阵，而且所有的Identity Mapping都换成这种形式</li>\n</ul>\n<p>实验结果表明，模型表现A&lt;B&lt;C。但是性能差距较小。由于C引入了很多额外的参数，所以并不使用这种方法（聚焦主要矛盾）。</p>\n<h2 id=\"Bottleneck结构\"><a href=\"#Bottleneck结构\" class=\"headerlink\" title=\"Bottleneck结构\"></a>Bottleneck结构</h2><p>为了节省训练时间，作者提出了一种新的变形——Bottleneck结构。见下图右侧。首先将两层结构扩展为三层，最前面和最后面都是$1\\times 1$的卷积核，来进行channel的变形。通过前面的$1\\times 1$卷积核，将channel降下来。和$3\\times 3$卷积核作用后，再用最后的$1\\times 1$卷积核升上去。</p>\n<p><img src=\"/img/residualnet_bottleneck_unit.png\" alt=\"Bottleneck单元结构\"></p>\n<p>使用这一单元结构，作者构建了50层，101层和152层的深层网络，并最终取得了很好的成绩。</p>\n<h2 id=\"附录\"><a href=\"#附录\" class=\"headerlink\" title=\"附录\"></a>附录</h2><p>在附录中，作者描述了在Pascal VOC和COCO目标检测和定位任务中使用Residual Net的情况。对于目标检测这个任务，后续可以参见MSRA的R-FCN那篇文章。</p>\n","excerpt":"<p>Residuel Net是MSRA HeKaiming组的作品，斩获了ImageNet挑战赛的所有项目的第一，并荣获CVPR的best paper，成为state of the ar的网络结构。这篇文章记录了阅读最初论文“Deep Residual Learning for Image Recongnition”的重点。<br><img src=\"/img/residualnet_unit.png\" alt=\"ResidualNet Unit\"><br>","more":"</p>\n<h2 id=\"更深的网络-gt-更好的性能\"><a href=\"#更深的网络-gt-更好的性能\" class=\"headerlink\" title=\"更深的网络 -&gt; 更好的性能\"></a>更深的网络 -&gt; 更好的性能</h2><p>在ImageNet等比赛上，大家已经发现了一个现象，就是更深的网络往往能够获得更好的成绩。从LeNet到AlexNet再到VGG Net和GoogLeNet，网络层次越来越深，然而增加网络深度在实际中遇到了很多的问题。</p>\n<p>在序言部分，这篇论文也是首先提出了一个问题：我们只需要不断在现有结构基础上堆叠更多的layer就可以获得更好的网络吗？</p>\n<blockquote>\n<p> Is learning better networks as easy as stacking more layers?</p>\n</blockquote>\n<p>很明显，答案是否定的。一个问题就是梯度的消失（或爆炸）。在bp过程中，过深的网络结构会导致传导到底层的梯度变的很小（或飞升），导致训练失败。这一问题在BN层提出之后得到了一定的解决。</p>\n<p>另一个问题是在实验过程中观测到的。通过实验，作者发现，当不断增加网络深度的时候，网络的性能会不再提升。如果再继续添加深度，网络的性能甚至会下降！下面是作者在CIFAR-10上做的实验，使用56层的网络比20层的网络，无论在训练集还是测试集上都落于下风。<br><img src=\"/img/residualnet_deepnet_problem.png\" alt=\"更深的网络表现反而不好\"></p>\n<p>个人觉得，这个现象看上去意料之外，情理之中。并不是说56层的网络的学习能力不如20层，而是训练不同深度的神经网络的难度是不同的。作者联想到（这里的想法很好！），如果我们已经有了一个较浅的网络（shadow net），然后我们在其后面接上若干的等同映射（Identity Mapping），那么新得到的更深的网络应该是和前者有相同的表现的。这个思想实验，巧妙地说明了并不是更深的网络变坏了，而是我们现有的方法不能很好地训练更深的网络。</p>\n<h2 id=\"残差单元\"><a href=\"#残差单元\" class=\"headerlink\" title=\"残差单元\"></a>残差单元</h2><p>也是受上面这个思想实验中的Identity Mapping的启发，作者设计了一种残差网络结构，以它为基本单元构建更深的网络，以期解决第二个问题。<br><img src=\"/img/residualnet_unit.png\" alt=\"残差结构单元\"></p>\n<p>使用残差单元时，我们不再让网络去直接学映射$\\mathcal{H}(x)$，而是学习映射$\\mathcal{F}(x) = \\mathcal{H}(x) - x$。作者也简单说了为何使用这种残差结构。这种方法给要学的映射加上了一个等同映射作为参考。同时考虑极端情况，如果最优的结构真的是等同映射的话，那么学习到的$\\mathcal{F}(x)$为$0$就好了。这个网络的性能起码是不输于那个浅层网络的。</p>\n<p>应用这种残差结构就可以很容易地搭建深层网络了。使用这种技术，作者构建了多达$1000$多层的网络，同时在多项比赛中狂揽桂冠，在实践中证明了它的威力。</p>\n<h2 id=\"残差学习\"><a href=\"#残差学习\" class=\"headerlink\" title=\"残差学习\"></a>残差学习</h2><p>从上面的介绍看出，使用残差结构后，网络不再直接学习最终的映射$\\mathcal{H}$，而是这个映射和输入的残差$\\mathcal{F}$。这里叫做残差学习（Residual Learning）。</p>\n<p>神经网络可以近似任意复杂的函数（作者指出此处存疑，还是作为假设）所以，它不仅可以逼近$\\mathcal{H}$，当然也可以逼近残差。这两者虽然都可以通过网络近似，但是训练难度是不同的。</p>\n<p>上面图中的残差单元结构可以写成下面的式子，其中的$W_i$就是决定残差映射$\\mathcal{F}$的参数，也是训练中要优化的东西。这里为了书写简单，省略了偏置项。</p>\n<script type=\"math/tex; mode=display\">y = \\mathcal{F(x,\\lbrace W_i\\rbrace)}+x</script><p>上面的式子要求$\\mathcal{F}(x)$与$x$有相同的维度，如果维度不同的话，可以给输入$x$乘上一个权重参数矩阵$W_s$，做一下维度匹配。当然，即使维度相同，我们也可以乘上一个方阵$W_s$，但是这样一来，一是给网络引入了更多的参数（这样，我们的残差结构打脸效果不就打折扣了？），同时在论文中的实验部分也证明了加入这个矩阵对提升性能没用（Identity Mapping已经够用了）。</p>\n<p>同时，在设计残差结构的时候，也不必非要像上面的图那样设计两层，完全可以设计更多（比如下面的bottleneck结构，只是别减少得只剩一层了，那样的话$\\mathcal{F}$只剩一个线性映射可以学了。。。）。</p>\n<h2 id=\"ImageNet实验和比较\"><a href=\"#ImageNet实验和比较\" class=\"headerlink\" title=\"ImageNet实验和比较\"></a>ImageNet实验和比较</h2><p>由此，我们可以构建残差网络。这里开始，作者通过一系列实验，来证明残差结构的优越性：更少的参数，更深的层数，更优秀的性能。</p>\n<p>这里着重介绍作者在ImageNet上的实验结果。</p>\n<p>作者首先对比了18层和34层plain网络和残差网络的表现，进一步验证了序言中的结论。采用普通的结构，更深的网络（34层）表现反而不如较浅的网络，而使用残差结构则没有这个问题。从下图左右的对比可以很清楚地看出这个现象。作者同时指出这一现象不大可能是由于梯度消失造成的。</p>\n<p><img src=\"/img/residualnet_comparison_with_plainnet.png\" alt=\"残差网络和普通网络不同深度的比较\"></p>\n<p>另外一个从实验中观察到的现象指出，对于18层这种较浅的网络，使用残差结构能够加快收敛速度，使得训练更加容易。</p>\n<p>同时，对于上面提到的维度不匹配的问题，作者提出了三个解决方案并进行了对比。</p>\n<ul>\n<li>方案A使用zero-padding的方法</li>\n<li>方案B使用乘上权重矩阵的方法</li>\n<li>方案C不止在维度不匹配时乘权重矩阵，而且所有的Identity Mapping都换成这种形式</li>\n</ul>\n<p>实验结果表明，模型表现A&lt;B&lt;C。但是性能差距较小。由于C引入了很多额外的参数，所以并不使用这种方法（聚焦主要矛盾）。</p>\n<h2 id=\"Bottleneck结构\"><a href=\"#Bottleneck结构\" class=\"headerlink\" title=\"Bottleneck结构\"></a>Bottleneck结构</h2><p>为了节省训练时间，作者提出了一种新的变形——Bottleneck结构。见下图右侧。首先将两层结构扩展为三层，最前面和最后面都是$1\\times 1$的卷积核，来进行channel的变形。通过前面的$1\\times 1$卷积核，将channel降下来。和$3\\times 3$卷积核作用后，再用最后的$1\\times 1$卷积核升上去。</p>\n<p><img src=\"/img/residualnet_bottleneck_unit.png\" alt=\"Bottleneck单元结构\"></p>\n<p>使用这一单元结构，作者构建了50层，101层和152层的深层网络，并最终取得了很好的成绩。</p>\n<h2 id=\"附录\"><a href=\"#附录\" class=\"headerlink\" title=\"附录\"></a>附录</h2><p>在附录中，作者描述了在Pascal VOC和COCO目标检测和定位任务中使用Residual Net的情况。对于目标检测这个任务，后续可以参见MSRA的R-FCN那篇文章。</p>"},{"title":"Residual Net论文阅读 - Identity Mapping in Deep Residual Networks","date":"2017-03-07T08:16:48.000Z","_content":"这篇文章是He Kaiming继最初的那篇ResidualNet的论文后又发的一篇。这篇论文较为详细地探讨了由第一篇文章所引入的Identity Mapping，结合具体实验，测试了很多不同的组合结构，从实践方面验证了Identity Mapping的重要性。同时，也试图对Identity Mapping如此好的作用做一解释。尤其是本文在上篇文章的基础上，提出了新的残差单元结构，并指出这种新的结构具有更优秀的性能。\n<!-- more -->\n\n## 从残差到残差\n在第一篇文章中，作者创造性地提出了残差网络结构。简洁的网络结构，优异的性能，实在是一篇佳作，得到CVPR best paper实至名归。在这篇文章的开头，作者回顾了残差网络的一般结构，如下所示。其中，$x\\_l$和$x\\_{l+1}$表示第$l$个残差单元的输入和输出，$\\mathcal{F}$为残差函数。\n$$y_l = h(x_l) + \\mathcal{F}(x_l, W_l)$$\n$$x_{l+1} = f(y_l)$$\n\n在上篇文章中，作者将$f(x)$取作ReLU函数，将$h(x)$取作Identity Mapping，即，\n$$h(x_l) = x_l$$\n\n在这篇文章中，作者提出了新的网络结构，和原有结构比较如下。\n![新的网络结构，将BN和ReLU看做是前激活](/img/residualnet_improved_structure.png)\n\n作者提出，将BN层和ReLU看做是后面带参数的卷积层的”前激活”（pre-activation），取代原先的“后激活”（post-activation）。这样就得到了上图右侧的新结构。\n\n从右图可以看到，本单元的输入$x\\_l$首先经过了BN层和ReLU层的处理，然后才通过卷积层，之后又是BN-ReLU-conv的结构。这些处理过后得到的$y\\_l$，直接和$x\\_l$相加，得到该层的输出$x\\_{l+1}$。\n\n利用这种新的残差单元结构，作者构建了1001层的残差网络，在CIFAR-10/100上进行了测试，验证了新结构能够更容易地训练（收敛速度快），并且拥有更好的泛化能力（测试集合error低）。\n\n\n## Identity Mapping: Why Always me?\n作者在后面的文章中对这种新结构进行了理论分析和实验测试，试图解释Identity Mapping为何这么重要。实验部分不再多介绍了，无非就是把作者的实验配置和结果贴出来，好麻烦。。。这里只把作者的理论分析整理如下。如果想要复现坐着的工作，还是要结合论文去好好看一下实验部分。\n\n在使用了这种新结构之后，我们有，\n\n$$x_{l+1} = x_l + \\mathcal{F}(x_l, W_l)$$\n\n如果我们的网络都是由这种残差网络组成的，递归地去倒到前面较浅的某一层，则：\n\n$$x_L = x_l +\\sum_{i=l}^{L-1}\\mathcal{F}(x_i, W_i)$$\n\n从上面的式子可以看出，\n\n- 深层单元$L$的特征$x_L$可以被表示为浅层单元的特征$x_l$j加上它们之间各层的残差函数$\\mathcal{F}$。\n- 我们有，$x\\_L=x\\_0+\\sum\\_{i=0}^{L-1}\\mathcal{F}(x\\_i, W\\_i)$，而普通网络$x\\_L$和$x\\_l$的关系比较复杂，$x\\_L = \\prod\\_{i=0}^{L-1}W\\_ix\\_0$。看上去，前者的优化应该更加简单。\n\n计算bp的时候，有，\n$$\\frac{\\partial \\epsilon}{\\partial x_l} = \\frac{\\partial \\epsilon} {\\partial x_L}\\frac{\\partial x_L}{\\partial x_l} = \\frac{\\partial \\epsilon}{\\partial x_L}(1+\\frac{\\partial}{\\partial x_l}\\sum_{i=l}^{L-1}\\mathcal{F}(x_i, W_i))$$\n\n上式表明，由于残差单元的短路连接（shortcut），$x_l$处的梯度基本不会出现消失的情况（除非后面一项正好等于-1）。\n\n如果不做Identity Mapping，而是乘上一个系数$\\lambda$呢？作者发现这会在上面的式子上出现$\\lambda^k$的形式，造成梯度以指数规律vanish或者爆炸。同样的，如果乘上一个权重，也会有类似的效应。\n\n所以，Identity Mapping是坠吼的！\n\n## 花式跑实验\n论文的后半部分，作者开始花式做实验，调研了很多不同的结构，具体实验方案和对比结果可以参看原论文。这里不再罗列了。附上自己用PyTorch实现的164层ResNet在CIFAR10上的训练代码：[Gist Code: ResNet-164 training experiment on CIFAR10 using PyTorch](https://gist.github.com/xmfbit/67c407e34cbaf56e7820f09e774e56d8)。\n\n下面的可视化结果由[DMLC/tensorboard](https://github.com/dmlc/tensorboard)实现。图上在$30K$次迭代附近有明显的性能提升，对应于学习率的调整，变为原来的$0.1$。\n![训练集损失和精度](/img/resnet-164layer-cifar10-training.jpg)\n![测试集损失和精度](/img/resnet-164layer-cifar10-testing.jpg)\n","source":"_posts/residualnet-paper2-identitymapping.md","raw":"---\ntitle: Residual Net论文阅读 - Identity Mapping in Deep Residual Networks\ndate: 2017-03-07 16:16:48\ntags:\n    - paper\n    - deep learning\n---\n这篇文章是He Kaiming继最初的那篇ResidualNet的论文后又发的一篇。这篇论文较为详细地探讨了由第一篇文章所引入的Identity Mapping，结合具体实验，测试了很多不同的组合结构，从实践方面验证了Identity Mapping的重要性。同时，也试图对Identity Mapping如此好的作用做一解释。尤其是本文在上篇文章的基础上，提出了新的残差单元结构，并指出这种新的结构具有更优秀的性能。\n<!-- more -->\n\n## 从残差到残差\n在第一篇文章中，作者创造性地提出了残差网络结构。简洁的网络结构，优异的性能，实在是一篇佳作，得到CVPR best paper实至名归。在这篇文章的开头，作者回顾了残差网络的一般结构，如下所示。其中，$x\\_l$和$x\\_{l+1}$表示第$l$个残差单元的输入和输出，$\\mathcal{F}$为残差函数。\n$$y_l = h(x_l) + \\mathcal{F}(x_l, W_l)$$\n$$x_{l+1} = f(y_l)$$\n\n在上篇文章中，作者将$f(x)$取作ReLU函数，将$h(x)$取作Identity Mapping，即，\n$$h(x_l) = x_l$$\n\n在这篇文章中，作者提出了新的网络结构，和原有结构比较如下。\n![新的网络结构，将BN和ReLU看做是前激活](/img/residualnet_improved_structure.png)\n\n作者提出，将BN层和ReLU看做是后面带参数的卷积层的”前激活”（pre-activation），取代原先的“后激活”（post-activation）。这样就得到了上图右侧的新结构。\n\n从右图可以看到，本单元的输入$x\\_l$首先经过了BN层和ReLU层的处理，然后才通过卷积层，之后又是BN-ReLU-conv的结构。这些处理过后得到的$y\\_l$，直接和$x\\_l$相加，得到该层的输出$x\\_{l+1}$。\n\n利用这种新的残差单元结构，作者构建了1001层的残差网络，在CIFAR-10/100上进行了测试，验证了新结构能够更容易地训练（收敛速度快），并且拥有更好的泛化能力（测试集合error低）。\n\n\n## Identity Mapping: Why Always me?\n作者在后面的文章中对这种新结构进行了理论分析和实验测试，试图解释Identity Mapping为何这么重要。实验部分不再多介绍了，无非就是把作者的实验配置和结果贴出来，好麻烦。。。这里只把作者的理论分析整理如下。如果想要复现坐着的工作，还是要结合论文去好好看一下实验部分。\n\n在使用了这种新结构之后，我们有，\n\n$$x_{l+1} = x_l + \\mathcal{F}(x_l, W_l)$$\n\n如果我们的网络都是由这种残差网络组成的，递归地去倒到前面较浅的某一层，则：\n\n$$x_L = x_l +\\sum_{i=l}^{L-1}\\mathcal{F}(x_i, W_i)$$\n\n从上面的式子可以看出，\n\n- 深层单元$L$的特征$x_L$可以被表示为浅层单元的特征$x_l$j加上它们之间各层的残差函数$\\mathcal{F}$。\n- 我们有，$x\\_L=x\\_0+\\sum\\_{i=0}^{L-1}\\mathcal{F}(x\\_i, W\\_i)$，而普通网络$x\\_L$和$x\\_l$的关系比较复杂，$x\\_L = \\prod\\_{i=0}^{L-1}W\\_ix\\_0$。看上去，前者的优化应该更加简单。\n\n计算bp的时候，有，\n$$\\frac{\\partial \\epsilon}{\\partial x_l} = \\frac{\\partial \\epsilon} {\\partial x_L}\\frac{\\partial x_L}{\\partial x_l} = \\frac{\\partial \\epsilon}{\\partial x_L}(1+\\frac{\\partial}{\\partial x_l}\\sum_{i=l}^{L-1}\\mathcal{F}(x_i, W_i))$$\n\n上式表明，由于残差单元的短路连接（shortcut），$x_l$处的梯度基本不会出现消失的情况（除非后面一项正好等于-1）。\n\n如果不做Identity Mapping，而是乘上一个系数$\\lambda$呢？作者发现这会在上面的式子上出现$\\lambda^k$的形式，造成梯度以指数规律vanish或者爆炸。同样的，如果乘上一个权重，也会有类似的效应。\n\n所以，Identity Mapping是坠吼的！\n\n## 花式跑实验\n论文的后半部分，作者开始花式做实验，调研了很多不同的结构，具体实验方案和对比结果可以参看原论文。这里不再罗列了。附上自己用PyTorch实现的164层ResNet在CIFAR10上的训练代码：[Gist Code: ResNet-164 training experiment on CIFAR10 using PyTorch](https://gist.github.com/xmfbit/67c407e34cbaf56e7820f09e774e56d8)。\n\n下面的可视化结果由[DMLC/tensorboard](https://github.com/dmlc/tensorboard)实现。图上在$30K$次迭代附近有明显的性能提升，对应于学习率的调整，变为原来的$0.1$。\n![训练集损失和精度](/img/resnet-164layer-cifar10-training.jpg)\n![测试集损失和精度](/img/resnet-164layer-cifar10-testing.jpg)\n","slug":"residualnet-paper2-identitymapping","published":1,"updated":"2018-01-12T06:22:20.480Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjcu6vcu5002aqu46r1varffd","content":"<p>这篇文章是He Kaiming继最初的那篇ResidualNet的论文后又发的一篇。这篇论文较为详细地探讨了由第一篇文章所引入的Identity Mapping，结合具体实验，测试了很多不同的组合结构，从实践方面验证了Identity Mapping的重要性。同时，也试图对Identity Mapping如此好的作用做一解释。尤其是本文在上篇文章的基础上，提出了新的残差单元结构，并指出这种新的结构具有更优秀的性能。<br><a id=\"more\"></a></p>\n<h2 id=\"从残差到残差\"><a href=\"#从残差到残差\" class=\"headerlink\" title=\"从残差到残差\"></a>从残差到残差</h2><p>在第一篇文章中，作者创造性地提出了残差网络结构。简洁的网络结构，优异的性能，实在是一篇佳作，得到CVPR best paper实至名归。在这篇文章的开头，作者回顾了残差网络的一般结构，如下所示。其中，$x_l$和$x_{l+1}$表示第$l$个残差单元的输入和输出，$\\mathcal{F}$为残差函数。</p>\n<script type=\"math/tex; mode=display\">y_l = h(x_l) + \\mathcal{F}(x_l, W_l)</script><script type=\"math/tex; mode=display\">x_{l+1} = f(y_l)</script><p>在上篇文章中，作者将$f(x)$取作ReLU函数，将$h(x)$取作Identity Mapping，即，</p>\n<script type=\"math/tex; mode=display\">h(x_l) = x_l</script><p>在这篇文章中，作者提出了新的网络结构，和原有结构比较如下。<br><img src=\"/img/residualnet_improved_structure.png\" alt=\"新的网络结构，将BN和ReLU看做是前激活\"></p>\n<p>作者提出，将BN层和ReLU看做是后面带参数的卷积层的”前激活”（pre-activation），取代原先的“后激活”（post-activation）。这样就得到了上图右侧的新结构。</p>\n<p>从右图可以看到，本单元的输入$x_l$首先经过了BN层和ReLU层的处理，然后才通过卷积层，之后又是BN-ReLU-conv的结构。这些处理过后得到的$y_l$，直接和$x_l$相加，得到该层的输出$x_{l+1}$。</p>\n<p>利用这种新的残差单元结构，作者构建了1001层的残差网络，在CIFAR-10/100上进行了测试，验证了新结构能够更容易地训练（收敛速度快），并且拥有更好的泛化能力（测试集合error低）。</p>\n<h2 id=\"Identity-Mapping-Why-Always-me\"><a href=\"#Identity-Mapping-Why-Always-me\" class=\"headerlink\" title=\"Identity Mapping: Why Always me?\"></a>Identity Mapping: Why Always me?</h2><p>作者在后面的文章中对这种新结构进行了理论分析和实验测试，试图解释Identity Mapping为何这么重要。实验部分不再多介绍了，无非就是把作者的实验配置和结果贴出来，好麻烦。。。这里只把作者的理论分析整理如下。如果想要复现坐着的工作，还是要结合论文去好好看一下实验部分。</p>\n<p>在使用了这种新结构之后，我们有，</p>\n<script type=\"math/tex; mode=display\">x_{l+1} = x_l + \\mathcal{F}(x_l, W_l)</script><p>如果我们的网络都是由这种残差网络组成的，递归地去倒到前面较浅的某一层，则：</p>\n<script type=\"math/tex; mode=display\">x_L = x_l +\\sum_{i=l}^{L-1}\\mathcal{F}(x_i, W_i)</script><p>从上面的式子可以看出，</p>\n<ul>\n<li>深层单元$L$的特征$x_L$可以被表示为浅层单元的特征$x_l$j加上它们之间各层的残差函数$\\mathcal{F}$。</li>\n<li>我们有，$x_L=x_0+\\sum_{i=0}^{L-1}\\mathcal{F}(x_i, W_i)$，而普通网络$x_L$和$x_l$的关系比较复杂，$x_L = \\prod_{i=0}^{L-1}W_ix_0$。看上去，前者的优化应该更加简单。</li>\n</ul>\n<p>计算bp的时候，有，</p>\n<script type=\"math/tex; mode=display\">\\frac{\\partial \\epsilon}{\\partial x_l} = \\frac{\\partial \\epsilon} {\\partial x_L}\\frac{\\partial x_L}{\\partial x_l} = \\frac{\\partial \\epsilon}{\\partial x_L}(1+\\frac{\\partial}{\\partial x_l}\\sum_{i=l}^{L-1}\\mathcal{F}(x_i, W_i))</script><p>上式表明，由于残差单元的短路连接（shortcut），$x_l$处的梯度基本不会出现消失的情况（除非后面一项正好等于-1）。</p>\n<p>如果不做Identity Mapping，而是乘上一个系数$\\lambda$呢？作者发现这会在上面的式子上出现$\\lambda^k$的形式，造成梯度以指数规律vanish或者爆炸。同样的，如果乘上一个权重，也会有类似的效应。</p>\n<p>所以，Identity Mapping是坠吼的！</p>\n<h2 id=\"花式跑实验\"><a href=\"#花式跑实验\" class=\"headerlink\" title=\"花式跑实验\"></a>花式跑实验</h2><p>论文的后半部分，作者开始花式做实验，调研了很多不同的结构，具体实验方案和对比结果可以参看原论文。这里不再罗列了。附上自己用PyTorch实现的164层ResNet在CIFAR10上的训练代码：<a href=\"https://gist.github.com/xmfbit/67c407e34cbaf56e7820f09e774e56d8\" target=\"_blank\" rel=\"external\">Gist Code: ResNet-164 training experiment on CIFAR10 using PyTorch</a>。</p>\n<p>下面的可视化结果由<a href=\"https://github.com/dmlc/tensorboard\" target=\"_blank\" rel=\"external\">DMLC/tensorboard</a>实现。图上在$30K$次迭代附近有明显的性能提升，对应于学习率的调整，变为原来的$0.1$。<br><img src=\"/img/resnet-164layer-cifar10-training.jpg\" alt=\"训练集损失和精度\"><br><img src=\"/img/resnet-164layer-cifar10-testing.jpg\" alt=\"测试集损失和精度\"></p>\n","excerpt":"<p>这篇文章是He Kaiming继最初的那篇ResidualNet的论文后又发的一篇。这篇论文较为详细地探讨了由第一篇文章所引入的Identity Mapping，结合具体实验，测试了很多不同的组合结构，从实践方面验证了Identity Mapping的重要性。同时，也试图对Identity Mapping如此好的作用做一解释。尤其是本文在上篇文章的基础上，提出了新的残差单元结构，并指出这种新的结构具有更优秀的性能。<br>","more":"</p>\n<h2 id=\"从残差到残差\"><a href=\"#从残差到残差\" class=\"headerlink\" title=\"从残差到残差\"></a>从残差到残差</h2><p>在第一篇文章中，作者创造性地提出了残差网络结构。简洁的网络结构，优异的性能，实在是一篇佳作，得到CVPR best paper实至名归。在这篇文章的开头，作者回顾了残差网络的一般结构，如下所示。其中，$x_l$和$x_{l+1}$表示第$l$个残差单元的输入和输出，$\\mathcal{F}$为残差函数。</p>\n<script type=\"math/tex; mode=display\">y_l = h(x_l) + \\mathcal{F}(x_l, W_l)</script><script type=\"math/tex; mode=display\">x_{l+1} = f(y_l)</script><p>在上篇文章中，作者将$f(x)$取作ReLU函数，将$h(x)$取作Identity Mapping，即，</p>\n<script type=\"math/tex; mode=display\">h(x_l) = x_l</script><p>在这篇文章中，作者提出了新的网络结构，和原有结构比较如下。<br><img src=\"/img/residualnet_improved_structure.png\" alt=\"新的网络结构，将BN和ReLU看做是前激活\"></p>\n<p>作者提出，将BN层和ReLU看做是后面带参数的卷积层的”前激活”（pre-activation），取代原先的“后激活”（post-activation）。这样就得到了上图右侧的新结构。</p>\n<p>从右图可以看到，本单元的输入$x_l$首先经过了BN层和ReLU层的处理，然后才通过卷积层，之后又是BN-ReLU-conv的结构。这些处理过后得到的$y_l$，直接和$x_l$相加，得到该层的输出$x_{l+1}$。</p>\n<p>利用这种新的残差单元结构，作者构建了1001层的残差网络，在CIFAR-10/100上进行了测试，验证了新结构能够更容易地训练（收敛速度快），并且拥有更好的泛化能力（测试集合error低）。</p>\n<h2 id=\"Identity-Mapping-Why-Always-me\"><a href=\"#Identity-Mapping-Why-Always-me\" class=\"headerlink\" title=\"Identity Mapping: Why Always me?\"></a>Identity Mapping: Why Always me?</h2><p>作者在后面的文章中对这种新结构进行了理论分析和实验测试，试图解释Identity Mapping为何这么重要。实验部分不再多介绍了，无非就是把作者的实验配置和结果贴出来，好麻烦。。。这里只把作者的理论分析整理如下。如果想要复现坐着的工作，还是要结合论文去好好看一下实验部分。</p>\n<p>在使用了这种新结构之后，我们有，</p>\n<script type=\"math/tex; mode=display\">x_{l+1} = x_l + \\mathcal{F}(x_l, W_l)</script><p>如果我们的网络都是由这种残差网络组成的，递归地去倒到前面较浅的某一层，则：</p>\n<script type=\"math/tex; mode=display\">x_L = x_l +\\sum_{i=l}^{L-1}\\mathcal{F}(x_i, W_i)</script><p>从上面的式子可以看出，</p>\n<ul>\n<li>深层单元$L$的特征$x_L$可以被表示为浅层单元的特征$x_l$j加上它们之间各层的残差函数$\\mathcal{F}$。</li>\n<li>我们有，$x_L=x_0+\\sum_{i=0}^{L-1}\\mathcal{F}(x_i, W_i)$，而普通网络$x_L$和$x_l$的关系比较复杂，$x_L = \\prod_{i=0}^{L-1}W_ix_0$。看上去，前者的优化应该更加简单。</li>\n</ul>\n<p>计算bp的时候，有，</p>\n<script type=\"math/tex; mode=display\">\\frac{\\partial \\epsilon}{\\partial x_l} = \\frac{\\partial \\epsilon} {\\partial x_L}\\frac{\\partial x_L}{\\partial x_l} = \\frac{\\partial \\epsilon}{\\partial x_L}(1+\\frac{\\partial}{\\partial x_l}\\sum_{i=l}^{L-1}\\mathcal{F}(x_i, W_i))</script><p>上式表明，由于残差单元的短路连接（shortcut），$x_l$处的梯度基本不会出现消失的情况（除非后面一项正好等于-1）。</p>\n<p>如果不做Identity Mapping，而是乘上一个系数$\\lambda$呢？作者发现这会在上面的式子上出现$\\lambda^k$的形式，造成梯度以指数规律vanish或者爆炸。同样的，如果乘上一个权重，也会有类似的效应。</p>\n<p>所以，Identity Mapping是坠吼的！</p>\n<h2 id=\"花式跑实验\"><a href=\"#花式跑实验\" class=\"headerlink\" title=\"花式跑实验\"></a>花式跑实验</h2><p>论文的后半部分，作者开始花式做实验，调研了很多不同的结构，具体实验方案和对比结果可以参看原论文。这里不再罗列了。附上自己用PyTorch实现的164层ResNet在CIFAR10上的训练代码：<a href=\"https://gist.github.com/xmfbit/67c407e34cbaf56e7820f09e774e56d8\">Gist Code: ResNet-164 training experiment on CIFAR10 using PyTorch</a>。</p>\n<p>下面的可视化结果由<a href=\"https://github.com/dmlc/tensorboard\">DMLC/tensorboard</a>实现。图上在$30K$次迭代附近有明显的性能提升，对应于学习率的调整，变为原来的$0.1$。<br><img src=\"/img/resnet-164layer-cifar10-training.jpg\" alt=\"训练集损失和精度\"><br><img src=\"/img/resnet-164layer-cifar10-testing.jpg\" alt=\"测试集损失和精度\"></p>"},{"title":"Silver RL课程 - DP Planning","date":"2017-06-06T08:57:49.000Z","_content":"上讲中介绍了MDP这一基本概念。之后的lecture以此出发，介绍不同情况下的最优策略求解方法。本节假设我们对MDP过程的所有参数都是已知的，这时候问题较为简单，可以直接得到确定的解。这种问题叫做planning问题，求解方法是动态规划。\n<!-- more -->\n\n## 动态规划\n动态规划是计算机科学中常用的思想方法。对于一个复杂的问题，我们可以将它划分成若干的子问题，然后再将子问题的解答合并为原问题的解。要想用动态规划解决问题，该问题必须满足以下两个条件：\n\n- 最优子结构。能够分解为若干子问题。\n- 子问题重叠。分解后的子问题存在重叠，我们可以通过记忆化的方法进行缓存和重用。\n\nMDP问题的求解符合上述要求。贝尔曼方程给出了原问题递归的分解（考虑状态$S$时，我们可以考虑从状态$s$出发的下一个状态$s^\\prime$，而在考虑状态$s^\\prime$的时候，问题和原问题是一样的，只不过问题规模变小了）；而使用值函数我们相当于记录了中间结果。值函数充当了缓存与记事簿的作用。\n\n## 迭代策略估计\n给定一个策略，我们想要知道该策略的期望回报是多少，也就是其对应的值函数$v_\\pi(s)$。首先回顾一下上讲中得到的值函数的贝尔曼方程如下（全概率公式）：\n\n$$v_{k+1}(s) = \\sum_{a\\in \\mathcal{A}}\\pi(a|s)(R_s^a+\\gamma\\sum_{s^\\prime \\in \\mathcal{S}} P_{ss^\\prime}^av_k(s^\\prime))$$\n\n我们有如下的迭代估计方法：在每一轮迭代中，对于所有状态$s\\in \\mathcal{S}$，使用上式利用上轮中的$v(s^\\prime)$更新$v_{k+1}(s)$，直到收敛。\n\n给出下面的算例。$4\\times 4$的格子中，$0$和$15$是出口。在状态$0$和$15$向自身转移时，奖赏为$0$。其他状态来回转换时，奖赏均为$-1$。如果当前移动使得更新后的位置超过格子的边界，则状态仍然保持原状。求采取随机策略$\\pi$，即每个状态下，上下左右四个方向移动的概率均为$0.25$时候各个状态的值函数$v_\\pi(s)$。\n![Demo](/img/silver_rl_dp_policy_evaluating_demo.png)\n\n这里直接将Python实现的计算过程贴在下面，注意在每一轮迭代开始前，暂存当前值函数的副本。\n\n``` py\nimport numpy as np\n\nv = [0 for _ in xrange(16)]\nline1 = range(1, 4)\nline4 = range(12, 15)\ncol1 = [4, 8, 12]\ncol4 = [3, 7, 11]\n\n\n# the environment simulator\ndef get_new_loc(idx, action):\n    if idx == 0 or idx == 15:\n        ret = idx\n        reward = 0\n        return ret, reward\n\n    if action == 0:\n        # up\n        if idx in line1:\n            ret = idx\n        else:\n            ret = idx-4\n    elif action == 1:\n        # down\n        if idx in line4:\n            ret = idx\n        else:\n            ret = idx+4\n    elif action == 2:\n        # left\n        if idx in col1:\n            ret = idx\n        else:\n            ret = idx-1\n    elif action == 3:\n        # right\n        if idx in col4:\n            ret = idx\n        else:\n            ret = idx+1\n\n    reward = -1\n    return ret, reward\n\n\ngamma = 1.\n\nK = [1, 2, 3, 10, 100]\nfor k in xrange(1, 101):\n    # in each iteration, update v(s) via:\n    # v(s) = \\sum_a \\pi(a|s) + \\gamma \\sum_s^\\prime P_{ss^\\prime}^a v(s^\\prime)\n    v_aux = v[:]\n    for i in xrange(16):\n        v_aux[i] = 0.\n        for action in range(4):\n            j, r = get_new_loc(i, action)\n            v_aux[i] += 0.25*(r+gamma*v[j])\n    v = v_aux\n    if k in K:\n        print 'k = {} '.format(k),\n        print ', '.join(map(lambda x: '{:.1f}'.format(x), v))\n\n```\n\n## 策略的改进\n评估过某个策略的值函数后，我们可以改进该策略，使用的方法为贪心法。具体来说，在某个状态$s$时，我们更新此时的动作为能够使得$Q(s,a)$取得最大，接下来继续执行原策略的那个动作（也就是我们只看一步）。如下所示：\n$$\\pi^\\prime = arg\\max_{a\\in \\mathcal{A}}q_\\pi(s,a)$$\n\n以上小节中给出的算例为例，最终值函数结果为：\n![策略估计结果](/img/silver_rl_dp_policy_evaluating_demo_result.png)\n\n那么对于位置$1$，由于其左方的状态值函数最大，为$0$。所以，我们认为从位置$1$出发的最优策略应该是向左移动。其他同理。这样，对于任何一个状态，它都可以通过选取$q(s,a)$最大的那个动作达到下一个状态，再递推地走下去（如右侧图中的箭头所示）。\n\n为什么这种贪心方法有效呢？这里直接把证明过程粘贴如下。\n![贪心方法work的证明](/img/silver_rl_dp_improve_policy_greedily_proof.png)\n\n当上述单步提升不再满足时，上图中的不等号就变成了等号，算法收敛到了最优解。\n![贪心方法的终止](/img/silver_rl_dp_improve_policy_greedily_proof_2.png)\n\n## 值迭代\n首先介绍最优化定理（也可以解释上述贪心方法为什么work，类比图中最短路径的分析）。这条定理是说某个策略对于状态$s$是最优的，当且仅当，对于每个由$s$出发可达的状态$s^\\prime$，都有，该策略对$s^\\prime$也是最优的。这提示我们，可以通过下面的式子更新$s$处的最优值函数的值。\n$$v^\\ast(s) = \\max_{a\\in \\mathcal{A}}R_s^a+\\gamma\\sum_{s^\\prime\\in \\mathcal{S}}P_{ss^\\prime}^av^\\ast(s^\\prime)$$\n\n通过迭代地进行这个步骤，就能够收敛到最优值函数。每次迭代中，都首先计算最后一个状态的值函数，然后逐渐回滚，更新前面的。如下图所示（求取最短路径）：\n![值迭代方法示例](/img/silver_rl_dp_value_iteration_demo.png)\n\n每轮迭代，都从$1$号开始。考虑$1$号，第一轮时候，大家都是$0$。当选取动作为向左移动时候，上式取得最大值。所以$s^\\prime=0$。更新之后，其值变为了$-1$（因为把reward加上去了），接下来更新其他。并开始新的迭代轮次，最终收敛。\n\n注意到，这里和上面策略迭代-改进来求取最优策略不同，这里并不存在一个显式的策略。或者说，在策略迭代的时候，我们是要选取某个动作$a$，使得值-动作函数$q(s,a)$取值最大。而在值迭代的过程中，我们只关心下个状态的值函数和在这个转换过程中得到的奖励。\n\n## 异步DP\n上面我们讨论的是同步迭代更新。也就是说，在更新前，我们要先备份各个状态的值函数，更新时是使用状态$s^\\prime$的旧值来计算$s$的新值。如下图所示：\n![同步更新](/img/silver_rl_dp_synchronous_value_iteration.png)\n上面讨论了同步迭代的三个主要问题：\n![同步更新问题](/img/silver_rl_dp_synchronous_dp_algorithms.png)\n\n我们也可以使用异步方法。主要包括以下三种：\n\n### 就地（in-place） DP\n就地DP只存储一份值函数，在更新时，有可能在使用新的状态值函数$v_{\\text{new}}(s^\\orime)$来更新$v(s)$。\n![就地更新迭代值函数](/img/silver_rl_dp_inplace_value_iteration.png)\n\n### 带有优先级的状态扫描（Prioritized Sweeping）\n根据贝尔曼方程的误差来指示更新先更新哪个状态的值函数，即\n$$|\\max\\_{a\\in A}(R\\_s^a+\\gamma\\sum\\_{s^\\prime\\in S}P\\_{ss^\\prime}^a v(s^\\prime)-v(s)|$$\n\n实现的具体细节如下：\n![](/img/silver_rl_dp_detailed_prioritized_dp.png)\n### 实时DP\n使用智能体与环境交互的经验（experience）来挑选状态。\n![实时DP](/img/silver_rl_dp_realtime_dp.png)\n","source":"_posts/silver-rl-dp.md","raw":"---\ntitle: Silver RL课程 - DP Planning\ndate: 2017-06-06 16:57:49\ntags:\n     - reinforcement learning\n---\n上讲中介绍了MDP这一基本概念。之后的lecture以此出发，介绍不同情况下的最优策略求解方法。本节假设我们对MDP过程的所有参数都是已知的，这时候问题较为简单，可以直接得到确定的解。这种问题叫做planning问题，求解方法是动态规划。\n<!-- more -->\n\n## 动态规划\n动态规划是计算机科学中常用的思想方法。对于一个复杂的问题，我们可以将它划分成若干的子问题，然后再将子问题的解答合并为原问题的解。要想用动态规划解决问题，该问题必须满足以下两个条件：\n\n- 最优子结构。能够分解为若干子问题。\n- 子问题重叠。分解后的子问题存在重叠，我们可以通过记忆化的方法进行缓存和重用。\n\nMDP问题的求解符合上述要求。贝尔曼方程给出了原问题递归的分解（考虑状态$S$时，我们可以考虑从状态$s$出发的下一个状态$s^\\prime$，而在考虑状态$s^\\prime$的时候，问题和原问题是一样的，只不过问题规模变小了）；而使用值函数我们相当于记录了中间结果。值函数充当了缓存与记事簿的作用。\n\n## 迭代策略估计\n给定一个策略，我们想要知道该策略的期望回报是多少，也就是其对应的值函数$v_\\pi(s)$。首先回顾一下上讲中得到的值函数的贝尔曼方程如下（全概率公式）：\n\n$$v_{k+1}(s) = \\sum_{a\\in \\mathcal{A}}\\pi(a|s)(R_s^a+\\gamma\\sum_{s^\\prime \\in \\mathcal{S}} P_{ss^\\prime}^av_k(s^\\prime))$$\n\n我们有如下的迭代估计方法：在每一轮迭代中，对于所有状态$s\\in \\mathcal{S}$，使用上式利用上轮中的$v(s^\\prime)$更新$v_{k+1}(s)$，直到收敛。\n\n给出下面的算例。$4\\times 4$的格子中，$0$和$15$是出口。在状态$0$和$15$向自身转移时，奖赏为$0$。其他状态来回转换时，奖赏均为$-1$。如果当前移动使得更新后的位置超过格子的边界，则状态仍然保持原状。求采取随机策略$\\pi$，即每个状态下，上下左右四个方向移动的概率均为$0.25$时候各个状态的值函数$v_\\pi(s)$。\n![Demo](/img/silver_rl_dp_policy_evaluating_demo.png)\n\n这里直接将Python实现的计算过程贴在下面，注意在每一轮迭代开始前，暂存当前值函数的副本。\n\n``` py\nimport numpy as np\n\nv = [0 for _ in xrange(16)]\nline1 = range(1, 4)\nline4 = range(12, 15)\ncol1 = [4, 8, 12]\ncol4 = [3, 7, 11]\n\n\n# the environment simulator\ndef get_new_loc(idx, action):\n    if idx == 0 or idx == 15:\n        ret = idx\n        reward = 0\n        return ret, reward\n\n    if action == 0:\n        # up\n        if idx in line1:\n            ret = idx\n        else:\n            ret = idx-4\n    elif action == 1:\n        # down\n        if idx in line4:\n            ret = idx\n        else:\n            ret = idx+4\n    elif action == 2:\n        # left\n        if idx in col1:\n            ret = idx\n        else:\n            ret = idx-1\n    elif action == 3:\n        # right\n        if idx in col4:\n            ret = idx\n        else:\n            ret = idx+1\n\n    reward = -1\n    return ret, reward\n\n\ngamma = 1.\n\nK = [1, 2, 3, 10, 100]\nfor k in xrange(1, 101):\n    # in each iteration, update v(s) via:\n    # v(s) = \\sum_a \\pi(a|s) + \\gamma \\sum_s^\\prime P_{ss^\\prime}^a v(s^\\prime)\n    v_aux = v[:]\n    for i in xrange(16):\n        v_aux[i] = 0.\n        for action in range(4):\n            j, r = get_new_loc(i, action)\n            v_aux[i] += 0.25*(r+gamma*v[j])\n    v = v_aux\n    if k in K:\n        print 'k = {} '.format(k),\n        print ', '.join(map(lambda x: '{:.1f}'.format(x), v))\n\n```\n\n## 策略的改进\n评估过某个策略的值函数后，我们可以改进该策略，使用的方法为贪心法。具体来说，在某个状态$s$时，我们更新此时的动作为能够使得$Q(s,a)$取得最大，接下来继续执行原策略的那个动作（也就是我们只看一步）。如下所示：\n$$\\pi^\\prime = arg\\max_{a\\in \\mathcal{A}}q_\\pi(s,a)$$\n\n以上小节中给出的算例为例，最终值函数结果为：\n![策略估计结果](/img/silver_rl_dp_policy_evaluating_demo_result.png)\n\n那么对于位置$1$，由于其左方的状态值函数最大，为$0$。所以，我们认为从位置$1$出发的最优策略应该是向左移动。其他同理。这样，对于任何一个状态，它都可以通过选取$q(s,a)$最大的那个动作达到下一个状态，再递推地走下去（如右侧图中的箭头所示）。\n\n为什么这种贪心方法有效呢？这里直接把证明过程粘贴如下。\n![贪心方法work的证明](/img/silver_rl_dp_improve_policy_greedily_proof.png)\n\n当上述单步提升不再满足时，上图中的不等号就变成了等号，算法收敛到了最优解。\n![贪心方法的终止](/img/silver_rl_dp_improve_policy_greedily_proof_2.png)\n\n## 值迭代\n首先介绍最优化定理（也可以解释上述贪心方法为什么work，类比图中最短路径的分析）。这条定理是说某个策略对于状态$s$是最优的，当且仅当，对于每个由$s$出发可达的状态$s^\\prime$，都有，该策略对$s^\\prime$也是最优的。这提示我们，可以通过下面的式子更新$s$处的最优值函数的值。\n$$v^\\ast(s) = \\max_{a\\in \\mathcal{A}}R_s^a+\\gamma\\sum_{s^\\prime\\in \\mathcal{S}}P_{ss^\\prime}^av^\\ast(s^\\prime)$$\n\n通过迭代地进行这个步骤，就能够收敛到最优值函数。每次迭代中，都首先计算最后一个状态的值函数，然后逐渐回滚，更新前面的。如下图所示（求取最短路径）：\n![值迭代方法示例](/img/silver_rl_dp_value_iteration_demo.png)\n\n每轮迭代，都从$1$号开始。考虑$1$号，第一轮时候，大家都是$0$。当选取动作为向左移动时候，上式取得最大值。所以$s^\\prime=0$。更新之后，其值变为了$-1$（因为把reward加上去了），接下来更新其他。并开始新的迭代轮次，最终收敛。\n\n注意到，这里和上面策略迭代-改进来求取最优策略不同，这里并不存在一个显式的策略。或者说，在策略迭代的时候，我们是要选取某个动作$a$，使得值-动作函数$q(s,a)$取值最大。而在值迭代的过程中，我们只关心下个状态的值函数和在这个转换过程中得到的奖励。\n\n## 异步DP\n上面我们讨论的是同步迭代更新。也就是说，在更新前，我们要先备份各个状态的值函数，更新时是使用状态$s^\\prime$的旧值来计算$s$的新值。如下图所示：\n![同步更新](/img/silver_rl_dp_synchronous_value_iteration.png)\n上面讨论了同步迭代的三个主要问题：\n![同步更新问题](/img/silver_rl_dp_synchronous_dp_algorithms.png)\n\n我们也可以使用异步方法。主要包括以下三种：\n\n### 就地（in-place） DP\n就地DP只存储一份值函数，在更新时，有可能在使用新的状态值函数$v_{\\text{new}}(s^\\orime)$来更新$v(s)$。\n![就地更新迭代值函数](/img/silver_rl_dp_inplace_value_iteration.png)\n\n### 带有优先级的状态扫描（Prioritized Sweeping）\n根据贝尔曼方程的误差来指示更新先更新哪个状态的值函数，即\n$$|\\max\\_{a\\in A}(R\\_s^a+\\gamma\\sum\\_{s^\\prime\\in S}P\\_{ss^\\prime}^a v(s^\\prime)-v(s)|$$\n\n实现的具体细节如下：\n![](/img/silver_rl_dp_detailed_prioritized_dp.png)\n### 实时DP\n使用智能体与环境交互的经验（experience）来挑选状态。\n![实时DP](/img/silver_rl_dp_realtime_dp.png)\n","slug":"silver-rl-dp","published":1,"updated":"2018-01-12T06:22:20.481Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjcu6vcu6002cqu46jq3vqhj8","content":"<p>上讲中介绍了MDP这一基本概念。之后的lecture以此出发，介绍不同情况下的最优策略求解方法。本节假设我们对MDP过程的所有参数都是已知的，这时候问题较为简单，可以直接得到确定的解。这种问题叫做planning问题，求解方法是动态规划。<br><a id=\"more\"></a></p>\n<h2 id=\"动态规划\"><a href=\"#动态规划\" class=\"headerlink\" title=\"动态规划\"></a>动态规划</h2><p>动态规划是计算机科学中常用的思想方法。对于一个复杂的问题，我们可以将它划分成若干的子问题，然后再将子问题的解答合并为原问题的解。要想用动态规划解决问题，该问题必须满足以下两个条件：</p>\n<ul>\n<li>最优子结构。能够分解为若干子问题。</li>\n<li>子问题重叠。分解后的子问题存在重叠，我们可以通过记忆化的方法进行缓存和重用。</li>\n</ul>\n<p>MDP问题的求解符合上述要求。贝尔曼方程给出了原问题递归的分解（考虑状态$S$时，我们可以考虑从状态$s$出发的下一个状态$s^\\prime$，而在考虑状态$s^\\prime$的时候，问题和原问题是一样的，只不过问题规模变小了）；而使用值函数我们相当于记录了中间结果。值函数充当了缓存与记事簿的作用。</p>\n<h2 id=\"迭代策略估计\"><a href=\"#迭代策略估计\" class=\"headerlink\" title=\"迭代策略估计\"></a>迭代策略估计</h2><p>给定一个策略，我们想要知道该策略的期望回报是多少，也就是其对应的值函数$v_\\pi(s)$。首先回顾一下上讲中得到的值函数的贝尔曼方程如下（全概率公式）：</p>\n<script type=\"math/tex; mode=display\">v_{k+1}(s) = \\sum_{a\\in \\mathcal{A}}\\pi(a|s)(R_s^a+\\gamma\\sum_{s^\\prime \\in \\mathcal{S}} P_{ss^\\prime}^av_k(s^\\prime))</script><p>我们有如下的迭代估计方法：在每一轮迭代中，对于所有状态$s\\in \\mathcal{S}$，使用上式利用上轮中的$v(s^\\prime)$更新$v_{k+1}(s)$，直到收敛。</p>\n<p>给出下面的算例。$4\\times 4$的格子中，$0$和$15$是出口。在状态$0$和$15$向自身转移时，奖赏为$0$。其他状态来回转换时，奖赏均为$-1$。如果当前移动使得更新后的位置超过格子的边界，则状态仍然保持原状。求采取随机策略$\\pi$，即每个状态下，上下左右四个方向移动的概率均为$0.25$时候各个状态的值函数$v_\\pi(s)$。<br><img src=\"/img/silver_rl_dp_policy_evaluating_demo.png\" alt=\"Demo\"></p>\n<p>这里直接将Python实现的计算过程贴在下面，注意在每一轮迭代开始前，暂存当前值函数的副本。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div><div class=\"line\">47</div><div class=\"line\">48</div><div class=\"line\">49</div><div class=\"line\">50</div><div class=\"line\">51</div><div class=\"line\">52</div><div class=\"line\">53</div><div class=\"line\">54</div><div class=\"line\">55</div><div class=\"line\">56</div><div class=\"line\">57</div><div class=\"line\">58</div><div class=\"line\">59</div><div class=\"line\">60</div><div class=\"line\">61</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</div><div class=\"line\"></div><div class=\"line\">v = [<span class=\"number\">0</span> <span class=\"keyword\">for</span> _ <span class=\"keyword\">in</span> xrange(<span class=\"number\">16</span>)]</div><div class=\"line\">line1 = range(<span class=\"number\">1</span>, <span class=\"number\">4</span>)</div><div class=\"line\">line4 = range(<span class=\"number\">12</span>, <span class=\"number\">15</span>)</div><div class=\"line\">col1 = [<span class=\"number\">4</span>, <span class=\"number\">8</span>, <span class=\"number\">12</span>]</div><div class=\"line\">col4 = [<span class=\"number\">3</span>, <span class=\"number\">7</span>, <span class=\"number\">11</span>]</div><div class=\"line\"></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># the environment simulator</span></div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">get_new_loc</span><span class=\"params\">(idx, action)</span>:</span></div><div class=\"line\">    <span class=\"keyword\">if</span> idx == <span class=\"number\">0</span> <span class=\"keyword\">or</span> idx == <span class=\"number\">15</span>:</div><div class=\"line\">        ret = idx</div><div class=\"line\">        reward = <span class=\"number\">0</span></div><div class=\"line\">        <span class=\"keyword\">return</span> ret, reward</div><div class=\"line\"></div><div class=\"line\">    <span class=\"keyword\">if</span> action == <span class=\"number\">0</span>:</div><div class=\"line\">        <span class=\"comment\"># up</span></div><div class=\"line\">        <span class=\"keyword\">if</span> idx <span class=\"keyword\">in</span> line1:</div><div class=\"line\">            ret = idx</div><div class=\"line\">        <span class=\"keyword\">else</span>:</div><div class=\"line\">            ret = idx<span class=\"number\">-4</span></div><div class=\"line\">    <span class=\"keyword\">elif</span> action == <span class=\"number\">1</span>:</div><div class=\"line\">        <span class=\"comment\"># down</span></div><div class=\"line\">        <span class=\"keyword\">if</span> idx <span class=\"keyword\">in</span> line4:</div><div class=\"line\">            ret = idx</div><div class=\"line\">        <span class=\"keyword\">else</span>:</div><div class=\"line\">            ret = idx+<span class=\"number\">4</span></div><div class=\"line\">    <span class=\"keyword\">elif</span> action == <span class=\"number\">2</span>:</div><div class=\"line\">        <span class=\"comment\"># left</span></div><div class=\"line\">        <span class=\"keyword\">if</span> idx <span class=\"keyword\">in</span> col1:</div><div class=\"line\">            ret = idx</div><div class=\"line\">        <span class=\"keyword\">else</span>:</div><div class=\"line\">            ret = idx<span class=\"number\">-1</span></div><div class=\"line\">    <span class=\"keyword\">elif</span> action == <span class=\"number\">3</span>:</div><div class=\"line\">        <span class=\"comment\"># right</span></div><div class=\"line\">        <span class=\"keyword\">if</span> idx <span class=\"keyword\">in</span> col4:</div><div class=\"line\">            ret = idx</div><div class=\"line\">        <span class=\"keyword\">else</span>:</div><div class=\"line\">            ret = idx+<span class=\"number\">1</span></div><div class=\"line\"></div><div class=\"line\">    reward = <span class=\"number\">-1</span></div><div class=\"line\">    <span class=\"keyword\">return</span> ret, reward</div><div class=\"line\"></div><div class=\"line\"></div><div class=\"line\">gamma = <span class=\"number\">1.</span></div><div class=\"line\"></div><div class=\"line\">K = [<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">10</span>, <span class=\"number\">100</span>]</div><div class=\"line\"><span class=\"keyword\">for</span> k <span class=\"keyword\">in</span> xrange(<span class=\"number\">1</span>, <span class=\"number\">101</span>):</div><div class=\"line\">    <span class=\"comment\"># in each iteration, update v(s) via:</span></div><div class=\"line\">    <span class=\"comment\"># v(s) = \\sum_a \\pi(a|s) + \\gamma \\sum_s^\\prime P_&#123;ss^\\prime&#125;^a v(s^\\prime)</span></div><div class=\"line\">    v_aux = v[:]</div><div class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> xrange(<span class=\"number\">16</span>):</div><div class=\"line\">        v_aux[i] = <span class=\"number\">0.</span></div><div class=\"line\">        <span class=\"keyword\">for</span> action <span class=\"keyword\">in</span> range(<span class=\"number\">4</span>):</div><div class=\"line\">            j, r = get_new_loc(i, action)</div><div class=\"line\">            v_aux[i] += <span class=\"number\">0.25</span>*(r+gamma*v[j])</div><div class=\"line\">    v = v_aux</div><div class=\"line\">    <span class=\"keyword\">if</span> k <span class=\"keyword\">in</span> K:</div><div class=\"line\">        <span class=\"keyword\">print</span> <span class=\"string\">'k = &#123;&#125; '</span>.format(k),</div><div class=\"line\">        <span class=\"keyword\">print</span> <span class=\"string\">', '</span>.join(map(<span class=\"keyword\">lambda</span> x: <span class=\"string\">'&#123;:.1f&#125;'</span>.format(x), v))</div></pre></td></tr></table></figure>\n<h2 id=\"策略的改进\"><a href=\"#策略的改进\" class=\"headerlink\" title=\"策略的改进\"></a>策略的改进</h2><p>评估过某个策略的值函数后，我们可以改进该策略，使用的方法为贪心法。具体来说，在某个状态$s$时，我们更新此时的动作为能够使得$Q(s,a)$取得最大，接下来继续执行原策略的那个动作（也就是我们只看一步）。如下所示：</p>\n<script type=\"math/tex; mode=display\">\\pi^\\prime = arg\\max_{a\\in \\mathcal{A}}q_\\pi(s,a)</script><p>以上小节中给出的算例为例，最终值函数结果为：<br><img src=\"/img/silver_rl_dp_policy_evaluating_demo_result.png\" alt=\"策略估计结果\"></p>\n<p>那么对于位置$1$，由于其左方的状态值函数最大，为$0$。所以，我们认为从位置$1$出发的最优策略应该是向左移动。其他同理。这样，对于任何一个状态，它都可以通过选取$q(s,a)$最大的那个动作达到下一个状态，再递推地走下去（如右侧图中的箭头所示）。</p>\n<p>为什么这种贪心方法有效呢？这里直接把证明过程粘贴如下。<br><img src=\"/img/silver_rl_dp_improve_policy_greedily_proof.png\" alt=\"贪心方法work的证明\"></p>\n<p>当上述单步提升不再满足时，上图中的不等号就变成了等号，算法收敛到了最优解。<br><img src=\"/img/silver_rl_dp_improve_policy_greedily_proof_2.png\" alt=\"贪心方法的终止\"></p>\n<h2 id=\"值迭代\"><a href=\"#值迭代\" class=\"headerlink\" title=\"值迭代\"></a>值迭代</h2><p>首先介绍最优化定理（也可以解释上述贪心方法为什么work，类比图中最短路径的分析）。这条定理是说某个策略对于状态$s$是最优的，当且仅当，对于每个由$s$出发可达的状态$s^\\prime$，都有，该策略对$s^\\prime$也是最优的。这提示我们，可以通过下面的式子更新$s$处的最优值函数的值。</p>\n<script type=\"math/tex; mode=display\">v^\\ast(s) = \\max_{a\\in \\mathcal{A}}R_s^a+\\gamma\\sum_{s^\\prime\\in \\mathcal{S}}P_{ss^\\prime}^av^\\ast(s^\\prime)</script><p>通过迭代地进行这个步骤，就能够收敛到最优值函数。每次迭代中，都首先计算最后一个状态的值函数，然后逐渐回滚，更新前面的。如下图所示（求取最短路径）：<br><img src=\"/img/silver_rl_dp_value_iteration_demo.png\" alt=\"值迭代方法示例\"></p>\n<p>每轮迭代，都从$1$号开始。考虑$1$号，第一轮时候，大家都是$0$。当选取动作为向左移动时候，上式取得最大值。所以$s^\\prime=0$。更新之后，其值变为了$-1$（因为把reward加上去了），接下来更新其他。并开始新的迭代轮次，最终收敛。</p>\n<p>注意到，这里和上面策略迭代-改进来求取最优策略不同，这里并不存在一个显式的策略。或者说，在策略迭代的时候，我们是要选取某个动作$a$，使得值-动作函数$q(s,a)$取值最大。而在值迭代的过程中，我们只关心下个状态的值函数和在这个转换过程中得到的奖励。</p>\n<h2 id=\"异步DP\"><a href=\"#异步DP\" class=\"headerlink\" title=\"异步DP\"></a>异步DP</h2><p>上面我们讨论的是同步迭代更新。也就是说，在更新前，我们要先备份各个状态的值函数，更新时是使用状态$s^\\prime$的旧值来计算$s$的新值。如下图所示：<br><img src=\"/img/silver_rl_dp_synchronous_value_iteration.png\" alt=\"同步更新\"><br>上面讨论了同步迭代的三个主要问题：<br><img src=\"/img/silver_rl_dp_synchronous_dp_algorithms.png\" alt=\"同步更新问题\"></p>\n<p>我们也可以使用异步方法。主要包括以下三种：</p>\n<h3 id=\"就地（in-place）-DP\"><a href=\"#就地（in-place）-DP\" class=\"headerlink\" title=\"就地（in-place） DP\"></a>就地（in-place） DP</h3><p>就地DP只存储一份值函数，在更新时，有可能在使用新的状态值函数$v_{\\text{new}}(s^\\orime)$来更新$v(s)$。<br><img src=\"/img/silver_rl_dp_inplace_value_iteration.png\" alt=\"就地更新迭代值函数\"></p>\n<h3 id=\"带有优先级的状态扫描（Prioritized-Sweeping）\"><a href=\"#带有优先级的状态扫描（Prioritized-Sweeping）\" class=\"headerlink\" title=\"带有优先级的状态扫描（Prioritized Sweeping）\"></a>带有优先级的状态扫描（Prioritized Sweeping）</h3><p>根据贝尔曼方程的误差来指示更新先更新哪个状态的值函数，即</p>\n<script type=\"math/tex; mode=display\">|\\max\\_{a\\in A}(R\\_s^a+\\gamma\\sum\\_{s^\\prime\\in S}P\\_{ss^\\prime}^a v(s^\\prime)-v(s)|</script><p>实现的具体细节如下：<br><img src=\"/img/silver_rl_dp_detailed_prioritized_dp.png\" alt=\"\"></p>\n<h3 id=\"实时DP\"><a href=\"#实时DP\" class=\"headerlink\" title=\"实时DP\"></a>实时DP</h3><p>使用智能体与环境交互的经验（experience）来挑选状态。<br><img src=\"/img/silver_rl_dp_realtime_dp.png\" alt=\"实时DP\"></p>\n","excerpt":"<p>上讲中介绍了MDP这一基本概念。之后的lecture以此出发，介绍不同情况下的最优策略求解方法。本节假设我们对MDP过程的所有参数都是已知的，这时候问题较为简单，可以直接得到确定的解。这种问题叫做planning问题，求解方法是动态规划。<br>","more":"</p>\n<h2 id=\"动态规划\"><a href=\"#动态规划\" class=\"headerlink\" title=\"动态规划\"></a>动态规划</h2><p>动态规划是计算机科学中常用的思想方法。对于一个复杂的问题，我们可以将它划分成若干的子问题，然后再将子问题的解答合并为原问题的解。要想用动态规划解决问题，该问题必须满足以下两个条件：</p>\n<ul>\n<li>最优子结构。能够分解为若干子问题。</li>\n<li>子问题重叠。分解后的子问题存在重叠，我们可以通过记忆化的方法进行缓存和重用。</li>\n</ul>\n<p>MDP问题的求解符合上述要求。贝尔曼方程给出了原问题递归的分解（考虑状态$S$时，我们可以考虑从状态$s$出发的下一个状态$s^\\prime$，而在考虑状态$s^\\prime$的时候，问题和原问题是一样的，只不过问题规模变小了）；而使用值函数我们相当于记录了中间结果。值函数充当了缓存与记事簿的作用。</p>\n<h2 id=\"迭代策略估计\"><a href=\"#迭代策略估计\" class=\"headerlink\" title=\"迭代策略估计\"></a>迭代策略估计</h2><p>给定一个策略，我们想要知道该策略的期望回报是多少，也就是其对应的值函数$v_\\pi(s)$。首先回顾一下上讲中得到的值函数的贝尔曼方程如下（全概率公式）：</p>\n<script type=\"math/tex; mode=display\">v_{k+1}(s) = \\sum_{a\\in \\mathcal{A}}\\pi(a|s)(R_s^a+\\gamma\\sum_{s^\\prime \\in \\mathcal{S}} P_{ss^\\prime}^av_k(s^\\prime))</script><p>我们有如下的迭代估计方法：在每一轮迭代中，对于所有状态$s\\in \\mathcal{S}$，使用上式利用上轮中的$v(s^\\prime)$更新$v_{k+1}(s)$，直到收敛。</p>\n<p>给出下面的算例。$4\\times 4$的格子中，$0$和$15$是出口。在状态$0$和$15$向自身转移时，奖赏为$0$。其他状态来回转换时，奖赏均为$-1$。如果当前移动使得更新后的位置超过格子的边界，则状态仍然保持原状。求采取随机策略$\\pi$，即每个状态下，上下左右四个方向移动的概率均为$0.25$时候各个状态的值函数$v_\\pi(s)$。<br><img src=\"/img/silver_rl_dp_policy_evaluating_demo.png\" alt=\"Demo\"></p>\n<p>这里直接将Python实现的计算过程贴在下面，注意在每一轮迭代开始前，暂存当前值函数的副本。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div><div class=\"line\">47</div><div class=\"line\">48</div><div class=\"line\">49</div><div class=\"line\">50</div><div class=\"line\">51</div><div class=\"line\">52</div><div class=\"line\">53</div><div class=\"line\">54</div><div class=\"line\">55</div><div class=\"line\">56</div><div class=\"line\">57</div><div class=\"line\">58</div><div class=\"line\">59</div><div class=\"line\">60</div><div class=\"line\">61</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</div><div class=\"line\"></div><div class=\"line\">v = [<span class=\"number\">0</span> <span class=\"keyword\">for</span> _ <span class=\"keyword\">in</span> xrange(<span class=\"number\">16</span>)]</div><div class=\"line\">line1 = range(<span class=\"number\">1</span>, <span class=\"number\">4</span>)</div><div class=\"line\">line4 = range(<span class=\"number\">12</span>, <span class=\"number\">15</span>)</div><div class=\"line\">col1 = [<span class=\"number\">4</span>, <span class=\"number\">8</span>, <span class=\"number\">12</span>]</div><div class=\"line\">col4 = [<span class=\"number\">3</span>, <span class=\"number\">7</span>, <span class=\"number\">11</span>]</div><div class=\"line\"></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># the environment simulator</span></div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">get_new_loc</span><span class=\"params\">(idx, action)</span>:</span></div><div class=\"line\">    <span class=\"keyword\">if</span> idx == <span class=\"number\">0</span> <span class=\"keyword\">or</span> idx == <span class=\"number\">15</span>:</div><div class=\"line\">        ret = idx</div><div class=\"line\">        reward = <span class=\"number\">0</span></div><div class=\"line\">        <span class=\"keyword\">return</span> ret, reward</div><div class=\"line\"></div><div class=\"line\">    <span class=\"keyword\">if</span> action == <span class=\"number\">0</span>:</div><div class=\"line\">        <span class=\"comment\"># up</span></div><div class=\"line\">        <span class=\"keyword\">if</span> idx <span class=\"keyword\">in</span> line1:</div><div class=\"line\">            ret = idx</div><div class=\"line\">        <span class=\"keyword\">else</span>:</div><div class=\"line\">            ret = idx<span class=\"number\">-4</span></div><div class=\"line\">    <span class=\"keyword\">elif</span> action == <span class=\"number\">1</span>:</div><div class=\"line\">        <span class=\"comment\"># down</span></div><div class=\"line\">        <span class=\"keyword\">if</span> idx <span class=\"keyword\">in</span> line4:</div><div class=\"line\">            ret = idx</div><div class=\"line\">        <span class=\"keyword\">else</span>:</div><div class=\"line\">            ret = idx+<span class=\"number\">4</span></div><div class=\"line\">    <span class=\"keyword\">elif</span> action == <span class=\"number\">2</span>:</div><div class=\"line\">        <span class=\"comment\"># left</span></div><div class=\"line\">        <span class=\"keyword\">if</span> idx <span class=\"keyword\">in</span> col1:</div><div class=\"line\">            ret = idx</div><div class=\"line\">        <span class=\"keyword\">else</span>:</div><div class=\"line\">            ret = idx<span class=\"number\">-1</span></div><div class=\"line\">    <span class=\"keyword\">elif</span> action == <span class=\"number\">3</span>:</div><div class=\"line\">        <span class=\"comment\"># right</span></div><div class=\"line\">        <span class=\"keyword\">if</span> idx <span class=\"keyword\">in</span> col4:</div><div class=\"line\">            ret = idx</div><div class=\"line\">        <span class=\"keyword\">else</span>:</div><div class=\"line\">            ret = idx+<span class=\"number\">1</span></div><div class=\"line\"></div><div class=\"line\">    reward = <span class=\"number\">-1</span></div><div class=\"line\">    <span class=\"keyword\">return</span> ret, reward</div><div class=\"line\"></div><div class=\"line\"></div><div class=\"line\">gamma = <span class=\"number\">1.</span></div><div class=\"line\"></div><div class=\"line\">K = [<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">10</span>, <span class=\"number\">100</span>]</div><div class=\"line\"><span class=\"keyword\">for</span> k <span class=\"keyword\">in</span> xrange(<span class=\"number\">1</span>, <span class=\"number\">101</span>):</div><div class=\"line\">    <span class=\"comment\"># in each iteration, update v(s) via:</span></div><div class=\"line\">    <span class=\"comment\"># v(s) = \\sum_a \\pi(a|s) + \\gamma \\sum_s^\\prime P_&#123;ss^\\prime&#125;^a v(s^\\prime)</span></div><div class=\"line\">    v_aux = v[:]</div><div class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> xrange(<span class=\"number\">16</span>):</div><div class=\"line\">        v_aux[i] = <span class=\"number\">0.</span></div><div class=\"line\">        <span class=\"keyword\">for</span> action <span class=\"keyword\">in</span> range(<span class=\"number\">4</span>):</div><div class=\"line\">            j, r = get_new_loc(i, action)</div><div class=\"line\">            v_aux[i] += <span class=\"number\">0.25</span>*(r+gamma*v[j])</div><div class=\"line\">    v = v_aux</div><div class=\"line\">    <span class=\"keyword\">if</span> k <span class=\"keyword\">in</span> K:</div><div class=\"line\">        <span class=\"keyword\">print</span> <span class=\"string\">'k = &#123;&#125; '</span>.format(k),</div><div class=\"line\">        <span class=\"keyword\">print</span> <span class=\"string\">', '</span>.join(map(<span class=\"keyword\">lambda</span> x: <span class=\"string\">'&#123;:.1f&#125;'</span>.format(x), v))</div></pre></td></tr></table></figure>\n<h2 id=\"策略的改进\"><a href=\"#策略的改进\" class=\"headerlink\" title=\"策略的改进\"></a>策略的改进</h2><p>评估过某个策略的值函数后，我们可以改进该策略，使用的方法为贪心法。具体来说，在某个状态$s$时，我们更新此时的动作为能够使得$Q(s,a)$取得最大，接下来继续执行原策略的那个动作（也就是我们只看一步）。如下所示：</p>\n<script type=\"math/tex; mode=display\">\\pi^\\prime = arg\\max_{a\\in \\mathcal{A}}q_\\pi(s,a)</script><p>以上小节中给出的算例为例，最终值函数结果为：<br><img src=\"/img/silver_rl_dp_policy_evaluating_demo_result.png\" alt=\"策略估计结果\"></p>\n<p>那么对于位置$1$，由于其左方的状态值函数最大，为$0$。所以，我们认为从位置$1$出发的最优策略应该是向左移动。其他同理。这样，对于任何一个状态，它都可以通过选取$q(s,a)$最大的那个动作达到下一个状态，再递推地走下去（如右侧图中的箭头所示）。</p>\n<p>为什么这种贪心方法有效呢？这里直接把证明过程粘贴如下。<br><img src=\"/img/silver_rl_dp_improve_policy_greedily_proof.png\" alt=\"贪心方法work的证明\"></p>\n<p>当上述单步提升不再满足时，上图中的不等号就变成了等号，算法收敛到了最优解。<br><img src=\"/img/silver_rl_dp_improve_policy_greedily_proof_2.png\" alt=\"贪心方法的终止\"></p>\n<h2 id=\"值迭代\"><a href=\"#值迭代\" class=\"headerlink\" title=\"值迭代\"></a>值迭代</h2><p>首先介绍最优化定理（也可以解释上述贪心方法为什么work，类比图中最短路径的分析）。这条定理是说某个策略对于状态$s$是最优的，当且仅当，对于每个由$s$出发可达的状态$s^\\prime$，都有，该策略对$s^\\prime$也是最优的。这提示我们，可以通过下面的式子更新$s$处的最优值函数的值。</p>\n<script type=\"math/tex; mode=display\">v^\\ast(s) = \\max_{a\\in \\mathcal{A}}R_s^a+\\gamma\\sum_{s^\\prime\\in \\mathcal{S}}P_{ss^\\prime}^av^\\ast(s^\\prime)</script><p>通过迭代地进行这个步骤，就能够收敛到最优值函数。每次迭代中，都首先计算最后一个状态的值函数，然后逐渐回滚，更新前面的。如下图所示（求取最短路径）：<br><img src=\"/img/silver_rl_dp_value_iteration_demo.png\" alt=\"值迭代方法示例\"></p>\n<p>每轮迭代，都从$1$号开始。考虑$1$号，第一轮时候，大家都是$0$。当选取动作为向左移动时候，上式取得最大值。所以$s^\\prime=0$。更新之后，其值变为了$-1$（因为把reward加上去了），接下来更新其他。并开始新的迭代轮次，最终收敛。</p>\n<p>注意到，这里和上面策略迭代-改进来求取最优策略不同，这里并不存在一个显式的策略。或者说，在策略迭代的时候，我们是要选取某个动作$a$，使得值-动作函数$q(s,a)$取值最大。而在值迭代的过程中，我们只关心下个状态的值函数和在这个转换过程中得到的奖励。</p>\n<h2 id=\"异步DP\"><a href=\"#异步DP\" class=\"headerlink\" title=\"异步DP\"></a>异步DP</h2><p>上面我们讨论的是同步迭代更新。也就是说，在更新前，我们要先备份各个状态的值函数，更新时是使用状态$s^\\prime$的旧值来计算$s$的新值。如下图所示：<br><img src=\"/img/silver_rl_dp_synchronous_value_iteration.png\" alt=\"同步更新\"><br>上面讨论了同步迭代的三个主要问题：<br><img src=\"/img/silver_rl_dp_synchronous_dp_algorithms.png\" alt=\"同步更新问题\"></p>\n<p>我们也可以使用异步方法。主要包括以下三种：</p>\n<h3 id=\"就地（in-place）-DP\"><a href=\"#就地（in-place）-DP\" class=\"headerlink\" title=\"就地（in-place） DP\"></a>就地（in-place） DP</h3><p>就地DP只存储一份值函数，在更新时，有可能在使用新的状态值函数$v_{\\text{new}}(s^\\orime)$来更新$v(s)$。<br><img src=\"/img/silver_rl_dp_inplace_value_iteration.png\" alt=\"就地更新迭代值函数\"></p>\n<h3 id=\"带有优先级的状态扫描（Prioritized-Sweeping）\"><a href=\"#带有优先级的状态扫描（Prioritized-Sweeping）\" class=\"headerlink\" title=\"带有优先级的状态扫描（Prioritized Sweeping）\"></a>带有优先级的状态扫描（Prioritized Sweeping）</h3><p>根据贝尔曼方程的误差来指示更新先更新哪个状态的值函数，即</p>\n<script type=\"math/tex; mode=display\">|\\max\\_{a\\in A}(R\\_s^a+\\gamma\\sum\\_{s^\\prime\\in S}P\\_{ss^\\prime}^a v(s^\\prime)-v(s)|</script><p>实现的具体细节如下：<br><img src=\"/img/silver_rl_dp_detailed_prioritized_dp.png\" alt=\"\"></p>\n<h3 id=\"实时DP\"><a href=\"#实时DP\" class=\"headerlink\" title=\"实时DP\"></a>实时DP</h3><p>使用智能体与环境交互的经验（experience）来挑选状态。<br><img src=\"/img/silver_rl_dp_realtime_dp.png\" alt=\"实时DP\"></p>"},{"title":"shell编程","date":"2017-11-10T05:06:30.000Z","_content":"介绍基本的shell编程方法，参考的教程是[Linux Shell Scripting Tutorial, A Beginner's handbook](http://www.freeos.com/guides/lsst/)。\n![Bash Logo](/img/shell-programming-bash-logo.png)\n<!-- more -->\n\n## 变量\n\n变量是代码的基本组成元素。可以认为shell中的变量类型都是字符串。\n\nshell中的变量可以分为两类：系统变量和用户自定义变量。下面分别进行介绍。\n\n在代码中使用变量值的时候，需要在前面加上`$`。`echo`命令可以在控制台打印相应输出。所以使用`echo $var`就可以输出变量`var`的值。\n\n### 系统变量\n\n系统变量是指Linux中自带的一些变量。例如`HOME`,`PATH`等。其中`PATH`又叫环境变量。更多的系统变量见下表：\n![系统变量列表](/img/shell-programming-system-variables.jpg)\n\n### 用户定义的变量\n\n用户自定义变量是用户命名并赋值的变量。使用下面的方法定义：\n\n``` bash\n# 注意不要在等号两边插入空格\nname=value\n# 如 n=10\n```\n\n### 局部变量和全局变量\n局部变量是指在当前代码块内可见的变量，使用`local`声明。例如下面的代码，将依次输出：111, 222, 111.\n```bash\n#! /bin/sh\nnum=111 # 全局变量\nfunc1()\n{\n  local num=222 # 局部变量\n  echo $num\n}\n\necho \"before---$num\"\nfunc1\necho \"after---$num\"\n```\n\n### 变量之间的运算\n\n使用`expr`可以进行变量之间的运算，如下所示：\n\n``` bash\n# 注意要在操作符两边空余空格\nexpr 1 + 3\n# 由于*是特殊字符，所以乘法要使用转义\nexpr 10 \\* 2\n```\n\n### \\`\\`和\"\"\n\n使用\\`\\`（也就是TAB键上面的那个）包起来的部分，是可执行的命令。而使用\"\"（引号）包起来的部分，是字符串。\n\n``` bash\na=`expr 10 \\* 3`\n# output: 3\necho $a\n# output: a\necho a\n# output: expr 10 \\* 3\na=\"expr 10 \\* 3\"\necho $a\n```\n\n另外，使用\"\"（双引号）括起来的字符串会发生变量替换，而用''（单引号）括起来的字符串则不会。\n\n``` bash\na=1\necho \"$a\"  # 输出 1\necho '$a'  # 输出 $a\n```\n\n### 读取输入\n\n使用`read var1, var2, ...`的方式从键盘的输入读取变量的值。\n\n``` bash\n# input a=1\nread a\n# ouptut: 2\necho `expr $a + 1`\n```\n\n## 基本概念\n\n### 命令的返回值\n\n当bash命令成功执行后，返回给系统的返回值为`0`；否则为非零。可以据此判断上步操作的状态。使用`$?`可以取出上一步执行的返回值。\n\n``` bash\n# 将echo 错输为ecoh\necoh \"hello\"\n# output: 非零(127)\necho $?\n# output: 0\necho $?\n```\n\n### 通配符\n\n通配符是指`*`,`?`和`[...]`这三类。\n\n`*`可以匹配任意多的字符，`?`用来匹配一个字符。`[...]`用来匹配括号内的字符。见下表。\n![通配符](/img/shell-programming-wild-cards.jpg)\n\n`[...]`表示法还有如下变形：\n\n- 使用`-`用来指示范围。如`[a-z]`，表示`a`到`z`间任意一个字符。\n- 使用`^`或`!`表示取反。如`[!a-p]`表示除了`a`到`p`间字符的其他字符。\n\n### 输入输出重定向\n\n重定向是指改变命令的输出位置。使用`>`进行输出重定向。使用`<`进行输入重定向。例如，`ls -l > a.txt`，将本目录下的文件信息输出到文本文件`a.txt`中，而不再输出到终端。\n\n此外，`>>`同样是输出重定向。但是它会在文件末尾追加写入，不会覆盖文件的原有内容。\n\n搭配使用`<`和`>`可以做文件处理。例如，`tr group1 group2`命令可以将`group1`中的字符变换为`group2`中对应位置的字符。使用如下命令：\n\n``` bash\ntr \"[a-z]\" \"A-Z\" < ori.txt > out.txt\n```\n\n可以将`ori.txt`中的小写字母转换为大写字母输出到`out.txt`中。\n\n### 管道（pipeline）\n\n管道`|`可以将第一个程序的输出作为第二个程序的输入。例如：\n\n``` bash\ncat ori.txt | tr \"[a-z]\" \"A-Z\"\n```\n\n会将`ori.txt`中的小写字母转换为大写，并在终端输出。\n\n### 过滤器（Filter）\n\nFilter是指那些输入和输出都是控制台的命令。通过Filter和输入输出重定向，可以很方便地对文件内容进行整理。例如：\n\n``` bash\nsort < names.txt | uniq > u_names.txt\n```\n\n`uniq`命令可以实现去重，但是需要首先对输入数据进行排序。上面的Filter可以将输入文件`names.txt`中的行文本去重后输出到`u_names.txt`中去。\n\n## 控制流\n\n### if 条件控制\n\n在bash中使用`if`条件控制的语法和MATLAB等很像，要在末尾加上类似`end`的指示符，如下：\n\n``` bash\nif condition\nthen \nXXX\nfi\n```\n\n或者加上`else`，使用如下的形式：\n``` bash\nif condition\nthen\n    do something\nelif condition\nthen\n    do something\nelse\n    do something\nfi\n```\n\n那么，如何做逻辑运算呢？需要借助`test`关键字。\n\n对于整数来说，我们可以使用`if test op1 oprator op2`的方式，判断操作数`op1`和`op2`的大小关系。其中，`operator`可以是`-gt`，`-eq`等。\n\n或者另一种写法：`if [ op1 operator op2 ]`，但是注意后者`[]`与操作数之间有空格。\n如下表所示（点击可放大）：\n\n![比较整数的逻辑运算](/img/shell-programming-if-operators.jpg)\n\n对于字符串，支持的逻辑判断如下：\n![比较字符串的逻辑运算](/img/bash-programming-comparing-string.jpg)\n\n举个例子，我们想判断输入的值是否为1或2，可以使用如下的脚本。注意`[]`的两边一定要加空格。\n``` bash\n#! /bin/bash\na=1\nif [ $1=$a ]\nthen\n    echo \"you input 1\"\nelif [ $1=2 ]\nthen\n    echo \"you input 2\"\nelse\n    echo \"you input $1\"\nfi\n```\n\n\n","source":"_posts/shell-programming.md","raw":"---\ntitle: shell编程\ndate: 2017-11-10 13:06:30\ntags:\n    - linux\n    - shell\n---\n介绍基本的shell编程方法，参考的教程是[Linux Shell Scripting Tutorial, A Beginner's handbook](http://www.freeos.com/guides/lsst/)。\n![Bash Logo](/img/shell-programming-bash-logo.png)\n<!-- more -->\n\n## 变量\n\n变量是代码的基本组成元素。可以认为shell中的变量类型都是字符串。\n\nshell中的变量可以分为两类：系统变量和用户自定义变量。下面分别进行介绍。\n\n在代码中使用变量值的时候，需要在前面加上`$`。`echo`命令可以在控制台打印相应输出。所以使用`echo $var`就可以输出变量`var`的值。\n\n### 系统变量\n\n系统变量是指Linux中自带的一些变量。例如`HOME`,`PATH`等。其中`PATH`又叫环境变量。更多的系统变量见下表：\n![系统变量列表](/img/shell-programming-system-variables.jpg)\n\n### 用户定义的变量\n\n用户自定义变量是用户命名并赋值的变量。使用下面的方法定义：\n\n``` bash\n# 注意不要在等号两边插入空格\nname=value\n# 如 n=10\n```\n\n### 局部变量和全局变量\n局部变量是指在当前代码块内可见的变量，使用`local`声明。例如下面的代码，将依次输出：111, 222, 111.\n```bash\n#! /bin/sh\nnum=111 # 全局变量\nfunc1()\n{\n  local num=222 # 局部变量\n  echo $num\n}\n\necho \"before---$num\"\nfunc1\necho \"after---$num\"\n```\n\n### 变量之间的运算\n\n使用`expr`可以进行变量之间的运算，如下所示：\n\n``` bash\n# 注意要在操作符两边空余空格\nexpr 1 + 3\n# 由于*是特殊字符，所以乘法要使用转义\nexpr 10 \\* 2\n```\n\n### \\`\\`和\"\"\n\n使用\\`\\`（也就是TAB键上面的那个）包起来的部分，是可执行的命令。而使用\"\"（引号）包起来的部分，是字符串。\n\n``` bash\na=`expr 10 \\* 3`\n# output: 3\necho $a\n# output: a\necho a\n# output: expr 10 \\* 3\na=\"expr 10 \\* 3\"\necho $a\n```\n\n另外，使用\"\"（双引号）括起来的字符串会发生变量替换，而用''（单引号）括起来的字符串则不会。\n\n``` bash\na=1\necho \"$a\"  # 输出 1\necho '$a'  # 输出 $a\n```\n\n### 读取输入\n\n使用`read var1, var2, ...`的方式从键盘的输入读取变量的值。\n\n``` bash\n# input a=1\nread a\n# ouptut: 2\necho `expr $a + 1`\n```\n\n## 基本概念\n\n### 命令的返回值\n\n当bash命令成功执行后，返回给系统的返回值为`0`；否则为非零。可以据此判断上步操作的状态。使用`$?`可以取出上一步执行的返回值。\n\n``` bash\n# 将echo 错输为ecoh\necoh \"hello\"\n# output: 非零(127)\necho $?\n# output: 0\necho $?\n```\n\n### 通配符\n\n通配符是指`*`,`?`和`[...]`这三类。\n\n`*`可以匹配任意多的字符，`?`用来匹配一个字符。`[...]`用来匹配括号内的字符。见下表。\n![通配符](/img/shell-programming-wild-cards.jpg)\n\n`[...]`表示法还有如下变形：\n\n- 使用`-`用来指示范围。如`[a-z]`，表示`a`到`z`间任意一个字符。\n- 使用`^`或`!`表示取反。如`[!a-p]`表示除了`a`到`p`间字符的其他字符。\n\n### 输入输出重定向\n\n重定向是指改变命令的输出位置。使用`>`进行输出重定向。使用`<`进行输入重定向。例如，`ls -l > a.txt`，将本目录下的文件信息输出到文本文件`a.txt`中，而不再输出到终端。\n\n此外，`>>`同样是输出重定向。但是它会在文件末尾追加写入，不会覆盖文件的原有内容。\n\n搭配使用`<`和`>`可以做文件处理。例如，`tr group1 group2`命令可以将`group1`中的字符变换为`group2`中对应位置的字符。使用如下命令：\n\n``` bash\ntr \"[a-z]\" \"A-Z\" < ori.txt > out.txt\n```\n\n可以将`ori.txt`中的小写字母转换为大写字母输出到`out.txt`中。\n\n### 管道（pipeline）\n\n管道`|`可以将第一个程序的输出作为第二个程序的输入。例如：\n\n``` bash\ncat ori.txt | tr \"[a-z]\" \"A-Z\"\n```\n\n会将`ori.txt`中的小写字母转换为大写，并在终端输出。\n\n### 过滤器（Filter）\n\nFilter是指那些输入和输出都是控制台的命令。通过Filter和输入输出重定向，可以很方便地对文件内容进行整理。例如：\n\n``` bash\nsort < names.txt | uniq > u_names.txt\n```\n\n`uniq`命令可以实现去重，但是需要首先对输入数据进行排序。上面的Filter可以将输入文件`names.txt`中的行文本去重后输出到`u_names.txt`中去。\n\n## 控制流\n\n### if 条件控制\n\n在bash中使用`if`条件控制的语法和MATLAB等很像，要在末尾加上类似`end`的指示符，如下：\n\n``` bash\nif condition\nthen \nXXX\nfi\n```\n\n或者加上`else`，使用如下的形式：\n``` bash\nif condition\nthen\n    do something\nelif condition\nthen\n    do something\nelse\n    do something\nfi\n```\n\n那么，如何做逻辑运算呢？需要借助`test`关键字。\n\n对于整数来说，我们可以使用`if test op1 oprator op2`的方式，判断操作数`op1`和`op2`的大小关系。其中，`operator`可以是`-gt`，`-eq`等。\n\n或者另一种写法：`if [ op1 operator op2 ]`，但是注意后者`[]`与操作数之间有空格。\n如下表所示（点击可放大）：\n\n![比较整数的逻辑运算](/img/shell-programming-if-operators.jpg)\n\n对于字符串，支持的逻辑判断如下：\n![比较字符串的逻辑运算](/img/bash-programming-comparing-string.jpg)\n\n举个例子，我们想判断输入的值是否为1或2，可以使用如下的脚本。注意`[]`的两边一定要加空格。\n``` bash\n#! /bin/bash\na=1\nif [ $1=$a ]\nthen\n    echo \"you input 1\"\nelif [ $1=2 ]\nthen\n    echo \"you input 2\"\nelse\n    echo \"you input $1\"\nfi\n```\n\n\n","slug":"shell-programming","published":1,"updated":"2018-01-12T06:22:20.480Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjcu6vcu8002equ469lvnbar6","content":"<p>介绍基本的shell编程方法，参考的教程是<a href=\"http://www.freeos.com/guides/lsst/\" target=\"_blank\" rel=\"external\">Linux Shell Scripting Tutorial, A Beginner’s handbook</a>。<br><img src=\"/img/shell-programming-bash-logo.png\" alt=\"Bash Logo\"><br><a id=\"more\"></a></p>\n<h2 id=\"变量\"><a href=\"#变量\" class=\"headerlink\" title=\"变量\"></a>变量</h2><p>变量是代码的基本组成元素。可以认为shell中的变量类型都是字符串。</p>\n<p>shell中的变量可以分为两类：系统变量和用户自定义变量。下面分别进行介绍。</p>\n<p>在代码中使用变量值的时候，需要在前面加上<code>$</code>。<code>echo</code>命令可以在控制台打印相应输出。所以使用<code>echo $var</code>就可以输出变量<code>var</code>的值。</p>\n<h3 id=\"系统变量\"><a href=\"#系统变量\" class=\"headerlink\" title=\"系统变量\"></a>系统变量</h3><p>系统变量是指Linux中自带的一些变量。例如<code>HOME</code>,<code>PATH</code>等。其中<code>PATH</code>又叫环境变量。更多的系统变量见下表：<br><img src=\"/img/shell-programming-system-variables.jpg\" alt=\"系统变量列表\"></p>\n<h3 id=\"用户定义的变量\"><a href=\"#用户定义的变量\" class=\"headerlink\" title=\"用户定义的变量\"></a>用户定义的变量</h3><p>用户自定义变量是用户命名并赋值的变量。使用下面的方法定义：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\"># 注意不要在等号两边插入空格</span></div><div class=\"line\">name=value</div><div class=\"line\"><span class=\"comment\"># 如 n=10</span></div></pre></td></tr></table></figure>\n<h3 id=\"局部变量和全局变量\"><a href=\"#局部变量和全局变量\" class=\"headerlink\" title=\"局部变量和全局变量\"></a>局部变量和全局变量</h3><p>局部变量是指在当前代码块内可见的变量，使用<code>local</code>声明。例如下面的代码，将依次输出：111, 222, 111.<br><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"meta\">#! /bin/sh</span></div><div class=\"line\">num=111 <span class=\"comment\"># 全局变量</span></div><div class=\"line\"><span class=\"function\"><span class=\"title\">func1</span></span>()</div><div class=\"line\">&#123;</div><div class=\"line\">  <span class=\"built_in\">local</span> num=222 <span class=\"comment\"># 局部变量</span></div><div class=\"line\">  <span class=\"built_in\">echo</span> <span class=\"variable\">$num</span></div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\"><span class=\"built_in\">echo</span> <span class=\"string\">\"before---<span class=\"variable\">$num</span>\"</span></div><div class=\"line\">func1</div><div class=\"line\"><span class=\"built_in\">echo</span> <span class=\"string\">\"after---<span class=\"variable\">$num</span>\"</span></div></pre></td></tr></table></figure></p>\n<h3 id=\"变量之间的运算\"><a href=\"#变量之间的运算\" class=\"headerlink\" title=\"变量之间的运算\"></a>变量之间的运算</h3><p>使用<code>expr</code>可以进行变量之间的运算，如下所示：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\"># 注意要在操作符两边空余空格</span></div><div class=\"line\">expr 1 + 3</div><div class=\"line\"><span class=\"comment\"># 由于*是特殊字符，所以乘法要使用转义</span></div><div class=\"line\">expr 10 \\* 2</div></pre></td></tr></table></figure>\n<h3 id=\"和””\"><a href=\"#和””\" class=\"headerlink\" title=\"``和””\"></a>``和””</h3><p>使用``（也就是TAB键上面的那个）包起来的部分，是可执行的命令。而使用””（引号）包起来的部分，是字符串。</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div></pre></td><td class=\"code\"><pre><div class=\"line\">a=`expr 10 \\* 3`</div><div class=\"line\"><span class=\"comment\"># output: 3</span></div><div class=\"line\"><span class=\"built_in\">echo</span> <span class=\"variable\">$a</span></div><div class=\"line\"><span class=\"comment\"># output: a</span></div><div class=\"line\"><span class=\"built_in\">echo</span> a</div><div class=\"line\"><span class=\"comment\"># output: expr 10 \\* 3</span></div><div class=\"line\">a=<span class=\"string\">\"expr 10 \\* 3\"</span></div><div class=\"line\"><span class=\"built_in\">echo</span> <span class=\"variable\">$a</span></div></pre></td></tr></table></figure>\n<p>另外，使用””（双引号）括起来的字符串会发生变量替换，而用’’（单引号）括起来的字符串则不会。</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\">a=1</div><div class=\"line\"><span class=\"built_in\">echo</span> <span class=\"string\">\"<span class=\"variable\">$a</span>\"</span>  <span class=\"comment\"># 输出 1</span></div><div class=\"line\"><span class=\"built_in\">echo</span> <span class=\"string\">'$a'</span>  <span class=\"comment\"># 输出 $a</span></div></pre></td></tr></table></figure>\n<h3 id=\"读取输入\"><a href=\"#读取输入\" class=\"headerlink\" title=\"读取输入\"></a>读取输入</h3><p>使用<code>read var1, var2, ...</code>的方式从键盘的输入读取变量的值。</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\"># input a=1</span></div><div class=\"line\"><span class=\"built_in\">read</span> a</div><div class=\"line\"><span class=\"comment\"># ouptut: 2</span></div><div class=\"line\"><span class=\"built_in\">echo</span> `expr <span class=\"variable\">$a</span> + 1`</div></pre></td></tr></table></figure>\n<h2 id=\"基本概念\"><a href=\"#基本概念\" class=\"headerlink\" title=\"基本概念\"></a>基本概念</h2><h3 id=\"命令的返回值\"><a href=\"#命令的返回值\" class=\"headerlink\" title=\"命令的返回值\"></a>命令的返回值</h3><p>当bash命令成功执行后，返回给系统的返回值为<code>0</code>；否则为非零。可以据此判断上步操作的状态。使用<code>$?</code>可以取出上一步执行的返回值。</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\"># 将echo 错输为ecoh</span></div><div class=\"line\">ecoh <span class=\"string\">\"hello\"</span></div><div class=\"line\"><span class=\"comment\"># output: 非零(127)</span></div><div class=\"line\"><span class=\"built_in\">echo</span> $?</div><div class=\"line\"><span class=\"comment\"># output: 0</span></div><div class=\"line\"><span class=\"built_in\">echo</span> $?</div></pre></td></tr></table></figure>\n<h3 id=\"通配符\"><a href=\"#通配符\" class=\"headerlink\" title=\"通配符\"></a>通配符</h3><p>通配符是指<code>*</code>,<code>?</code>和<code>[...]</code>这三类。</p>\n<p><code>*</code>可以匹配任意多的字符，<code>?</code>用来匹配一个字符。<code>[...]</code>用来匹配括号内的字符。见下表。<br><img src=\"/img/shell-programming-wild-cards.jpg\" alt=\"通配符\"></p>\n<p><code>[...]</code>表示法还有如下变形：</p>\n<ul>\n<li>使用<code>-</code>用来指示范围。如<code>[a-z]</code>，表示<code>a</code>到<code>z</code>间任意一个字符。</li>\n<li>使用<code>^</code>或<code>!</code>表示取反。如<code>[!a-p]</code>表示除了<code>a</code>到<code>p</code>间字符的其他字符。</li>\n</ul>\n<h3 id=\"输入输出重定向\"><a href=\"#输入输出重定向\" class=\"headerlink\" title=\"输入输出重定向\"></a>输入输出重定向</h3><p>重定向是指改变命令的输出位置。使用<code>&gt;</code>进行输出重定向。使用<code>&lt;</code>进行输入重定向。例如，<code>ls -l &gt; a.txt</code>，将本目录下的文件信息输出到文本文件<code>a.txt</code>中，而不再输出到终端。</p>\n<p>此外，<code>&gt;&gt;</code>同样是输出重定向。但是它会在文件末尾追加写入，不会覆盖文件的原有内容。</p>\n<p>搭配使用<code>&lt;</code>和<code>&gt;</code>可以做文件处理。例如，<code>tr group1 group2</code>命令可以将<code>group1</code>中的字符变换为<code>group2</code>中对应位置的字符。使用如下命令：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">tr <span class=\"string\">\"[a-z]\"</span> <span class=\"string\">\"A-Z\"</span> &lt; ori.txt &gt; out.txt</div></pre></td></tr></table></figure>\n<p>可以将<code>ori.txt</code>中的小写字母转换为大写字母输出到<code>out.txt</code>中。</p>\n<h3 id=\"管道（pipeline）\"><a href=\"#管道（pipeline）\" class=\"headerlink\" title=\"管道（pipeline）\"></a>管道（pipeline）</h3><p>管道<code>|</code>可以将第一个程序的输出作为第二个程序的输入。例如：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">cat ori.txt | tr <span class=\"string\">\"[a-z]\"</span> <span class=\"string\">\"A-Z\"</span></div></pre></td></tr></table></figure>\n<p>会将<code>ori.txt</code>中的小写字母转换为大写，并在终端输出。</p>\n<h3 id=\"过滤器（Filter）\"><a href=\"#过滤器（Filter）\" class=\"headerlink\" title=\"过滤器（Filter）\"></a>过滤器（Filter）</h3><p>Filter是指那些输入和输出都是控制台的命令。通过Filter和输入输出重定向，可以很方便地对文件内容进行整理。例如：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">sort &lt; names.txt | uniq &gt; u_names.txt</div></pre></td></tr></table></figure>\n<p><code>uniq</code>命令可以实现去重，但是需要首先对输入数据进行排序。上面的Filter可以将输入文件<code>names.txt</code>中的行文本去重后输出到<code>u_names.txt</code>中去。</p>\n<h2 id=\"控制流\"><a href=\"#控制流\" class=\"headerlink\" title=\"控制流\"></a>控制流</h2><h3 id=\"if-条件控制\"><a href=\"#if-条件控制\" class=\"headerlink\" title=\"if 条件控制\"></a>if 条件控制</h3><p>在bash中使用<code>if</code>条件控制的语法和MATLAB等很像，要在末尾加上类似<code>end</code>的指示符，如下：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">if</span> condition</div><div class=\"line\"><span class=\"keyword\">then</span> </div><div class=\"line\">XXX</div><div class=\"line\"><span class=\"keyword\">fi</span></div></pre></td></tr></table></figure>\n<p>或者加上<code>else</code>，使用如下的形式：<br><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">if</span> condition</div><div class=\"line\"><span class=\"keyword\">then</span></div><div class=\"line\">    <span class=\"keyword\">do</span> something</div><div class=\"line\"><span class=\"keyword\">elif</span> condition</div><div class=\"line\"><span class=\"keyword\">then</span></div><div class=\"line\">    <span class=\"keyword\">do</span> something</div><div class=\"line\"><span class=\"keyword\">else</span></div><div class=\"line\">    <span class=\"keyword\">do</span> something</div><div class=\"line\"><span class=\"keyword\">fi</span></div></pre></td></tr></table></figure></p>\n<p>那么，如何做逻辑运算呢？需要借助<code>test</code>关键字。</p>\n<p>对于整数来说，我们可以使用<code>if test op1 oprator op2</code>的方式，判断操作数<code>op1</code>和<code>op2</code>的大小关系。其中，<code>operator</code>可以是<code>-gt</code>，<code>-eq</code>等。</p>\n<p>或者另一种写法：<code>if [ op1 operator op2 ]</code>，但是注意后者<code>[]</code>与操作数之间有空格。<br>如下表所示（点击可放大）：</p>\n<p><img src=\"/img/shell-programming-if-operators.jpg\" alt=\"比较整数的逻辑运算\"></p>\n<p>对于字符串，支持的逻辑判断如下：<br><img src=\"/img/bash-programming-comparing-string.jpg\" alt=\"比较字符串的逻辑运算\"></p>\n<p>举个例子，我们想判断输入的值是否为1或2，可以使用如下的脚本。注意<code>[]</code>的两边一定要加空格。<br><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"meta\">#! /bin/bash</span></div><div class=\"line\">a=1</div><div class=\"line\"><span class=\"keyword\">if</span> [ <span class=\"variable\">$1</span>=<span class=\"variable\">$a</span> ]</div><div class=\"line\"><span class=\"keyword\">then</span></div><div class=\"line\">    <span class=\"built_in\">echo</span> <span class=\"string\">\"you input 1\"</span></div><div class=\"line\"><span class=\"keyword\">elif</span> [ <span class=\"variable\">$1</span>=2 ]</div><div class=\"line\"><span class=\"keyword\">then</span></div><div class=\"line\">    <span class=\"built_in\">echo</span> <span class=\"string\">\"you input 2\"</span></div><div class=\"line\"><span class=\"keyword\">else</span></div><div class=\"line\">    <span class=\"built_in\">echo</span> <span class=\"string\">\"you input <span class=\"variable\">$1</span>\"</span></div><div class=\"line\"><span class=\"keyword\">fi</span></div></pre></td></tr></table></figure></p>\n","excerpt":"<p>介绍基本的shell编程方法，参考的教程是<a href=\"http://www.freeos.com/guides/lsst/\">Linux Shell Scripting Tutorial, A Beginner’s handbook</a>。<br><img src=\"/img/shell-programming-bash-logo.png\" alt=\"Bash Logo\"><br>","more":"</p>\n<h2 id=\"变量\"><a href=\"#变量\" class=\"headerlink\" title=\"变量\"></a>变量</h2><p>变量是代码的基本组成元素。可以认为shell中的变量类型都是字符串。</p>\n<p>shell中的变量可以分为两类：系统变量和用户自定义变量。下面分别进行介绍。</p>\n<p>在代码中使用变量值的时候，需要在前面加上<code>$</code>。<code>echo</code>命令可以在控制台打印相应输出。所以使用<code>echo $var</code>就可以输出变量<code>var</code>的值。</p>\n<h3 id=\"系统变量\"><a href=\"#系统变量\" class=\"headerlink\" title=\"系统变量\"></a>系统变量</h3><p>系统变量是指Linux中自带的一些变量。例如<code>HOME</code>,<code>PATH</code>等。其中<code>PATH</code>又叫环境变量。更多的系统变量见下表：<br><img src=\"/img/shell-programming-system-variables.jpg\" alt=\"系统变量列表\"></p>\n<h3 id=\"用户定义的变量\"><a href=\"#用户定义的变量\" class=\"headerlink\" title=\"用户定义的变量\"></a>用户定义的变量</h3><p>用户自定义变量是用户命名并赋值的变量。使用下面的方法定义：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\"># 注意不要在等号两边插入空格</span></div><div class=\"line\">name=value</div><div class=\"line\"><span class=\"comment\"># 如 n=10</span></div></pre></td></tr></table></figure>\n<h3 id=\"局部变量和全局变量\"><a href=\"#局部变量和全局变量\" class=\"headerlink\" title=\"局部变量和全局变量\"></a>局部变量和全局变量</h3><p>局部变量是指在当前代码块内可见的变量，使用<code>local</code>声明。例如下面的代码，将依次输出：111, 222, 111.<br><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"meta\">#! /bin/sh</span></div><div class=\"line\">num=111 <span class=\"comment\"># 全局变量</span></div><div class=\"line\"><span class=\"function\"><span class=\"title\">func1</span></span>()</div><div class=\"line\">&#123;</div><div class=\"line\">  <span class=\"built_in\">local</span> num=222 <span class=\"comment\"># 局部变量</span></div><div class=\"line\">  <span class=\"built_in\">echo</span> <span class=\"variable\">$num</span></div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\"><span class=\"built_in\">echo</span> <span class=\"string\">\"before---<span class=\"variable\">$num</span>\"</span></div><div class=\"line\">func1</div><div class=\"line\"><span class=\"built_in\">echo</span> <span class=\"string\">\"after---<span class=\"variable\">$num</span>\"</span></div></pre></td></tr></table></figure></p>\n<h3 id=\"变量之间的运算\"><a href=\"#变量之间的运算\" class=\"headerlink\" title=\"变量之间的运算\"></a>变量之间的运算</h3><p>使用<code>expr</code>可以进行变量之间的运算，如下所示：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\"># 注意要在操作符两边空余空格</span></div><div class=\"line\">expr 1 + 3</div><div class=\"line\"><span class=\"comment\"># 由于*是特殊字符，所以乘法要使用转义</span></div><div class=\"line\">expr 10 \\* 2</div></pre></td></tr></table></figure>\n<h3 id=\"和””\"><a href=\"#和””\" class=\"headerlink\" title=\"``和””\"></a>``和””</h3><p>使用``（也就是TAB键上面的那个）包起来的部分，是可执行的命令。而使用””（引号）包起来的部分，是字符串。</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div></pre></td><td class=\"code\"><pre><div class=\"line\">a=`expr 10 \\* 3`</div><div class=\"line\"><span class=\"comment\"># output: 3</span></div><div class=\"line\"><span class=\"built_in\">echo</span> <span class=\"variable\">$a</span></div><div class=\"line\"><span class=\"comment\"># output: a</span></div><div class=\"line\"><span class=\"built_in\">echo</span> a</div><div class=\"line\"><span class=\"comment\"># output: expr 10 \\* 3</span></div><div class=\"line\">a=<span class=\"string\">\"expr 10 \\* 3\"</span></div><div class=\"line\"><span class=\"built_in\">echo</span> <span class=\"variable\">$a</span></div></pre></td></tr></table></figure>\n<p>另外，使用””（双引号）括起来的字符串会发生变量替换，而用’’（单引号）括起来的字符串则不会。</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\">a=1</div><div class=\"line\"><span class=\"built_in\">echo</span> <span class=\"string\">\"<span class=\"variable\">$a</span>\"</span>  <span class=\"comment\"># 输出 1</span></div><div class=\"line\"><span class=\"built_in\">echo</span> <span class=\"string\">'$a'</span>  <span class=\"comment\"># 输出 $a</span></div></pre></td></tr></table></figure>\n<h3 id=\"读取输入\"><a href=\"#读取输入\" class=\"headerlink\" title=\"读取输入\"></a>读取输入</h3><p>使用<code>read var1, var2, ...</code>的方式从键盘的输入读取变量的值。</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\"># input a=1</span></div><div class=\"line\"><span class=\"built_in\">read</span> a</div><div class=\"line\"><span class=\"comment\"># ouptut: 2</span></div><div class=\"line\"><span class=\"built_in\">echo</span> `expr <span class=\"variable\">$a</span> + 1`</div></pre></td></tr></table></figure>\n<h2 id=\"基本概念\"><a href=\"#基本概念\" class=\"headerlink\" title=\"基本概念\"></a>基本概念</h2><h3 id=\"命令的返回值\"><a href=\"#命令的返回值\" class=\"headerlink\" title=\"命令的返回值\"></a>命令的返回值</h3><p>当bash命令成功执行后，返回给系统的返回值为<code>0</code>；否则为非零。可以据此判断上步操作的状态。使用<code>$?</code>可以取出上一步执行的返回值。</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\"># 将echo 错输为ecoh</span></div><div class=\"line\">ecoh <span class=\"string\">\"hello\"</span></div><div class=\"line\"><span class=\"comment\"># output: 非零(127)</span></div><div class=\"line\"><span class=\"built_in\">echo</span> $?</div><div class=\"line\"><span class=\"comment\"># output: 0</span></div><div class=\"line\"><span class=\"built_in\">echo</span> $?</div></pre></td></tr></table></figure>\n<h3 id=\"通配符\"><a href=\"#通配符\" class=\"headerlink\" title=\"通配符\"></a>通配符</h3><p>通配符是指<code>*</code>,<code>?</code>和<code>[...]</code>这三类。</p>\n<p><code>*</code>可以匹配任意多的字符，<code>?</code>用来匹配一个字符。<code>[...]</code>用来匹配括号内的字符。见下表。<br><img src=\"/img/shell-programming-wild-cards.jpg\" alt=\"通配符\"></p>\n<p><code>[...]</code>表示法还有如下变形：</p>\n<ul>\n<li>使用<code>-</code>用来指示范围。如<code>[a-z]</code>，表示<code>a</code>到<code>z</code>间任意一个字符。</li>\n<li>使用<code>^</code>或<code>!</code>表示取反。如<code>[!a-p]</code>表示除了<code>a</code>到<code>p</code>间字符的其他字符。</li>\n</ul>\n<h3 id=\"输入输出重定向\"><a href=\"#输入输出重定向\" class=\"headerlink\" title=\"输入输出重定向\"></a>输入输出重定向</h3><p>重定向是指改变命令的输出位置。使用<code>&gt;</code>进行输出重定向。使用<code>&lt;</code>进行输入重定向。例如，<code>ls -l &gt; a.txt</code>，将本目录下的文件信息输出到文本文件<code>a.txt</code>中，而不再输出到终端。</p>\n<p>此外，<code>&gt;&gt;</code>同样是输出重定向。但是它会在文件末尾追加写入，不会覆盖文件的原有内容。</p>\n<p>搭配使用<code>&lt;</code>和<code>&gt;</code>可以做文件处理。例如，<code>tr group1 group2</code>命令可以将<code>group1</code>中的字符变换为<code>group2</code>中对应位置的字符。使用如下命令：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">tr <span class=\"string\">\"[a-z]\"</span> <span class=\"string\">\"A-Z\"</span> &lt; ori.txt &gt; out.txt</div></pre></td></tr></table></figure>\n<p>可以将<code>ori.txt</code>中的小写字母转换为大写字母输出到<code>out.txt</code>中。</p>\n<h3 id=\"管道（pipeline）\"><a href=\"#管道（pipeline）\" class=\"headerlink\" title=\"管道（pipeline）\"></a>管道（pipeline）</h3><p>管道<code>|</code>可以将第一个程序的输出作为第二个程序的输入。例如：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">cat ori.txt | tr <span class=\"string\">\"[a-z]\"</span> <span class=\"string\">\"A-Z\"</span></div></pre></td></tr></table></figure>\n<p>会将<code>ori.txt</code>中的小写字母转换为大写，并在终端输出。</p>\n<h3 id=\"过滤器（Filter）\"><a href=\"#过滤器（Filter）\" class=\"headerlink\" title=\"过滤器（Filter）\"></a>过滤器（Filter）</h3><p>Filter是指那些输入和输出都是控制台的命令。通过Filter和输入输出重定向，可以很方便地对文件内容进行整理。例如：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">sort &lt; names.txt | uniq &gt; u_names.txt</div></pre></td></tr></table></figure>\n<p><code>uniq</code>命令可以实现去重，但是需要首先对输入数据进行排序。上面的Filter可以将输入文件<code>names.txt</code>中的行文本去重后输出到<code>u_names.txt</code>中去。</p>\n<h2 id=\"控制流\"><a href=\"#控制流\" class=\"headerlink\" title=\"控制流\"></a>控制流</h2><h3 id=\"if-条件控制\"><a href=\"#if-条件控制\" class=\"headerlink\" title=\"if 条件控制\"></a>if 条件控制</h3><p>在bash中使用<code>if</code>条件控制的语法和MATLAB等很像，要在末尾加上类似<code>end</code>的指示符，如下：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">if</span> condition</div><div class=\"line\"><span class=\"keyword\">then</span> </div><div class=\"line\">XXX</div><div class=\"line\"><span class=\"keyword\">fi</span></div></pre></td></tr></table></figure>\n<p>或者加上<code>else</code>，使用如下的形式：<br><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">if</span> condition</div><div class=\"line\"><span class=\"keyword\">then</span></div><div class=\"line\">    <span class=\"keyword\">do</span> something</div><div class=\"line\"><span class=\"keyword\">elif</span> condition</div><div class=\"line\"><span class=\"keyword\">then</span></div><div class=\"line\">    <span class=\"keyword\">do</span> something</div><div class=\"line\"><span class=\"keyword\">else</span></div><div class=\"line\">    <span class=\"keyword\">do</span> something</div><div class=\"line\"><span class=\"keyword\">fi</span></div></pre></td></tr></table></figure></p>\n<p>那么，如何做逻辑运算呢？需要借助<code>test</code>关键字。</p>\n<p>对于整数来说，我们可以使用<code>if test op1 oprator op2</code>的方式，判断操作数<code>op1</code>和<code>op2</code>的大小关系。其中，<code>operator</code>可以是<code>-gt</code>，<code>-eq</code>等。</p>\n<p>或者另一种写法：<code>if [ op1 operator op2 ]</code>，但是注意后者<code>[]</code>与操作数之间有空格。<br>如下表所示（点击可放大）：</p>\n<p><img src=\"/img/shell-programming-if-operators.jpg\" alt=\"比较整数的逻辑运算\"></p>\n<p>对于字符串，支持的逻辑判断如下：<br><img src=\"/img/bash-programming-comparing-string.jpg\" alt=\"比较字符串的逻辑运算\"></p>\n<p>举个例子，我们想判断输入的值是否为1或2，可以使用如下的脚本。注意<code>[]</code>的两边一定要加空格。<br><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"meta\">#! /bin/bash</span></div><div class=\"line\">a=1</div><div class=\"line\"><span class=\"keyword\">if</span> [ <span class=\"variable\">$1</span>=<span class=\"variable\">$a</span> ]</div><div class=\"line\"><span class=\"keyword\">then</span></div><div class=\"line\">    <span class=\"built_in\">echo</span> <span class=\"string\">\"you input 1\"</span></div><div class=\"line\"><span class=\"keyword\">elif</span> [ <span class=\"variable\">$1</span>=2 ]</div><div class=\"line\"><span class=\"keyword\">then</span></div><div class=\"line\">    <span class=\"built_in\">echo</span> <span class=\"string\">\"you input 2\"</span></div><div class=\"line\"><span class=\"keyword\">else</span></div><div class=\"line\">    <span class=\"built_in\">echo</span> <span class=\"string\">\"you input <span class=\"variable\">$1</span>\"</span></div><div class=\"line\"><span class=\"keyword\">fi</span></div></pre></td></tr></table></figure></p>"},{"title":"Silver RL课程 - MDP","date":"2017-05-31T05:03:31.000Z","_content":"Silver在英国UCL讲授强化学习的slide总结。背景介绍部分略去不表，第一篇首先介绍强化学习中重要的数学基础-马尔科夫决策过程（MDP）。\n![MDP](/img/silver_rl_mdp.png)\n<!-- more -->\n\n## 马尔科夫性质\n不严谨地来说，马尔科夫性质是指未来与过去无关，只与当前的状态有关。我们说某个State是Markov的，等价于下面的等式成立：\n\n$$P[S_{t+1}|S_t] = P[S_{t+1}|S_1, \\dots, S_t]$$\n\n定义状态转移概率（State Transition Probability）如下：\n$$P_{ss^\\prime} = P[S_{t+1}=s^\\prime|S_t=s]$$\n\n前后两个时刻的状态不同取值的状态转移概率可以写成一个矩阵的形式。矩阵中的任意元素$P_{i,j}$表示$t$时刻状态$i$在$t+1$时刻转移到状态$j$的概率。矩阵满足行和为$1$的约束。\n\n下面，我们从马尔科夫性质展开，逐步地加入一些额外的参量，一步步引出强化学习中的马尔科夫决策过程。\n\n## 马尔科夫过程\n马尔科夫过程（或者叫做马尔科夫链）是指随机过程中的状态满足马尔科夫性质。我们可以使用二元组$(S, P)$来描述马氏过程。其中，\n- $S$是一个有限状态集合。\n- $P$是状态转移矩阵，定义如上。\n\n## 马尔科夫奖赏过程\n马尔科夫奖赏过程（不知道如何翻译，Markov reward process）在马氏过程基础上加上了状态转移过程中的奖赏reward。可以使用四元组$(S, P, R, \\gamma)$来表示。其中，\n- $R$代表奖励函数，$R\\_s = E[R\\_{t+1}|S\\_t=s]$，是指当前状态为$s$时，下一步状态转移过程中的期望奖励。\n- $\\gamma$是折旧率（discount），$\\gamma \\in [0,1]$\n\n定义回报（Return）为当前时刻往后得到的折旧总奖励，即：\n$$G_t = R_{t+1}+\\gamma R_{t+2}+... = \\sum_{k=0}^{\\infty}\\gamma^kR_{t+k+1}$$\n\n折旧率的引入，有以下几点考虑：\n- 在有环存在的马氏过程中，避免了无穷大回报的出现。\n- 未来的不确定性对当前的影响较小。\n- 事实上的考虑，例如投资市场上，即时的奖励比迟滞的奖励能够有更多的利息。\n- 人类行为倾向于即时奖励。\n- 如果马氏过程是存在终止的，有的时候也可以使用$\\gamma=1$，也就是不打折。\n\n## 值函数\n值函数（Value function）的意义是以期望的形式（条件期望）给出了状态$s$的长期回报，如下：\n$$v(s) = E[G_t|S_t=s]$$\n\n值函数可以分为两个部分，即时奖励$R_{t+1}$和后续状态的折旧值函数。如下所示：\n![推导过程](/img/silver_mdp_value_function.png)\n\n最后一步推导时，第二项的变形从直觉上推断还是比较容易的，但是还是把比较严格的推导过程写在下面：\n$$\\begin{aligned}\nE[G_{t+1}|S_t = s]  &= \\sum_{s^\\prime\\in S}E[G_{t+1}|S_{t+1}=s^\\prime]P(S_{t+1}=s^\\prime|S_t=s)\\\\\n&=\\sum_{s^\\prime}v(S_{t+1}=s^\\prime)P(S_{t+1}=s^\\prime|S_t=s)\\\\\n&=E[v(S_{t+1})|S_t=s]\n\\end{aligned}$$\n\n上面的结论就是贝尔曼方程，它给出了计算值函数的递归公式。如下图所示，状态节点$s$处的值函数可以分为两个部分，分别是转换到状态$s^\\prime$过程中收到的奖励$r$，和从新的状态$s^\\prime$出发，得到的值函数。我们想要知道$t$时刻某个状态$s$的值函数，只需要从后向前遍历，递归地去计算。\n![值函数的递归过程可视化](/img/silver_rl_bellman_equation_figure.png)\n\n把上面形式求期望的过程展开，可以得到下面的等价形式（更像上面补充的证明过程的思路）。其中，后面一项就是状态转移构成的树结构中以当前状态节点$s$为父节点的所有子节点的值函数，用转移概率进行加权。这个比上式更为直观。\n$$v(s) = R_s + \\gamma\\sum_{s^\\prime \\in S}P_{ss^\\prime}v(s^\\prime)$$\n\n或者写成下面的矩阵形式，更加紧凑：\n$$v = R+\\gamma Pv$$\n![bellman方程的矩阵形式](/img/silver_rl_bellman_equation_matrix.png)\n\n当我们对系统模型（包括奖励函数和概率转换矩阵）全部知道时，可以直接求解贝尔曼方程如下：\n![Bellman方程求解](/img/silver_rl_bellman_equation_solution.png)\n\n对于含有$n$个状态的系统，求解复杂度是$\\mathcal{O}(n^3)$。当$n$较大时，常用的替代的迭代求解方法有：\n- 动态规划 DP\n- 蒙特卡洛仿真（Monte-Carlo evaluation）\n- 时间差分学习（Temporal Difference Learning）\n\n## 马尔科夫决策过程\n马尔科夫决策过程（MDP）是带有决策的马尔科夫奖励过程。其中有一个env（环境），其状态量满足马尔科夫性质。MDP可以用五元组$(S,A,P,R,\\gamma)$描述。其中，\n- $A$是一个有限决策集合。\n- $P\\_{ss^\\prime}^a = P(S\\_{t+1}=s^\\prime|S\\_t=s, A\\_t=a)$是状态转移概率矩阵。\n- $R\\_{s}^a = E[R\\_{t+1}|S\\_t=s, A\\_t=a]$是奖励函数（与动作也挂钩）\n\n### 策略\n策略（Policy）$\\pi$是指在给定状态情况下，采取动作的概率分布，如下：\n$$\\pi(a|s)=P(A_t=a|S_t=s)$$\n\n对于一个智能体，如果策略确定了，那么它对环境的表现也就决定了。MDP的策略与历史无关，只与当前的状态有关。同时，策略是平稳过程，与时间无关。例如，无论在开局，还是终局，只要棋盘上的落子一样（也就是状态一样），那么围棋程序应该给出相同的落子动作决策。\n\n当我们给定一个MDP和相应的策略$\\pi$时，状态转移过程$S_1,S_2,\\dots$是一个马氏过程$(S, P^\\pi)$（上标$\\pi$表示$P$由$\\pi$决定）。而状态和奖励构成的过程$S_1,R_1,\\dots$是一个马氏奖赏过程$(S, P^\\pi, R^\\pi, \\gamma)$。具体来说，如下（就是全概率公式）：\n$$\\begin{aligned}\nP_{ss^\\prime}^\\pi &= \\sum_{a\\in A}\\pi(a|s)P_{ss^\\prime}^a\\\\\nR_s^\\pi &=\\sum_{a\\in A}\\pi(a|s)R_s^a\n\\end{aligned}$$\n\n### 值函数\nMDP的值函数$v_\\pi(s)$是指在当前状态$s$出发，使用策略$\\pi$得到的回报期望，即，\n$$v_\\pi(s) = E_\\pi[G_t|S_t=s]$$\n\n引入“动作-值”函数（action-value function）$q_\\pi(s,a)$，意义是从当前状态$s$出发，执行动作$a$，再使用策略$\\pi$得到的回报期望，即，\n$$q_\\pi(s,a) = E_\\pi[G_t|S_t=s,A_t=a]$$\n\n### 贝尔曼方程\n两者的关系如下如所示（通过全概率公式联系）：\n![Q函数和值函数的关系](/img/silver_rl_mdp_vq_relationship.png)\n\n注意上图描述的是$t$时刻的状态$s$下，$v(s)$和$q(s,a)$的关系。我们继续顺着状态链往前，可以得到下图所示$q(s,a)$和$t+1$时刻的状态$s^\\prime$的值函数$v(s^\\prime)$之间的关系。同样是一个全概率公式：\n![相邻时刻Q函数和值函数的关系](/img/silver_rl_mdp_vq_relationship2.png)\n\n综合上面两幅图中给出的关系，我们有相邻时刻值函数$v(s)$和$v(s^\\prime)$的关系：\n![相邻时刻值函数关系](/img/silver_rl_mdp_vv_relationship.png)\n\n同样，相邻时刻Q函数的关系：\n![相邻时刻Q函数关系](/img/silver_rl_mdp_qq_relationship.png)\n\n写成紧凑的矩阵形式：\n$$v_\\pi = R^\\pi + \\gamma P^\\pi v_\\pi$$\n这个方程的解是：\n$$v_\\pi = (1-\\gamma P^\\pi)^{-1}R^\\pi$$\n\n和上面对策略函数的分解类似，我们有下面两式成立：\n$$v_\\pi(s) = E_\\pi[R_{t+1} + \\gamma v_\\pi(sS_{t+1})]$$\n\n\n### 最优值函数\n最优的值函数是指在所有的策略中，使得$v_\\pi(s)$取得最大值的那个，即：\n$$v_\\ast(s) = \\max_\\pi v_\\pi(s)$$\n\n最优Q函数的定义同理：\n$$q_\\ast(s,a) = \\max_\\pi q_\\pi(s,a)$$\n\n定义策略$\\pi$集合上的一个偏序为\n$$\\pi > \\pi^\\prime \\quad \\text{if} \\quad v_\\pi(s) > v_{\\pi^\\prime}(s), \\forall s$$\n\n如果我们已经知道了最优的值函数，那么我们可以在每一步选取动作的时候，选取那个使得当前Q函数取得最大值的动作即可。这很straight forward，用数学语言表达就是：\n\n![最优策略的取法](/img/silver_rl_mdp_optimal_policy.png)\n同样地，对于最优值函数，也有贝尔曼递归方程成立。下面是一个形象化的推导，和上面导出贝尔曼方程的思路是一样的。\n![](/img/silver_rl_mdp_optimal_vq_relationship.png)\n![](/img/silver_rl_mdp_optimal_vq_relationship2.png)\n![](/img/silver_rl_mdp_optimal_vv_relationship.png)\n![](/img/silver_rl_mdp_optimal_qq_relationship.png)\n\n常用的求解方法包括：\n- 值迭代（Value Iteration）\n- 策略迭代（Policy Iteration）\n- Q Learning\n- Sarsa\n","source":"_posts/silver-rl-mdp.md","raw":"---\ntitle: Silver RL课程 - MDP\ndate: 2017-05-31 13:03:31\ntags:\n    - reinforcement learning\n---\nSilver在英国UCL讲授强化学习的slide总结。背景介绍部分略去不表，第一篇首先介绍强化学习中重要的数学基础-马尔科夫决策过程（MDP）。\n![MDP](/img/silver_rl_mdp.png)\n<!-- more -->\n\n## 马尔科夫性质\n不严谨地来说，马尔科夫性质是指未来与过去无关，只与当前的状态有关。我们说某个State是Markov的，等价于下面的等式成立：\n\n$$P[S_{t+1}|S_t] = P[S_{t+1}|S_1, \\dots, S_t]$$\n\n定义状态转移概率（State Transition Probability）如下：\n$$P_{ss^\\prime} = P[S_{t+1}=s^\\prime|S_t=s]$$\n\n前后两个时刻的状态不同取值的状态转移概率可以写成一个矩阵的形式。矩阵中的任意元素$P_{i,j}$表示$t$时刻状态$i$在$t+1$时刻转移到状态$j$的概率。矩阵满足行和为$1$的约束。\n\n下面，我们从马尔科夫性质展开，逐步地加入一些额外的参量，一步步引出强化学习中的马尔科夫决策过程。\n\n## 马尔科夫过程\n马尔科夫过程（或者叫做马尔科夫链）是指随机过程中的状态满足马尔科夫性质。我们可以使用二元组$(S, P)$来描述马氏过程。其中，\n- $S$是一个有限状态集合。\n- $P$是状态转移矩阵，定义如上。\n\n## 马尔科夫奖赏过程\n马尔科夫奖赏过程（不知道如何翻译，Markov reward process）在马氏过程基础上加上了状态转移过程中的奖赏reward。可以使用四元组$(S, P, R, \\gamma)$来表示。其中，\n- $R$代表奖励函数，$R\\_s = E[R\\_{t+1}|S\\_t=s]$，是指当前状态为$s$时，下一步状态转移过程中的期望奖励。\n- $\\gamma$是折旧率（discount），$\\gamma \\in [0,1]$\n\n定义回报（Return）为当前时刻往后得到的折旧总奖励，即：\n$$G_t = R_{t+1}+\\gamma R_{t+2}+... = \\sum_{k=0}^{\\infty}\\gamma^kR_{t+k+1}$$\n\n折旧率的引入，有以下几点考虑：\n- 在有环存在的马氏过程中，避免了无穷大回报的出现。\n- 未来的不确定性对当前的影响较小。\n- 事实上的考虑，例如投资市场上，即时的奖励比迟滞的奖励能够有更多的利息。\n- 人类行为倾向于即时奖励。\n- 如果马氏过程是存在终止的，有的时候也可以使用$\\gamma=1$，也就是不打折。\n\n## 值函数\n值函数（Value function）的意义是以期望的形式（条件期望）给出了状态$s$的长期回报，如下：\n$$v(s) = E[G_t|S_t=s]$$\n\n值函数可以分为两个部分，即时奖励$R_{t+1}$和后续状态的折旧值函数。如下所示：\n![推导过程](/img/silver_mdp_value_function.png)\n\n最后一步推导时，第二项的变形从直觉上推断还是比较容易的，但是还是把比较严格的推导过程写在下面：\n$$\\begin{aligned}\nE[G_{t+1}|S_t = s]  &= \\sum_{s^\\prime\\in S}E[G_{t+1}|S_{t+1}=s^\\prime]P(S_{t+1}=s^\\prime|S_t=s)\\\\\n&=\\sum_{s^\\prime}v(S_{t+1}=s^\\prime)P(S_{t+1}=s^\\prime|S_t=s)\\\\\n&=E[v(S_{t+1})|S_t=s]\n\\end{aligned}$$\n\n上面的结论就是贝尔曼方程，它给出了计算值函数的递归公式。如下图所示，状态节点$s$处的值函数可以分为两个部分，分别是转换到状态$s^\\prime$过程中收到的奖励$r$，和从新的状态$s^\\prime$出发，得到的值函数。我们想要知道$t$时刻某个状态$s$的值函数，只需要从后向前遍历，递归地去计算。\n![值函数的递归过程可视化](/img/silver_rl_bellman_equation_figure.png)\n\n把上面形式求期望的过程展开，可以得到下面的等价形式（更像上面补充的证明过程的思路）。其中，后面一项就是状态转移构成的树结构中以当前状态节点$s$为父节点的所有子节点的值函数，用转移概率进行加权。这个比上式更为直观。\n$$v(s) = R_s + \\gamma\\sum_{s^\\prime \\in S}P_{ss^\\prime}v(s^\\prime)$$\n\n或者写成下面的矩阵形式，更加紧凑：\n$$v = R+\\gamma Pv$$\n![bellman方程的矩阵形式](/img/silver_rl_bellman_equation_matrix.png)\n\n当我们对系统模型（包括奖励函数和概率转换矩阵）全部知道时，可以直接求解贝尔曼方程如下：\n![Bellman方程求解](/img/silver_rl_bellman_equation_solution.png)\n\n对于含有$n$个状态的系统，求解复杂度是$\\mathcal{O}(n^3)$。当$n$较大时，常用的替代的迭代求解方法有：\n- 动态规划 DP\n- 蒙特卡洛仿真（Monte-Carlo evaluation）\n- 时间差分学习（Temporal Difference Learning）\n\n## 马尔科夫决策过程\n马尔科夫决策过程（MDP）是带有决策的马尔科夫奖励过程。其中有一个env（环境），其状态量满足马尔科夫性质。MDP可以用五元组$(S,A,P,R,\\gamma)$描述。其中，\n- $A$是一个有限决策集合。\n- $P\\_{ss^\\prime}^a = P(S\\_{t+1}=s^\\prime|S\\_t=s, A\\_t=a)$是状态转移概率矩阵。\n- $R\\_{s}^a = E[R\\_{t+1}|S\\_t=s, A\\_t=a]$是奖励函数（与动作也挂钩）\n\n### 策略\n策略（Policy）$\\pi$是指在给定状态情况下，采取动作的概率分布，如下：\n$$\\pi(a|s)=P(A_t=a|S_t=s)$$\n\n对于一个智能体，如果策略确定了，那么它对环境的表现也就决定了。MDP的策略与历史无关，只与当前的状态有关。同时，策略是平稳过程，与时间无关。例如，无论在开局，还是终局，只要棋盘上的落子一样（也就是状态一样），那么围棋程序应该给出相同的落子动作决策。\n\n当我们给定一个MDP和相应的策略$\\pi$时，状态转移过程$S_1,S_2,\\dots$是一个马氏过程$(S, P^\\pi)$（上标$\\pi$表示$P$由$\\pi$决定）。而状态和奖励构成的过程$S_1,R_1,\\dots$是一个马氏奖赏过程$(S, P^\\pi, R^\\pi, \\gamma)$。具体来说，如下（就是全概率公式）：\n$$\\begin{aligned}\nP_{ss^\\prime}^\\pi &= \\sum_{a\\in A}\\pi(a|s)P_{ss^\\prime}^a\\\\\nR_s^\\pi &=\\sum_{a\\in A}\\pi(a|s)R_s^a\n\\end{aligned}$$\n\n### 值函数\nMDP的值函数$v_\\pi(s)$是指在当前状态$s$出发，使用策略$\\pi$得到的回报期望，即，\n$$v_\\pi(s) = E_\\pi[G_t|S_t=s]$$\n\n引入“动作-值”函数（action-value function）$q_\\pi(s,a)$，意义是从当前状态$s$出发，执行动作$a$，再使用策略$\\pi$得到的回报期望，即，\n$$q_\\pi(s,a) = E_\\pi[G_t|S_t=s,A_t=a]$$\n\n### 贝尔曼方程\n两者的关系如下如所示（通过全概率公式联系）：\n![Q函数和值函数的关系](/img/silver_rl_mdp_vq_relationship.png)\n\n注意上图描述的是$t$时刻的状态$s$下，$v(s)$和$q(s,a)$的关系。我们继续顺着状态链往前，可以得到下图所示$q(s,a)$和$t+1$时刻的状态$s^\\prime$的值函数$v(s^\\prime)$之间的关系。同样是一个全概率公式：\n![相邻时刻Q函数和值函数的关系](/img/silver_rl_mdp_vq_relationship2.png)\n\n综合上面两幅图中给出的关系，我们有相邻时刻值函数$v(s)$和$v(s^\\prime)$的关系：\n![相邻时刻值函数关系](/img/silver_rl_mdp_vv_relationship.png)\n\n同样，相邻时刻Q函数的关系：\n![相邻时刻Q函数关系](/img/silver_rl_mdp_qq_relationship.png)\n\n写成紧凑的矩阵形式：\n$$v_\\pi = R^\\pi + \\gamma P^\\pi v_\\pi$$\n这个方程的解是：\n$$v_\\pi = (1-\\gamma P^\\pi)^{-1}R^\\pi$$\n\n和上面对策略函数的分解类似，我们有下面两式成立：\n$$v_\\pi(s) = E_\\pi[R_{t+1} + \\gamma v_\\pi(sS_{t+1})]$$\n\n\n### 最优值函数\n最优的值函数是指在所有的策略中，使得$v_\\pi(s)$取得最大值的那个，即：\n$$v_\\ast(s) = \\max_\\pi v_\\pi(s)$$\n\n最优Q函数的定义同理：\n$$q_\\ast(s,a) = \\max_\\pi q_\\pi(s,a)$$\n\n定义策略$\\pi$集合上的一个偏序为\n$$\\pi > \\pi^\\prime \\quad \\text{if} \\quad v_\\pi(s) > v_{\\pi^\\prime}(s), \\forall s$$\n\n如果我们已经知道了最优的值函数，那么我们可以在每一步选取动作的时候，选取那个使得当前Q函数取得最大值的动作即可。这很straight forward，用数学语言表达就是：\n\n![最优策略的取法](/img/silver_rl_mdp_optimal_policy.png)\n同样地，对于最优值函数，也有贝尔曼递归方程成立。下面是一个形象化的推导，和上面导出贝尔曼方程的思路是一样的。\n![](/img/silver_rl_mdp_optimal_vq_relationship.png)\n![](/img/silver_rl_mdp_optimal_vq_relationship2.png)\n![](/img/silver_rl_mdp_optimal_vv_relationship.png)\n![](/img/silver_rl_mdp_optimal_qq_relationship.png)\n\n常用的求解方法包括：\n- 值迭代（Value Iteration）\n- 策略迭代（Policy Iteration）\n- Q Learning\n- Sarsa\n","slug":"silver-rl-mdp","published":1,"updated":"2018-01-12T06:22:20.482Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjcu6vcuc002fqu46u6jthijo","content":"<p>Silver在英国UCL讲授强化学习的slide总结。背景介绍部分略去不表，第一篇首先介绍强化学习中重要的数学基础-马尔科夫决策过程（MDP）。<br><img src=\"/img/silver_rl_mdp.png\" alt=\"MDP\"><br><a id=\"more\"></a></p>\n<h2 id=\"马尔科夫性质\"><a href=\"#马尔科夫性质\" class=\"headerlink\" title=\"马尔科夫性质\"></a>马尔科夫性质</h2><p>不严谨地来说，马尔科夫性质是指未来与过去无关，只与当前的状态有关。我们说某个State是Markov的，等价于下面的等式成立：</p>\n<script type=\"math/tex; mode=display\">P[S_{t+1}|S_t] = P[S_{t+1}|S_1, \\dots, S_t]</script><p>定义状态转移概率（State Transition Probability）如下：</p>\n<script type=\"math/tex; mode=display\">P_{ss^\\prime} = P[S_{t+1}=s^\\prime|S_t=s]</script><p>前后两个时刻的状态不同取值的状态转移概率可以写成一个矩阵的形式。矩阵中的任意元素$P_{i,j}$表示$t$时刻状态$i$在$t+1$时刻转移到状态$j$的概率。矩阵满足行和为$1$的约束。</p>\n<p>下面，我们从马尔科夫性质展开，逐步地加入一些额外的参量，一步步引出强化学习中的马尔科夫决策过程。</p>\n<h2 id=\"马尔科夫过程\"><a href=\"#马尔科夫过程\" class=\"headerlink\" title=\"马尔科夫过程\"></a>马尔科夫过程</h2><p>马尔科夫过程（或者叫做马尔科夫链）是指随机过程中的状态满足马尔科夫性质。我们可以使用二元组$(S, P)$来描述马氏过程。其中，</p>\n<ul>\n<li>$S$是一个有限状态集合。</li>\n<li>$P$是状态转移矩阵，定义如上。</li>\n</ul>\n<h2 id=\"马尔科夫奖赏过程\"><a href=\"#马尔科夫奖赏过程\" class=\"headerlink\" title=\"马尔科夫奖赏过程\"></a>马尔科夫奖赏过程</h2><p>马尔科夫奖赏过程（不知道如何翻译，Markov reward process）在马氏过程基础上加上了状态转移过程中的奖赏reward。可以使用四元组$(S, P, R, \\gamma)$来表示。其中，</p>\n<ul>\n<li>$R$代表奖励函数，$R_s = E[R_{t+1}|S_t=s]$，是指当前状态为$s$时，下一步状态转移过程中的期望奖励。</li>\n<li>$\\gamma$是折旧率（discount），$\\gamma \\in [0,1]$</li>\n</ul>\n<p>定义回报（Return）为当前时刻往后得到的折旧总奖励，即：</p>\n<script type=\"math/tex; mode=display\">G_t = R_{t+1}+\\gamma R_{t+2}+... = \\sum_{k=0}^{\\infty}\\gamma^kR_{t+k+1}</script><p>折旧率的引入，有以下几点考虑：</p>\n<ul>\n<li>在有环存在的马氏过程中，避免了无穷大回报的出现。</li>\n<li>未来的不确定性对当前的影响较小。</li>\n<li>事实上的考虑，例如投资市场上，即时的奖励比迟滞的奖励能够有更多的利息。</li>\n<li>人类行为倾向于即时奖励。</li>\n<li>如果马氏过程是存在终止的，有的时候也可以使用$\\gamma=1$，也就是不打折。</li>\n</ul>\n<h2 id=\"值函数\"><a href=\"#值函数\" class=\"headerlink\" title=\"值函数\"></a>值函数</h2><p>值函数（Value function）的意义是以期望的形式（条件期望）给出了状态$s$的长期回报，如下：</p>\n<script type=\"math/tex; mode=display\">v(s) = E[G_t|S_t=s]</script><p>值函数可以分为两个部分，即时奖励$R_{t+1}$和后续状态的折旧值函数。如下所示：<br><img src=\"/img/silver_mdp_value_function.png\" alt=\"推导过程\"></p>\n<p>最后一步推导时，第二项的变形从直觉上推断还是比较容易的，但是还是把比较严格的推导过程写在下面：</p>\n<script type=\"math/tex; mode=display\">\\begin{aligned}\nE[G_{t+1}|S_t = s]  &= \\sum_{s^\\prime\\in S}E[G_{t+1}|S_{t+1}=s^\\prime]P(S_{t+1}=s^\\prime|S_t=s)\\\\\n&=\\sum_{s^\\prime}v(S_{t+1}=s^\\prime)P(S_{t+1}=s^\\prime|S_t=s)\\\\\n&=E[v(S_{t+1})|S_t=s]\n\\end{aligned}</script><p>上面的结论就是贝尔曼方程，它给出了计算值函数的递归公式。如下图所示，状态节点$s$处的值函数可以分为两个部分，分别是转换到状态$s^\\prime$过程中收到的奖励$r$，和从新的状态$s^\\prime$出发，得到的值函数。我们想要知道$t$时刻某个状态$s$的值函数，只需要从后向前遍历，递归地去计算。<br><img src=\"/img/silver_rl_bellman_equation_figure.png\" alt=\"值函数的递归过程可视化\"></p>\n<p>把上面形式求期望的过程展开，可以得到下面的等价形式（更像上面补充的证明过程的思路）。其中，后面一项就是状态转移构成的树结构中以当前状态节点$s$为父节点的所有子节点的值函数，用转移概率进行加权。这个比上式更为直观。</p>\n<script type=\"math/tex; mode=display\">v(s) = R_s + \\gamma\\sum_{s^\\prime \\in S}P_{ss^\\prime}v(s^\\prime)</script><p>或者写成下面的矩阵形式，更加紧凑：</p>\n<script type=\"math/tex; mode=display\">v = R+\\gamma Pv</script><p><img src=\"/img/silver_rl_bellman_equation_matrix.png\" alt=\"bellman方程的矩阵形式\"></p>\n<p>当我们对系统模型（包括奖励函数和概率转换矩阵）全部知道时，可以直接求解贝尔曼方程如下：<br><img src=\"/img/silver_rl_bellman_equation_solution.png\" alt=\"Bellman方程求解\"></p>\n<p>对于含有$n$个状态的系统，求解复杂度是$\\mathcal{O}(n^3)$。当$n$较大时，常用的替代的迭代求解方法有：</p>\n<ul>\n<li>动态规划 DP</li>\n<li>蒙特卡洛仿真（Monte-Carlo evaluation）</li>\n<li>时间差分学习（Temporal Difference Learning）</li>\n</ul>\n<h2 id=\"马尔科夫决策过程\"><a href=\"#马尔科夫决策过程\" class=\"headerlink\" title=\"马尔科夫决策过程\"></a>马尔科夫决策过程</h2><p>马尔科夫决策过程（MDP）是带有决策的马尔科夫奖励过程。其中有一个env（环境），其状态量满足马尔科夫性质。MDP可以用五元组$(S,A,P,R,\\gamma)$描述。其中，</p>\n<ul>\n<li>$A$是一个有限决策集合。</li>\n<li>$P_{ss^\\prime}^a = P(S_{t+1}=s^\\prime|S_t=s, A_t=a)$是状态转移概率矩阵。</li>\n<li>$R_{s}^a = E[R_{t+1}|S_t=s, A_t=a]$是奖励函数（与动作也挂钩）</li>\n</ul>\n<h3 id=\"策略\"><a href=\"#策略\" class=\"headerlink\" title=\"策略\"></a>策略</h3><p>策略（Policy）$\\pi$是指在给定状态情况下，采取动作的概率分布，如下：</p>\n<script type=\"math/tex; mode=display\">\\pi(a|s)=P(A_t=a|S_t=s)</script><p>对于一个智能体，如果策略确定了，那么它对环境的表现也就决定了。MDP的策略与历史无关，只与当前的状态有关。同时，策略是平稳过程，与时间无关。例如，无论在开局，还是终局，只要棋盘上的落子一样（也就是状态一样），那么围棋程序应该给出相同的落子动作决策。</p>\n<p>当我们给定一个MDP和相应的策略$\\pi$时，状态转移过程$S_1,S_2,\\dots$是一个马氏过程$(S, P^\\pi)$（上标$\\pi$表示$P$由$\\pi$决定）。而状态和奖励构成的过程$S_1,R_1,\\dots$是一个马氏奖赏过程$(S, P^\\pi, R^\\pi, \\gamma)$。具体来说，如下（就是全概率公式）：</p>\n<script type=\"math/tex; mode=display\">\\begin{aligned}\nP_{ss^\\prime}^\\pi &= \\sum_{a\\in A}\\pi(a|s)P_{ss^\\prime}^a\\\\\nR_s^\\pi &=\\sum_{a\\in A}\\pi(a|s)R_s^a\n\\end{aligned}</script><h3 id=\"值函数-1\"><a href=\"#值函数-1\" class=\"headerlink\" title=\"值函数\"></a>值函数</h3><p>MDP的值函数$v_\\pi(s)$是指在当前状态$s$出发，使用策略$\\pi$得到的回报期望，即，</p>\n<script type=\"math/tex; mode=display\">v_\\pi(s) = E_\\pi[G_t|S_t=s]</script><p>引入“动作-值”函数（action-value function）$q_\\pi(s,a)$，意义是从当前状态$s$出发，执行动作$a$，再使用策略$\\pi$得到的回报期望，即，</p>\n<script type=\"math/tex; mode=display\">q_\\pi(s,a) = E_\\pi[G_t|S_t=s,A_t=a]</script><h3 id=\"贝尔曼方程\"><a href=\"#贝尔曼方程\" class=\"headerlink\" title=\"贝尔曼方程\"></a>贝尔曼方程</h3><p>两者的关系如下如所示（通过全概率公式联系）：<br><img src=\"/img/silver_rl_mdp_vq_relationship.png\" alt=\"Q函数和值函数的关系\"></p>\n<p>注意上图描述的是$t$时刻的状态$s$下，$v(s)$和$q(s,a)$的关系。我们继续顺着状态链往前，可以得到下图所示$q(s,a)$和$t+1$时刻的状态$s^\\prime$的值函数$v(s^\\prime)$之间的关系。同样是一个全概率公式：<br><img src=\"/img/silver_rl_mdp_vq_relationship2.png\" alt=\"相邻时刻Q函数和值函数的关系\"></p>\n<p>综合上面两幅图中给出的关系，我们有相邻时刻值函数$v(s)$和$v(s^\\prime)$的关系：<br><img src=\"/img/silver_rl_mdp_vv_relationship.png\" alt=\"相邻时刻值函数关系\"></p>\n<p>同样，相邻时刻Q函数的关系：<br><img src=\"/img/silver_rl_mdp_qq_relationship.png\" alt=\"相邻时刻Q函数关系\"></p>\n<p>写成紧凑的矩阵形式：</p>\n<script type=\"math/tex; mode=display\">v_\\pi = R^\\pi + \\gamma P^\\pi v_\\pi</script><p>这个方程的解是：</p>\n<script type=\"math/tex; mode=display\">v_\\pi = (1-\\gamma P^\\pi)^{-1}R^\\pi</script><p>和上面对策略函数的分解类似，我们有下面两式成立：</p>\n<script type=\"math/tex; mode=display\">v_\\pi(s) = E_\\pi[R_{t+1} + \\gamma v_\\pi(sS_{t+1})]</script><h3 id=\"最优值函数\"><a href=\"#最优值函数\" class=\"headerlink\" title=\"最优值函数\"></a>最优值函数</h3><p>最优的值函数是指在所有的策略中，使得$v_\\pi(s)$取得最大值的那个，即：</p>\n<script type=\"math/tex; mode=display\">v_\\ast(s) = \\max_\\pi v_\\pi(s)</script><p>最优Q函数的定义同理：</p>\n<script type=\"math/tex; mode=display\">q_\\ast(s,a) = \\max_\\pi q_\\pi(s,a)</script><p>定义策略$\\pi$集合上的一个偏序为</p>\n<script type=\"math/tex; mode=display\">\\pi > \\pi^\\prime \\quad \\text{if} \\quad v_\\pi(s) > v_{\\pi^\\prime}(s), \\forall s</script><p>如果我们已经知道了最优的值函数，那么我们可以在每一步选取动作的时候，选取那个使得当前Q函数取得最大值的动作即可。这很straight forward，用数学语言表达就是：</p>\n<p><img src=\"/img/silver_rl_mdp_optimal_policy.png\" alt=\"最优策略的取法\"><br>同样地，对于最优值函数，也有贝尔曼递归方程成立。下面是一个形象化的推导，和上面导出贝尔曼方程的思路是一样的。<br><img src=\"/img/silver_rl_mdp_optimal_vq_relationship.png\" alt=\"\"><br><img src=\"/img/silver_rl_mdp_optimal_vq_relationship2.png\" alt=\"\"><br><img src=\"/img/silver_rl_mdp_optimal_vv_relationship.png\" alt=\"\"><br><img src=\"/img/silver_rl_mdp_optimal_qq_relationship.png\" alt=\"\"></p>\n<p>常用的求解方法包括：</p>\n<ul>\n<li>值迭代（Value Iteration）</li>\n<li>策略迭代（Policy Iteration）</li>\n<li>Q Learning</li>\n<li>Sarsa</li>\n</ul>\n","excerpt":"<p>Silver在英国UCL讲授强化学习的slide总结。背景介绍部分略去不表，第一篇首先介绍强化学习中重要的数学基础-马尔科夫决策过程（MDP）。<br><img src=\"/img/silver_rl_mdp.png\" alt=\"MDP\"><br>","more":"</p>\n<h2 id=\"马尔科夫性质\"><a href=\"#马尔科夫性质\" class=\"headerlink\" title=\"马尔科夫性质\"></a>马尔科夫性质</h2><p>不严谨地来说，马尔科夫性质是指未来与过去无关，只与当前的状态有关。我们说某个State是Markov的，等价于下面的等式成立：</p>\n<script type=\"math/tex; mode=display\">P[S_{t+1}|S_t] = P[S_{t+1}|S_1, \\dots, S_t]</script><p>定义状态转移概率（State Transition Probability）如下：</p>\n<script type=\"math/tex; mode=display\">P_{ss^\\prime} = P[S_{t+1}=s^\\prime|S_t=s]</script><p>前后两个时刻的状态不同取值的状态转移概率可以写成一个矩阵的形式。矩阵中的任意元素$P_{i,j}$表示$t$时刻状态$i$在$t+1$时刻转移到状态$j$的概率。矩阵满足行和为$1$的约束。</p>\n<p>下面，我们从马尔科夫性质展开，逐步地加入一些额外的参量，一步步引出强化学习中的马尔科夫决策过程。</p>\n<h2 id=\"马尔科夫过程\"><a href=\"#马尔科夫过程\" class=\"headerlink\" title=\"马尔科夫过程\"></a>马尔科夫过程</h2><p>马尔科夫过程（或者叫做马尔科夫链）是指随机过程中的状态满足马尔科夫性质。我们可以使用二元组$(S, P)$来描述马氏过程。其中，</p>\n<ul>\n<li>$S$是一个有限状态集合。</li>\n<li>$P$是状态转移矩阵，定义如上。</li>\n</ul>\n<h2 id=\"马尔科夫奖赏过程\"><a href=\"#马尔科夫奖赏过程\" class=\"headerlink\" title=\"马尔科夫奖赏过程\"></a>马尔科夫奖赏过程</h2><p>马尔科夫奖赏过程（不知道如何翻译，Markov reward process）在马氏过程基础上加上了状态转移过程中的奖赏reward。可以使用四元组$(S, P, R, \\gamma)$来表示。其中，</p>\n<ul>\n<li>$R$代表奖励函数，$R_s = E[R_{t+1}|S_t=s]$，是指当前状态为$s$时，下一步状态转移过程中的期望奖励。</li>\n<li>$\\gamma$是折旧率（discount），$\\gamma \\in [0,1]$</li>\n</ul>\n<p>定义回报（Return）为当前时刻往后得到的折旧总奖励，即：</p>\n<script type=\"math/tex; mode=display\">G_t = R_{t+1}+\\gamma R_{t+2}+... = \\sum_{k=0}^{\\infty}\\gamma^kR_{t+k+1}</script><p>折旧率的引入，有以下几点考虑：</p>\n<ul>\n<li>在有环存在的马氏过程中，避免了无穷大回报的出现。</li>\n<li>未来的不确定性对当前的影响较小。</li>\n<li>事实上的考虑，例如投资市场上，即时的奖励比迟滞的奖励能够有更多的利息。</li>\n<li>人类行为倾向于即时奖励。</li>\n<li>如果马氏过程是存在终止的，有的时候也可以使用$\\gamma=1$，也就是不打折。</li>\n</ul>\n<h2 id=\"值函数\"><a href=\"#值函数\" class=\"headerlink\" title=\"值函数\"></a>值函数</h2><p>值函数（Value function）的意义是以期望的形式（条件期望）给出了状态$s$的长期回报，如下：</p>\n<script type=\"math/tex; mode=display\">v(s) = E[G_t|S_t=s]</script><p>值函数可以分为两个部分，即时奖励$R_{t+1}$和后续状态的折旧值函数。如下所示：<br><img src=\"/img/silver_mdp_value_function.png\" alt=\"推导过程\"></p>\n<p>最后一步推导时，第二项的变形从直觉上推断还是比较容易的，但是还是把比较严格的推导过程写在下面：</p>\n<script type=\"math/tex; mode=display\">\\begin{aligned}\nE[G_{t+1}|S_t = s]  &= \\sum_{s^\\prime\\in S}E[G_{t+1}|S_{t+1}=s^\\prime]P(S_{t+1}=s^\\prime|S_t=s)\\\\\n&=\\sum_{s^\\prime}v(S_{t+1}=s^\\prime)P(S_{t+1}=s^\\prime|S_t=s)\\\\\n&=E[v(S_{t+1})|S_t=s]\n\\end{aligned}</script><p>上面的结论就是贝尔曼方程，它给出了计算值函数的递归公式。如下图所示，状态节点$s$处的值函数可以分为两个部分，分别是转换到状态$s^\\prime$过程中收到的奖励$r$，和从新的状态$s^\\prime$出发，得到的值函数。我们想要知道$t$时刻某个状态$s$的值函数，只需要从后向前遍历，递归地去计算。<br><img src=\"/img/silver_rl_bellman_equation_figure.png\" alt=\"值函数的递归过程可视化\"></p>\n<p>把上面形式求期望的过程展开，可以得到下面的等价形式（更像上面补充的证明过程的思路）。其中，后面一项就是状态转移构成的树结构中以当前状态节点$s$为父节点的所有子节点的值函数，用转移概率进行加权。这个比上式更为直观。</p>\n<script type=\"math/tex; mode=display\">v(s) = R_s + \\gamma\\sum_{s^\\prime \\in S}P_{ss^\\prime}v(s^\\prime)</script><p>或者写成下面的矩阵形式，更加紧凑：</p>\n<script type=\"math/tex; mode=display\">v = R+\\gamma Pv</script><p><img src=\"/img/silver_rl_bellman_equation_matrix.png\" alt=\"bellman方程的矩阵形式\"></p>\n<p>当我们对系统模型（包括奖励函数和概率转换矩阵）全部知道时，可以直接求解贝尔曼方程如下：<br><img src=\"/img/silver_rl_bellman_equation_solution.png\" alt=\"Bellman方程求解\"></p>\n<p>对于含有$n$个状态的系统，求解复杂度是$\\mathcal{O}(n^3)$。当$n$较大时，常用的替代的迭代求解方法有：</p>\n<ul>\n<li>动态规划 DP</li>\n<li>蒙特卡洛仿真（Monte-Carlo evaluation）</li>\n<li>时间差分学习（Temporal Difference Learning）</li>\n</ul>\n<h2 id=\"马尔科夫决策过程\"><a href=\"#马尔科夫决策过程\" class=\"headerlink\" title=\"马尔科夫决策过程\"></a>马尔科夫决策过程</h2><p>马尔科夫决策过程（MDP）是带有决策的马尔科夫奖励过程。其中有一个env（环境），其状态量满足马尔科夫性质。MDP可以用五元组$(S,A,P,R,\\gamma)$描述。其中，</p>\n<ul>\n<li>$A$是一个有限决策集合。</li>\n<li>$P_{ss^\\prime}^a = P(S_{t+1}=s^\\prime|S_t=s, A_t=a)$是状态转移概率矩阵。</li>\n<li>$R_{s}^a = E[R_{t+1}|S_t=s, A_t=a]$是奖励函数（与动作也挂钩）</li>\n</ul>\n<h3 id=\"策略\"><a href=\"#策略\" class=\"headerlink\" title=\"策略\"></a>策略</h3><p>策略（Policy）$\\pi$是指在给定状态情况下，采取动作的概率分布，如下：</p>\n<script type=\"math/tex; mode=display\">\\pi(a|s)=P(A_t=a|S_t=s)</script><p>对于一个智能体，如果策略确定了，那么它对环境的表现也就决定了。MDP的策略与历史无关，只与当前的状态有关。同时，策略是平稳过程，与时间无关。例如，无论在开局，还是终局，只要棋盘上的落子一样（也就是状态一样），那么围棋程序应该给出相同的落子动作决策。</p>\n<p>当我们给定一个MDP和相应的策略$\\pi$时，状态转移过程$S_1,S_2,\\dots$是一个马氏过程$(S, P^\\pi)$（上标$\\pi$表示$P$由$\\pi$决定）。而状态和奖励构成的过程$S_1,R_1,\\dots$是一个马氏奖赏过程$(S, P^\\pi, R^\\pi, \\gamma)$。具体来说，如下（就是全概率公式）：</p>\n<script type=\"math/tex; mode=display\">\\begin{aligned}\nP_{ss^\\prime}^\\pi &= \\sum_{a\\in A}\\pi(a|s)P_{ss^\\prime}^a\\\\\nR_s^\\pi &=\\sum_{a\\in A}\\pi(a|s)R_s^a\n\\end{aligned}</script><h3 id=\"值函数-1\"><a href=\"#值函数-1\" class=\"headerlink\" title=\"值函数\"></a>值函数</h3><p>MDP的值函数$v_\\pi(s)$是指在当前状态$s$出发，使用策略$\\pi$得到的回报期望，即，</p>\n<script type=\"math/tex; mode=display\">v_\\pi(s) = E_\\pi[G_t|S_t=s]</script><p>引入“动作-值”函数（action-value function）$q_\\pi(s,a)$，意义是从当前状态$s$出发，执行动作$a$，再使用策略$\\pi$得到的回报期望，即，</p>\n<script type=\"math/tex; mode=display\">q_\\pi(s,a) = E_\\pi[G_t|S_t=s,A_t=a]</script><h3 id=\"贝尔曼方程\"><a href=\"#贝尔曼方程\" class=\"headerlink\" title=\"贝尔曼方程\"></a>贝尔曼方程</h3><p>两者的关系如下如所示（通过全概率公式联系）：<br><img src=\"/img/silver_rl_mdp_vq_relationship.png\" alt=\"Q函数和值函数的关系\"></p>\n<p>注意上图描述的是$t$时刻的状态$s$下，$v(s)$和$q(s,a)$的关系。我们继续顺着状态链往前，可以得到下图所示$q(s,a)$和$t+1$时刻的状态$s^\\prime$的值函数$v(s^\\prime)$之间的关系。同样是一个全概率公式：<br><img src=\"/img/silver_rl_mdp_vq_relationship2.png\" alt=\"相邻时刻Q函数和值函数的关系\"></p>\n<p>综合上面两幅图中给出的关系，我们有相邻时刻值函数$v(s)$和$v(s^\\prime)$的关系：<br><img src=\"/img/silver_rl_mdp_vv_relationship.png\" alt=\"相邻时刻值函数关系\"></p>\n<p>同样，相邻时刻Q函数的关系：<br><img src=\"/img/silver_rl_mdp_qq_relationship.png\" alt=\"相邻时刻Q函数关系\"></p>\n<p>写成紧凑的矩阵形式：</p>\n<script type=\"math/tex; mode=display\">v_\\pi = R^\\pi + \\gamma P^\\pi v_\\pi</script><p>这个方程的解是：</p>\n<script type=\"math/tex; mode=display\">v_\\pi = (1-\\gamma P^\\pi)^{-1}R^\\pi</script><p>和上面对策略函数的分解类似，我们有下面两式成立：</p>\n<script type=\"math/tex; mode=display\">v_\\pi(s) = E_\\pi[R_{t+1} + \\gamma v_\\pi(sS_{t+1})]</script><h3 id=\"最优值函数\"><a href=\"#最优值函数\" class=\"headerlink\" title=\"最优值函数\"></a>最优值函数</h3><p>最优的值函数是指在所有的策略中，使得$v_\\pi(s)$取得最大值的那个，即：</p>\n<script type=\"math/tex; mode=display\">v_\\ast(s) = \\max_\\pi v_\\pi(s)</script><p>最优Q函数的定义同理：</p>\n<script type=\"math/tex; mode=display\">q_\\ast(s,a) = \\max_\\pi q_\\pi(s,a)</script><p>定义策略$\\pi$集合上的一个偏序为</p>\n<script type=\"math/tex; mode=display\">\\pi > \\pi^\\prime \\quad \\text{if} \\quad v_\\pi(s) > v_{\\pi^\\prime}(s), \\forall s</script><p>如果我们已经知道了最优的值函数，那么我们可以在每一步选取动作的时候，选取那个使得当前Q函数取得最大值的动作即可。这很straight forward，用数学语言表达就是：</p>\n<p><img src=\"/img/silver_rl_mdp_optimal_policy.png\" alt=\"最优策略的取法\"><br>同样地，对于最优值函数，也有贝尔曼递归方程成立。下面是一个形象化的推导，和上面导出贝尔曼方程的思路是一样的。<br><img src=\"/img/silver_rl_mdp_optimal_vq_relationship.png\" alt=\"\"><br><img src=\"/img/silver_rl_mdp_optimal_vq_relationship2.png\" alt=\"\"><br><img src=\"/img/silver_rl_mdp_optimal_vv_relationship.png\" alt=\"\"><br><img src=\"/img/silver_rl_mdp_optimal_qq_relationship.png\" alt=\"\"></p>\n<p>常用的求解方法包括：</p>\n<ul>\n<li>值迭代（Value Iteration）</li>\n<li>策略迭代（Policy Iteration）</li>\n<li>Q Learning</li>\n<li>Sarsa</li>\n</ul>"},{"title":"Ubuntu Cannot Mount exfat格式硬盘的解决办法","date":"2017-05-04T10:36:29.000Z","_content":"我的移动硬盘为东芝1TB容量，为了能够在Windows和MacOS下使用，我将其格式化为exfat格式。然而我发现这样一来，在Ubuntu14.04下不能挂载。虽然可见盘符，但是却提示`unable to mount`。这篇文章是对解决办法的记录。\n![Ubuntu Exfat](/img/ubuntu_exfat.png)\n\n<!-- more -->\n\n## 解决方法\n参见[页面](https://askubuntu.com/questions/531919/ubuntu-14-04-cant-mount-exfat-external-hard-disk)，运行以下命令：\n\n```\n$ sudo -i  # 获取root权限\n# apt-get update\n# apt-get install --reinstall exfat-fuse exfat-utils\n# mkdir -p /media/user/exfat\n# chmod -Rf 777 /media/user/exfat\n# fdisk -l\n```\n\n之后我发现直接点击盘符的挂载即可，而无需使用他的后续命令。\n\n在弹出驱动器的时候，会出现虽然顺利弹出，但是马上（大概3s），移动硬盘又被读取的情况。所以只能利用间隙，很快地将硬盘取下。不知道会不会有什么损害。所以如果方便的话，还是格式为NTFS格式，再花一些大洋去买Mac上读写NTFS格式硬盘的软件工具吧。。。\n","source":"_posts/ubuntu-cannot-mount-exfat-disk.md","raw":"---\ntitle: Ubuntu Cannot Mount exfat格式硬盘的解决办法\ndate: 2017-05-04 18:36:29\ntags:\n    - tool\n---\n我的移动硬盘为东芝1TB容量，为了能够在Windows和MacOS下使用，我将其格式化为exfat格式。然而我发现这样一来，在Ubuntu14.04下不能挂载。虽然可见盘符，但是却提示`unable to mount`。这篇文章是对解决办法的记录。\n![Ubuntu Exfat](/img/ubuntu_exfat.png)\n\n<!-- more -->\n\n## 解决方法\n参见[页面](https://askubuntu.com/questions/531919/ubuntu-14-04-cant-mount-exfat-external-hard-disk)，运行以下命令：\n\n```\n$ sudo -i  # 获取root权限\n# apt-get update\n# apt-get install --reinstall exfat-fuse exfat-utils\n# mkdir -p /media/user/exfat\n# chmod -Rf 777 /media/user/exfat\n# fdisk -l\n```\n\n之后我发现直接点击盘符的挂载即可，而无需使用他的后续命令。\n\n在弹出驱动器的时候，会出现虽然顺利弹出，但是马上（大概3s），移动硬盘又被读取的情况。所以只能利用间隙，很快地将硬盘取下。不知道会不会有什么损害。所以如果方便的话，还是格式为NTFS格式，再花一些大洋去买Mac上读写NTFS格式硬盘的软件工具吧。。。\n","slug":"ubuntu-cannot-mount-exfat-disk","published":1,"updated":"2018-01-12T06:22:20.482Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjcu6vcug002iqu46ylakbwvg","content":"<p>我的移动硬盘为东芝1TB容量，为了能够在Windows和MacOS下使用，我将其格式化为exfat格式。然而我发现这样一来，在Ubuntu14.04下不能挂载。虽然可见盘符，但是却提示<code>unable to mount</code>。这篇文章是对解决办法的记录。<br><img src=\"/img/ubuntu_exfat.png\" alt=\"Ubuntu Exfat\"></p>\n<a id=\"more\"></a>\n<h2 id=\"解决方法\"><a href=\"#解决方法\" class=\"headerlink\" title=\"解决方法\"></a>解决方法</h2><p>参见<a href=\"https://askubuntu.com/questions/531919/ubuntu-14-04-cant-mount-exfat-external-hard-disk\" target=\"_blank\" rel=\"external\">页面</a>，运行以下命令：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div></pre></td><td class=\"code\"><pre><div class=\"line\">$ sudo -i  # 获取root权限</div><div class=\"line\"># apt-get update</div><div class=\"line\"># apt-get install --reinstall exfat-fuse exfat-utils</div><div class=\"line\"># mkdir -p /media/user/exfat</div><div class=\"line\"># chmod -Rf 777 /media/user/exfat</div><div class=\"line\"># fdisk -l</div></pre></td></tr></table></figure>\n<p>之后我发现直接点击盘符的挂载即可，而无需使用他的后续命令。</p>\n<p>在弹出驱动器的时候，会出现虽然顺利弹出，但是马上（大概3s），移动硬盘又被读取的情况。所以只能利用间隙，很快地将硬盘取下。不知道会不会有什么损害。所以如果方便的话，还是格式为NTFS格式，再花一些大洋去买Mac上读写NTFS格式硬盘的软件工具吧。。。</p>\n","excerpt":"<p>我的移动硬盘为东芝1TB容量，为了能够在Windows和MacOS下使用，我将其格式化为exfat格式。然而我发现这样一来，在Ubuntu14.04下不能挂载。虽然可见盘符，但是却提示<code>unable to mount</code>。这篇文章是对解决办法的记录。<br><img src=\"/img/ubuntu_exfat.png\" alt=\"Ubuntu Exfat\"></p>","more":"<h2 id=\"解决方法\"><a href=\"#解决方法\" class=\"headerlink\" title=\"解决方法\"></a>解决方法</h2><p>参见<a href=\"https://askubuntu.com/questions/531919/ubuntu-14-04-cant-mount-exfat-external-hard-disk\">页面</a>，运行以下命令：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div></pre></td><td class=\"code\"><pre><div class=\"line\">$ sudo -i  # 获取root权限</div><div class=\"line\"># apt-get update</div><div class=\"line\"># apt-get install --reinstall exfat-fuse exfat-utils</div><div class=\"line\"># mkdir -p /media/user/exfat</div><div class=\"line\"># chmod -Rf 777 /media/user/exfat</div><div class=\"line\"># fdisk -l</div></pre></td></tr></table></figure>\n<p>之后我发现直接点击盘符的挂载即可，而无需使用他的后续命令。</p>\n<p>在弹出驱动器的时候，会出现虽然顺利弹出，但是马上（大概3s），移动硬盘又被读取的情况。所以只能利用间隙，很快地将硬盘取下。不知道会不会有什么损害。所以如果方便的话，还是格式为NTFS格式，再花一些大洋去买Mac上读写NTFS格式硬盘的软件工具吧。。。</p>"},{"title":"Windows环境下使用Doxygen生成注释文档","date":"2016-12-16T11:00:00.000Z","_content":"\nDoxygen 是一种很好用的代码注释生成工具，然而和很多国外的工具软件一样，在中文环境下，它的使用总是会出现一些问题，也就是中文注释文档出现乱码。经过调试，终于是解决了这个问题。\n\n![Doxygen](/img/doxygen_picture.png)\n<!-- more -->\n## 安装 Doxygen\n\nDoxygen 在Windows平台下的安装比较简单，[Doxygen的项目主页](http://www.doxygen.nl/)提供了下载和安装的使用说明，可以下载它们的官方使用手册进行阅读。对于Windows，提供了源代码编译安装和直接安装程序安装两种方式，可以自行选择。\n\n安装成功后，使用命令行命令\n\n``` bash\ndoxygen --help\n```\n\n就可以查看帮助文档，对应参数含义一目了然，降低了入手难度。\n\n使用命令，\n\n\n``` bash\ndoxygen -g doxygen_filename\n```\n\n就可以在当前目录下建立一个doxygen配置文件，用文本编辑器打开就可以编辑里面的配置选项。\n\n使用命令，\n\n``` bash\ndoxygen doxygen_filename\n```\n\n就可以生成注释文档了。\n\n下面就来说一说对中文的支持。\n\n## 生成 HTML 格式文档\n\n中文之所以乱码，很多时候是由于编码和译码格式不同，所以我们需要先知道自己代码文件的编码方式。我的代码都是建立在Visual Studio上的，可以通过VS的高级保存选项查看自己代码文件的存储编码格式。对于中文版的VS，一般应该是GB2312。\n\n我们打开 Doxygen 的配置文件，将里面的 INPUT_ENCODING 改为我们代码文件的编码格式，这里就改成 GB2312。\n\n这样一来，编译出来的 HTML 页面就不会有中文乱码了。\n\n## 生成Latex 格式文档\n\n生成 Latex 需要本机上安装有 Latex 的编译环境。如果是中文用户，推荐的是CTEX套件，可以到他们的网站上去下载。\n\n可以看到，Doxygen为Latex文件的编译生成了make文件，我们在命令行窗口中执行make命令就可以完成编译，然而这时候会发现编译出错，pdf文档无法生成。\n\n打开生成的refman.latex文档，添加宏包 CJKutf8。然后找到 `\\begin{document}`一行，将其改为\n\n``` latex\n\\begin{document}\n\\begin{CJK}{UTF8}{gbsn}\n```\n\n也就是说为正文提供了CJK环境，这样中文文本就可以正常编译了。\n\n相应的，我们要将结尾的 `\\end{document)`改为：\n``` latex\n\\end{CJK}\n\\end{document}\n```\n\n这样，运行make命令之后，就可以看到中文的注释文档了。\n","source":"_posts/use-doxygen.md","raw":"---\ntitle: Windows环境下使用Doxygen生成注释文档\ndate: 2016-12-16 19:00:00\ntags:\n    - tool\n    - doxygen\n---\n\nDoxygen 是一种很好用的代码注释生成工具，然而和很多国外的工具软件一样，在中文环境下，它的使用总是会出现一些问题，也就是中文注释文档出现乱码。经过调试，终于是解决了这个问题。\n\n![Doxygen](/img/doxygen_picture.png)\n<!-- more -->\n## 安装 Doxygen\n\nDoxygen 在Windows平台下的安装比较简单，[Doxygen的项目主页](http://www.doxygen.nl/)提供了下载和安装的使用说明，可以下载它们的官方使用手册进行阅读。对于Windows，提供了源代码编译安装和直接安装程序安装两种方式，可以自行选择。\n\n安装成功后，使用命令行命令\n\n``` bash\ndoxygen --help\n```\n\n就可以查看帮助文档，对应参数含义一目了然，降低了入手难度。\n\n使用命令，\n\n\n``` bash\ndoxygen -g doxygen_filename\n```\n\n就可以在当前目录下建立一个doxygen配置文件，用文本编辑器打开就可以编辑里面的配置选项。\n\n使用命令，\n\n``` bash\ndoxygen doxygen_filename\n```\n\n就可以生成注释文档了。\n\n下面就来说一说对中文的支持。\n\n## 生成 HTML 格式文档\n\n中文之所以乱码，很多时候是由于编码和译码格式不同，所以我们需要先知道自己代码文件的编码方式。我的代码都是建立在Visual Studio上的，可以通过VS的高级保存选项查看自己代码文件的存储编码格式。对于中文版的VS，一般应该是GB2312。\n\n我们打开 Doxygen 的配置文件，将里面的 INPUT_ENCODING 改为我们代码文件的编码格式，这里就改成 GB2312。\n\n这样一来，编译出来的 HTML 页面就不会有中文乱码了。\n\n## 生成Latex 格式文档\n\n生成 Latex 需要本机上安装有 Latex 的编译环境。如果是中文用户，推荐的是CTEX套件，可以到他们的网站上去下载。\n\n可以看到，Doxygen为Latex文件的编译生成了make文件，我们在命令行窗口中执行make命令就可以完成编译，然而这时候会发现编译出错，pdf文档无法生成。\n\n打开生成的refman.latex文档，添加宏包 CJKutf8。然后找到 `\\begin{document}`一行，将其改为\n\n``` latex\n\\begin{document}\n\\begin{CJK}{UTF8}{gbsn}\n```\n\n也就是说为正文提供了CJK环境，这样中文文本就可以正常编译了。\n\n相应的，我们要将结尾的 `\\end{document)`改为：\n``` latex\n\\end{CJK}\n\\end{document}\n```\n\n这样，运行make命令之后，就可以看到中文的注释文档了。\n","slug":"use-doxygen","published":1,"updated":"2018-01-12T06:22:20.482Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjcu6vcui002kqu4623ztmsf6","content":"<p>Doxygen 是一种很好用的代码注释生成工具，然而和很多国外的工具软件一样，在中文环境下，它的使用总是会出现一些问题，也就是中文注释文档出现乱码。经过调试，终于是解决了这个问题。</p>\n<p><img src=\"/img/doxygen_picture.png\" alt=\"Doxygen\"><br><a id=\"more\"></a></p>\n<h2 id=\"安装-Doxygen\"><a href=\"#安装-Doxygen\" class=\"headerlink\" title=\"安装 Doxygen\"></a>安装 Doxygen</h2><p>Doxygen 在Windows平台下的安装比较简单，<a href=\"http://www.doxygen.nl/\" target=\"_blank\" rel=\"external\">Doxygen的项目主页</a>提供了下载和安装的使用说明，可以下载它们的官方使用手册进行阅读。对于Windows，提供了源代码编译安装和直接安装程序安装两种方式，可以自行选择。</p>\n<p>安装成功后，使用命令行命令</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">doxygen --help</div></pre></td></tr></table></figure>\n<p>就可以查看帮助文档，对应参数含义一目了然，降低了入手难度。</p>\n<p>使用命令，</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">doxygen -g doxygen_filename</div></pre></td></tr></table></figure>\n<p>就可以在当前目录下建立一个doxygen配置文件，用文本编辑器打开就可以编辑里面的配置选项。</p>\n<p>使用命令，</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">doxygen doxygen_filename</div></pre></td></tr></table></figure>\n<p>就可以生成注释文档了。</p>\n<p>下面就来说一说对中文的支持。</p>\n<h2 id=\"生成-HTML-格式文档\"><a href=\"#生成-HTML-格式文档\" class=\"headerlink\" title=\"生成 HTML 格式文档\"></a>生成 HTML 格式文档</h2><p>中文之所以乱码，很多时候是由于编码和译码格式不同，所以我们需要先知道自己代码文件的编码方式。我的代码都是建立在Visual Studio上的，可以通过VS的高级保存选项查看自己代码文件的存储编码格式。对于中文版的VS，一般应该是GB2312。</p>\n<p>我们打开 Doxygen 的配置文件，将里面的 INPUT_ENCODING 改为我们代码文件的编码格式，这里就改成 GB2312。</p>\n<p>这样一来，编译出来的 HTML 页面就不会有中文乱码了。</p>\n<h2 id=\"生成Latex-格式文档\"><a href=\"#生成Latex-格式文档\" class=\"headerlink\" title=\"生成Latex 格式文档\"></a>生成Latex 格式文档</h2><p>生成 Latex 需要本机上安装有 Latex 的编译环境。如果是中文用户，推荐的是CTEX套件，可以到他们的网站上去下载。</p>\n<p>可以看到，Doxygen为Latex文件的编译生成了make文件，我们在命令行窗口中执行make命令就可以完成编译，然而这时候会发现编译出错，pdf文档无法生成。</p>\n<p>打开生成的refman.latex文档，添加宏包 CJKutf8。然后找到 <code>\\begin{document}</code>一行，将其改为</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">\\begin&#123;document&#125;</div><div class=\"line\">\\begin&#123;CJK&#125;&#123;UTF8&#125;&#123;gbsn&#125;</div></pre></td></tr></table></figure>\n<p>也就是说为正文提供了CJK环境，这样中文文本就可以正常编译了。</p>\n<p>相应的，我们要将结尾的 <code>\\end{document)</code>改为：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">\\end&#123;CJK&#125;</div><div class=\"line\">\\end&#123;document&#125;</div></pre></td></tr></table></figure></p>\n<p>这样，运行make命令之后，就可以看到中文的注释文档了。</p>\n","excerpt":"<p>Doxygen 是一种很好用的代码注释生成工具，然而和很多国外的工具软件一样，在中文环境下，它的使用总是会出现一些问题，也就是中文注释文档出现乱码。经过调试，终于是解决了这个问题。</p>\n<p><img src=\"/img/doxygen_picture.png\" alt=\"Doxygen\"><br>","more":"</p>\n<h2 id=\"安装-Doxygen\"><a href=\"#安装-Doxygen\" class=\"headerlink\" title=\"安装 Doxygen\"></a>安装 Doxygen</h2><p>Doxygen 在Windows平台下的安装比较简单，<a href=\"http://www.doxygen.nl/\">Doxygen的项目主页</a>提供了下载和安装的使用说明，可以下载它们的官方使用手册进行阅读。对于Windows，提供了源代码编译安装和直接安装程序安装两种方式，可以自行选择。</p>\n<p>安装成功后，使用命令行命令</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">doxygen --help</div></pre></td></tr></table></figure>\n<p>就可以查看帮助文档，对应参数含义一目了然，降低了入手难度。</p>\n<p>使用命令，</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">doxygen -g doxygen_filename</div></pre></td></tr></table></figure>\n<p>就可以在当前目录下建立一个doxygen配置文件，用文本编辑器打开就可以编辑里面的配置选项。</p>\n<p>使用命令，</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">doxygen doxygen_filename</div></pre></td></tr></table></figure>\n<p>就可以生成注释文档了。</p>\n<p>下面就来说一说对中文的支持。</p>\n<h2 id=\"生成-HTML-格式文档\"><a href=\"#生成-HTML-格式文档\" class=\"headerlink\" title=\"生成 HTML 格式文档\"></a>生成 HTML 格式文档</h2><p>中文之所以乱码，很多时候是由于编码和译码格式不同，所以我们需要先知道自己代码文件的编码方式。我的代码都是建立在Visual Studio上的，可以通过VS的高级保存选项查看自己代码文件的存储编码格式。对于中文版的VS，一般应该是GB2312。</p>\n<p>我们打开 Doxygen 的配置文件，将里面的 INPUT_ENCODING 改为我们代码文件的编码格式，这里就改成 GB2312。</p>\n<p>这样一来，编译出来的 HTML 页面就不会有中文乱码了。</p>\n<h2 id=\"生成Latex-格式文档\"><a href=\"#生成Latex-格式文档\" class=\"headerlink\" title=\"生成Latex 格式文档\"></a>生成Latex 格式文档</h2><p>生成 Latex 需要本机上安装有 Latex 的编译环境。如果是中文用户，推荐的是CTEX套件，可以到他们的网站上去下载。</p>\n<p>可以看到，Doxygen为Latex文件的编译生成了make文件，我们在命令行窗口中执行make命令就可以完成编译，然而这时候会发现编译出错，pdf文档无法生成。</p>\n<p>打开生成的refman.latex文档，添加宏包 CJKutf8。然后找到 <code>\\begin{document}</code>一行，将其改为</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">\\begin&#123;document&#125;</div><div class=\"line\">\\begin&#123;CJK&#125;&#123;UTF8&#125;&#123;gbsn&#125;</div></pre></td></tr></table></figure>\n<p>也就是说为正文提供了CJK环境，这样中文文本就可以正常编译了。</p>\n<p>相应的，我们要将结尾的 <code>\\end{document)</code>改为：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">\\end&#123;CJK&#125;</div><div class=\"line\">\\end&#123;document&#125;</div></pre></td></tr></table></figure></p>\n<p>这样，运行make命令之后，就可以看到中文的注释文档了。</p>"},{"title":"Ubuntu/Mac 工具软件列表","date":"2017-10-22T12:12:09.000Z","_content":"工作环境大部分在Ubuntu和MacOS上进行，这里是一个我觉得在这两个平台上很有用的工具的整理列表，大部分都可以在两个系统上找到。这里并不会给出具体安装方式，因为最好的文档总是软件的官方Document或者GitHub的README。\n<!-- more -->\n\n## zsh和Oh-my-zsh\n\n如果经常在终端敲命令而且还在用系统自带的Bash？可以考虑试一下zsh替代bash，并使用[oh-my-zsh](https://github.com/robbyrussell/oh-my-zsh)武装zsh。\n\n关于oh-my-zsh的帖子网上已经有很多，不过我还并没有用到太多的功能。oh-my-zsh中可以配置插件，不过我只是使用了`colored-man-pages`。顾名思义，它可以将使用`man`查询时的页面彩色输出。如下所示。\n![彩色的cp man页面](/img/useful_tools_colored_man_pages.jpg)\n\n## autojump\n\n使用[autojump](https://github.com/wting/autojump)，可以很方便地在已经访问过的文件夹间快速跳转。甚至都不需要输入目标文件夹的全名，支持自动联想。\n\n除了自动跳转功能，我还将其作为终端到文件资源管理器(Mac: Finder)的跳转功能。\n```\n# 跳转到path并使用文件资源管理器打开\njo path\n```\n\n## tldr\n\n[tldr](https://github.com/tldr-pages/tldr) (too long don't read)是一款能够给出bash命令常用功能的工具。在Linux系统中，很多命令都有一长串参数。这其中很多都是不常用的。而我们使用时，常常是使用某几个常见的功能选项。tldr就能够给出命令的简要描述和例子。\n\n例如，使用其查询`tar`的常用方法：\n\n``` bash\ntldr tar\n# output\ntar\n\nArchiving utility.\nOften combined with a compression method, such as gzip or bzip.\n\n- Create an archive from files:\n    tar cf target.tar file1 file2 file3\n\n- Create a gzipped archive:\n    tar czf target.tar.gz file1 file2 file3\n\n- Extract an archive in a target folder:\n    tar xf source.tar -C folder\n\n- Extract a gzipped archive in the current directory:\n    tar xzf source.tar.gz\n\n- Extract a bzipped archive in the current directory:\n    tar xjf source.tar.bz2\n\n- Create a compressed archive, using archive suffix to determine the compression program:\n    tar caf target.tar.xz file1 file2 file3\n\n- List the contents of a tar file:\n    tar tvf source.tar\n```\n\ntldr支持多种语言，我使用了python包安装。但是不知为何，tldr在我这里总显示奇怪的背景颜色，看上去很别扭。所以我实际使用的是[tldr-py](https://github.com/lord63/tldr.py)。\n\n## tmux\n\n用SSH登录到服务器上时，如果网络连接不稳定或是自己的主机意外断电，会造成正在跑的代码死掉。因为进程是依附于SSH的会话Session的。tmux是一个终端的“分线器”，可以很方便地将正在进行的终端会话detach掉，使其转入后台运行。正是有这一特点，所以我们可以在SSH会话时，新建tmux会话，在其中跑一些耗时很长的代码，而不必担心SSH掉线。当然，也可以将tmux作为一款终端多任务的管理软件，方便地在多个任务中进行跳转。不过这个功能，我更加常用的是下面的guake。\n\n虽然Ubuntu14.04可以通过`apt-get`的方式安装tmux，不过为了能够使用一款好用的配置[oh-my-tmux](https://github.com/gpakosz/.tmux)（要求tmux>=2.1），还是推荐去GitHub上自己编译安装[tmux](https://github.com/tmux/tmux)。\n\n## guake\n\n[guake](https://github.com/Guake/guake)是一款Ubuntu上可以方便呼出终端的应用（按下F12，终端将以全屏的方式铺满桌面，F11可以切换全屏或半屏）。\n\n## Dash/Zeal\n\nDash是Mac上一款用于查询API文档的软件。在Ubuntu或Windows上，我们可以使用Zeal这个替代软件。Zeal和Dash基本上无缝衔接，但是却是免费的（Mac上的软件真是好贵。。。）。之前我已经写过一篇[博客](https://xmfbit.github.io/2017/08/26/doc2dash-usage/)，介绍如何自己制作文档导入Zeal中。\n\n## sshfs\n\n使用sshfs可以在本地机器上挂载远程服务器某个文件夹。这样，操作本地的该文件夹就相当于操作远程服务器上的该文件夹（小心使用`rm`）。\n\n## Alfred/Mutate\n\nAlfred是Mac上一款非常好用的软件，就像蝙蝠侠身边的老管家一样，可以帮你自动化处理很多事情。除了原生\n功能，还可以自己编写脚本实现扩展。例如查询豆瓣电影，查询ip，计算器等。鉴于这款软件的大名，这里不再多说。\n\n[Mutate](https://github.com/qdore/Mutate)是Ubuntu上的一款替代软件。同时，它也提供了方便的扩展接口，只需要按照模板编写python/shell代码，可以很方便地将自己的自动化处理功能加入软件中。\n","source":"_posts/useful-tools-list.md","raw":"---\ntitle: Ubuntu/Mac 工具软件列表\ndate: 2017-10-22 20:12:09\ntags:\n---\n工作环境大部分在Ubuntu和MacOS上进行，这里是一个我觉得在这两个平台上很有用的工具的整理列表，大部分都可以在两个系统上找到。这里并不会给出具体安装方式，因为最好的文档总是软件的官方Document或者GitHub的README。\n<!-- more -->\n\n## zsh和Oh-my-zsh\n\n如果经常在终端敲命令而且还在用系统自带的Bash？可以考虑试一下zsh替代bash，并使用[oh-my-zsh](https://github.com/robbyrussell/oh-my-zsh)武装zsh。\n\n关于oh-my-zsh的帖子网上已经有很多，不过我还并没有用到太多的功能。oh-my-zsh中可以配置插件，不过我只是使用了`colored-man-pages`。顾名思义，它可以将使用`man`查询时的页面彩色输出。如下所示。\n![彩色的cp man页面](/img/useful_tools_colored_man_pages.jpg)\n\n## autojump\n\n使用[autojump](https://github.com/wting/autojump)，可以很方便地在已经访问过的文件夹间快速跳转。甚至都不需要输入目标文件夹的全名，支持自动联想。\n\n除了自动跳转功能，我还将其作为终端到文件资源管理器(Mac: Finder)的跳转功能。\n```\n# 跳转到path并使用文件资源管理器打开\njo path\n```\n\n## tldr\n\n[tldr](https://github.com/tldr-pages/tldr) (too long don't read)是一款能够给出bash命令常用功能的工具。在Linux系统中，很多命令都有一长串参数。这其中很多都是不常用的。而我们使用时，常常是使用某几个常见的功能选项。tldr就能够给出命令的简要描述和例子。\n\n例如，使用其查询`tar`的常用方法：\n\n``` bash\ntldr tar\n# output\ntar\n\nArchiving utility.\nOften combined with a compression method, such as gzip or bzip.\n\n- Create an archive from files:\n    tar cf target.tar file1 file2 file3\n\n- Create a gzipped archive:\n    tar czf target.tar.gz file1 file2 file3\n\n- Extract an archive in a target folder:\n    tar xf source.tar -C folder\n\n- Extract a gzipped archive in the current directory:\n    tar xzf source.tar.gz\n\n- Extract a bzipped archive in the current directory:\n    tar xjf source.tar.bz2\n\n- Create a compressed archive, using archive suffix to determine the compression program:\n    tar caf target.tar.xz file1 file2 file3\n\n- List the contents of a tar file:\n    tar tvf source.tar\n```\n\ntldr支持多种语言，我使用了python包安装。但是不知为何，tldr在我这里总显示奇怪的背景颜色，看上去很别扭。所以我实际使用的是[tldr-py](https://github.com/lord63/tldr.py)。\n\n## tmux\n\n用SSH登录到服务器上时，如果网络连接不稳定或是自己的主机意外断电，会造成正在跑的代码死掉。因为进程是依附于SSH的会话Session的。tmux是一个终端的“分线器”，可以很方便地将正在进行的终端会话detach掉，使其转入后台运行。正是有这一特点，所以我们可以在SSH会话时，新建tmux会话，在其中跑一些耗时很长的代码，而不必担心SSH掉线。当然，也可以将tmux作为一款终端多任务的管理软件，方便地在多个任务中进行跳转。不过这个功能，我更加常用的是下面的guake。\n\n虽然Ubuntu14.04可以通过`apt-get`的方式安装tmux，不过为了能够使用一款好用的配置[oh-my-tmux](https://github.com/gpakosz/.tmux)（要求tmux>=2.1），还是推荐去GitHub上自己编译安装[tmux](https://github.com/tmux/tmux)。\n\n## guake\n\n[guake](https://github.com/Guake/guake)是一款Ubuntu上可以方便呼出终端的应用（按下F12，终端将以全屏的方式铺满桌面，F11可以切换全屏或半屏）。\n\n## Dash/Zeal\n\nDash是Mac上一款用于查询API文档的软件。在Ubuntu或Windows上，我们可以使用Zeal这个替代软件。Zeal和Dash基本上无缝衔接，但是却是免费的（Mac上的软件真是好贵。。。）。之前我已经写过一篇[博客](https://xmfbit.github.io/2017/08/26/doc2dash-usage/)，介绍如何自己制作文档导入Zeal中。\n\n## sshfs\n\n使用sshfs可以在本地机器上挂载远程服务器某个文件夹。这样，操作本地的该文件夹就相当于操作远程服务器上的该文件夹（小心使用`rm`）。\n\n## Alfred/Mutate\n\nAlfred是Mac上一款非常好用的软件，就像蝙蝠侠身边的老管家一样，可以帮你自动化处理很多事情。除了原生\n功能，还可以自己编写脚本实现扩展。例如查询豆瓣电影，查询ip，计算器等。鉴于这款软件的大名，这里不再多说。\n\n[Mutate](https://github.com/qdore/Mutate)是Ubuntu上的一款替代软件。同时，它也提供了方便的扩展接口，只需要按照模板编写python/shell代码，可以很方便地将自己的自动化处理功能加入软件中。\n","slug":"useful-tools-list","published":1,"updated":"2018-01-12T06:22:20.483Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjcu6vcuk002nqu46im291vz0","content":"<p>工作环境大部分在Ubuntu和MacOS上进行，这里是一个我觉得在这两个平台上很有用的工具的整理列表，大部分都可以在两个系统上找到。这里并不会给出具体安装方式，因为最好的文档总是软件的官方Document或者GitHub的README。<br><a id=\"more\"></a></p>\n<h2 id=\"zsh和Oh-my-zsh\"><a href=\"#zsh和Oh-my-zsh\" class=\"headerlink\" title=\"zsh和Oh-my-zsh\"></a>zsh和Oh-my-zsh</h2><p>如果经常在终端敲命令而且还在用系统自带的Bash？可以考虑试一下zsh替代bash，并使用<a href=\"https://github.com/robbyrussell/oh-my-zsh\" target=\"_blank\" rel=\"external\">oh-my-zsh</a>武装zsh。</p>\n<p>关于oh-my-zsh的帖子网上已经有很多，不过我还并没有用到太多的功能。oh-my-zsh中可以配置插件，不过我只是使用了<code>colored-man-pages</code>。顾名思义，它可以将使用<code>man</code>查询时的页面彩色输出。如下所示。<br><img src=\"/img/useful_tools_colored_man_pages.jpg\" alt=\"彩色的cp man页面\"></p>\n<h2 id=\"autojump\"><a href=\"#autojump\" class=\"headerlink\" title=\"autojump\"></a>autojump</h2><p>使用<a href=\"https://github.com/wting/autojump\" target=\"_blank\" rel=\"external\">autojump</a>，可以很方便地在已经访问过的文件夹间快速跳转。甚至都不需要输入目标文件夹的全名，支持自动联想。</p>\n<p>除了自动跳转功能，我还将其作为终端到文件资源管理器(Mac: Finder)的跳转功能。<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\"># 跳转到path并使用文件资源管理器打开</div><div class=\"line\">jo path</div></pre></td></tr></table></figure></p>\n<h2 id=\"tldr\"><a href=\"#tldr\" class=\"headerlink\" title=\"tldr\"></a>tldr</h2><p><a href=\"https://github.com/tldr-pages/tldr\" target=\"_blank\" rel=\"external\">tldr</a> (too long don’t read)是一款能够给出bash命令常用功能的工具。在Linux系统中，很多命令都有一长串参数。这其中很多都是不常用的。而我们使用时，常常是使用某几个常见的功能选项。tldr就能够给出命令的简要描述和例子。</p>\n<p>例如，使用其查询<code>tar</code>的常用方法：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div></pre></td><td class=\"code\"><pre><div class=\"line\">tldr tar</div><div class=\"line\"><span class=\"comment\"># output</span></div><div class=\"line\">tar</div><div class=\"line\"></div><div class=\"line\">Archiving utility.</div><div class=\"line\">Often combined with a compression method, such as gzip or bzip.</div><div class=\"line\"></div><div class=\"line\">- Create an archive from files:</div><div class=\"line\">    tar cf target.tar file1 file2 file3</div><div class=\"line\"></div><div class=\"line\">- Create a gzipped archive:</div><div class=\"line\">    tar czf target.tar.gz file1 file2 file3</div><div class=\"line\"></div><div class=\"line\">- Extract an archive <span class=\"keyword\">in</span> a target folder:</div><div class=\"line\">    tar xf source.tar -C folder</div><div class=\"line\"></div><div class=\"line\">- Extract a gzipped archive <span class=\"keyword\">in</span> the current directory:</div><div class=\"line\">    tar xzf source.tar.gz</div><div class=\"line\"></div><div class=\"line\">- Extract a bzipped archive <span class=\"keyword\">in</span> the current directory:</div><div class=\"line\">    tar xjf source.tar.bz2</div><div class=\"line\"></div><div class=\"line\">- Create a compressed archive, using archive suffix to determine the compression program:</div><div class=\"line\">    tar caf target.tar.xz file1 file2 file3</div><div class=\"line\"></div><div class=\"line\">- List the contents of a tar file:</div><div class=\"line\">    tar tvf source.tar</div></pre></td></tr></table></figure>\n<p>tldr支持多种语言，我使用了python包安装。但是不知为何，tldr在我这里总显示奇怪的背景颜色，看上去很别扭。所以我实际使用的是<a href=\"https://github.com/lord63/tldr.py\" target=\"_blank\" rel=\"external\">tldr-py</a>。</p>\n<h2 id=\"tmux\"><a href=\"#tmux\" class=\"headerlink\" title=\"tmux\"></a>tmux</h2><p>用SSH登录到服务器上时，如果网络连接不稳定或是自己的主机意外断电，会造成正在跑的代码死掉。因为进程是依附于SSH的会话Session的。tmux是一个终端的“分线器”，可以很方便地将正在进行的终端会话detach掉，使其转入后台运行。正是有这一特点，所以我们可以在SSH会话时，新建tmux会话，在其中跑一些耗时很长的代码，而不必担心SSH掉线。当然，也可以将tmux作为一款终端多任务的管理软件，方便地在多个任务中进行跳转。不过这个功能，我更加常用的是下面的guake。</p>\n<p>虽然Ubuntu14.04可以通过<code>apt-get</code>的方式安装tmux，不过为了能够使用一款好用的配置<a href=\"https://github.com/gpakosz/.tmux\" target=\"_blank\" rel=\"external\">oh-my-tmux</a>（要求tmux&gt;=2.1），还是推荐去GitHub上自己编译安装<a href=\"https://github.com/tmux/tmux\" target=\"_blank\" rel=\"external\">tmux</a>。</p>\n<h2 id=\"guake\"><a href=\"#guake\" class=\"headerlink\" title=\"guake\"></a>guake</h2><p><a href=\"https://github.com/Guake/guake\" target=\"_blank\" rel=\"external\">guake</a>是一款Ubuntu上可以方便呼出终端的应用（按下F12，终端将以全屏的方式铺满桌面，F11可以切换全屏或半屏）。</p>\n<h2 id=\"Dash-Zeal\"><a href=\"#Dash-Zeal\" class=\"headerlink\" title=\"Dash/Zeal\"></a>Dash/Zeal</h2><p>Dash是Mac上一款用于查询API文档的软件。在Ubuntu或Windows上，我们可以使用Zeal这个替代软件。Zeal和Dash基本上无缝衔接，但是却是免费的（Mac上的软件真是好贵。。。）。之前我已经写过一篇<a href=\"https://xmfbit.github.io/2017/08/26/doc2dash-usage/\">博客</a>，介绍如何自己制作文档导入Zeal中。</p>\n<h2 id=\"sshfs\"><a href=\"#sshfs\" class=\"headerlink\" title=\"sshfs\"></a>sshfs</h2><p>使用sshfs可以在本地机器上挂载远程服务器某个文件夹。这样，操作本地的该文件夹就相当于操作远程服务器上的该文件夹（小心使用<code>rm</code>）。</p>\n<h2 id=\"Alfred-Mutate\"><a href=\"#Alfred-Mutate\" class=\"headerlink\" title=\"Alfred/Mutate\"></a>Alfred/Mutate</h2><p>Alfred是Mac上一款非常好用的软件，就像蝙蝠侠身边的老管家一样，可以帮你自动化处理很多事情。除了原生<br>功能，还可以自己编写脚本实现扩展。例如查询豆瓣电影，查询ip，计算器等。鉴于这款软件的大名，这里不再多说。</p>\n<p><a href=\"https://github.com/qdore/Mutate\" target=\"_blank\" rel=\"external\">Mutate</a>是Ubuntu上的一款替代软件。同时，它也提供了方便的扩展接口，只需要按照模板编写python/shell代码，可以很方便地将自己的自动化处理功能加入软件中。</p>\n","excerpt":"<p>工作环境大部分在Ubuntu和MacOS上进行，这里是一个我觉得在这两个平台上很有用的工具的整理列表，大部分都可以在两个系统上找到。这里并不会给出具体安装方式，因为最好的文档总是软件的官方Document或者GitHub的README。<br>","more":"</p>\n<h2 id=\"zsh和Oh-my-zsh\"><a href=\"#zsh和Oh-my-zsh\" class=\"headerlink\" title=\"zsh和Oh-my-zsh\"></a>zsh和Oh-my-zsh</h2><p>如果经常在终端敲命令而且还在用系统自带的Bash？可以考虑试一下zsh替代bash，并使用<a href=\"https://github.com/robbyrussell/oh-my-zsh\">oh-my-zsh</a>武装zsh。</p>\n<p>关于oh-my-zsh的帖子网上已经有很多，不过我还并没有用到太多的功能。oh-my-zsh中可以配置插件，不过我只是使用了<code>colored-man-pages</code>。顾名思义，它可以将使用<code>man</code>查询时的页面彩色输出。如下所示。<br><img src=\"/img/useful_tools_colored_man_pages.jpg\" alt=\"彩色的cp man页面\"></p>\n<h2 id=\"autojump\"><a href=\"#autojump\" class=\"headerlink\" title=\"autojump\"></a>autojump</h2><p>使用<a href=\"https://github.com/wting/autojump\">autojump</a>，可以很方便地在已经访问过的文件夹间快速跳转。甚至都不需要输入目标文件夹的全名，支持自动联想。</p>\n<p>除了自动跳转功能，我还将其作为终端到文件资源管理器(Mac: Finder)的跳转功能。<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\"># 跳转到path并使用文件资源管理器打开</div><div class=\"line\">jo path</div></pre></td></tr></table></figure></p>\n<h2 id=\"tldr\"><a href=\"#tldr\" class=\"headerlink\" title=\"tldr\"></a>tldr</h2><p><a href=\"https://github.com/tldr-pages/tldr\">tldr</a> (too long don’t read)是一款能够给出bash命令常用功能的工具。在Linux系统中，很多命令都有一长串参数。这其中很多都是不常用的。而我们使用时，常常是使用某几个常见的功能选项。tldr就能够给出命令的简要描述和例子。</p>\n<p>例如，使用其查询<code>tar</code>的常用方法：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div></pre></td><td class=\"code\"><pre><div class=\"line\">tldr tar</div><div class=\"line\"><span class=\"comment\"># output</span></div><div class=\"line\">tar</div><div class=\"line\"></div><div class=\"line\">Archiving utility.</div><div class=\"line\">Often combined with a compression method, such as gzip or bzip.</div><div class=\"line\"></div><div class=\"line\">- Create an archive from files:</div><div class=\"line\">    tar cf target.tar file1 file2 file3</div><div class=\"line\"></div><div class=\"line\">- Create a gzipped archive:</div><div class=\"line\">    tar czf target.tar.gz file1 file2 file3</div><div class=\"line\"></div><div class=\"line\">- Extract an archive <span class=\"keyword\">in</span> a target folder:</div><div class=\"line\">    tar xf source.tar -C folder</div><div class=\"line\"></div><div class=\"line\">- Extract a gzipped archive <span class=\"keyword\">in</span> the current directory:</div><div class=\"line\">    tar xzf source.tar.gz</div><div class=\"line\"></div><div class=\"line\">- Extract a bzipped archive <span class=\"keyword\">in</span> the current directory:</div><div class=\"line\">    tar xjf source.tar.bz2</div><div class=\"line\"></div><div class=\"line\">- Create a compressed archive, using archive suffix to determine the compression program:</div><div class=\"line\">    tar caf target.tar.xz file1 file2 file3</div><div class=\"line\"></div><div class=\"line\">- List the contents of a tar file:</div><div class=\"line\">    tar tvf source.tar</div></pre></td></tr></table></figure>\n<p>tldr支持多种语言，我使用了python包安装。但是不知为何，tldr在我这里总显示奇怪的背景颜色，看上去很别扭。所以我实际使用的是<a href=\"https://github.com/lord63/tldr.py\">tldr-py</a>。</p>\n<h2 id=\"tmux\"><a href=\"#tmux\" class=\"headerlink\" title=\"tmux\"></a>tmux</h2><p>用SSH登录到服务器上时，如果网络连接不稳定或是自己的主机意外断电，会造成正在跑的代码死掉。因为进程是依附于SSH的会话Session的。tmux是一个终端的“分线器”，可以很方便地将正在进行的终端会话detach掉，使其转入后台运行。正是有这一特点，所以我们可以在SSH会话时，新建tmux会话，在其中跑一些耗时很长的代码，而不必担心SSH掉线。当然，也可以将tmux作为一款终端多任务的管理软件，方便地在多个任务中进行跳转。不过这个功能，我更加常用的是下面的guake。</p>\n<p>虽然Ubuntu14.04可以通过<code>apt-get</code>的方式安装tmux，不过为了能够使用一款好用的配置<a href=\"https://github.com/gpakosz/.tmux\">oh-my-tmux</a>（要求tmux&gt;=2.1），还是推荐去GitHub上自己编译安装<a href=\"https://github.com/tmux/tmux\">tmux</a>。</p>\n<h2 id=\"guake\"><a href=\"#guake\" class=\"headerlink\" title=\"guake\"></a>guake</h2><p><a href=\"https://github.com/Guake/guake\">guake</a>是一款Ubuntu上可以方便呼出终端的应用（按下F12，终端将以全屏的方式铺满桌面，F11可以切换全屏或半屏）。</p>\n<h2 id=\"Dash-Zeal\"><a href=\"#Dash-Zeal\" class=\"headerlink\" title=\"Dash/Zeal\"></a>Dash/Zeal</h2><p>Dash是Mac上一款用于查询API文档的软件。在Ubuntu或Windows上，我们可以使用Zeal这个替代软件。Zeal和Dash基本上无缝衔接，但是却是免费的（Mac上的软件真是好贵。。。）。之前我已经写过一篇<a href=\"https://xmfbit.github.io/2017/08/26/doc2dash-usage/\">博客</a>，介绍如何自己制作文档导入Zeal中。</p>\n<h2 id=\"sshfs\"><a href=\"#sshfs\" class=\"headerlink\" title=\"sshfs\"></a>sshfs</h2><p>使用sshfs可以在本地机器上挂载远程服务器某个文件夹。这样，操作本地的该文件夹就相当于操作远程服务器上的该文件夹（小心使用<code>rm</code>）。</p>\n<h2 id=\"Alfred-Mutate\"><a href=\"#Alfred-Mutate\" class=\"headerlink\" title=\"Alfred/Mutate\"></a>Alfred/Mutate</h2><p>Alfred是Mac上一款非常好用的软件，就像蝙蝠侠身边的老管家一样，可以帮你自动化处理很多事情。除了原生<br>功能，还可以自己编写脚本实现扩展。例如查询豆瓣电影，查询ip，计算器等。鉴于这款软件的大名，这里不再多说。</p>\n<p><a href=\"https://github.com/qdore/Mutate\">Mutate</a>是Ubuntu上的一款替代软件。同时，它也提供了方便的扩展接口，只需要按照模板编写python/shell代码，可以很方便地将自己的自动化处理功能加入软件中。</p>"},{"title":"B站视频“线性代数的本质”观后感","date":"2017-02-05T11:16:32.000Z","_content":"线性代数对于现代科技的重要性不言而喻，在机器学习领域也是重要的工具。最近，在B站上发现了这样的一个视频：[线性代数的本质](http://www.bilibili.com/video/av6731067/index_1.html)。这个视频共有十集左右，每集大约10分钟，从线性空间入手，介绍线性变换，并由此介绍矩阵的相关性质，动画做的很棒，翻译得也很好，用了几天时间零散地把几集视频看完，记录一些心得。\n![](/img/video_linear_alg_essential.png)\n\n<!-- more -->\n## 从线性空间和线性变换讲起\nBIT的教学水平，不同的老师参差不齐。所幸的是，大一教授线性代数一课的吴惠彬老师，是一位既有深厚学问又善于教课的老师。我们所用的教材，正是吴老师主编的。和这个视频不同，教材是以线性方程组入手来介绍矩阵，而后引入到线性空间和线性变换。吴老师还曾经告诫大家，线性空间和线性变换一章较为抽象晦涩，不易理解。当然，很多年过去了，我也在研究生阶段继续修了线性代数的深入课程，如今想来，其实线性变换和矩阵的联系更为紧密一些。但是我们的教材之所以不用线性变换为引子，可能正是由于对于大一新生来说，过去十二年所受的基础教育都是操作一些看得见摸得着的数和几何体，一时难以接受线性空间这种东西吧~而也正是从这门课和高数开始，思考问题的方式和初等数学不一样了。我们更多地考察运动和极限，将具有共性的东西抽象出来，研究它们的等价性质。\n\n而抽象这个问题，在本视频中，通过将问题聚焦在二维（偶有提及三维）平面并辅之以动画的形式避开了（所以这个视频也只能算是有趣的不严肃的科普性质）。视频从线性空间入手，首先介绍了向量和基的概念，而后引入出贯穿视频系列始终的**线性变换**。什么是线性变换呢？从“数”的角度来看，定义在线性空间上，满足叠加性和齐次性就是线性变换。而本视频则从“形”的角度出发（注意，这也是本视频贯穿始终的指导思想：用“形”来思考），给出了下面两个条件：\n- 变换前后原点不动\n- 变换前后平行等距线仍然保持平行等距（但是距离值可能会变）。\n\n## 线性变换与矩阵的关系\n视频在阐述线性变换和矩阵关系的时候一带而过，不是很清楚（所以还是要去看严肃的教材啊）。下面是我写的一个补充说明。这也是整个视频系列的基础。\n\n在由一组基向量$\\alpha_i, i = 1,2,\\dots,n$张成的线性空间$\\mathcal{V}$上，任何一个向量$v$都可以表示为这组基的线性组合，也就是\n$$v = \\sum_{i=1}^{n}k_i\\alpha_i$$\n\n则线性变换$\\mathcal{T}$对$v$作用之后，有，\n$$u = \\mathcal{T}(v) = \\mathcal{T}(\\sum_{i=1}^{n}k_i\\alpha_i)$$\n\n根据线性变换的叠加性，有，\n$$u = \\sum_{i=1}^{n}k_i\\mathcal{T}(\\alpha_i)$$\n\n设$\\alpha_i$经过线性变换$\\mathcal{T}$作用后，变换为$\\beta_i$，那么，\n$$u = \\sum_{i=1}^{n}k_i\\beta_i$$\n\n也就是说，\n$$u = \\begin{bmatrix}\\mathcal{T}(\\alpha_1), \\mathcal{T}(\\alpha_2), \\cdots, \\mathcal{T}(\\alpha_n)\\end{bmatrix}\n\\begin{bmatrix}k_1\\\\\\\\ k_2\\\\\\\\ \\vdots\\\\\\\\ k_n\\end{bmatrix}$$\n\n上式说明，一个抽象的线性变换可以使用一个具体的矩阵来代表。这个矩阵的列，就是线性变换之后的那组基。\n\n举个例子，旋转变换。如果旋转$\\frac{\\pi}{4}$，那么原来坐标轴方向的单位向量的$i$和$j$分别转到了$(\\frac{\\sqrt{2}}{2},\\frac{\\sqrt{2}}{2})$和$(-\\frac{\\sqrt{2}}{2},\\frac{\\sqrt{2}}{2})$。所以描述这个线性变换的矩阵为\n\n$$A = \\begin{bmatrix}\\frac{\\sqrt{2}}{2} & -\\frac{\\sqrt{2}}{2}\\\\\\\\ \\frac{\\sqrt{2}}{2}& \\frac{\\sqrt{2}}{2}\\end{bmatrix}$$\n\n矩阵$A$的两列分别为变换后的基向量坐标。\n\n## 矩阵乘法\n那么矩阵乘法的意义也就有了，那就是对向量做线性变换。例如上面的矩阵$A$与$x = \\begin{bmatrix}-1\\\\\\\\ 0\\end{bmatrix}$相乘，就是求取向量$-1i+0j$在旋转变换之后的新坐标。这时候，$-1$和$0$就是上面推导过程中的$k_1$和$k_2$，所以，\n\n$$Ax = -1\\begin{bmatrix}\\frac{\\sqrt{2}}{2}\\\\\\\\ \\frac{\\sqrt{2}}{2}\\end{bmatrix} + 0\\begin{bmatrix}-\\frac{\\sqrt{2}}{2}\\\\\\\\\\frac{\\sqrt{2}}{2}\\end{bmatrix} $$\n\n而矩阵与矩阵相乘呢？只要把右边的矩阵按列分块即可。其实这是矩阵与多个向量相乘的简写形式。\n\n所以，对于任意向量$x$，$Ax$的结果就是组成矩阵$A$的各列向量的线性组合。这个由矩阵$A$的各列张成的空间叫做矩阵的列空间。\n\n而那些使得方程$Ax=0$成立的$x$，张成的空间为矩阵的核空间。\n\n矩阵的秩的意义就是矩阵列空间的维数。\n\n同时，从这个角度看来，解决多元线性方程组的过程就变成了这样一个问题：即给定代表线性变换的矩阵以及变换后的向量，求解变换前向量。这个转换如下所示：\n![线性方程组与矩阵乘法](/img/video_linear_alg_essential_linear_equation.png)\n\n既然矩阵代表了某种线性变换，那么很自然的，可以想到，我们可以求取这个线性变换的逆变换，这个逆变换作用到$v$上，就可以得到原始的向量$x$了（而且这样的向量只有一个）！自然，这个逆变换也是有矩阵与其对应的，这个矩阵就是原矩阵的逆矩阵。那么是不是所有的矩阵都有逆矩阵呢？我们可以通过行列式来分析。\n\n## 行列式\n仍然考虑二维矩阵，我们已经知道它表示了二维平面上的线性变换。而矩阵行列式的意义，就表明了变换前后，单位面积的正方形被放缩的比例。如果变换之后，$i$向量跑到了$j$向量的左手边，那么需要加上负号，表明在这个变换过程中，二维平面其实进过了一次翻转。\n\n我们已经知道，行列式为$0$的矩阵实际对线性空间进行了“降维打击”。以二维平面举例，变换之后变成了一条直线（甚至变成了一个点），也就是说对于任意给定的一个变换后向量，有这样两种情况：\n- 当这个向量不在这条直线上的时候，说明没有原始向量与其对应（否则矛盾），此时原方程组无解。\n- 当这个向量在这条直线上的时候，说明很多的原始向量（而且必然是无穷多）与其对应，此时原方程组有无穷多组解。\n\n你不能将一条直线“解压缩”为原始平面，所以行列式为$0$的矩阵，不存在逆矩阵。\n\n## 点积叉积和对偶性\n这部分的对偶性这个概念比较玄一些，这里就不讲了。。。对于向量的点积，我们已经知道，\n$$\\langle v, u \\rangle = \\sum_{i=1}^{n}v_iu_i$$\n\n从几何的角度看，向量的点积是将向量$u$向向量$v$的方向投影（有正负），投影长度乘上$v$的长度得到的。\n\n按照上面矩阵乘法的思路，$u$可以看做代表一个从2维到1维的线性变换（也就是从$\\mathbb{R}^2$到$\\mathbb{R}$）。而根据上述矩阵变换和矩阵乘法的对应关系可知，变换后的结果就等于$v$的两个分量分别乘上两个基向量（也就是$u$的两个分量），这样我们就得到了向量点积的数值计算方法。\n","source":"_posts/video-linear-alg-essential-property.md","raw":"---\ntitle: B站视频“线性代数的本质”观后感\ndate: 2017-02-05 19:16:32\ntags:\n    - math\n---\n线性代数对于现代科技的重要性不言而喻，在机器学习领域也是重要的工具。最近，在B站上发现了这样的一个视频：[线性代数的本质](http://www.bilibili.com/video/av6731067/index_1.html)。这个视频共有十集左右，每集大约10分钟，从线性空间入手，介绍线性变换，并由此介绍矩阵的相关性质，动画做的很棒，翻译得也很好，用了几天时间零散地把几集视频看完，记录一些心得。\n![](/img/video_linear_alg_essential.png)\n\n<!-- more -->\n## 从线性空间和线性变换讲起\nBIT的教学水平，不同的老师参差不齐。所幸的是，大一教授线性代数一课的吴惠彬老师，是一位既有深厚学问又善于教课的老师。我们所用的教材，正是吴老师主编的。和这个视频不同，教材是以线性方程组入手来介绍矩阵，而后引入到线性空间和线性变换。吴老师还曾经告诫大家，线性空间和线性变换一章较为抽象晦涩，不易理解。当然，很多年过去了，我也在研究生阶段继续修了线性代数的深入课程，如今想来，其实线性变换和矩阵的联系更为紧密一些。但是我们的教材之所以不用线性变换为引子，可能正是由于对于大一新生来说，过去十二年所受的基础教育都是操作一些看得见摸得着的数和几何体，一时难以接受线性空间这种东西吧~而也正是从这门课和高数开始，思考问题的方式和初等数学不一样了。我们更多地考察运动和极限，将具有共性的东西抽象出来，研究它们的等价性质。\n\n而抽象这个问题，在本视频中，通过将问题聚焦在二维（偶有提及三维）平面并辅之以动画的形式避开了（所以这个视频也只能算是有趣的不严肃的科普性质）。视频从线性空间入手，首先介绍了向量和基的概念，而后引入出贯穿视频系列始终的**线性变换**。什么是线性变换呢？从“数”的角度来看，定义在线性空间上，满足叠加性和齐次性就是线性变换。而本视频则从“形”的角度出发（注意，这也是本视频贯穿始终的指导思想：用“形”来思考），给出了下面两个条件：\n- 变换前后原点不动\n- 变换前后平行等距线仍然保持平行等距（但是距离值可能会变）。\n\n## 线性变换与矩阵的关系\n视频在阐述线性变换和矩阵关系的时候一带而过，不是很清楚（所以还是要去看严肃的教材啊）。下面是我写的一个补充说明。这也是整个视频系列的基础。\n\n在由一组基向量$\\alpha_i, i = 1,2,\\dots,n$张成的线性空间$\\mathcal{V}$上，任何一个向量$v$都可以表示为这组基的线性组合，也就是\n$$v = \\sum_{i=1}^{n}k_i\\alpha_i$$\n\n则线性变换$\\mathcal{T}$对$v$作用之后，有，\n$$u = \\mathcal{T}(v) = \\mathcal{T}(\\sum_{i=1}^{n}k_i\\alpha_i)$$\n\n根据线性变换的叠加性，有，\n$$u = \\sum_{i=1}^{n}k_i\\mathcal{T}(\\alpha_i)$$\n\n设$\\alpha_i$经过线性变换$\\mathcal{T}$作用后，变换为$\\beta_i$，那么，\n$$u = \\sum_{i=1}^{n}k_i\\beta_i$$\n\n也就是说，\n$$u = \\begin{bmatrix}\\mathcal{T}(\\alpha_1), \\mathcal{T}(\\alpha_2), \\cdots, \\mathcal{T}(\\alpha_n)\\end{bmatrix}\n\\begin{bmatrix}k_1\\\\\\\\ k_2\\\\\\\\ \\vdots\\\\\\\\ k_n\\end{bmatrix}$$\n\n上式说明，一个抽象的线性变换可以使用一个具体的矩阵来代表。这个矩阵的列，就是线性变换之后的那组基。\n\n举个例子，旋转变换。如果旋转$\\frac{\\pi}{4}$，那么原来坐标轴方向的单位向量的$i$和$j$分别转到了$(\\frac{\\sqrt{2}}{2},\\frac{\\sqrt{2}}{2})$和$(-\\frac{\\sqrt{2}}{2},\\frac{\\sqrt{2}}{2})$。所以描述这个线性变换的矩阵为\n\n$$A = \\begin{bmatrix}\\frac{\\sqrt{2}}{2} & -\\frac{\\sqrt{2}}{2}\\\\\\\\ \\frac{\\sqrt{2}}{2}& \\frac{\\sqrt{2}}{2}\\end{bmatrix}$$\n\n矩阵$A$的两列分别为变换后的基向量坐标。\n\n## 矩阵乘法\n那么矩阵乘法的意义也就有了，那就是对向量做线性变换。例如上面的矩阵$A$与$x = \\begin{bmatrix}-1\\\\\\\\ 0\\end{bmatrix}$相乘，就是求取向量$-1i+0j$在旋转变换之后的新坐标。这时候，$-1$和$0$就是上面推导过程中的$k_1$和$k_2$，所以，\n\n$$Ax = -1\\begin{bmatrix}\\frac{\\sqrt{2}}{2}\\\\\\\\ \\frac{\\sqrt{2}}{2}\\end{bmatrix} + 0\\begin{bmatrix}-\\frac{\\sqrt{2}}{2}\\\\\\\\\\frac{\\sqrt{2}}{2}\\end{bmatrix} $$\n\n而矩阵与矩阵相乘呢？只要把右边的矩阵按列分块即可。其实这是矩阵与多个向量相乘的简写形式。\n\n所以，对于任意向量$x$，$Ax$的结果就是组成矩阵$A$的各列向量的线性组合。这个由矩阵$A$的各列张成的空间叫做矩阵的列空间。\n\n而那些使得方程$Ax=0$成立的$x$，张成的空间为矩阵的核空间。\n\n矩阵的秩的意义就是矩阵列空间的维数。\n\n同时，从这个角度看来，解决多元线性方程组的过程就变成了这样一个问题：即给定代表线性变换的矩阵以及变换后的向量，求解变换前向量。这个转换如下所示：\n![线性方程组与矩阵乘法](/img/video_linear_alg_essential_linear_equation.png)\n\n既然矩阵代表了某种线性变换，那么很自然的，可以想到，我们可以求取这个线性变换的逆变换，这个逆变换作用到$v$上，就可以得到原始的向量$x$了（而且这样的向量只有一个）！自然，这个逆变换也是有矩阵与其对应的，这个矩阵就是原矩阵的逆矩阵。那么是不是所有的矩阵都有逆矩阵呢？我们可以通过行列式来分析。\n\n## 行列式\n仍然考虑二维矩阵，我们已经知道它表示了二维平面上的线性变换。而矩阵行列式的意义，就表明了变换前后，单位面积的正方形被放缩的比例。如果变换之后，$i$向量跑到了$j$向量的左手边，那么需要加上负号，表明在这个变换过程中，二维平面其实进过了一次翻转。\n\n我们已经知道，行列式为$0$的矩阵实际对线性空间进行了“降维打击”。以二维平面举例，变换之后变成了一条直线（甚至变成了一个点），也就是说对于任意给定的一个变换后向量，有这样两种情况：\n- 当这个向量不在这条直线上的时候，说明没有原始向量与其对应（否则矛盾），此时原方程组无解。\n- 当这个向量在这条直线上的时候，说明很多的原始向量（而且必然是无穷多）与其对应，此时原方程组有无穷多组解。\n\n你不能将一条直线“解压缩”为原始平面，所以行列式为$0$的矩阵，不存在逆矩阵。\n\n## 点积叉积和对偶性\n这部分的对偶性这个概念比较玄一些，这里就不讲了。。。对于向量的点积，我们已经知道，\n$$\\langle v, u \\rangle = \\sum_{i=1}^{n}v_iu_i$$\n\n从几何的角度看，向量的点积是将向量$u$向向量$v$的方向投影（有正负），投影长度乘上$v$的长度得到的。\n\n按照上面矩阵乘法的思路，$u$可以看做代表一个从2维到1维的线性变换（也就是从$\\mathbb{R}^2$到$\\mathbb{R}$）。而根据上述矩阵变换和矩阵乘法的对应关系可知，变换后的结果就等于$v$的两个分量分别乘上两个基向量（也就是$u$的两个分量），这样我们就得到了向量点积的数值计算方法。\n","slug":"video-linear-alg-essential-property","published":1,"updated":"2018-01-12T06:22:20.483Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjcu6vcuk002pqu46e7tdx1v8","content":"<p>线性代数对于现代科技的重要性不言而喻，在机器学习领域也是重要的工具。最近，在B站上发现了这样的一个视频：<a href=\"http://www.bilibili.com/video/av6731067/index_1.html\" target=\"_blank\" rel=\"external\">线性代数的本质</a>。这个视频共有十集左右，每集大约10分钟，从线性空间入手，介绍线性变换，并由此介绍矩阵的相关性质，动画做的很棒，翻译得也很好，用了几天时间零散地把几集视频看完，记录一些心得。<br><img src=\"/img/video_linear_alg_essential.png\" alt=\"\"></p>\n<a id=\"more\"></a>\n<h2 id=\"从线性空间和线性变换讲起\"><a href=\"#从线性空间和线性变换讲起\" class=\"headerlink\" title=\"从线性空间和线性变换讲起\"></a>从线性空间和线性变换讲起</h2><p>BIT的教学水平，不同的老师参差不齐。所幸的是，大一教授线性代数一课的吴惠彬老师，是一位既有深厚学问又善于教课的老师。我们所用的教材，正是吴老师主编的。和这个视频不同，教材是以线性方程组入手来介绍矩阵，而后引入到线性空间和线性变换。吴老师还曾经告诫大家，线性空间和线性变换一章较为抽象晦涩，不易理解。当然，很多年过去了，我也在研究生阶段继续修了线性代数的深入课程，如今想来，其实线性变换和矩阵的联系更为紧密一些。但是我们的教材之所以不用线性变换为引子，可能正是由于对于大一新生来说，过去十二年所受的基础教育都是操作一些看得见摸得着的数和几何体，一时难以接受线性空间这种东西吧~而也正是从这门课和高数开始，思考问题的方式和初等数学不一样了。我们更多地考察运动和极限，将具有共性的东西抽象出来，研究它们的等价性质。</p>\n<p>而抽象这个问题，在本视频中，通过将问题聚焦在二维（偶有提及三维）平面并辅之以动画的形式避开了（所以这个视频也只能算是有趣的不严肃的科普性质）。视频从线性空间入手，首先介绍了向量和基的概念，而后引入出贯穿视频系列始终的<strong>线性变换</strong>。什么是线性变换呢？从“数”的角度来看，定义在线性空间上，满足叠加性和齐次性就是线性变换。而本视频则从“形”的角度出发（注意，这也是本视频贯穿始终的指导思想：用“形”来思考），给出了下面两个条件：</p>\n<ul>\n<li>变换前后原点不动</li>\n<li>变换前后平行等距线仍然保持平行等距（但是距离值可能会变）。</li>\n</ul>\n<h2 id=\"线性变换与矩阵的关系\"><a href=\"#线性变换与矩阵的关系\" class=\"headerlink\" title=\"线性变换与矩阵的关系\"></a>线性变换与矩阵的关系</h2><p>视频在阐述线性变换和矩阵关系的时候一带而过，不是很清楚（所以还是要去看严肃的教材啊）。下面是我写的一个补充说明。这也是整个视频系列的基础。</p>\n<p>在由一组基向量$\\alpha_i, i = 1,2,\\dots,n$张成的线性空间$\\mathcal{V}$上，任何一个向量$v$都可以表示为这组基的线性组合，也就是</p>\n<script type=\"math/tex; mode=display\">v = \\sum_{i=1}^{n}k_i\\alpha_i</script><p>则线性变换$\\mathcal{T}$对$v$作用之后，有，</p>\n<script type=\"math/tex; mode=display\">u = \\mathcal{T}(v) = \\mathcal{T}(\\sum_{i=1}^{n}k_i\\alpha_i)</script><p>根据线性变换的叠加性，有，</p>\n<script type=\"math/tex; mode=display\">u = \\sum_{i=1}^{n}k_i\\mathcal{T}(\\alpha_i)</script><p>设$\\alpha_i$经过线性变换$\\mathcal{T}$作用后，变换为$\\beta_i$，那么，</p>\n<script type=\"math/tex; mode=display\">u = \\sum_{i=1}^{n}k_i\\beta_i</script><p>也就是说，</p>\n<script type=\"math/tex; mode=display\">u = \\begin{bmatrix}\\mathcal{T}(\\alpha_1), \\mathcal{T}(\\alpha_2), \\cdots, \\mathcal{T}(\\alpha_n)\\end{bmatrix}\n\\begin{bmatrix}k_1\\\\\\\\ k_2\\\\\\\\ \\vdots\\\\\\\\ k_n\\end{bmatrix}</script><p>上式说明，一个抽象的线性变换可以使用一个具体的矩阵来代表。这个矩阵的列，就是线性变换之后的那组基。</p>\n<p>举个例子，旋转变换。如果旋转$\\frac{\\pi}{4}$，那么原来坐标轴方向的单位向量的$i$和$j$分别转到了$(\\frac{\\sqrt{2}}{2},\\frac{\\sqrt{2}}{2})$和$(-\\frac{\\sqrt{2}}{2},\\frac{\\sqrt{2}}{2})$。所以描述这个线性变换的矩阵为</p>\n<script type=\"math/tex; mode=display\">A = \\begin{bmatrix}\\frac{\\sqrt{2}}{2} & -\\frac{\\sqrt{2}}{2}\\\\\\\\ \\frac{\\sqrt{2}}{2}& \\frac{\\sqrt{2}}{2}\\end{bmatrix}</script><p>矩阵$A$的两列分别为变换后的基向量坐标。</p>\n<h2 id=\"矩阵乘法\"><a href=\"#矩阵乘法\" class=\"headerlink\" title=\"矩阵乘法\"></a>矩阵乘法</h2><p>那么矩阵乘法的意义也就有了，那就是对向量做线性变换。例如上面的矩阵$A$与$x = \\begin{bmatrix}-1\\\\ 0\\end{bmatrix}$相乘，就是求取向量$-1i+0j$在旋转变换之后的新坐标。这时候，$-1$和$0$就是上面推导过程中的$k_1$和$k_2$，所以，</p>\n<script type=\"math/tex; mode=display\">Ax = -1\\begin{bmatrix}\\frac{\\sqrt{2}}{2}\\\\\\\\ \\frac{\\sqrt{2}}{2}\\end{bmatrix} + 0\\begin{bmatrix}-\\frac{\\sqrt{2}}{2}\\\\\\\\\\frac{\\sqrt{2}}{2}\\end{bmatrix}</script><p>而矩阵与矩阵相乘呢？只要把右边的矩阵按列分块即可。其实这是矩阵与多个向量相乘的简写形式。</p>\n<p>所以，对于任意向量$x$，$Ax$的结果就是组成矩阵$A$的各列向量的线性组合。这个由矩阵$A$的各列张成的空间叫做矩阵的列空间。</p>\n<p>而那些使得方程$Ax=0$成立的$x$，张成的空间为矩阵的核空间。</p>\n<p>矩阵的秩的意义就是矩阵列空间的维数。</p>\n<p>同时，从这个角度看来，解决多元线性方程组的过程就变成了这样一个问题：即给定代表线性变换的矩阵以及变换后的向量，求解变换前向量。这个转换如下所示：<br><img src=\"/img/video_linear_alg_essential_linear_equation.png\" alt=\"线性方程组与矩阵乘法\"></p>\n<p>既然矩阵代表了某种线性变换，那么很自然的，可以想到，我们可以求取这个线性变换的逆变换，这个逆变换作用到$v$上，就可以得到原始的向量$x$了（而且这样的向量只有一个）！自然，这个逆变换也是有矩阵与其对应的，这个矩阵就是原矩阵的逆矩阵。那么是不是所有的矩阵都有逆矩阵呢？我们可以通过行列式来分析。</p>\n<h2 id=\"行列式\"><a href=\"#行列式\" class=\"headerlink\" title=\"行列式\"></a>行列式</h2><p>仍然考虑二维矩阵，我们已经知道它表示了二维平面上的线性变换。而矩阵行列式的意义，就表明了变换前后，单位面积的正方形被放缩的比例。如果变换之后，$i$向量跑到了$j$向量的左手边，那么需要加上负号，表明在这个变换过程中，二维平面其实进过了一次翻转。</p>\n<p>我们已经知道，行列式为$0$的矩阵实际对线性空间进行了“降维打击”。以二维平面举例，变换之后变成了一条直线（甚至变成了一个点），也就是说对于任意给定的一个变换后向量，有这样两种情况：</p>\n<ul>\n<li>当这个向量不在这条直线上的时候，说明没有原始向量与其对应（否则矛盾），此时原方程组无解。</li>\n<li>当这个向量在这条直线上的时候，说明很多的原始向量（而且必然是无穷多）与其对应，此时原方程组有无穷多组解。</li>\n</ul>\n<p>你不能将一条直线“解压缩”为原始平面，所以行列式为$0$的矩阵，不存在逆矩阵。</p>\n<h2 id=\"点积叉积和对偶性\"><a href=\"#点积叉积和对偶性\" class=\"headerlink\" title=\"点积叉积和对偶性\"></a>点积叉积和对偶性</h2><p>这部分的对偶性这个概念比较玄一些，这里就不讲了。。。对于向量的点积，我们已经知道，</p>\n<script type=\"math/tex; mode=display\">\\langle v, u \\rangle = \\sum_{i=1}^{n}v_iu_i</script><p>从几何的角度看，向量的点积是将向量$u$向向量$v$的方向投影（有正负），投影长度乘上$v$的长度得到的。</p>\n<p>按照上面矩阵乘法的思路，$u$可以看做代表一个从2维到1维的线性变换（也就是从$\\mathbb{R}^2$到$\\mathbb{R}$）。而根据上述矩阵变换和矩阵乘法的对应关系可知，变换后的结果就等于$v$的两个分量分别乘上两个基向量（也就是$u$的两个分量），这样我们就得到了向量点积的数值计算方法。</p>\n","excerpt":"<p>线性代数对于现代科技的重要性不言而喻，在机器学习领域也是重要的工具。最近，在B站上发现了这样的一个视频：<a href=\"http://www.bilibili.com/video/av6731067/index_1.html\">线性代数的本质</a>。这个视频共有十集左右，每集大约10分钟，从线性空间入手，介绍线性变换，并由此介绍矩阵的相关性质，动画做的很棒，翻译得也很好，用了几天时间零散地把几集视频看完，记录一些心得。<br><img src=\"/img/video_linear_alg_essential.png\" alt=\"\"></p>","more":"<h2 id=\"从线性空间和线性变换讲起\"><a href=\"#从线性空间和线性变换讲起\" class=\"headerlink\" title=\"从线性空间和线性变换讲起\"></a>从线性空间和线性变换讲起</h2><p>BIT的教学水平，不同的老师参差不齐。所幸的是，大一教授线性代数一课的吴惠彬老师，是一位既有深厚学问又善于教课的老师。我们所用的教材，正是吴老师主编的。和这个视频不同，教材是以线性方程组入手来介绍矩阵，而后引入到线性空间和线性变换。吴老师还曾经告诫大家，线性空间和线性变换一章较为抽象晦涩，不易理解。当然，很多年过去了，我也在研究生阶段继续修了线性代数的深入课程，如今想来，其实线性变换和矩阵的联系更为紧密一些。但是我们的教材之所以不用线性变换为引子，可能正是由于对于大一新生来说，过去十二年所受的基础教育都是操作一些看得见摸得着的数和几何体，一时难以接受线性空间这种东西吧~而也正是从这门课和高数开始，思考问题的方式和初等数学不一样了。我们更多地考察运动和极限，将具有共性的东西抽象出来，研究它们的等价性质。</p>\n<p>而抽象这个问题，在本视频中，通过将问题聚焦在二维（偶有提及三维）平面并辅之以动画的形式避开了（所以这个视频也只能算是有趣的不严肃的科普性质）。视频从线性空间入手，首先介绍了向量和基的概念，而后引入出贯穿视频系列始终的<strong>线性变换</strong>。什么是线性变换呢？从“数”的角度来看，定义在线性空间上，满足叠加性和齐次性就是线性变换。而本视频则从“形”的角度出发（注意，这也是本视频贯穿始终的指导思想：用“形”来思考），给出了下面两个条件：</p>\n<ul>\n<li>变换前后原点不动</li>\n<li>变换前后平行等距线仍然保持平行等距（但是距离值可能会变）。</li>\n</ul>\n<h2 id=\"线性变换与矩阵的关系\"><a href=\"#线性变换与矩阵的关系\" class=\"headerlink\" title=\"线性变换与矩阵的关系\"></a>线性变换与矩阵的关系</h2><p>视频在阐述线性变换和矩阵关系的时候一带而过，不是很清楚（所以还是要去看严肃的教材啊）。下面是我写的一个补充说明。这也是整个视频系列的基础。</p>\n<p>在由一组基向量$\\alpha_i, i = 1,2,\\dots,n$张成的线性空间$\\mathcal{V}$上，任何一个向量$v$都可以表示为这组基的线性组合，也就是</p>\n<script type=\"math/tex; mode=display\">v = \\sum_{i=1}^{n}k_i\\alpha_i</script><p>则线性变换$\\mathcal{T}$对$v$作用之后，有，</p>\n<script type=\"math/tex; mode=display\">u = \\mathcal{T}(v) = \\mathcal{T}(\\sum_{i=1}^{n}k_i\\alpha_i)</script><p>根据线性变换的叠加性，有，</p>\n<script type=\"math/tex; mode=display\">u = \\sum_{i=1}^{n}k_i\\mathcal{T}(\\alpha_i)</script><p>设$\\alpha_i$经过线性变换$\\mathcal{T}$作用后，变换为$\\beta_i$，那么，</p>\n<script type=\"math/tex; mode=display\">u = \\sum_{i=1}^{n}k_i\\beta_i</script><p>也就是说，</p>\n<script type=\"math/tex; mode=display\">u = \\begin{bmatrix}\\mathcal{T}(\\alpha_1), \\mathcal{T}(\\alpha_2), \\cdots, \\mathcal{T}(\\alpha_n)\\end{bmatrix}\n\\begin{bmatrix}k_1\\\\\\\\ k_2\\\\\\\\ \\vdots\\\\\\\\ k_n\\end{bmatrix}</script><p>上式说明，一个抽象的线性变换可以使用一个具体的矩阵来代表。这个矩阵的列，就是线性变换之后的那组基。</p>\n<p>举个例子，旋转变换。如果旋转$\\frac{\\pi}{4}$，那么原来坐标轴方向的单位向量的$i$和$j$分别转到了$(\\frac{\\sqrt{2}}{2},\\frac{\\sqrt{2}}{2})$和$(-\\frac{\\sqrt{2}}{2},\\frac{\\sqrt{2}}{2})$。所以描述这个线性变换的矩阵为</p>\n<script type=\"math/tex; mode=display\">A = \\begin{bmatrix}\\frac{\\sqrt{2}}{2} & -\\frac{\\sqrt{2}}{2}\\\\\\\\ \\frac{\\sqrt{2}}{2}& \\frac{\\sqrt{2}}{2}\\end{bmatrix}</script><p>矩阵$A$的两列分别为变换后的基向量坐标。</p>\n<h2 id=\"矩阵乘法\"><a href=\"#矩阵乘法\" class=\"headerlink\" title=\"矩阵乘法\"></a>矩阵乘法</h2><p>那么矩阵乘法的意义也就有了，那就是对向量做线性变换。例如上面的矩阵$A$与$x = \\begin{bmatrix}-1\\\\ 0\\end{bmatrix}$相乘，就是求取向量$-1i+0j$在旋转变换之后的新坐标。这时候，$-1$和$0$就是上面推导过程中的$k_1$和$k_2$，所以，</p>\n<script type=\"math/tex; mode=display\">Ax = -1\\begin{bmatrix}\\frac{\\sqrt{2}}{2}\\\\\\\\ \\frac{\\sqrt{2}}{2}\\end{bmatrix} + 0\\begin{bmatrix}-\\frac{\\sqrt{2}}{2}\\\\\\\\\\frac{\\sqrt{2}}{2}\\end{bmatrix}</script><p>而矩阵与矩阵相乘呢？只要把右边的矩阵按列分块即可。其实这是矩阵与多个向量相乘的简写形式。</p>\n<p>所以，对于任意向量$x$，$Ax$的结果就是组成矩阵$A$的各列向量的线性组合。这个由矩阵$A$的各列张成的空间叫做矩阵的列空间。</p>\n<p>而那些使得方程$Ax=0$成立的$x$，张成的空间为矩阵的核空间。</p>\n<p>矩阵的秩的意义就是矩阵列空间的维数。</p>\n<p>同时，从这个角度看来，解决多元线性方程组的过程就变成了这样一个问题：即给定代表线性变换的矩阵以及变换后的向量，求解变换前向量。这个转换如下所示：<br><img src=\"/img/video_linear_alg_essential_linear_equation.png\" alt=\"线性方程组与矩阵乘法\"></p>\n<p>既然矩阵代表了某种线性变换，那么很自然的，可以想到，我们可以求取这个线性变换的逆变换，这个逆变换作用到$v$上，就可以得到原始的向量$x$了（而且这样的向量只有一个）！自然，这个逆变换也是有矩阵与其对应的，这个矩阵就是原矩阵的逆矩阵。那么是不是所有的矩阵都有逆矩阵呢？我们可以通过行列式来分析。</p>\n<h2 id=\"行列式\"><a href=\"#行列式\" class=\"headerlink\" title=\"行列式\"></a>行列式</h2><p>仍然考虑二维矩阵，我们已经知道它表示了二维平面上的线性变换。而矩阵行列式的意义，就表明了变换前后，单位面积的正方形被放缩的比例。如果变换之后，$i$向量跑到了$j$向量的左手边，那么需要加上负号，表明在这个变换过程中，二维平面其实进过了一次翻转。</p>\n<p>我们已经知道，行列式为$0$的矩阵实际对线性空间进行了“降维打击”。以二维平面举例，变换之后变成了一条直线（甚至变成了一个点），也就是说对于任意给定的一个变换后向量，有这样两种情况：</p>\n<ul>\n<li>当这个向量不在这条直线上的时候，说明没有原始向量与其对应（否则矛盾），此时原方程组无解。</li>\n<li>当这个向量在这条直线上的时候，说明很多的原始向量（而且必然是无穷多）与其对应，此时原方程组有无穷多组解。</li>\n</ul>\n<p>你不能将一条直线“解压缩”为原始平面，所以行列式为$0$的矩阵，不存在逆矩阵。</p>\n<h2 id=\"点积叉积和对偶性\"><a href=\"#点积叉积和对偶性\" class=\"headerlink\" title=\"点积叉积和对偶性\"></a>点积叉积和对偶性</h2><p>这部分的对偶性这个概念比较玄一些，这里就不讲了。。。对于向量的点积，我们已经知道，</p>\n<script type=\"math/tex; mode=display\">\\langle v, u \\rangle = \\sum_{i=1}^{n}v_iu_i</script><p>从几何的角度看，向量的点积是将向量$u$向向量$v$的方向投影（有正负），投影长度乘上$v$的长度得到的。</p>\n<p>按照上面矩阵乘法的思路，$u$可以看做代表一个从2维到1维的线性变换（也就是从$\\mathbb{R}^2$到$\\mathbb{R}$）。而根据上述矩阵变换和矩阵乘法的对应关系可知，变换后的结果就等于$v$的两个分量分别乘上两个基向量（也就是$u$的两个分量），这样我们就得到了向量点积的数值计算方法。</p>"},{"title":"在Caffe中使用Baidu warpctc实现CTC Loss的计算","date":"2017-02-22T07:34:32.000Z","_content":"CTC(Connectionist Temporal Classification) Loss 函数多用于序列有监督学习，优点是不需要对齐输入数据及标签。本文内容并不涉及CTC Loss的原理介绍，而是关于如何在Caffe中移植Baidu美研院实现的[warp-ctc](https://github.com/baidu-research/warp-ctc)，并利用其实现一个LSTM + CTC Loss的验证码识别demo。下面这张图引用自warp-ctc的[项目页面](https://github.com/baidu-research/warp-ctc)。本文介绍内容的相关代码可以参见我的GitHub项目[warpctc-caffe](https://github.com/xmfbit/warpctc-caffe)\n![CTC Loss](/img/warpctc_intro.png)\n<!-- more -->\n\n## 移植warp-ctc\n本节介绍了如何将`warp-ctc`的源码在Caffe中进行编译。\n\n首先，我们将`warp-ctc`的项目代码从GitHub上clone下来。在Caffe的`include/caffe`和`src/caffe`下分别创建名为`3rdparty`的文件夹，将warp-ctc中的头文件和实现文件分别放到对应的文件夹下。之后，我们需要对其代码和配置进行修改，才能在Caffe中顺利编译。\n\n由于`warp-ctc`中使用了`C++11`的相关技术，所以需要修改Caffe的`Makefile`文件，添加`C++11`支持，可以参见[Makefile](https://github.com/xmfbit/warpctc-caffe/blob/master/Makefile)。\n\n对Caffe的修改就是这么简单，之后我们需要修改`warp-ctc`中的代码文件。这里的修改多且乱，边改边试着编译，所以可能其中也有不必要的修改。最后的目的就是能够使用GPU进行CTC Loss的计算。\n\n`warp-ctc`提供了CPU多线程的计算，这里我直接将相应的`openmp`并行化语句删掉了。\n\n另外，需要将warp-ctc中包含CUDA代码的相关头文件后缀名改为`cuh`，这样才能够通过编译。否则编译器会给出找不到`__host__`和`__device__`等等关键字的错误。\n\n对于详细的修改配置，还请参见GitHub相应的[代码文件](https://github.com/xmfbit/warpctc-caffe/blob/master/include/caffe/3rdparty/detail/hostdevice.cuh)。\n\n## 实现CTC Loss计算\n编译没有问题后，我们可以编写`ctc_loss_layer`实现CTC Loss的计算。在实现时，注意参考文件`ctc.h`。这个文件中给出了使用`warp-ctc`进行CTC Loss计算的全部API接口。\n\n`ctc_loss_layer`继承自`loss_layer`，主要是前向和反向计算的实现。由于`warp-ctc`中只对单精度浮点数`float`进行支持，所以，对于双精度网络参数，直接将其设置为`NOT_IMPLEMENTED`，如下所示。\n\n``` cpp\ntemplate <>\nvoid CtcLossLayer<double>::Forward_cpu(\n    const vector<Blob<double>*>& bottom, const vector<Blob<double>*>& top) {\n    NOT_IMPLEMENTED;\n}\n\ntemplate <>\nvoid CtcLossLayer<double>::Backward_cpu(const vector<Blob<double>*>& top,\n    const vector<bool>& propagate_down, const vector<Blob<double>*>& bottom) {\n    NOT_IMPLEMENTED;\n}\n```\n\n使用`warp-ctc`相关接口进行CTC Loss计算的步骤如下：\n\n- 设置`ctcOptions`，指定使用CPU或GPU进行计算，并指定CPU计算的线程数和GPU计算的CUDA stream。\n- 调用`get_workspace_size()`函数，预先为计算分配空间（分配的内存根据计算平台不同位于CPU或GPU）。\n- 调用`compute_ctc_loss()`函数，计算`loss`和`gradient`。\n\n其中，在第三步中计算`gradient`时，可以直接将对应`blob`的`cpu/gpu_diff`指针传入，作为`gradient`。\n\n这部分的实现代码分别位于`include/caffe/layers`和`src/caffe/layers/`下。\n\n## 验证码数字识别\n本部分相关代码位于`examples/warpctc`文件夹下。实验方案如下。\n\n- 使用`Python`中的`capycha`进行包含`0-9`数字的验证码图片的产生，图片中数字个数从`1`到`MAX_LEN`不等。\n- 使用`10`作为`blank_label`，将所有的标签序列在后面补`blank_label`以达到同样的长度`MAX_LEN`。\n- 将图像的每一列看做一个time step，网络模型使用`image data->2LSTM->fc->CTC Loss`，简单粗暴。\n- 模型训练过程中，数据输入使用`HDF5`格式。\n\n### 数据产生\n使用`captcha`生成验证码图片。[这里](https://pypi.python.org/pypi/captcha/0.1.1)是一个简单的API demo。默认生成的图片大小为`160x60`。我们将其长宽缩小一半，使用`80x30`的彩色图片作为输入。\n\n使用`python`中的`h5py`模块生成`HDF5`格式的数据文件。将全部图片分为两部分，80%作为训练集，20%作为验证集。\n\n### LSTM的输入\n在Caffe中已经有了`lstm_layer`的实现。`lstm_layer`要求输入的序列`blob`为`TxNx...`，也就是说我们需要将输入的image进行转置。\n\nCaffe中Batch的内存布局顺序为`NxCxHxW`。我们将图像中的每一列作为一个time step输入的$x$向量。所以，在代码中使用了[liuwei的SSD工作中实现的`permute_layer`](https://github.com/weiliu89/caffe/blob/ssd/include/caffe/layers/permute_layer.hpp)进行转置，将`W`维度放到最前方。与之对应的参数定义如下：\n\n```\nlayer {\n    name: \"permuted_data\"\n    type: \"Permute\"\n    bottom: \"data\"\n    top: \"permuted_data\"\n    permute_param {\n        order: 3   # W\n        order: 0   # N\n        order: 1   # C\n        order: 2   # H\n    }\n}\n```\n\n另外，LSTM需要第二个输入，用于指示时序信号的起始位置。在代码中，我新加入了一个名为`ContinuationIndicator`的layer，产生对应的time indicator序列。\n\n### 训练\n在某次试验中，迭代50,000次，实验过程中的损失函数变化如下：\n\n![train loss](/img/captcha_train_loss.png)\n\n在验证集上的精度变化如下：\n\n![test accuracy](/img/captcha_test_accuracy.png)\n\n最终模型的精度在98%左右。考虑到本实验只是简单堆叠了两层的LSTM，并使用CTC Loss进行训练，能够轻易达到这一精度，可以在一定程度上说明CTC Loss的强大。\n\n至于该实验的具体细节，可以参考repo的相关具体代码实现。\n","source":"_posts/warpctc-caffe.md","raw":"---\ntitle: 在Caffe中使用Baidu warpctc实现CTC Loss的计算\ndate: 2017-02-22 15:34:32\ntags:\n     - caffe\n     - deep learning\n---\nCTC(Connectionist Temporal Classification) Loss 函数多用于序列有监督学习，优点是不需要对齐输入数据及标签。本文内容并不涉及CTC Loss的原理介绍，而是关于如何在Caffe中移植Baidu美研院实现的[warp-ctc](https://github.com/baidu-research/warp-ctc)，并利用其实现一个LSTM + CTC Loss的验证码识别demo。下面这张图引用自warp-ctc的[项目页面](https://github.com/baidu-research/warp-ctc)。本文介绍内容的相关代码可以参见我的GitHub项目[warpctc-caffe](https://github.com/xmfbit/warpctc-caffe)\n![CTC Loss](/img/warpctc_intro.png)\n<!-- more -->\n\n## 移植warp-ctc\n本节介绍了如何将`warp-ctc`的源码在Caffe中进行编译。\n\n首先，我们将`warp-ctc`的项目代码从GitHub上clone下来。在Caffe的`include/caffe`和`src/caffe`下分别创建名为`3rdparty`的文件夹，将warp-ctc中的头文件和实现文件分别放到对应的文件夹下。之后，我们需要对其代码和配置进行修改，才能在Caffe中顺利编译。\n\n由于`warp-ctc`中使用了`C++11`的相关技术，所以需要修改Caffe的`Makefile`文件，添加`C++11`支持，可以参见[Makefile](https://github.com/xmfbit/warpctc-caffe/blob/master/Makefile)。\n\n对Caffe的修改就是这么简单，之后我们需要修改`warp-ctc`中的代码文件。这里的修改多且乱，边改边试着编译，所以可能其中也有不必要的修改。最后的目的就是能够使用GPU进行CTC Loss的计算。\n\n`warp-ctc`提供了CPU多线程的计算，这里我直接将相应的`openmp`并行化语句删掉了。\n\n另外，需要将warp-ctc中包含CUDA代码的相关头文件后缀名改为`cuh`，这样才能够通过编译。否则编译器会给出找不到`__host__`和`__device__`等等关键字的错误。\n\n对于详细的修改配置，还请参见GitHub相应的[代码文件](https://github.com/xmfbit/warpctc-caffe/blob/master/include/caffe/3rdparty/detail/hostdevice.cuh)。\n\n## 实现CTC Loss计算\n编译没有问题后，我们可以编写`ctc_loss_layer`实现CTC Loss的计算。在实现时，注意参考文件`ctc.h`。这个文件中给出了使用`warp-ctc`进行CTC Loss计算的全部API接口。\n\n`ctc_loss_layer`继承自`loss_layer`，主要是前向和反向计算的实现。由于`warp-ctc`中只对单精度浮点数`float`进行支持，所以，对于双精度网络参数，直接将其设置为`NOT_IMPLEMENTED`，如下所示。\n\n``` cpp\ntemplate <>\nvoid CtcLossLayer<double>::Forward_cpu(\n    const vector<Blob<double>*>& bottom, const vector<Blob<double>*>& top) {\n    NOT_IMPLEMENTED;\n}\n\ntemplate <>\nvoid CtcLossLayer<double>::Backward_cpu(const vector<Blob<double>*>& top,\n    const vector<bool>& propagate_down, const vector<Blob<double>*>& bottom) {\n    NOT_IMPLEMENTED;\n}\n```\n\n使用`warp-ctc`相关接口进行CTC Loss计算的步骤如下：\n\n- 设置`ctcOptions`，指定使用CPU或GPU进行计算，并指定CPU计算的线程数和GPU计算的CUDA stream。\n- 调用`get_workspace_size()`函数，预先为计算分配空间（分配的内存根据计算平台不同位于CPU或GPU）。\n- 调用`compute_ctc_loss()`函数，计算`loss`和`gradient`。\n\n其中，在第三步中计算`gradient`时，可以直接将对应`blob`的`cpu/gpu_diff`指针传入，作为`gradient`。\n\n这部分的实现代码分别位于`include/caffe/layers`和`src/caffe/layers/`下。\n\n## 验证码数字识别\n本部分相关代码位于`examples/warpctc`文件夹下。实验方案如下。\n\n- 使用`Python`中的`capycha`进行包含`0-9`数字的验证码图片的产生，图片中数字个数从`1`到`MAX_LEN`不等。\n- 使用`10`作为`blank_label`，将所有的标签序列在后面补`blank_label`以达到同样的长度`MAX_LEN`。\n- 将图像的每一列看做一个time step，网络模型使用`image data->2LSTM->fc->CTC Loss`，简单粗暴。\n- 模型训练过程中，数据输入使用`HDF5`格式。\n\n### 数据产生\n使用`captcha`生成验证码图片。[这里](https://pypi.python.org/pypi/captcha/0.1.1)是一个简单的API demo。默认生成的图片大小为`160x60`。我们将其长宽缩小一半，使用`80x30`的彩色图片作为输入。\n\n使用`python`中的`h5py`模块生成`HDF5`格式的数据文件。将全部图片分为两部分，80%作为训练集，20%作为验证集。\n\n### LSTM的输入\n在Caffe中已经有了`lstm_layer`的实现。`lstm_layer`要求输入的序列`blob`为`TxNx...`，也就是说我们需要将输入的image进行转置。\n\nCaffe中Batch的内存布局顺序为`NxCxHxW`。我们将图像中的每一列作为一个time step输入的$x$向量。所以，在代码中使用了[liuwei的SSD工作中实现的`permute_layer`](https://github.com/weiliu89/caffe/blob/ssd/include/caffe/layers/permute_layer.hpp)进行转置，将`W`维度放到最前方。与之对应的参数定义如下：\n\n```\nlayer {\n    name: \"permuted_data\"\n    type: \"Permute\"\n    bottom: \"data\"\n    top: \"permuted_data\"\n    permute_param {\n        order: 3   # W\n        order: 0   # N\n        order: 1   # C\n        order: 2   # H\n    }\n}\n```\n\n另外，LSTM需要第二个输入，用于指示时序信号的起始位置。在代码中，我新加入了一个名为`ContinuationIndicator`的layer，产生对应的time indicator序列。\n\n### 训练\n在某次试验中，迭代50,000次，实验过程中的损失函数变化如下：\n\n![train loss](/img/captcha_train_loss.png)\n\n在验证集上的精度变化如下：\n\n![test accuracy](/img/captcha_test_accuracy.png)\n\n最终模型的精度在98%左右。考虑到本实验只是简单堆叠了两层的LSTM，并使用CTC Loss进行训练，能够轻易达到这一精度，可以在一定程度上说明CTC Loss的强大。\n\n至于该实验的具体细节，可以参考repo的相关具体代码实现。\n","slug":"warpctc-caffe","published":1,"updated":"2018-01-12T06:22:20.484Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjcu6vcum002squ46wwfda2l0","content":"<p>CTC(Connectionist Temporal Classification) Loss 函数多用于序列有监督学习，优点是不需要对齐输入数据及标签。本文内容并不涉及CTC Loss的原理介绍，而是关于如何在Caffe中移植Baidu美研院实现的<a href=\"https://github.com/baidu-research/warp-ctc\" target=\"_blank\" rel=\"external\">warp-ctc</a>，并利用其实现一个LSTM + CTC Loss的验证码识别demo。下面这张图引用自warp-ctc的<a href=\"https://github.com/baidu-research/warp-ctc\" target=\"_blank\" rel=\"external\">项目页面</a>。本文介绍内容的相关代码可以参见我的GitHub项目<a href=\"https://github.com/xmfbit/warpctc-caffe\" target=\"_blank\" rel=\"external\">warpctc-caffe</a><br><img src=\"/img/warpctc_intro.png\" alt=\"CTC Loss\"><br><a id=\"more\"></a></p>\n<h2 id=\"移植warp-ctc\"><a href=\"#移植warp-ctc\" class=\"headerlink\" title=\"移植warp-ctc\"></a>移植warp-ctc</h2><p>本节介绍了如何将<code>warp-ctc</code>的源码在Caffe中进行编译。</p>\n<p>首先，我们将<code>warp-ctc</code>的项目代码从GitHub上clone下来。在Caffe的<code>include/caffe</code>和<code>src/caffe</code>下分别创建名为<code>3rdparty</code>的文件夹，将warp-ctc中的头文件和实现文件分别放到对应的文件夹下。之后，我们需要对其代码和配置进行修改，才能在Caffe中顺利编译。</p>\n<p>由于<code>warp-ctc</code>中使用了<code>C++11</code>的相关技术，所以需要修改Caffe的<code>Makefile</code>文件，添加<code>C++11</code>支持，可以参见<a href=\"https://github.com/xmfbit/warpctc-caffe/blob/master/Makefile\" target=\"_blank\" rel=\"external\">Makefile</a>。</p>\n<p>对Caffe的修改就是这么简单，之后我们需要修改<code>warp-ctc</code>中的代码文件。这里的修改多且乱，边改边试着编译，所以可能其中也有不必要的修改。最后的目的就是能够使用GPU进行CTC Loss的计算。</p>\n<p><code>warp-ctc</code>提供了CPU多线程的计算，这里我直接将相应的<code>openmp</code>并行化语句删掉了。</p>\n<p>另外，需要将warp-ctc中包含CUDA代码的相关头文件后缀名改为<code>cuh</code>，这样才能够通过编译。否则编译器会给出找不到<code>__host__</code>和<code>__device__</code>等等关键字的错误。</p>\n<p>对于详细的修改配置，还请参见GitHub相应的<a href=\"https://github.com/xmfbit/warpctc-caffe/blob/master/include/caffe/3rdparty/detail/hostdevice.cuh\" target=\"_blank\" rel=\"external\">代码文件</a>。</p>\n<h2 id=\"实现CTC-Loss计算\"><a href=\"#实现CTC-Loss计算\" class=\"headerlink\" title=\"实现CTC Loss计算\"></a>实现CTC Loss计算</h2><p>编译没有问题后，我们可以编写<code>ctc_loss_layer</code>实现CTC Loss的计算。在实现时，注意参考文件<code>ctc.h</code>。这个文件中给出了使用<code>warp-ctc</code>进行CTC Loss计算的全部API接口。</p>\n<p><code>ctc_loss_layer</code>继承自<code>loss_layer</code>，主要是前向和反向计算的实现。由于<code>warp-ctc</code>中只对单精度浮点数<code>float</code>进行支持，所以，对于双精度网络参数，直接将其设置为<code>NOT_IMPLEMENTED</code>，如下所示。</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">template</span> &lt;&gt;</div><div class=\"line\"><span class=\"keyword\">void</span> CtcLossLayer&lt;<span class=\"keyword\">double</span>&gt;::Forward_cpu(</div><div class=\"line\">    <span class=\"keyword\">const</span> <span class=\"built_in\">vector</span>&lt;Blob&lt;<span class=\"keyword\">double</span>&gt;*&gt;&amp; bottom, <span class=\"keyword\">const</span> <span class=\"built_in\">vector</span>&lt;Blob&lt;<span class=\"keyword\">double</span>&gt;*&gt;&amp; top) &#123;</div><div class=\"line\">    NOT_IMPLEMENTED;</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\"><span class=\"keyword\">template</span> &lt;&gt;</div><div class=\"line\"><span class=\"keyword\">void</span> CtcLossLayer&lt;<span class=\"keyword\">double</span>&gt;::Backward_cpu(<span class=\"keyword\">const</span> <span class=\"built_in\">vector</span>&lt;Blob&lt;<span class=\"keyword\">double</span>&gt;*&gt;&amp; top,</div><div class=\"line\">    <span class=\"keyword\">const</span> <span class=\"built_in\">vector</span>&lt;<span class=\"keyword\">bool</span>&gt;&amp; propagate_down, <span class=\"keyword\">const</span> <span class=\"built_in\">vector</span>&lt;Blob&lt;<span class=\"keyword\">double</span>&gt;*&gt;&amp; bottom) &#123;</div><div class=\"line\">    NOT_IMPLEMENTED;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<p>使用<code>warp-ctc</code>相关接口进行CTC Loss计算的步骤如下：</p>\n<ul>\n<li>设置<code>ctcOptions</code>，指定使用CPU或GPU进行计算，并指定CPU计算的线程数和GPU计算的CUDA stream。</li>\n<li>调用<code>get_workspace_size()</code>函数，预先为计算分配空间（分配的内存根据计算平台不同位于CPU或GPU）。</li>\n<li>调用<code>compute_ctc_loss()</code>函数，计算<code>loss</code>和<code>gradient</code>。</li>\n</ul>\n<p>其中，在第三步中计算<code>gradient</code>时，可以直接将对应<code>blob</code>的<code>cpu/gpu_diff</code>指针传入，作为<code>gradient</code>。</p>\n<p>这部分的实现代码分别位于<code>include/caffe/layers</code>和<code>src/caffe/layers/</code>下。</p>\n<h2 id=\"验证码数字识别\"><a href=\"#验证码数字识别\" class=\"headerlink\" title=\"验证码数字识别\"></a>验证码数字识别</h2><p>本部分相关代码位于<code>examples/warpctc</code>文件夹下。实验方案如下。</p>\n<ul>\n<li>使用<code>Python</code>中的<code>capycha</code>进行包含<code>0-9</code>数字的验证码图片的产生，图片中数字个数从<code>1</code>到<code>MAX_LEN</code>不等。</li>\n<li>使用<code>10</code>作为<code>blank_label</code>，将所有的标签序列在后面补<code>blank_label</code>以达到同样的长度<code>MAX_LEN</code>。</li>\n<li>将图像的每一列看做一个time step，网络模型使用<code>image data-&gt;2LSTM-&gt;fc-&gt;CTC Loss</code>，简单粗暴。</li>\n<li>模型训练过程中，数据输入使用<code>HDF5</code>格式。</li>\n</ul>\n<h3 id=\"数据产生\"><a href=\"#数据产生\" class=\"headerlink\" title=\"数据产生\"></a>数据产生</h3><p>使用<code>captcha</code>生成验证码图片。<a href=\"https://pypi.python.org/pypi/captcha/0.1.1\" target=\"_blank\" rel=\"external\">这里</a>是一个简单的API demo。默认生成的图片大小为<code>160x60</code>。我们将其长宽缩小一半，使用<code>80x30</code>的彩色图片作为输入。</p>\n<p>使用<code>python</code>中的<code>h5py</code>模块生成<code>HDF5</code>格式的数据文件。将全部图片分为两部分，80%作为训练集，20%作为验证集。</p>\n<h3 id=\"LSTM的输入\"><a href=\"#LSTM的输入\" class=\"headerlink\" title=\"LSTM的输入\"></a>LSTM的输入</h3><p>在Caffe中已经有了<code>lstm_layer</code>的实现。<code>lstm_layer</code>要求输入的序列<code>blob</code>为<code>TxNx...</code>，也就是说我们需要将输入的image进行转置。</p>\n<p>Caffe中Batch的内存布局顺序为<code>NxCxHxW</code>。我们将图像中的每一列作为一个time step输入的$x$向量。所以，在代码中使用了<a href=\"https://github.com/weiliu89/caffe/blob/ssd/include/caffe/layers/permute_layer.hpp\" target=\"_blank\" rel=\"external\">liuwei的SSD工作中实现的<code>permute_layer</code></a>进行转置，将<code>W</code>维度放到最前方。与之对应的参数定义如下：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div></pre></td><td class=\"code\"><pre><div class=\"line\">layer &#123;</div><div class=\"line\">    name: &quot;permuted_data&quot;</div><div class=\"line\">    type: &quot;Permute&quot;</div><div class=\"line\">    bottom: &quot;data&quot;</div><div class=\"line\">    top: &quot;permuted_data&quot;</div><div class=\"line\">    permute_param &#123;</div><div class=\"line\">        order: 3   # W</div><div class=\"line\">        order: 0   # N</div><div class=\"line\">        order: 1   # C</div><div class=\"line\">        order: 2   # H</div><div class=\"line\">    &#125;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<p>另外，LSTM需要第二个输入，用于指示时序信号的起始位置。在代码中，我新加入了一个名为<code>ContinuationIndicator</code>的layer，产生对应的time indicator序列。</p>\n<h3 id=\"训练\"><a href=\"#训练\" class=\"headerlink\" title=\"训练\"></a>训练</h3><p>在某次试验中，迭代50,000次，实验过程中的损失函数变化如下：</p>\n<p><img src=\"/img/captcha_train_loss.png\" alt=\"train loss\"></p>\n<p>在验证集上的精度变化如下：</p>\n<p><img src=\"/img/captcha_test_accuracy.png\" alt=\"test accuracy\"></p>\n<p>最终模型的精度在98%左右。考虑到本实验只是简单堆叠了两层的LSTM，并使用CTC Loss进行训练，能够轻易达到这一精度，可以在一定程度上说明CTC Loss的强大。</p>\n<p>至于该实验的具体细节，可以参考repo的相关具体代码实现。</p>\n","excerpt":"<p>CTC(Connectionist Temporal Classification) Loss 函数多用于序列有监督学习，优点是不需要对齐输入数据及标签。本文内容并不涉及CTC Loss的原理介绍，而是关于如何在Caffe中移植Baidu美研院实现的<a href=\"https://github.com/baidu-research/warp-ctc\">warp-ctc</a>，并利用其实现一个LSTM + CTC Loss的验证码识别demo。下面这张图引用自warp-ctc的<a href=\"https://github.com/baidu-research/warp-ctc\">项目页面</a>。本文介绍内容的相关代码可以参见我的GitHub项目<a href=\"https://github.com/xmfbit/warpctc-caffe\">warpctc-caffe</a><br><img src=\"/img/warpctc_intro.png\" alt=\"CTC Loss\"><br>","more":"</p>\n<h2 id=\"移植warp-ctc\"><a href=\"#移植warp-ctc\" class=\"headerlink\" title=\"移植warp-ctc\"></a>移植warp-ctc</h2><p>本节介绍了如何将<code>warp-ctc</code>的源码在Caffe中进行编译。</p>\n<p>首先，我们将<code>warp-ctc</code>的项目代码从GitHub上clone下来。在Caffe的<code>include/caffe</code>和<code>src/caffe</code>下分别创建名为<code>3rdparty</code>的文件夹，将warp-ctc中的头文件和实现文件分别放到对应的文件夹下。之后，我们需要对其代码和配置进行修改，才能在Caffe中顺利编译。</p>\n<p>由于<code>warp-ctc</code>中使用了<code>C++11</code>的相关技术，所以需要修改Caffe的<code>Makefile</code>文件，添加<code>C++11</code>支持，可以参见<a href=\"https://github.com/xmfbit/warpctc-caffe/blob/master/Makefile\">Makefile</a>。</p>\n<p>对Caffe的修改就是这么简单，之后我们需要修改<code>warp-ctc</code>中的代码文件。这里的修改多且乱，边改边试着编译，所以可能其中也有不必要的修改。最后的目的就是能够使用GPU进行CTC Loss的计算。</p>\n<p><code>warp-ctc</code>提供了CPU多线程的计算，这里我直接将相应的<code>openmp</code>并行化语句删掉了。</p>\n<p>另外，需要将warp-ctc中包含CUDA代码的相关头文件后缀名改为<code>cuh</code>，这样才能够通过编译。否则编译器会给出找不到<code>__host__</code>和<code>__device__</code>等等关键字的错误。</p>\n<p>对于详细的修改配置，还请参见GitHub相应的<a href=\"https://github.com/xmfbit/warpctc-caffe/blob/master/include/caffe/3rdparty/detail/hostdevice.cuh\">代码文件</a>。</p>\n<h2 id=\"实现CTC-Loss计算\"><a href=\"#实现CTC-Loss计算\" class=\"headerlink\" title=\"实现CTC Loss计算\"></a>实现CTC Loss计算</h2><p>编译没有问题后，我们可以编写<code>ctc_loss_layer</code>实现CTC Loss的计算。在实现时，注意参考文件<code>ctc.h</code>。这个文件中给出了使用<code>warp-ctc</code>进行CTC Loss计算的全部API接口。</p>\n<p><code>ctc_loss_layer</code>继承自<code>loss_layer</code>，主要是前向和反向计算的实现。由于<code>warp-ctc</code>中只对单精度浮点数<code>float</code>进行支持，所以，对于双精度网络参数，直接将其设置为<code>NOT_IMPLEMENTED</code>，如下所示。</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">template</span> &lt;&gt;</div><div class=\"line\"><span class=\"keyword\">void</span> CtcLossLayer&lt;<span class=\"keyword\">double</span>&gt;::Forward_cpu(</div><div class=\"line\">    <span class=\"keyword\">const</span> <span class=\"built_in\">vector</span>&lt;Blob&lt;<span class=\"keyword\">double</span>&gt;*&gt;&amp; bottom, <span class=\"keyword\">const</span> <span class=\"built_in\">vector</span>&lt;Blob&lt;<span class=\"keyword\">double</span>&gt;*&gt;&amp; top) &#123;</div><div class=\"line\">    NOT_IMPLEMENTED;</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\"><span class=\"keyword\">template</span> &lt;&gt;</div><div class=\"line\"><span class=\"keyword\">void</span> CtcLossLayer&lt;<span class=\"keyword\">double</span>&gt;::Backward_cpu(<span class=\"keyword\">const</span> <span class=\"built_in\">vector</span>&lt;Blob&lt;<span class=\"keyword\">double</span>&gt;*&gt;&amp; top,</div><div class=\"line\">    <span class=\"keyword\">const</span> <span class=\"built_in\">vector</span>&lt;<span class=\"keyword\">bool</span>&gt;&amp; propagate_down, <span class=\"keyword\">const</span> <span class=\"built_in\">vector</span>&lt;Blob&lt;<span class=\"keyword\">double</span>&gt;*&gt;&amp; bottom) &#123;</div><div class=\"line\">    NOT_IMPLEMENTED;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<p>使用<code>warp-ctc</code>相关接口进行CTC Loss计算的步骤如下：</p>\n<ul>\n<li>设置<code>ctcOptions</code>，指定使用CPU或GPU进行计算，并指定CPU计算的线程数和GPU计算的CUDA stream。</li>\n<li>调用<code>get_workspace_size()</code>函数，预先为计算分配空间（分配的内存根据计算平台不同位于CPU或GPU）。</li>\n<li>调用<code>compute_ctc_loss()</code>函数，计算<code>loss</code>和<code>gradient</code>。</li>\n</ul>\n<p>其中，在第三步中计算<code>gradient</code>时，可以直接将对应<code>blob</code>的<code>cpu/gpu_diff</code>指针传入，作为<code>gradient</code>。</p>\n<p>这部分的实现代码分别位于<code>include/caffe/layers</code>和<code>src/caffe/layers/</code>下。</p>\n<h2 id=\"验证码数字识别\"><a href=\"#验证码数字识别\" class=\"headerlink\" title=\"验证码数字识别\"></a>验证码数字识别</h2><p>本部分相关代码位于<code>examples/warpctc</code>文件夹下。实验方案如下。</p>\n<ul>\n<li>使用<code>Python</code>中的<code>capycha</code>进行包含<code>0-9</code>数字的验证码图片的产生，图片中数字个数从<code>1</code>到<code>MAX_LEN</code>不等。</li>\n<li>使用<code>10</code>作为<code>blank_label</code>，将所有的标签序列在后面补<code>blank_label</code>以达到同样的长度<code>MAX_LEN</code>。</li>\n<li>将图像的每一列看做一个time step，网络模型使用<code>image data-&gt;2LSTM-&gt;fc-&gt;CTC Loss</code>，简单粗暴。</li>\n<li>模型训练过程中，数据输入使用<code>HDF5</code>格式。</li>\n</ul>\n<h3 id=\"数据产生\"><a href=\"#数据产生\" class=\"headerlink\" title=\"数据产生\"></a>数据产生</h3><p>使用<code>captcha</code>生成验证码图片。<a href=\"https://pypi.python.org/pypi/captcha/0.1.1\">这里</a>是一个简单的API demo。默认生成的图片大小为<code>160x60</code>。我们将其长宽缩小一半，使用<code>80x30</code>的彩色图片作为输入。</p>\n<p>使用<code>python</code>中的<code>h5py</code>模块生成<code>HDF5</code>格式的数据文件。将全部图片分为两部分，80%作为训练集，20%作为验证集。</p>\n<h3 id=\"LSTM的输入\"><a href=\"#LSTM的输入\" class=\"headerlink\" title=\"LSTM的输入\"></a>LSTM的输入</h3><p>在Caffe中已经有了<code>lstm_layer</code>的实现。<code>lstm_layer</code>要求输入的序列<code>blob</code>为<code>TxNx...</code>，也就是说我们需要将输入的image进行转置。</p>\n<p>Caffe中Batch的内存布局顺序为<code>NxCxHxW</code>。我们将图像中的每一列作为一个time step输入的$x$向量。所以，在代码中使用了<a href=\"https://github.com/weiliu89/caffe/blob/ssd/include/caffe/layers/permute_layer.hpp\">liuwei的SSD工作中实现的<code>permute_layer</code></a>进行转置，将<code>W</code>维度放到最前方。与之对应的参数定义如下：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div></pre></td><td class=\"code\"><pre><div class=\"line\">layer &#123;</div><div class=\"line\">    name: &quot;permuted_data&quot;</div><div class=\"line\">    type: &quot;Permute&quot;</div><div class=\"line\">    bottom: &quot;data&quot;</div><div class=\"line\">    top: &quot;permuted_data&quot;</div><div class=\"line\">    permute_param &#123;</div><div class=\"line\">        order: 3   # W</div><div class=\"line\">        order: 0   # N</div><div class=\"line\">        order: 1   # C</div><div class=\"line\">        order: 2   # H</div><div class=\"line\">    &#125;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<p>另外，LSTM需要第二个输入，用于指示时序信号的起始位置。在代码中，我新加入了一个名为<code>ContinuationIndicator</code>的layer，产生对应的time indicator序列。</p>\n<h3 id=\"训练\"><a href=\"#训练\" class=\"headerlink\" title=\"训练\"></a>训练</h3><p>在某次试验中，迭代50,000次，实验过程中的损失函数变化如下：</p>\n<p><img src=\"/img/captcha_train_loss.png\" alt=\"train loss\"></p>\n<p>在验证集上的精度变化如下：</p>\n<p><img src=\"/img/captcha_test_accuracy.png\" alt=\"test accuracy\"></p>\n<p>最终模型的精度在98%左右。考虑到本实验只是简单堆叠了两层的LSTM，并使用CTC Loss进行训练，能够轻易达到这一精度，可以在一定程度上说明CTC Loss的强大。</p>\n<p>至于该实验的具体细节，可以参考repo的相关具体代码实现。</p>"},{"title":"YOLO网络参数的解析与存储","date":"2017-03-06T07:51:22.000Z","_content":"YOLO的原作者使用了自己开发的Darknet框架，而没有选取当前流行的其他深度学习框架实现算法，所以有必要对其网络模型的参数解析与存储方式做一了解，方便阅读源码和在其他流行的框架下的算法移植。\n<!-- more -->\n\n## YOLO网络结构定义的CFG文件\nYOLO中的网络定义采用和Caffe类似的方式，都是通过一个顺序堆叠layer来对神经网络结构进行定义的文件来描述。不同的地方在于，Caffe中使用了Google家出品的protobuf，省时省力，无需自己实现解析文件的功能，但是也使得Caffe对第三方库的依赖更加严重。相信很多人在编译Caffe的时候都出现过无法链接等蛋疼无比的问题。而YOLO的作者则是使用了自己定义的一种CFG文件格式，需要自己实现解析功能。\n\nCFG文件的格式可以归纳如下（可以打开某个[CFG文件](https://github.com/pjreddie/darknet/blob/master/cfg/yolo-voc.cfg)进行对照）：\n```\n[net]\n# 这里会对net的参数进行配置\n# 同时YOLO将对net的求解器的参数也放在了这里\n[conv]\n# 一些conv层的参数描述\n[maxpool]\n# 一些池化层的参数描述\n\n# 顺序堆叠的其他layer描述\n```\n在Darknet的代码中，将每个`[]`符号导引的参数列表叫做section。\n\n## 网络结构解析器 Parser\n具体的解析实现参见[parser.c文件](https://github.com/pjreddie/darknet/blob/master/src/parser.c)。我们先以`convolutional_layer parse_convolutional(list *options, size_params params)`函数为例，看一下Darknet是如何完成对卷积层参数的解析的。\n\n从函数签名可以看出，这个函数接受一个`list`的变量（Darknet中将堆叠起来的这些层描述抽象成链表），而`size_params`类型的变量`params`指示了该层上一层的参数情况，其具体定义如下：\n``` cpp\ntypedef struct size_params{\n    int batch;\n    int inputs;\n    int h;\n    int w;\n    int c;\n    int index;\n    int time_steps;\n    network net;\n} size_params;\n```\n这样，在构建该层卷积层的时候，我们就能够知道上一层的输入维度等信息，方便做一些参数检查和layer初始化等的工作。\n\n进入函数内部，会发现频繁出现`option_find_int`这个函数。从函数名字面意义看，应该是要解析字符串中的整型数。\n\n我们首先来看一下这个函数的定义吧~这个函数并不在`parser.c`中，而是在[option_list.c 文件](https://github.com/pjreddie/darknet/blob/master/src/option_list.c)中。\n\n``` cpp\n// l: data pointer to the list\n// key: the key to find, example: \"filters\", \"padding\"\n// def: default value\nint option_find_int(list *l, char *key, int def)\n{\n    // 去找到该key对应的数值，使用atoi转换为整型数\n    char *v = option_find(l, key);\n    if(v) return atoi(v);\n    // 使用XXX_quiet版本可以不打印此信息\n    fprintf(stderr, \"%s: Using default '%d'\\n\", key, def);\n    // 没有找到key，返回默认值\n    return def;\n}\n```\n\n而其中的`option_find`函数则是逐项顺序查找，匹配字符串来实现的。\n\n``` cpp\nchar *option_find(list *l, char *key)\n{\n    node *n = l->front;\n    while(n){\n        kvp *p = (kvp *)n->val;\n        if(strcmp(p->key, key) == 0){\n            p->used = 1;\n            return p->val;\n        }\n        n = n->next;\n    }\n    return 0;\n}\n```\n\n## 构建conv层\n由此，我们可以通过CFG文件得到卷积层的参数了。接下来需要调用其初始化函数，进行构建。\n\n``` cpp\n    // 首先得到参数\n    int n = option_find_int(options, \"filters\",1);\n    int size = option_find_int(options, \"size\",1);\n    int stride = option_find_int(options, \"stride\",1);\n    int pad = option_find_int_quiet(options, \"pad\",0);\n    int padding = option_find_int_quiet(options, \"padding\",0);\n    if(pad) padding = size/2;\n\t// 激活函数是通过匹配其名称的方法得到的\n    char *activation_s = option_find_str(options, \"activation\", \"logistic\");\n    ACTIVATION activation = get_activation(activation_s);\n    // 通过上层的信息得到batch size，做参数检查\n    int batch,h,w,c;\n    h = params.h;\n    w = params.w;\n    c = params.c;\n    batch=params.batch;\n    if(!(h && w && c)) error(\"Layer before convolutional layer must output image.\");\n    int batch_normalize = option_find_int_quiet(options, \"batch_normalize\", 0);\n    int binary = option_find_int_quiet(options, \"binary\", 0);\n    int xnor = option_find_int_quiet(options, \"xnor\", 0);\n    // 调用初始化函数\n    convolutional_layer layer = make_convolutional_layer(batch,h,w,c,n,size,stride,padding,activation, batch_normalize, binary, xnor, params.net.adam);\n    layer.flipped = option_find_int_quiet(options, \"flipped\", 0);\n    layer.dot = option_find_float_quiet(options, \"dot\", 0);\n```\n\n所以，如果在阅读源码时候，对layer的某个成员变量不知道什么意思的话，可以参考此文件，看一下原始解析对应的字符串是什么，一般这个字符串描述是比较具体的。\n\n## 构建网络\n有了各个layer的解析方法，接下来就可以逐层读取参数信息并构建网络了。\n\nDarknet中对应的函数为`network parse_network_cfg(char *filename)`，这个函数接受文件名为参数，进行网络结构的解析。\n\n首先，调用`read_cfg(filename)`得到CFG文件的一个层次链表，接着只要对这个链表进行解析就好了。不过对第一个section，也就是`[net]` section，要特殊对待。这里不再多说了。\n\n## 保存参数信息\nDarknet中保存带参数的layer的信息是直接写入二进制文件。仍然以卷积层为例，其保存代码如下所示：\n\n``` cpp\nvoid save_convolutional_weights(layer l, FILE *fp)\n{\n    if(l.binary){\n        //save_convolutional_weights_binary(l, fp);\n        //return;\n    }\n#ifdef GPU\n    if(gpu_index >= 0){\n        pull_convolutional_layer(l);\n    }\n#endif\n    int num = l.n*l.c*l.size*l.size;\n    fwrite(l.biases, sizeof(float), l.n, fp);\n    // 由于darknet设计时，没有单独设计BN层，所以BN的参数也是和其所在的层一起保存的，如果读取时候要注意分别讨论\n    if (l.batch_normalize){\n        fwrite(l.scales, sizeof(float), l.n, fp);\n        fwrite(l.rolling_mean, sizeof(float), l.n, fp);\n        fwrite(l.rolling_variance, sizeof(float), l.n, fp);\n    }\n    fwrite(l.weights, sizeof(float), num, fp);\n    if(l.adam){\n        fwrite(l.m, sizeof(float), num, fp);\n        fwrite(l.v, sizeof(float), num, fp);\n    }\n}\n```\n\n在保存整个网络的参数信息的时候，同样逐层保存到同一个二进制文件中就好了。\n","source":"_posts/yolo-cfg-parser.md","raw":"---\ntitle: YOLO网络参数的解析与存储\ndate: 2017-03-06 15:51:22\ntags:\n     - yolo\n     - deep learning\n---\nYOLO的原作者使用了自己开发的Darknet框架，而没有选取当前流行的其他深度学习框架实现算法，所以有必要对其网络模型的参数解析与存储方式做一了解，方便阅读源码和在其他流行的框架下的算法移植。\n<!-- more -->\n\n## YOLO网络结构定义的CFG文件\nYOLO中的网络定义采用和Caffe类似的方式，都是通过一个顺序堆叠layer来对神经网络结构进行定义的文件来描述。不同的地方在于，Caffe中使用了Google家出品的protobuf，省时省力，无需自己实现解析文件的功能，但是也使得Caffe对第三方库的依赖更加严重。相信很多人在编译Caffe的时候都出现过无法链接等蛋疼无比的问题。而YOLO的作者则是使用了自己定义的一种CFG文件格式，需要自己实现解析功能。\n\nCFG文件的格式可以归纳如下（可以打开某个[CFG文件](https://github.com/pjreddie/darknet/blob/master/cfg/yolo-voc.cfg)进行对照）：\n```\n[net]\n# 这里会对net的参数进行配置\n# 同时YOLO将对net的求解器的参数也放在了这里\n[conv]\n# 一些conv层的参数描述\n[maxpool]\n# 一些池化层的参数描述\n\n# 顺序堆叠的其他layer描述\n```\n在Darknet的代码中，将每个`[]`符号导引的参数列表叫做section。\n\n## 网络结构解析器 Parser\n具体的解析实现参见[parser.c文件](https://github.com/pjreddie/darknet/blob/master/src/parser.c)。我们先以`convolutional_layer parse_convolutional(list *options, size_params params)`函数为例，看一下Darknet是如何完成对卷积层参数的解析的。\n\n从函数签名可以看出，这个函数接受一个`list`的变量（Darknet中将堆叠起来的这些层描述抽象成链表），而`size_params`类型的变量`params`指示了该层上一层的参数情况，其具体定义如下：\n``` cpp\ntypedef struct size_params{\n    int batch;\n    int inputs;\n    int h;\n    int w;\n    int c;\n    int index;\n    int time_steps;\n    network net;\n} size_params;\n```\n这样，在构建该层卷积层的时候，我们就能够知道上一层的输入维度等信息，方便做一些参数检查和layer初始化等的工作。\n\n进入函数内部，会发现频繁出现`option_find_int`这个函数。从函数名字面意义看，应该是要解析字符串中的整型数。\n\n我们首先来看一下这个函数的定义吧~这个函数并不在`parser.c`中，而是在[option_list.c 文件](https://github.com/pjreddie/darknet/blob/master/src/option_list.c)中。\n\n``` cpp\n// l: data pointer to the list\n// key: the key to find, example: \"filters\", \"padding\"\n// def: default value\nint option_find_int(list *l, char *key, int def)\n{\n    // 去找到该key对应的数值，使用atoi转换为整型数\n    char *v = option_find(l, key);\n    if(v) return atoi(v);\n    // 使用XXX_quiet版本可以不打印此信息\n    fprintf(stderr, \"%s: Using default '%d'\\n\", key, def);\n    // 没有找到key，返回默认值\n    return def;\n}\n```\n\n而其中的`option_find`函数则是逐项顺序查找，匹配字符串来实现的。\n\n``` cpp\nchar *option_find(list *l, char *key)\n{\n    node *n = l->front;\n    while(n){\n        kvp *p = (kvp *)n->val;\n        if(strcmp(p->key, key) == 0){\n            p->used = 1;\n            return p->val;\n        }\n        n = n->next;\n    }\n    return 0;\n}\n```\n\n## 构建conv层\n由此，我们可以通过CFG文件得到卷积层的参数了。接下来需要调用其初始化函数，进行构建。\n\n``` cpp\n    // 首先得到参数\n    int n = option_find_int(options, \"filters\",1);\n    int size = option_find_int(options, \"size\",1);\n    int stride = option_find_int(options, \"stride\",1);\n    int pad = option_find_int_quiet(options, \"pad\",0);\n    int padding = option_find_int_quiet(options, \"padding\",0);\n    if(pad) padding = size/2;\n\t// 激活函数是通过匹配其名称的方法得到的\n    char *activation_s = option_find_str(options, \"activation\", \"logistic\");\n    ACTIVATION activation = get_activation(activation_s);\n    // 通过上层的信息得到batch size，做参数检查\n    int batch,h,w,c;\n    h = params.h;\n    w = params.w;\n    c = params.c;\n    batch=params.batch;\n    if(!(h && w && c)) error(\"Layer before convolutional layer must output image.\");\n    int batch_normalize = option_find_int_quiet(options, \"batch_normalize\", 0);\n    int binary = option_find_int_quiet(options, \"binary\", 0);\n    int xnor = option_find_int_quiet(options, \"xnor\", 0);\n    // 调用初始化函数\n    convolutional_layer layer = make_convolutional_layer(batch,h,w,c,n,size,stride,padding,activation, batch_normalize, binary, xnor, params.net.adam);\n    layer.flipped = option_find_int_quiet(options, \"flipped\", 0);\n    layer.dot = option_find_float_quiet(options, \"dot\", 0);\n```\n\n所以，如果在阅读源码时候，对layer的某个成员变量不知道什么意思的话，可以参考此文件，看一下原始解析对应的字符串是什么，一般这个字符串描述是比较具体的。\n\n## 构建网络\n有了各个layer的解析方法，接下来就可以逐层读取参数信息并构建网络了。\n\nDarknet中对应的函数为`network parse_network_cfg(char *filename)`，这个函数接受文件名为参数，进行网络结构的解析。\n\n首先，调用`read_cfg(filename)`得到CFG文件的一个层次链表，接着只要对这个链表进行解析就好了。不过对第一个section，也就是`[net]` section，要特殊对待。这里不再多说了。\n\n## 保存参数信息\nDarknet中保存带参数的layer的信息是直接写入二进制文件。仍然以卷积层为例，其保存代码如下所示：\n\n``` cpp\nvoid save_convolutional_weights(layer l, FILE *fp)\n{\n    if(l.binary){\n        //save_convolutional_weights_binary(l, fp);\n        //return;\n    }\n#ifdef GPU\n    if(gpu_index >= 0){\n        pull_convolutional_layer(l);\n    }\n#endif\n    int num = l.n*l.c*l.size*l.size;\n    fwrite(l.biases, sizeof(float), l.n, fp);\n    // 由于darknet设计时，没有单独设计BN层，所以BN的参数也是和其所在的层一起保存的，如果读取时候要注意分别讨论\n    if (l.batch_normalize){\n        fwrite(l.scales, sizeof(float), l.n, fp);\n        fwrite(l.rolling_mean, sizeof(float), l.n, fp);\n        fwrite(l.rolling_variance, sizeof(float), l.n, fp);\n    }\n    fwrite(l.weights, sizeof(float), num, fp);\n    if(l.adam){\n        fwrite(l.m, sizeof(float), num, fp);\n        fwrite(l.v, sizeof(float), num, fp);\n    }\n}\n```\n\n在保存整个网络的参数信息的时候，同样逐层保存到同一个二进制文件中就好了。\n","slug":"yolo-cfg-parser","published":1,"updated":"2018-01-12T06:22:20.484Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjcu6vcup002uqu46r43pyll7","content":"<p>YOLO的原作者使用了自己开发的Darknet框架，而没有选取当前流行的其他深度学习框架实现算法，所以有必要对其网络模型的参数解析与存储方式做一了解，方便阅读源码和在其他流行的框架下的算法移植。<br><a id=\"more\"></a></p>\n<h2 id=\"YOLO网络结构定义的CFG文件\"><a href=\"#YOLO网络结构定义的CFG文件\" class=\"headerlink\" title=\"YOLO网络结构定义的CFG文件\"></a>YOLO网络结构定义的CFG文件</h2><p>YOLO中的网络定义采用和Caffe类似的方式，都是通过一个顺序堆叠layer来对神经网络结构进行定义的文件来描述。不同的地方在于，Caffe中使用了Google家出品的protobuf，省时省力，无需自己实现解析文件的功能，但是也使得Caffe对第三方库的依赖更加严重。相信很多人在编译Caffe的时候都出现过无法链接等蛋疼无比的问题。而YOLO的作者则是使用了自己定义的一种CFG文件格式，需要自己实现解析功能。</p>\n<p>CFG文件的格式可以归纳如下（可以打开某个<a href=\"https://github.com/pjreddie/darknet/blob/master/cfg/yolo-voc.cfg\" target=\"_blank\" rel=\"external\">CFG文件</a>进行对照）：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div></pre></td><td class=\"code\"><pre><div class=\"line\">[net]</div><div class=\"line\"># 这里会对net的参数进行配置</div><div class=\"line\"># 同时YOLO将对net的求解器的参数也放在了这里</div><div class=\"line\">[conv]</div><div class=\"line\"># 一些conv层的参数描述</div><div class=\"line\">[maxpool]</div><div class=\"line\"># 一些池化层的参数描述</div><div class=\"line\"></div><div class=\"line\"># 顺序堆叠的其他layer描述</div></pre></td></tr></table></figure></p>\n<p>在Darknet的代码中，将每个<code>[]</code>符号导引的参数列表叫做section。</p>\n<h2 id=\"网络结构解析器-Parser\"><a href=\"#网络结构解析器-Parser\" class=\"headerlink\" title=\"网络结构解析器 Parser\"></a>网络结构解析器 Parser</h2><p>具体的解析实现参见<a href=\"https://github.com/pjreddie/darknet/blob/master/src/parser.c\" target=\"_blank\" rel=\"external\">parser.c文件</a>。我们先以<code>convolutional_layer parse_convolutional(list *options, size_params params)</code>函数为例，看一下Darknet是如何完成对卷积层参数的解析的。</p>\n<p>从函数签名可以看出，这个函数接受一个<code>list</code>的变量（Darknet中将堆叠起来的这些层描述抽象成链表），而<code>size_params</code>类型的变量<code>params</code>指示了该层上一层的参数情况，其具体定义如下：<br><figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">typedef</span> <span class=\"keyword\">struct</span> size_params&#123;</div><div class=\"line\">    <span class=\"keyword\">int</span> batch;</div><div class=\"line\">    <span class=\"keyword\">int</span> inputs;</div><div class=\"line\">    <span class=\"keyword\">int</span> h;</div><div class=\"line\">    <span class=\"keyword\">int</span> w;</div><div class=\"line\">    <span class=\"keyword\">int</span> c;</div><div class=\"line\">    <span class=\"keyword\">int</span> index;</div><div class=\"line\">    <span class=\"keyword\">int</span> time_steps;</div><div class=\"line\">    network net;</div><div class=\"line\">&#125; size_params;</div></pre></td></tr></table></figure></p>\n<p>这样，在构建该层卷积层的时候，我们就能够知道上一层的输入维度等信息，方便做一些参数检查和layer初始化等的工作。</p>\n<p>进入函数内部，会发现频繁出现<code>option_find_int</code>这个函数。从函数名字面意义看，应该是要解析字符串中的整型数。</p>\n<p>我们首先来看一下这个函数的定义吧~这个函数并不在<code>parser.c</code>中，而是在<a href=\"https://github.com/pjreddie/darknet/blob/master/src/option_list.c\" target=\"_blank\" rel=\"external\">option_list.c 文件</a>中。</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\">// l: data pointer to the list</span></div><div class=\"line\"><span class=\"comment\">// key: the key to find, example: \"filters\", \"padding\"</span></div><div class=\"line\"><span class=\"comment\">// def: default value</span></div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">int</span> <span class=\"title\">option_find_int</span><span class=\"params\">(<span class=\"built_in\">list</span> *l, <span class=\"keyword\">char</span> *key, <span class=\"keyword\">int</span> def)</span></span></div><div class=\"line\">&#123;</div><div class=\"line\">    <span class=\"comment\">// 去找到该key对应的数值，使用atoi转换为整型数</span></div><div class=\"line\">    <span class=\"keyword\">char</span> *v = option_find(l, key);</div><div class=\"line\">    <span class=\"keyword\">if</span>(v) <span class=\"keyword\">return</span> atoi(v);</div><div class=\"line\">    <span class=\"comment\">// 使用XXX_quiet版本可以不打印此信息</span></div><div class=\"line\">    <span class=\"built_in\">fprintf</span>(<span class=\"built_in\">stderr</span>, <span class=\"string\">\"%s: Using default '%d'\\n\"</span>, key, def);</div><div class=\"line\">    <span class=\"comment\">// 没有找到key，返回默认值</span></div><div class=\"line\">    <span class=\"keyword\">return</span> def;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<p>而其中的<code>option_find</code>函数则是逐项顺序查找，匹配字符串来实现的。</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"function\"><span class=\"keyword\">char</span> *<span class=\"title\">option_find</span><span class=\"params\">(<span class=\"built_in\">list</span> *l, <span class=\"keyword\">char</span> *key)</span></span></div><div class=\"line\">&#123;</div><div class=\"line\">    node *n = l-&gt;front;</div><div class=\"line\">    <span class=\"keyword\">while</span>(n)&#123;</div><div class=\"line\">        kvp *p = (kvp *)n-&gt;val;</div><div class=\"line\">        <span class=\"keyword\">if</span>(<span class=\"built_in\">strcmp</span>(p-&gt;key, key) == <span class=\"number\">0</span>)&#123;</div><div class=\"line\">            p-&gt;used = <span class=\"number\">1</span>;</div><div class=\"line\">            <span class=\"keyword\">return</span> p-&gt;val;</div><div class=\"line\">        &#125;</div><div class=\"line\">        n = n-&gt;next;</div><div class=\"line\">    &#125;</div><div class=\"line\">    <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<h2 id=\"构建conv层\"><a href=\"#构建conv层\" class=\"headerlink\" title=\"构建conv层\"></a>构建conv层</h2><p>由此，我们可以通过CFG文件得到卷积层的参数了。接下来需要调用其初始化函数，进行构建。</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div></pre></td><td class=\"code\"><pre><div class=\"line\">   <span class=\"comment\">// 首先得到参数</span></div><div class=\"line\">   <span class=\"keyword\">int</span> n = option_find_int(options, <span class=\"string\">\"filters\"</span>,<span class=\"number\">1</span>);</div><div class=\"line\">   <span class=\"keyword\">int</span> size = option_find_int(options, <span class=\"string\">\"size\"</span>,<span class=\"number\">1</span>);</div><div class=\"line\">   <span class=\"keyword\">int</span> stride = option_find_int(options, <span class=\"string\">\"stride\"</span>,<span class=\"number\">1</span>);</div><div class=\"line\">   <span class=\"keyword\">int</span> pad = option_find_int_quiet(options, <span class=\"string\">\"pad\"</span>,<span class=\"number\">0</span>);</div><div class=\"line\">   <span class=\"keyword\">int</span> padding = option_find_int_quiet(options, <span class=\"string\">\"padding\"</span>,<span class=\"number\">0</span>);</div><div class=\"line\">   <span class=\"keyword\">if</span>(pad) padding = size/<span class=\"number\">2</span>;</div><div class=\"line\"><span class=\"comment\">// 激活函数是通过匹配其名称的方法得到的</span></div><div class=\"line\">   <span class=\"keyword\">char</span> *activation_s = option_find_str(options, <span class=\"string\">\"activation\"</span>, <span class=\"string\">\"logistic\"</span>);</div><div class=\"line\">   ACTIVATION activation = get_activation(activation_s);</div><div class=\"line\">   <span class=\"comment\">// 通过上层的信息得到batch size，做参数检查</span></div><div class=\"line\">   <span class=\"keyword\">int</span> batch,h,w,c;</div><div class=\"line\">   h = params.h;</div><div class=\"line\">   w = params.w;</div><div class=\"line\">   c = params.c;</div><div class=\"line\">   batch=params.batch;</div><div class=\"line\">   <span class=\"keyword\">if</span>(!(h &amp;&amp; w &amp;&amp; c)) error(<span class=\"string\">\"Layer before convolutional layer must output image.\"</span>);</div><div class=\"line\">   <span class=\"keyword\">int</span> batch_normalize = option_find_int_quiet(options, <span class=\"string\">\"batch_normalize\"</span>, <span class=\"number\">0</span>);</div><div class=\"line\">   <span class=\"keyword\">int</span> binary = option_find_int_quiet(options, <span class=\"string\">\"binary\"</span>, <span class=\"number\">0</span>);</div><div class=\"line\">   <span class=\"keyword\">int</span> xnor = option_find_int_quiet(options, <span class=\"string\">\"xnor\"</span>, <span class=\"number\">0</span>);</div><div class=\"line\">   <span class=\"comment\">// 调用初始化函数</span></div><div class=\"line\">   convolutional_layer layer = make_convolutional_layer(batch,h,w,c,n,size,stride,padding,activation, batch_normalize, binary, xnor, params.net.adam);</div><div class=\"line\">   layer.flipped = option_find_int_quiet(options, <span class=\"string\">\"flipped\"</span>, <span class=\"number\">0</span>);</div><div class=\"line\">   layer.dot = option_find_float_quiet(options, <span class=\"string\">\"dot\"</span>, <span class=\"number\">0</span>);</div></pre></td></tr></table></figure>\n<p>所以，如果在阅读源码时候，对layer的某个成员变量不知道什么意思的话，可以参考此文件，看一下原始解析对应的字符串是什么，一般这个字符串描述是比较具体的。</p>\n<h2 id=\"构建网络\"><a href=\"#构建网络\" class=\"headerlink\" title=\"构建网络\"></a>构建网络</h2><p>有了各个layer的解析方法，接下来就可以逐层读取参数信息并构建网络了。</p>\n<p>Darknet中对应的函数为<code>network parse_network_cfg(char *filename)</code>，这个函数接受文件名为参数，进行网络结构的解析。</p>\n<p>首先，调用<code>read_cfg(filename)</code>得到CFG文件的一个层次链表，接着只要对这个链表进行解析就好了。不过对第一个section，也就是<code>[net]</code> section，要特殊对待。这里不再多说了。</p>\n<h2 id=\"保存参数信息\"><a href=\"#保存参数信息\" class=\"headerlink\" title=\"保存参数信息\"></a>保存参数信息</h2><p>Darknet中保存带参数的layer的信息是直接写入二进制文件。仍然以卷积层为例，其保存代码如下所示：</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">save_convolutional_weights</span><span class=\"params\">(layer l, FILE *fp)</span></span></div><div class=\"line\">&#123;</div><div class=\"line\">    <span class=\"keyword\">if</span>(l.binary)&#123;</div><div class=\"line\">        <span class=\"comment\">//save_convolutional_weights_binary(l, fp);</span></div><div class=\"line\">        <span class=\"comment\">//return;</span></div><div class=\"line\">    &#125;</div><div class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">ifdef</span> GPU</span></div><div class=\"line\">    <span class=\"keyword\">if</span>(gpu_index &gt;= <span class=\"number\">0</span>)&#123;</div><div class=\"line\">        pull_convolutional_layer(l);</div><div class=\"line\">    &#125;</div><div class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">endif</span></span></div><div class=\"line\">    <span class=\"keyword\">int</span> num = l.n*l.c*l.size*l.size;</div><div class=\"line\">    fwrite(l.biases, <span class=\"keyword\">sizeof</span>(<span class=\"keyword\">float</span>), l.n, fp);</div><div class=\"line\">    <span class=\"comment\">// 由于darknet设计时，没有单独设计BN层，所以BN的参数也是和其所在的层一起保存的，如果读取时候要注意分别讨论</span></div><div class=\"line\">    <span class=\"keyword\">if</span> (l.batch_normalize)&#123;</div><div class=\"line\">        fwrite(l.scales, <span class=\"keyword\">sizeof</span>(<span class=\"keyword\">float</span>), l.n, fp);</div><div class=\"line\">        fwrite(l.rolling_mean, <span class=\"keyword\">sizeof</span>(<span class=\"keyword\">float</span>), l.n, fp);</div><div class=\"line\">        fwrite(l.rolling_variance, <span class=\"keyword\">sizeof</span>(<span class=\"keyword\">float</span>), l.n, fp);</div><div class=\"line\">    &#125;</div><div class=\"line\">    fwrite(l.weights, <span class=\"keyword\">sizeof</span>(<span class=\"keyword\">float</span>), num, fp);</div><div class=\"line\">    <span class=\"keyword\">if</span>(l.adam)&#123;</div><div class=\"line\">        fwrite(l.m, <span class=\"keyword\">sizeof</span>(<span class=\"keyword\">float</span>), num, fp);</div><div class=\"line\">        fwrite(l.v, <span class=\"keyword\">sizeof</span>(<span class=\"keyword\">float</span>), num, fp);</div><div class=\"line\">    &#125;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<p>在保存整个网络的参数信息的时候，同样逐层保存到同一个二进制文件中就好了。</p>\n","excerpt":"<p>YOLO的原作者使用了自己开发的Darknet框架，而没有选取当前流行的其他深度学习框架实现算法，所以有必要对其网络模型的参数解析与存储方式做一了解，方便阅读源码和在其他流行的框架下的算法移植。<br>","more":"</p>\n<h2 id=\"YOLO网络结构定义的CFG文件\"><a href=\"#YOLO网络结构定义的CFG文件\" class=\"headerlink\" title=\"YOLO网络结构定义的CFG文件\"></a>YOLO网络结构定义的CFG文件</h2><p>YOLO中的网络定义采用和Caffe类似的方式，都是通过一个顺序堆叠layer来对神经网络结构进行定义的文件来描述。不同的地方在于，Caffe中使用了Google家出品的protobuf，省时省力，无需自己实现解析文件的功能，但是也使得Caffe对第三方库的依赖更加严重。相信很多人在编译Caffe的时候都出现过无法链接等蛋疼无比的问题。而YOLO的作者则是使用了自己定义的一种CFG文件格式，需要自己实现解析功能。</p>\n<p>CFG文件的格式可以归纳如下（可以打开某个<a href=\"https://github.com/pjreddie/darknet/blob/master/cfg/yolo-voc.cfg\">CFG文件</a>进行对照）：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div></pre></td><td class=\"code\"><pre><div class=\"line\">[net]</div><div class=\"line\"># 这里会对net的参数进行配置</div><div class=\"line\"># 同时YOLO将对net的求解器的参数也放在了这里</div><div class=\"line\">[conv]</div><div class=\"line\"># 一些conv层的参数描述</div><div class=\"line\">[maxpool]</div><div class=\"line\"># 一些池化层的参数描述</div><div class=\"line\"></div><div class=\"line\"># 顺序堆叠的其他layer描述</div></pre></td></tr></table></figure></p>\n<p>在Darknet的代码中，将每个<code>[]</code>符号导引的参数列表叫做section。</p>\n<h2 id=\"网络结构解析器-Parser\"><a href=\"#网络结构解析器-Parser\" class=\"headerlink\" title=\"网络结构解析器 Parser\"></a>网络结构解析器 Parser</h2><p>具体的解析实现参见<a href=\"https://github.com/pjreddie/darknet/blob/master/src/parser.c\">parser.c文件</a>。我们先以<code>convolutional_layer parse_convolutional(list *options, size_params params)</code>函数为例，看一下Darknet是如何完成对卷积层参数的解析的。</p>\n<p>从函数签名可以看出，这个函数接受一个<code>list</code>的变量（Darknet中将堆叠起来的这些层描述抽象成链表），而<code>size_params</code>类型的变量<code>params</code>指示了该层上一层的参数情况，其具体定义如下：<br><figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">typedef</span> <span class=\"keyword\">struct</span> size_params&#123;</div><div class=\"line\">    <span class=\"keyword\">int</span> batch;</div><div class=\"line\">    <span class=\"keyword\">int</span> inputs;</div><div class=\"line\">    <span class=\"keyword\">int</span> h;</div><div class=\"line\">    <span class=\"keyword\">int</span> w;</div><div class=\"line\">    <span class=\"keyword\">int</span> c;</div><div class=\"line\">    <span class=\"keyword\">int</span> index;</div><div class=\"line\">    <span class=\"keyword\">int</span> time_steps;</div><div class=\"line\">    network net;</div><div class=\"line\">&#125; size_params;</div></pre></td></tr></table></figure></p>\n<p>这样，在构建该层卷积层的时候，我们就能够知道上一层的输入维度等信息，方便做一些参数检查和layer初始化等的工作。</p>\n<p>进入函数内部，会发现频繁出现<code>option_find_int</code>这个函数。从函数名字面意义看，应该是要解析字符串中的整型数。</p>\n<p>我们首先来看一下这个函数的定义吧~这个函数并不在<code>parser.c</code>中，而是在<a href=\"https://github.com/pjreddie/darknet/blob/master/src/option_list.c\">option_list.c 文件</a>中。</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\">// l: data pointer to the list</span></div><div class=\"line\"><span class=\"comment\">// key: the key to find, example: \"filters\", \"padding\"</span></div><div class=\"line\"><span class=\"comment\">// def: default value</span></div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">int</span> <span class=\"title\">option_find_int</span><span class=\"params\">(<span class=\"built_in\">list</span> *l, <span class=\"keyword\">char</span> *key, <span class=\"keyword\">int</span> def)</span></div><div class=\"line\"></span>&#123;</div><div class=\"line\">    <span class=\"comment\">// 去找到该key对应的数值，使用atoi转换为整型数</span></div><div class=\"line\">    <span class=\"keyword\">char</span> *v = option_find(l, key);</div><div class=\"line\">    <span class=\"keyword\">if</span>(v) <span class=\"keyword\">return</span> atoi(v);</div><div class=\"line\">    <span class=\"comment\">// 使用XXX_quiet版本可以不打印此信息</span></div><div class=\"line\">    <span class=\"built_in\">fprintf</span>(<span class=\"built_in\">stderr</span>, <span class=\"string\">\"%s: Using default '%d'\\n\"</span>, key, def);</div><div class=\"line\">    <span class=\"comment\">// 没有找到key，返回默认值</span></div><div class=\"line\">    <span class=\"keyword\">return</span> def;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<p>而其中的<code>option_find</code>函数则是逐项顺序查找，匹配字符串来实现的。</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"function\"><span class=\"keyword\">char</span> *<span class=\"title\">option_find</span><span class=\"params\">(<span class=\"built_in\">list</span> *l, <span class=\"keyword\">char</span> *key)</span></div><div class=\"line\"></span>&#123;</div><div class=\"line\">    node *n = l-&gt;front;</div><div class=\"line\">    <span class=\"keyword\">while</span>(n)&#123;</div><div class=\"line\">        kvp *p = (kvp *)n-&gt;val;</div><div class=\"line\">        <span class=\"keyword\">if</span>(<span class=\"built_in\">strcmp</span>(p-&gt;key, key) == <span class=\"number\">0</span>)&#123;</div><div class=\"line\">            p-&gt;used = <span class=\"number\">1</span>;</div><div class=\"line\">            <span class=\"keyword\">return</span> p-&gt;val;</div><div class=\"line\">        &#125;</div><div class=\"line\">        n = n-&gt;next;</div><div class=\"line\">    &#125;</div><div class=\"line\">    <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<h2 id=\"构建conv层\"><a href=\"#构建conv层\" class=\"headerlink\" title=\"构建conv层\"></a>构建conv层</h2><p>由此，我们可以通过CFG文件得到卷积层的参数了。接下来需要调用其初始化函数，进行构建。</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div></pre></td><td class=\"code\"><pre><div class=\"line\">   <span class=\"comment\">// 首先得到参数</span></div><div class=\"line\">   <span class=\"keyword\">int</span> n = option_find_int(options, <span class=\"string\">\"filters\"</span>,<span class=\"number\">1</span>);</div><div class=\"line\">   <span class=\"keyword\">int</span> size = option_find_int(options, <span class=\"string\">\"size\"</span>,<span class=\"number\">1</span>);</div><div class=\"line\">   <span class=\"keyword\">int</span> stride = option_find_int(options, <span class=\"string\">\"stride\"</span>,<span class=\"number\">1</span>);</div><div class=\"line\">   <span class=\"keyword\">int</span> pad = option_find_int_quiet(options, <span class=\"string\">\"pad\"</span>,<span class=\"number\">0</span>);</div><div class=\"line\">   <span class=\"keyword\">int</span> padding = option_find_int_quiet(options, <span class=\"string\">\"padding\"</span>,<span class=\"number\">0</span>);</div><div class=\"line\">   <span class=\"keyword\">if</span>(pad) padding = size/<span class=\"number\">2</span>;</div><div class=\"line\"><span class=\"comment\">// 激活函数是通过匹配其名称的方法得到的</span></div><div class=\"line\">   <span class=\"keyword\">char</span> *activation_s = option_find_str(options, <span class=\"string\">\"activation\"</span>, <span class=\"string\">\"logistic\"</span>);</div><div class=\"line\">   ACTIVATION activation = get_activation(activation_s);</div><div class=\"line\">   <span class=\"comment\">// 通过上层的信息得到batch size，做参数检查</span></div><div class=\"line\">   <span class=\"keyword\">int</span> batch,h,w,c;</div><div class=\"line\">   h = params.h;</div><div class=\"line\">   w = params.w;</div><div class=\"line\">   c = params.c;</div><div class=\"line\">   batch=params.batch;</div><div class=\"line\">   <span class=\"keyword\">if</span>(!(h &amp;&amp; w &amp;&amp; c)) error(<span class=\"string\">\"Layer before convolutional layer must output image.\"</span>);</div><div class=\"line\">   <span class=\"keyword\">int</span> batch_normalize = option_find_int_quiet(options, <span class=\"string\">\"batch_normalize\"</span>, <span class=\"number\">0</span>);</div><div class=\"line\">   <span class=\"keyword\">int</span> binary = option_find_int_quiet(options, <span class=\"string\">\"binary\"</span>, <span class=\"number\">0</span>);</div><div class=\"line\">   <span class=\"keyword\">int</span> xnor = option_find_int_quiet(options, <span class=\"string\">\"xnor\"</span>, <span class=\"number\">0</span>);</div><div class=\"line\">   <span class=\"comment\">// 调用初始化函数</span></div><div class=\"line\">   convolutional_layer layer = make_convolutional_layer(batch,h,w,c,n,size,stride,padding,activation, batch_normalize, binary, xnor, params.net.adam);</div><div class=\"line\">   layer.flipped = option_find_int_quiet(options, <span class=\"string\">\"flipped\"</span>, <span class=\"number\">0</span>);</div><div class=\"line\">   layer.dot = option_find_float_quiet(options, <span class=\"string\">\"dot\"</span>, <span class=\"number\">0</span>);</div></pre></td></tr></table></figure>\n<p>所以，如果在阅读源码时候，对layer的某个成员变量不知道什么意思的话，可以参考此文件，看一下原始解析对应的字符串是什么，一般这个字符串描述是比较具体的。</p>\n<h2 id=\"构建网络\"><a href=\"#构建网络\" class=\"headerlink\" title=\"构建网络\"></a>构建网络</h2><p>有了各个layer的解析方法，接下来就可以逐层读取参数信息并构建网络了。</p>\n<p>Darknet中对应的函数为<code>network parse_network_cfg(char *filename)</code>，这个函数接受文件名为参数，进行网络结构的解析。</p>\n<p>首先，调用<code>read_cfg(filename)</code>得到CFG文件的一个层次链表，接着只要对这个链表进行解析就好了。不过对第一个section，也就是<code>[net]</code> section，要特殊对待。这里不再多说了。</p>\n<h2 id=\"保存参数信息\"><a href=\"#保存参数信息\" class=\"headerlink\" title=\"保存参数信息\"></a>保存参数信息</h2><p>Darknet中保存带参数的layer的信息是直接写入二进制文件。仍然以卷积层为例，其保存代码如下所示：</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">save_convolutional_weights</span><span class=\"params\">(layer l, FILE *fp)</span></div><div class=\"line\"></span>&#123;</div><div class=\"line\">    <span class=\"keyword\">if</span>(l.binary)&#123;</div><div class=\"line\">        <span class=\"comment\">//save_convolutional_weights_binary(l, fp);</span></div><div class=\"line\">        <span class=\"comment\">//return;</span></div><div class=\"line\">    &#125;</div><div class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">ifdef</span> GPU</span></div><div class=\"line\">    <span class=\"keyword\">if</span>(gpu_index &gt;= <span class=\"number\">0</span>)&#123;</div><div class=\"line\">        pull_convolutional_layer(l);</div><div class=\"line\">    &#125;</div><div class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">endif</span></span></div><div class=\"line\">    <span class=\"keyword\">int</span> num = l.n*l.c*l.size*l.size;</div><div class=\"line\">    fwrite(l.biases, <span class=\"keyword\">sizeof</span>(<span class=\"keyword\">float</span>), l.n, fp);</div><div class=\"line\">    <span class=\"comment\">// 由于darknet设计时，没有单独设计BN层，所以BN的参数也是和其所在的层一起保存的，如果读取时候要注意分别讨论</span></div><div class=\"line\">    <span class=\"keyword\">if</span> (l.batch_normalize)&#123;</div><div class=\"line\">        fwrite(l.scales, <span class=\"keyword\">sizeof</span>(<span class=\"keyword\">float</span>), l.n, fp);</div><div class=\"line\">        fwrite(l.rolling_mean, <span class=\"keyword\">sizeof</span>(<span class=\"keyword\">float</span>), l.n, fp);</div><div class=\"line\">        fwrite(l.rolling_variance, <span class=\"keyword\">sizeof</span>(<span class=\"keyword\">float</span>), l.n, fp);</div><div class=\"line\">    &#125;</div><div class=\"line\">    fwrite(l.weights, <span class=\"keyword\">sizeof</span>(<span class=\"keyword\">float</span>), num, fp);</div><div class=\"line\">    <span class=\"keyword\">if</span>(l.adam)&#123;</div><div class=\"line\">        fwrite(l.m, <span class=\"keyword\">sizeof</span>(<span class=\"keyword\">float</span>), num, fp);</div><div class=\"line\">        fwrite(l.v, <span class=\"keyword\">sizeof</span>(<span class=\"keyword\">float</span>), num, fp);</div><div class=\"line\">    &#125;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<p>在保存整个网络的参数信息的时候，同样逐层保存到同一个二进制文件中就好了。</p>"},{"title":"YOLO 论文阅读","date":"2017-02-04T10:49:22.000Z","_content":"YOLO(**Y**ou **O**nly **L**ook **O**nce)是一个流行的目标检测方法，和Faster RCNN等state of the art方法比起来，主打检测速度快。截止到目前为止（2017年2月初），YOLO已经发布了两个版本，在下文中分别称为[YOLO V1](https://arxiv.org/abs/1506.02640)和[YOLO V2](https://arxiv.org/abs/1612.08242)。YOLO V2的代码目前作为[Darknet](http://pjreddie.com/darknet/yolo/)的一部分开源在[GitHub]()。在这篇博客中，记录了阅读YOLO两个版本论文中的重点内容，并着重总结V2版本的改进。\n\n![YOLO V2的检测效果示意](/img/yolo2_result.png)\n<!-- more -->\n\n## YOLO V1\n这里不妨把YOLO V1论文[\"You Only Look Once: Unitied, Real-Time Object Detection\"](https://arxiv.org/abs/1506.02640)的摘要部分意译如下：\n\n> 我们提出了一种新的物体检测方法YOLO。之前的目标检测方法大多把分类器重新调整用于检测（这里应该是在说以前的方法大多将检测问题看做分类问题，使用滑动窗提取特征并使用分类器进行分类）。我们将检测问题看做回归，分别给出bounding box的位置和对应的类别概率。对于给定输入图像，只需使用CNN网络计算一次，就同时给出bounding box位置和类别概率。由于整个pipeline都是同一个网络，所以很容易进行端到端的训练。\n> YOLO相当快。base model可以达到45fps，一个更小的model（Fast YOLO），可以达到155fps，同时mAP是其他可以实现实时检测方法的两倍。和目前的State of the art方法相比，YOLO的localization误差较大，但是对于背景有更低的误检（False Positives）。同时，YOLO能够学习到更加泛化的特征，例如艺术品中物体的检测等任务，表现很好。\n\n和以往较为常见的方法，如HoG+SVM的行人检测，DPM，RCNN系方法等不同，YOLO接受image作为输入，直接输出bounding box的位置和对应位置为某一类的概率。我们可以把它和目前的State of the art的Faster RCNN方法对比。Faster RCNN方法需要两个网络RPN和Fast RCNN，其中前者负责接受图像输入，并提出proposal。后续Fast RCNN再给出这些proposal是某一类的概率。也正是因为这种“直来直去”的方法，YOLO才能达到这么快的速度。也真是令人感叹，网络参数够多，数据够多，什么映射关系都能学出来。。。下图就是YOLO的检测系统示意图。在test阶段，经过单个CNN网络前向计算后，再经过非极大值抑制，就可以给出检测结果。\n![YOLO V1检测系统示意图](/img/yolo1_detection_system.png)\n\n### 基本思路\n![基础思路示意图](/img/yolo1_basic_idea.png)\n\n- 网格划分：将输入image划分为$S \\times S$个grid cell，如果image中某个object box的中心落在某个grid cell内部，那么这个cell就对检测该object负责（responsible for detection that object）。同时，每个grid cell同时预测$B$个bounding box的位置和一个置信度。这个置信度并不只是该bounding box是待检测目标的概率，而是该bounding box是待检测目标的概率乘上该bounding box和真实位置的IoU的积。通过乘上这个交并比，反映出该bounding box预测位置的精度。如下式所示：\n\n$$\\text{confidence} = P(\\text{Object})\\times \\text{IoU}_{\\text{pred}}^{\\text{truth}}$$\n\n- 网络输出：每个bounding box对应于5个输出，分别是$x,y,w,h$和上述提到的置信度。其中，$x,y$代表bounding box的中心离开其所在grid cell边界的偏移。$w,h$代表bounding box真实宽高相对于整幅图像的比例。$x,y,w,h$这几个参数都已经被bounded到了区间$[0,1]$上。除此以外，每个grid cell还产生$C$个条件概率，$P(\\text{Class}_i|\\text{Object})$。注意，我们不管$B$的大小，每个grid cell只产生一组这样的概率。在test的非极大值抑制阶段，对于每个bounding box，我们应该按照下式衡量该框是否应该予以保留。\n$$\\text{confidence}\\times P(\\text{Class}_i|\\text{Object}) = P(\\text{Class}_i)\\times \\text{IoU}_{\\text{pred}}^{\\text{truth}}$$\n\n- 实际参数：在PASCAL VOC进行测试时，使用$S = 7$, $B=2$。由于共有20类，故$C=20$。所以，我们的网络输出大小为$7\\times 7 \\times 30$\n\n### 网络模型结构\nInspired by GoogLeNet，但是没有采取inception的结构，simply使用了$1\\times 1$的卷积核。base model共有24个卷积层，后面接2个全连接层，如下图所示。\n![YOLO的网络结构示意图](/img/yolo1_network_arch.png)\n\n另外，Fast YOLO使用了更小的网络结构（9个卷积层，且filter的数目也少了），其他部分完全一样。\n\n### 训练\n同很多人一样，这里作者也是先在ImageNet上做了预训练。使用上图网络结构中的前20个卷积层，后面接一个average-pooling层和全连接层，在ImageNet 1000类数据集上训练了一周，达到了88%的top-5准确率。\n\n由于[Ren的论文](https://arxiv.org/abs/1504.06066)提到给预训练的模型加上额外的卷积层和全连接层能够提升性能，所以我们加上了剩下的4个卷积层和2个全连接层，权值为随机初始化。同时，我们把网络输入的分辨率从$224\\times 224$提升到了$448 \\times 448$。\n\n在最后一层，我们使用了线性激活函数；其它层使用了leaky ReLU激活函数。如下所示：\n$$\nf(x)=\n\\begin{cases}\nx, &\\text{if}\\ x > 0 \\\\\\\\\n0.1x, &\\text{otherwise}\n\\end{cases}\n$$\n\n很重要的问题就是定义很好的loss function，为此，作者提出了以下几点说明：\n\n- loss的形式采用误差平方和的形式（真是把回归进行到底了。。。）\n- 由于很多的grid cell没有目标物体存在，所以给有目标存在的bounding box和没有目标存在的bounding box设置了不同的比例因子进行平衡。具体来说，\n$$\\lambda_{\\text{coord}} = 5，\\lambda_{\\text{noobj}} = 0.5$$\n- 直接使用$w$和$h$，这样大的box的差异在loss中占得比重会和小的box不平衡，所以这里使用$\\sqrt{w}$和$\\sqrt{h}$。\n- 上文中已经提到，同一个grid cell会提出多个bounding box。在training阶段，我们只想让一个bounding box对应object。所以，我们计算每个bounding box和ground truth的IoU，以此为标准得到最好的那个bounding box，其他的认为no obj。\n\nloss函数的具体形式见下图（实在是不想打这一大串公式。。）。其中，$\\mathbb{1}_i$表示是否有目标出现在第$i$个grid cell。\n\n$\\mathbb{1}_{i,j}$表示第$i$个grid cell的第$j$个bounding box是否对某个目标负责。\n![YOLO的损失函数定义](/img/yolo1_loss_fun.png)\n\n在进行反向传播时，由于loss都是二次形式，所以导数形式都是统一的。下面是Darknet中[detection_layer.c](https://github.com/pjreddie/darknet/blob/master/src/detection_layer.c)中训练部分的代码。在代码中，计算了cost（也就是loss）和delta（也就是反向的导数），\n\n``` c\nif(state.train){\n    float avg_iou = 0;\n    float avg_cat = 0;\n    float avg_allcat = 0;\n    float avg_obj = 0;\n    float avg_anyobj = 0;\n    int count = 0;\n    *(l.cost) = 0;\n    int size = l.inputs * l.batch;\n    memset(l.delta, 0, size * sizeof(float));\n    for (b = 0; b < l.batch; ++b){\n        int index = b*l.inputs;\n        // for each grid cell\n        for (i = 0; i < locations; ++i) {   // locations = S * S = 49\n            int truth_index = (b*locations + i)*(1+l.coords+l.classes);\n            int is_obj = state.truth[truth_index];\n            // for each bbox\n            for (j = 0; j < l.n; ++j) {     // l.n = B = 2\n                int p_index = index + locations*l.classes + i*l.n + j;\n                l.delta[p_index] = l.noobject_scale*(0 - l.output[p_index]);\n                // 因为no obj对应的bbox很多，而responsible的只有一个\n                // 这里统一加上，如果一会判断该bbox responsible for object，再把它减去\n                *(l.cost) += l.noobject_scale*pow(l.output[p_index], 2);  \n                avg_anyobj += l.output[p_index];\n            }\n\n            int best_index = -1;\n            float best_iou = 0;\n            float best_rmse = 20;\n            // 该grid cell没有目标，直接返回\n            if (!is_obj){\n                continue;\n            }\n            // 否则，找出responsible的bounding box，计算其他几项的loss\n            int class_index = index + i*l.classes;\n            for(j = 0; j < l.classes; ++j) {\n                l.delta[class_index+j] = l.class_scale * (state.truth[truth_index+1+j] - l.output[class_index+j]);\n                *(l.cost) += l.class_scale * pow(state.truth[truth_index+1+j] - l.output[class_index+j], 2);\n                if(state.truth[truth_index + 1 + j]) avg_cat += l.output[class_index+j];\n                avg_allcat += l.output[class_index+j];\n            }\n\n            box truth = float_to_box(state.truth + truth_index + 1 + l.classes);\n            truth.x /= l.side;\n            truth.y /= l.side;\n            // 找到最好的IoU，对应的bbox是responsible的，记录其index\n            for(j = 0; j < l.n; ++j){\n                int box_index = index + locations*(l.classes + l.n) + (i*l.n + j) * l.coords;\n                box out = float_to_box(l.output + box_index);\n                out.x /= l.side;\n                out.y /= l.side;\n\n                if (l.sqrt){\n                    out.w = out.w*out.w;\n                    out.h = out.h*out.h;\n                }\n\n                float iou  = box_iou(out, truth);\n                //iou = 0;\n                float rmse = box_rmse(out, truth);\n                if(best_iou > 0 || iou > 0){\n                    if(iou > best_iou){\n                        best_iou = iou;\n                        best_index = j;\n                    }\n                }else{\n                    if(rmse < best_rmse){\n                        best_rmse = rmse;\n                        best_index = j;\n                    }\n                }\n            }\n\n            if(l.forced){\n                if(truth.w*truth.h < .1){\n                    best_index = 1;\n                }else{\n                    best_index = 0;\n                }\n            }\n            if(l.random && *(state.net.seen) < 64000){\n                best_index = rand()%l.n;\n            }\n\n            int box_index = index + locations*(l.classes + l.n) + (i*l.n + best_index) * l.coords;\n            int tbox_index = truth_index + 1 + l.classes;\n\n            box out = float_to_box(l.output + box_index);\n            out.x /= l.side;\n            out.y /= l.side;\n            if (l.sqrt) {\n                out.w = out.w*out.w;\n                out.h = out.h*out.h;\n            }\n            float iou  = box_iou(out, truth);\n\n            //printf(\"%d,\", best_index);\n            int p_index = index + locations*l.classes + i*l.n + best_index;\n            *(l.cost) -= l.noobject_scale * pow(l.output[p_index], 2);  // 还记得我们曾经统一加过吗？这里需要减去了\n            *(l.cost) += l.object_scale * pow(1-l.output[p_index], 2);\n            avg_obj += l.output[p_index];\n            l.delta[p_index] = l.object_scale * (1.-l.output[p_index]);\n\n            if(l.rescore){\n                l.delta[p_index] = l.object_scale * (iou - l.output[p_index]);\n            }\n\n            l.delta[box_index+0] = l.coord_scale*(state.truth[tbox_index + 0] - l.output[box_index + 0]);\n            l.delta[box_index+1] = l.coord_scale*(state.truth[tbox_index + 1] - l.output[box_index + 1]);\n            l.delta[box_index+2] = l.coord_scale*(state.truth[tbox_index + 2] - l.output[box_index + 2]);\n            l.delta[box_index+3] = l.coord_scale*(state.truth[tbox_index + 3] - l.output[box_index + 3]);\n            if(l.sqrt){\n                l.delta[box_index+2] = l.coord_scale*(sqrt(state.truth[tbox_index + 2]) - l.output[box_index + 2]);\n                l.delta[box_index+3] = l.coord_scale*(sqrt(state.truth[tbox_index + 3]) - l.output[box_index + 3]);\n            }\n\n            *(l.cost) += pow(1-iou, 2);\n            avg_iou += iou;\n            ++count;\n        }\n    }\n\n```\n\n## YOLO V2\nYOLO V2是原作者在V1基础上做出改进后提出的。为了达到题目中所称的Better，Faster，Stronger的目标，主要改进点如下。当然，具体内容还是要深入论文。\n- 受到Faster RCNN方法的启发，引入了anchor。同时使用了K-Means方法，对anchor数量进行了讨论，在精度和速度之间做出折中；\n- 修改了网络结构，去掉了全连接层，改成了全卷积结构；\n- 引入了WordTree结构，将检测和分类问题做成了一个统一的框架，并充分利用ImageNet和COCO数据集的数据。\n\n下面，还是先把论文的摘要意译如下：\n>我们引入了YOLO 9000模型，它是实时物体检测的State of the art的工作，能够检测超过9K类目标。首先，我们对以前的工作（YOLO V1）做出了若干改进，使其成为了一种在实时检测方法内在PASCAL VOC和COCO上State of the art的效果。通过一种新颖的多尺度训练犯法（multi-scale training method）， YOLO V2适用于不同大小尺寸输入的image，在精度和效率上达到了很好地trade-off。在67fps的速度下，VOC2007上达到了76.8mAP的精度。40fps时，达到了78.6mAP，已经超过了Faster RCNN with ResNet和SSD的精度，同时比它们更快！最后，我们提出了一种能够同时进行检测任务和分类任务的联合训练方法。使用这种方法，我们在COCO detection dataset和ImageNet classification dataset上同时训练模型。这种方法使得我们能够对那些没有label上detection data的数据做出detection的预测。我们使用ImageNet的detection任务做了验证。YOLO 9000在仅有200类中44类的detection data的情况下，仍然在ImageNet datection任务中取得了19.7mAP的成绩。对于不在COCO中的156类，YOLO9000成绩为16.0mAP。我们使得YOLO9000能够在保证实时的前提下对9K类目标进行检测。\n\n根据论文结构的安排，将从Better，Faster和Stronger三个方面对论文提出的各条改进措施进行介绍。\n\n## Better\n在YOLO V1的基础上，作者提出了不少的改进来进一步提升算法的性能（mAP），主要改进措施包括网络结构的改进（第1，3，5，6条）和Anchor Box的引进（第3，4，5条）以及训练方法（第2，7条）。\n\n### 改进1：引入BN层（Batch Normalization）\nBatch Normalization能够加快模型收敛，并提供一定的正则化。作者在每个conv层都加上了了BN层，同时去掉了原来模型中的drop out部分，这带来了2%的性能提升。\n\n### 改进2：高分辨率分类器（High Resolution Classifier）\nYOLO V1首先在ImageNet上以$224\\times 224$大小图像作为输入进行训练，之后在检测任务中提升到$448\\times 448$。这里，作者在训练完224大小的分类网络后，首先调整网络大小为$448\\times 448$，然后在ImageNet上进行fine tuning（10个epoch）。也就是得到了一个高分辨率的cls。再把它用detection上训练。这样，能够提升4%。\n\n### 改进3：引入Anchor Box\nYOLO V1中直接在CNN后面街上全连接层，直接回归bounding box的参数。这里引入了Faster RCNN中的anchor box概念，不再直接回归bounding box的参数，而是相对于anchor box的参数。\n\n作者去掉后面的fc层和最后一个max pooling层，以期得到更高分辨率的feature map。同时，shrink网络接受$416\\times 416$（而不是448）大小的输入image。这是因为我们想要最后的feature map大小是奇数，这样就能够得到一个center cell（比较大的目标，更有可能占据中间的位置）。由于YOLO conv-pooling的效应是将image downsamplig 32倍，所以最后feature map大小为$416/32 = 13$。\n\n与YOLO V1不同的是，我们不再对同一个grid cell下的bounding box统一产生一个数量为$C$的类别概率，而是对于每一个bounding box都产生对应的$C$类概率。和YOLO V1一样的是，我们仍然产生confidence，意义也完全一样。\n\n使用anchor后，我们的精度accuracy降低了，不过recall上来了。（这也较好理解。原来每个grid cell内部只有2个bounding box，造成recall不高。现在recall高上来了，accuracy会下降一些）。\n\n### 改进4：Dimension Cluster\n在引入anchor box后，一个问题就是如何确定anchor的位置和大小？Faster RCNN中是手工选定的，每隔stride设定一个anchor，并根据不同的面积比例和长宽比例产生9个anchor box。在本文中，作者使用了聚类方法对如何选取anchor box做了探究。这点应该是论文中很有新意的地方。\n\n这里对作者使用的方法不再过多赘述，强调以下两点：\n- 作者使用的聚类方法是K-Means；\n- 相似性度量不用欧氏距离，而是用IoU，定义如下：\n$$d(\\text{box}, \\text{centroid}) = 1-\\text{IoU}(\\text{box}, \\text{centroid})$$\n\n使用不同的$k$，聚类实验结果如下，作者折中采用了$k = 5$。而且经过实验，发现当取$k=9$时候，已经能够超过Faster RCNN采用的手工固定anchor box的方法。下图右侧图是在COCO和VOC数据集上$k=5$的聚类后结果。这些box可以作为anchor box使用。\n![](/img/yolo2_cluster_result.png)\n\n### 改进5：直接位置预测（Direct Location Prediction）\n我们仍然延续了YOLO V1中的思路，预测box相对于grid cell的位置。使用sigmoid函数作为激活函数，使得最终输出值落在$[0, 1]$这个区间上。\n\n在output的feature map上，对于每个cell（共计$13\\times 13$个），给出对应每个bounding box的输出$t_x$, $t_y$, $t_w$, $t_h$。每个cell共计$k=5$个bounding box。如何由这几个参数确定bounding box的真实位置呢？见下图。\n\n![确定bbox的位置](/img/yolo2_bbox_location.png)\n\n设该grid cell距离图像左上角的offset是$(c_x, c_y)$，那么bounding box的位置和宽高计算如下。注意，box的位置是相对于grid cell的，而宽高是相对于anchor box的。\n![bounding box参数的计算方法](/img/yolo2_bbox_param.png)\n\nDarknet中的具体的实现代码如下（不停切换中英文输入实在是蛋疼，所以只有用我这蹩脚的英语来注释了。。。）：\n\n``` cpp\n// get bounding box\n// x: data pointer of feature map\n// biases: data pointer of anchor box data\n// biases[2*n] = width of anchor box\n// biases[2*n+1] = height of anchor box\n// n: output bounding box for each cell in the feature map\n// index: output bounding box index in the cell\n// i: `cx` in the paper\n// j: 'cy' in the paper\n// (cx, cy) is the offset from the left top corner of the feature map\n// (w, h) is the size of feature map (do normalization in the code)\nbox get_region_box(float *x, float *biases, int n, int index, int i, int j, int w, int h)\n{\n    box b;\n    // i <- cx, j <- cy\n    // index + 0: tx\n    // index + 1: ty\n    // index + 2: tw\n    // index + 3: th\n    // index + 4: to   // not used here\n    // index + 5, +6, ..., +(C+4)   // confidence of P(class c|Object), not used here\n    b.x = (i + logistic_activate(x[index + 0])) / w;    // bx = cx+sigmoid(tx)\n    b.y = (j + logistic_activate(x[index + 1])) / h;    // by = cy+sigmoid(ty)\n    b.w = exp(x[index + 2]) * biases[2*n]   / w;        // bw = exp(tw) * pw\n    b.h = exp(x[index + 3]) * biases[2*n+1] / h;        // bh = exp(th) * ph\n    // 注意这里都做了Normalization，将值化到[0, 1]，论文里面貌似没有提到\n    // 也就是说YOLO 用于detection层的bounding box大小和位置的输出参数都是相对值\n    return b;\n}\n```\n\n顺便说一下对bounding box的bp实现。具体代码如下：\n\n``` cpp\n// truth: ground truth\n// x: data pointer of feature map\n// biases: data pointer of anchor box data\n// n, index, i, j, w, h: same meaning with `get_region_box`\n// delta: data pointer of gradient\n// scale: just a weight, given by user\nfloat delta_region_box(box truth, float *x, float *biases,\n                       int n, int index, int i, int j, int w, int h,\n                       float *delta, float scale)\n{\n    box pred = get_region_box(x, biases, n, index, i, j, w, h);\n    // get iou of the bbox and truth\n    float iou = box_iou(pred, truth);\n    // ground truth of the parameters (tx, ty, tw, th)\n    float tx = (truth.x*w - i);\n    float ty = (truth.y*h - j);\n    float tw = log(truth.w*w / biases[2*n]);\n    float th = log(truth.h*h / biases[2*n + 1]);\n    // 这里是欧式距离损失的梯度回传\n    // 以tx为例。\n    // loss = 1/2*(bx^hat-bx)^2, 其中bx = cx + sigmoid(tx)\n    // d(loss)/d(tx) = -(bx^hat-bx) * d(bx)/d(tx)\n    // 注意，Darkent中的delta存储的是负梯度数值，所以下面的delta数组内数值实际是-d(loss)/d(tx)\n    // 也就是(bx^hat-bx) * d(bx)/d(tx)\n    // 前面的(bx^hat-bx)把cx约掉了（因为是同一个cell，偏移是一样的）\n    // 后面相当于是求sigmoid函数对输入自变量的梯度。\n    // 由于当初没有缓存 sigomid(tx)，所以作者又重新计算了一次 sigmoid(tx)，也就是下面的激活函数那里\n    delta[index + 0] = scale * (tx - logistic_activate(x[index + 0]))\n                             * logistic_gradient(logistic_activate(x[index + 0]));\n\n    delta[index + 1] = scale * (ty - logistic_activate(x[index + 1]))\n                             * logistic_gradient(logistic_activate(x[index + 1]));\n    // tw 相似，只不过这里的 loss = 1/2(tw^hat-tw)^2，而不是和上面一样使用bw^hat 和 bw\n    delta[index + 2] = scale * (tw - x[index + 2]);\n    delta[index + 3] = scale * (th - x[index + 3]);\n    return iou;\n}\n```\n\n接下来，我们看一下bp的计算。主要涉及到决定bounding box大小和位置的四个参数的回归，以及置信度$t_o$，以及$C$类分类概率。上面的代码中已经介绍了bounding box大小位置的四个参数的梯度计算。对于置信度$t_o$的计算，如下所示。\n\n``` cpp\n// 上面的代码遍历了所有的groundtruth，找出了与当前预测bounding box iou最大的那个\n// 首先，我们认为当前bounding box没有responsible for any groundtruth，\n// 那么，loss = 1/2*(0-sigmoid(to))^2\n// => d(loss)/d(to) = -(0-sigmoid(to)) * d(sigmoid)/d(to)\n// 由于之前的代码中已经将output取了sigmoid作用，所以就有了下面的代码\n// 其中，logistic_gradient(y) 是指dy/dx|(y=y0)的值。具体来说，logistic_gradient(y) = (1-y)*y\nl.delta[index + 4] = l.noobject_scale * ((0 - l.output[index + 4]) * logistic_gradient(l.output[index + 4]));\n// 如果best iou > thresh, 我们认为这个bounding box有了对应的groundtruth，把梯度直接设置为0即可\nif (best_iou > l.thresh) {\n    l.delta[index + 4] = 0;\n}\n```\n\n这里额外要说明的是，阅读代码可以发现，分类loss的计算方法和V1不同，不再使用MSELoss，而是使用了交叉熵损失函数。对应地，梯度计算的方法如下所示。不过这点在论文中貌似并没有体现。\n```\n// for each class\nfor(n = 0; n < classes; ++n){\n    // P_i = \\frac{exp^out_i}{sum of exp^out_j}\n    // SoftmaxLoss = -logP(class)\n    // ∂SoftmaxLoss/∂output = -(1(n==class)-P)\n    delta[index + n] = scale * (((n == class)?1 : 0) - output[index + n]);\n    if(n == class) *avg_cat += output[index + n];\n}\n```\n\n### 改进6：Fine-Gained Features\n这个trick是受Faster RCNN和SSD方法中使用多个不同feature map提高算法对不同分辨率目标物体的检测能力的启发，加入了一个pass-through层，直接将倒数第二层的$26\\times 26$大小的feature map加进来。\n\n在具体实现时，是将higher resolution（也就是$26\\times 26$）的feature map stacking在一起。比如，原大小为$26\\times 26 \\times 512$的feature map，因为我们要将其变为$13\\times 13$大小，所以，将在空间上相近的点移到后面的channel上去，这部分可以参考Darknet中`reorg_layer`的实现。\n\n使用这一扩展之后的feature map，提高了1%的性能提升。\n\n### 改进7：多尺度训练（Multi-Scale Training）\n在实际应用时，输入的图像大小有可能是变化的。我们也将这一点考虑进来。因为我们的网络是全卷积神经网络，只有conv和pooling层，没有全连接层，所以可以适应不同大小的图像输入。所以网络结构上是没问题的。\n\n具体来说，在训练的时候，我们每隔一定的epoch（例如10）就随机改变网络的输入图像大小。由于我们的网络最终降采样的比例是$32$，所以随机生成的图像大小为$32$的倍数，即$\\lbrace 320, 352, \\dots, 608\\rbrace$。\n\n在实际使用中，如果输入图像的分辨率较低，YOLO V2可以在不错的精度下达到很快的检测速度。这可以被用应在计算能力有限的场合（无GPU或者GPU很弱）或多路视频信号的实时处理。如果输入图像的分辨率较高，YOLO V2可以作为state of the art的检测器，并仍能获得不错的检测速度。对于目前流行的检测方法（Faster RCNN，SSD，YOLO）的精度和帧率之间的关系，见下图。可以看到，作者在30fps处画了一条竖线，这是算法能否达到实时处理的分水岭。Faster RCNN败下阵来，而YOLO V2的不同点代表了不同输入图像分辨率下算法的表现。对于详细数据，见图下的表格对比（VOC 2007上进行测试）。\n\n![不同检测方法的对比](/img/yolo2_different_methods_comparation.png)\n![不同检测方法的对比](/img/yolo2_different_methods_comparation_in_table.png)\n\n### 总结\n在Better这部分的末尾，作者给出了一个表格，指出了主要提升性能的措施。例外是网络结构上改为带Anchor box的全卷积网络结构（提升了recall，但对mAP基本无影响）和使用新的网络（计算量少了~33%）。\n![不同改进措施的影响](/img/yolo2_different_methods_improvement.png)\n\n## Faster\n这部分的改进为网络结构的变化。包括Faster RCNN在内的很多检测方法都使用VGG-16作为base network。VGG-16精度够高但是计算量较大（对于大小为$224\\times 224$的单幅输入图像，卷积层就需要30.69B次浮点运算）。在YOLO V1中，我们的网络结构较为简单，在精度上不如VGG-16（ImageNet测试，88.0% vs 90.0%）。\n\n在YOLO V2中，我们使用了一种新的网络结构Darknet-19（因为base network有19个卷积层）。和VGG相类似，我们的卷积核也都是$3\\times 3$大小，同时每次pooling操作后channel数double。另外，在NIN工作基础上，我们在网络最后使用了global average pooling层，同时使用$1\\times 1$大小的卷积核来做feature map的压缩（分辨率不变，channel减小，新的元素是原来相应位置不同channel的线性组合），同时使用了Batch Normalization技术。具体的网络结构见下表。Darknet-19计算量大大减小，同时精度超过了VGG-16。\n![Darknet-19的网络结构](/img/yolo2_dartnet_19_structure.png)\n\n在训练过程中，首先在ImageNet 1K上进行分类器的训练。使用数据增强技术（如随机裁剪、旋转、饱和度变化等）。和上面的讨论相对应，首先使用$224\\times 224$大小的图像进行训练，再使用$448\\times 448$的图像进行fine tuning，具体训练参数设置可以参见论文和对应的代码。这里不再多说。\n\n然后，我们对检测任务进行训练。对网络结构进行微调，去掉最后的卷积层（因为它是我们当初为了得到1K类的分类置信度加上去的），增加$3$个$3\\times 3$大小，channel数为$1024$的卷积层，并每个都跟着一个$1\\times 1$大小的卷积层，channel数由我们最终的检测任务需要的输出决定。对于VOC的检测任务来说，我们需要预测$5$个box，每个需要$5$个参数（相对位置，相对大小和置信度），同时$20$个类别的置信度，所以输出共$5\\times(5+20)=125$。从YOLO V2的`yolo_voc.cfg`[文件](https://github.com/pjreddie/darknet/blob/master/cfg/yolo.cfg)中，我们也可以看到如下的对应结构：\n\n```\n[convolutional]\nbatch_normalize=1\nsize=3\nstride=1\npad=1\nfilters=1024\nactivation=leaky\n\n[convolutional]\nsize=1\nstride=1\npad=1\nfilters=125\nactivation=linear\n```\n\n同时，加上上文提到的pass-through结构。\n\n## Stronger\n未完待续\n","source":"_posts/yolo-paper.md","raw":"---\ntitle: YOLO 论文阅读\ndate: 2017-02-04 18:49:22\ntags:\n    - paper\n    - yolo\n    - deep learning\n---\nYOLO(**Y**ou **O**nly **L**ook **O**nce)是一个流行的目标检测方法，和Faster RCNN等state of the art方法比起来，主打检测速度快。截止到目前为止（2017年2月初），YOLO已经发布了两个版本，在下文中分别称为[YOLO V1](https://arxiv.org/abs/1506.02640)和[YOLO V2](https://arxiv.org/abs/1612.08242)。YOLO V2的代码目前作为[Darknet](http://pjreddie.com/darknet/yolo/)的一部分开源在[GitHub]()。在这篇博客中，记录了阅读YOLO两个版本论文中的重点内容，并着重总结V2版本的改进。\n\n![YOLO V2的检测效果示意](/img/yolo2_result.png)\n<!-- more -->\n\n## YOLO V1\n这里不妨把YOLO V1论文[\"You Only Look Once: Unitied, Real-Time Object Detection\"](https://arxiv.org/abs/1506.02640)的摘要部分意译如下：\n\n> 我们提出了一种新的物体检测方法YOLO。之前的目标检测方法大多把分类器重新调整用于检测（这里应该是在说以前的方法大多将检测问题看做分类问题，使用滑动窗提取特征并使用分类器进行分类）。我们将检测问题看做回归，分别给出bounding box的位置和对应的类别概率。对于给定输入图像，只需使用CNN网络计算一次，就同时给出bounding box位置和类别概率。由于整个pipeline都是同一个网络，所以很容易进行端到端的训练。\n> YOLO相当快。base model可以达到45fps，一个更小的model（Fast YOLO），可以达到155fps，同时mAP是其他可以实现实时检测方法的两倍。和目前的State of the art方法相比，YOLO的localization误差较大，但是对于背景有更低的误检（False Positives）。同时，YOLO能够学习到更加泛化的特征，例如艺术品中物体的检测等任务，表现很好。\n\n和以往较为常见的方法，如HoG+SVM的行人检测，DPM，RCNN系方法等不同，YOLO接受image作为输入，直接输出bounding box的位置和对应位置为某一类的概率。我们可以把它和目前的State of the art的Faster RCNN方法对比。Faster RCNN方法需要两个网络RPN和Fast RCNN，其中前者负责接受图像输入，并提出proposal。后续Fast RCNN再给出这些proposal是某一类的概率。也正是因为这种“直来直去”的方法，YOLO才能达到这么快的速度。也真是令人感叹，网络参数够多，数据够多，什么映射关系都能学出来。。。下图就是YOLO的检测系统示意图。在test阶段，经过单个CNN网络前向计算后，再经过非极大值抑制，就可以给出检测结果。\n![YOLO V1检测系统示意图](/img/yolo1_detection_system.png)\n\n### 基本思路\n![基础思路示意图](/img/yolo1_basic_idea.png)\n\n- 网格划分：将输入image划分为$S \\times S$个grid cell，如果image中某个object box的中心落在某个grid cell内部，那么这个cell就对检测该object负责（responsible for detection that object）。同时，每个grid cell同时预测$B$个bounding box的位置和一个置信度。这个置信度并不只是该bounding box是待检测目标的概率，而是该bounding box是待检测目标的概率乘上该bounding box和真实位置的IoU的积。通过乘上这个交并比，反映出该bounding box预测位置的精度。如下式所示：\n\n$$\\text{confidence} = P(\\text{Object})\\times \\text{IoU}_{\\text{pred}}^{\\text{truth}}$$\n\n- 网络输出：每个bounding box对应于5个输出，分别是$x,y,w,h$和上述提到的置信度。其中，$x,y$代表bounding box的中心离开其所在grid cell边界的偏移。$w,h$代表bounding box真实宽高相对于整幅图像的比例。$x,y,w,h$这几个参数都已经被bounded到了区间$[0,1]$上。除此以外，每个grid cell还产生$C$个条件概率，$P(\\text{Class}_i|\\text{Object})$。注意，我们不管$B$的大小，每个grid cell只产生一组这样的概率。在test的非极大值抑制阶段，对于每个bounding box，我们应该按照下式衡量该框是否应该予以保留。\n$$\\text{confidence}\\times P(\\text{Class}_i|\\text{Object}) = P(\\text{Class}_i)\\times \\text{IoU}_{\\text{pred}}^{\\text{truth}}$$\n\n- 实际参数：在PASCAL VOC进行测试时，使用$S = 7$, $B=2$。由于共有20类，故$C=20$。所以，我们的网络输出大小为$7\\times 7 \\times 30$\n\n### 网络模型结构\nInspired by GoogLeNet，但是没有采取inception的结构，simply使用了$1\\times 1$的卷积核。base model共有24个卷积层，后面接2个全连接层，如下图所示。\n![YOLO的网络结构示意图](/img/yolo1_network_arch.png)\n\n另外，Fast YOLO使用了更小的网络结构（9个卷积层，且filter的数目也少了），其他部分完全一样。\n\n### 训练\n同很多人一样，这里作者也是先在ImageNet上做了预训练。使用上图网络结构中的前20个卷积层，后面接一个average-pooling层和全连接层，在ImageNet 1000类数据集上训练了一周，达到了88%的top-5准确率。\n\n由于[Ren的论文](https://arxiv.org/abs/1504.06066)提到给预训练的模型加上额外的卷积层和全连接层能够提升性能，所以我们加上了剩下的4个卷积层和2个全连接层，权值为随机初始化。同时，我们把网络输入的分辨率从$224\\times 224$提升到了$448 \\times 448$。\n\n在最后一层，我们使用了线性激活函数；其它层使用了leaky ReLU激活函数。如下所示：\n$$\nf(x)=\n\\begin{cases}\nx, &\\text{if}\\ x > 0 \\\\\\\\\n0.1x, &\\text{otherwise}\n\\end{cases}\n$$\n\n很重要的问题就是定义很好的loss function，为此，作者提出了以下几点说明：\n\n- loss的形式采用误差平方和的形式（真是把回归进行到底了。。。）\n- 由于很多的grid cell没有目标物体存在，所以给有目标存在的bounding box和没有目标存在的bounding box设置了不同的比例因子进行平衡。具体来说，\n$$\\lambda_{\\text{coord}} = 5，\\lambda_{\\text{noobj}} = 0.5$$\n- 直接使用$w$和$h$，这样大的box的差异在loss中占得比重会和小的box不平衡，所以这里使用$\\sqrt{w}$和$\\sqrt{h}$。\n- 上文中已经提到，同一个grid cell会提出多个bounding box。在training阶段，我们只想让一个bounding box对应object。所以，我们计算每个bounding box和ground truth的IoU，以此为标准得到最好的那个bounding box，其他的认为no obj。\n\nloss函数的具体形式见下图（实在是不想打这一大串公式。。）。其中，$\\mathbb{1}_i$表示是否有目标出现在第$i$个grid cell。\n\n$\\mathbb{1}_{i,j}$表示第$i$个grid cell的第$j$个bounding box是否对某个目标负责。\n![YOLO的损失函数定义](/img/yolo1_loss_fun.png)\n\n在进行反向传播时，由于loss都是二次形式，所以导数形式都是统一的。下面是Darknet中[detection_layer.c](https://github.com/pjreddie/darknet/blob/master/src/detection_layer.c)中训练部分的代码。在代码中，计算了cost（也就是loss）和delta（也就是反向的导数），\n\n``` c\nif(state.train){\n    float avg_iou = 0;\n    float avg_cat = 0;\n    float avg_allcat = 0;\n    float avg_obj = 0;\n    float avg_anyobj = 0;\n    int count = 0;\n    *(l.cost) = 0;\n    int size = l.inputs * l.batch;\n    memset(l.delta, 0, size * sizeof(float));\n    for (b = 0; b < l.batch; ++b){\n        int index = b*l.inputs;\n        // for each grid cell\n        for (i = 0; i < locations; ++i) {   // locations = S * S = 49\n            int truth_index = (b*locations + i)*(1+l.coords+l.classes);\n            int is_obj = state.truth[truth_index];\n            // for each bbox\n            for (j = 0; j < l.n; ++j) {     // l.n = B = 2\n                int p_index = index + locations*l.classes + i*l.n + j;\n                l.delta[p_index] = l.noobject_scale*(0 - l.output[p_index]);\n                // 因为no obj对应的bbox很多，而responsible的只有一个\n                // 这里统一加上，如果一会判断该bbox responsible for object，再把它减去\n                *(l.cost) += l.noobject_scale*pow(l.output[p_index], 2);  \n                avg_anyobj += l.output[p_index];\n            }\n\n            int best_index = -1;\n            float best_iou = 0;\n            float best_rmse = 20;\n            // 该grid cell没有目标，直接返回\n            if (!is_obj){\n                continue;\n            }\n            // 否则，找出responsible的bounding box，计算其他几项的loss\n            int class_index = index + i*l.classes;\n            for(j = 0; j < l.classes; ++j) {\n                l.delta[class_index+j] = l.class_scale * (state.truth[truth_index+1+j] - l.output[class_index+j]);\n                *(l.cost) += l.class_scale * pow(state.truth[truth_index+1+j] - l.output[class_index+j], 2);\n                if(state.truth[truth_index + 1 + j]) avg_cat += l.output[class_index+j];\n                avg_allcat += l.output[class_index+j];\n            }\n\n            box truth = float_to_box(state.truth + truth_index + 1 + l.classes);\n            truth.x /= l.side;\n            truth.y /= l.side;\n            // 找到最好的IoU，对应的bbox是responsible的，记录其index\n            for(j = 0; j < l.n; ++j){\n                int box_index = index + locations*(l.classes + l.n) + (i*l.n + j) * l.coords;\n                box out = float_to_box(l.output + box_index);\n                out.x /= l.side;\n                out.y /= l.side;\n\n                if (l.sqrt){\n                    out.w = out.w*out.w;\n                    out.h = out.h*out.h;\n                }\n\n                float iou  = box_iou(out, truth);\n                //iou = 0;\n                float rmse = box_rmse(out, truth);\n                if(best_iou > 0 || iou > 0){\n                    if(iou > best_iou){\n                        best_iou = iou;\n                        best_index = j;\n                    }\n                }else{\n                    if(rmse < best_rmse){\n                        best_rmse = rmse;\n                        best_index = j;\n                    }\n                }\n            }\n\n            if(l.forced){\n                if(truth.w*truth.h < .1){\n                    best_index = 1;\n                }else{\n                    best_index = 0;\n                }\n            }\n            if(l.random && *(state.net.seen) < 64000){\n                best_index = rand()%l.n;\n            }\n\n            int box_index = index + locations*(l.classes + l.n) + (i*l.n + best_index) * l.coords;\n            int tbox_index = truth_index + 1 + l.classes;\n\n            box out = float_to_box(l.output + box_index);\n            out.x /= l.side;\n            out.y /= l.side;\n            if (l.sqrt) {\n                out.w = out.w*out.w;\n                out.h = out.h*out.h;\n            }\n            float iou  = box_iou(out, truth);\n\n            //printf(\"%d,\", best_index);\n            int p_index = index + locations*l.classes + i*l.n + best_index;\n            *(l.cost) -= l.noobject_scale * pow(l.output[p_index], 2);  // 还记得我们曾经统一加过吗？这里需要减去了\n            *(l.cost) += l.object_scale * pow(1-l.output[p_index], 2);\n            avg_obj += l.output[p_index];\n            l.delta[p_index] = l.object_scale * (1.-l.output[p_index]);\n\n            if(l.rescore){\n                l.delta[p_index] = l.object_scale * (iou - l.output[p_index]);\n            }\n\n            l.delta[box_index+0] = l.coord_scale*(state.truth[tbox_index + 0] - l.output[box_index + 0]);\n            l.delta[box_index+1] = l.coord_scale*(state.truth[tbox_index + 1] - l.output[box_index + 1]);\n            l.delta[box_index+2] = l.coord_scale*(state.truth[tbox_index + 2] - l.output[box_index + 2]);\n            l.delta[box_index+3] = l.coord_scale*(state.truth[tbox_index + 3] - l.output[box_index + 3]);\n            if(l.sqrt){\n                l.delta[box_index+2] = l.coord_scale*(sqrt(state.truth[tbox_index + 2]) - l.output[box_index + 2]);\n                l.delta[box_index+3] = l.coord_scale*(sqrt(state.truth[tbox_index + 3]) - l.output[box_index + 3]);\n            }\n\n            *(l.cost) += pow(1-iou, 2);\n            avg_iou += iou;\n            ++count;\n        }\n    }\n\n```\n\n## YOLO V2\nYOLO V2是原作者在V1基础上做出改进后提出的。为了达到题目中所称的Better，Faster，Stronger的目标，主要改进点如下。当然，具体内容还是要深入论文。\n- 受到Faster RCNN方法的启发，引入了anchor。同时使用了K-Means方法，对anchor数量进行了讨论，在精度和速度之间做出折中；\n- 修改了网络结构，去掉了全连接层，改成了全卷积结构；\n- 引入了WordTree结构，将检测和分类问题做成了一个统一的框架，并充分利用ImageNet和COCO数据集的数据。\n\n下面，还是先把论文的摘要意译如下：\n>我们引入了YOLO 9000模型，它是实时物体检测的State of the art的工作，能够检测超过9K类目标。首先，我们对以前的工作（YOLO V1）做出了若干改进，使其成为了一种在实时检测方法内在PASCAL VOC和COCO上State of the art的效果。通过一种新颖的多尺度训练犯法（multi-scale training method）， YOLO V2适用于不同大小尺寸输入的image，在精度和效率上达到了很好地trade-off。在67fps的速度下，VOC2007上达到了76.8mAP的精度。40fps时，达到了78.6mAP，已经超过了Faster RCNN with ResNet和SSD的精度，同时比它们更快！最后，我们提出了一种能够同时进行检测任务和分类任务的联合训练方法。使用这种方法，我们在COCO detection dataset和ImageNet classification dataset上同时训练模型。这种方法使得我们能够对那些没有label上detection data的数据做出detection的预测。我们使用ImageNet的detection任务做了验证。YOLO 9000在仅有200类中44类的detection data的情况下，仍然在ImageNet datection任务中取得了19.7mAP的成绩。对于不在COCO中的156类，YOLO9000成绩为16.0mAP。我们使得YOLO9000能够在保证实时的前提下对9K类目标进行检测。\n\n根据论文结构的安排，将从Better，Faster和Stronger三个方面对论文提出的各条改进措施进行介绍。\n\n## Better\n在YOLO V1的基础上，作者提出了不少的改进来进一步提升算法的性能（mAP），主要改进措施包括网络结构的改进（第1，3，5，6条）和Anchor Box的引进（第3，4，5条）以及训练方法（第2，7条）。\n\n### 改进1：引入BN层（Batch Normalization）\nBatch Normalization能够加快模型收敛，并提供一定的正则化。作者在每个conv层都加上了了BN层，同时去掉了原来模型中的drop out部分，这带来了2%的性能提升。\n\n### 改进2：高分辨率分类器（High Resolution Classifier）\nYOLO V1首先在ImageNet上以$224\\times 224$大小图像作为输入进行训练，之后在检测任务中提升到$448\\times 448$。这里，作者在训练完224大小的分类网络后，首先调整网络大小为$448\\times 448$，然后在ImageNet上进行fine tuning（10个epoch）。也就是得到了一个高分辨率的cls。再把它用detection上训练。这样，能够提升4%。\n\n### 改进3：引入Anchor Box\nYOLO V1中直接在CNN后面街上全连接层，直接回归bounding box的参数。这里引入了Faster RCNN中的anchor box概念，不再直接回归bounding box的参数，而是相对于anchor box的参数。\n\n作者去掉后面的fc层和最后一个max pooling层，以期得到更高分辨率的feature map。同时，shrink网络接受$416\\times 416$（而不是448）大小的输入image。这是因为我们想要最后的feature map大小是奇数，这样就能够得到一个center cell（比较大的目标，更有可能占据中间的位置）。由于YOLO conv-pooling的效应是将image downsamplig 32倍，所以最后feature map大小为$416/32 = 13$。\n\n与YOLO V1不同的是，我们不再对同一个grid cell下的bounding box统一产生一个数量为$C$的类别概率，而是对于每一个bounding box都产生对应的$C$类概率。和YOLO V1一样的是，我们仍然产生confidence，意义也完全一样。\n\n使用anchor后，我们的精度accuracy降低了，不过recall上来了。（这也较好理解。原来每个grid cell内部只有2个bounding box，造成recall不高。现在recall高上来了，accuracy会下降一些）。\n\n### 改进4：Dimension Cluster\n在引入anchor box后，一个问题就是如何确定anchor的位置和大小？Faster RCNN中是手工选定的，每隔stride设定一个anchor，并根据不同的面积比例和长宽比例产生9个anchor box。在本文中，作者使用了聚类方法对如何选取anchor box做了探究。这点应该是论文中很有新意的地方。\n\n这里对作者使用的方法不再过多赘述，强调以下两点：\n- 作者使用的聚类方法是K-Means；\n- 相似性度量不用欧氏距离，而是用IoU，定义如下：\n$$d(\\text{box}, \\text{centroid}) = 1-\\text{IoU}(\\text{box}, \\text{centroid})$$\n\n使用不同的$k$，聚类实验结果如下，作者折中采用了$k = 5$。而且经过实验，发现当取$k=9$时候，已经能够超过Faster RCNN采用的手工固定anchor box的方法。下图右侧图是在COCO和VOC数据集上$k=5$的聚类后结果。这些box可以作为anchor box使用。\n![](/img/yolo2_cluster_result.png)\n\n### 改进5：直接位置预测（Direct Location Prediction）\n我们仍然延续了YOLO V1中的思路，预测box相对于grid cell的位置。使用sigmoid函数作为激活函数，使得最终输出值落在$[0, 1]$这个区间上。\n\n在output的feature map上，对于每个cell（共计$13\\times 13$个），给出对应每个bounding box的输出$t_x$, $t_y$, $t_w$, $t_h$。每个cell共计$k=5$个bounding box。如何由这几个参数确定bounding box的真实位置呢？见下图。\n\n![确定bbox的位置](/img/yolo2_bbox_location.png)\n\n设该grid cell距离图像左上角的offset是$(c_x, c_y)$，那么bounding box的位置和宽高计算如下。注意，box的位置是相对于grid cell的，而宽高是相对于anchor box的。\n![bounding box参数的计算方法](/img/yolo2_bbox_param.png)\n\nDarknet中的具体的实现代码如下（不停切换中英文输入实在是蛋疼，所以只有用我这蹩脚的英语来注释了。。。）：\n\n``` cpp\n// get bounding box\n// x: data pointer of feature map\n// biases: data pointer of anchor box data\n// biases[2*n] = width of anchor box\n// biases[2*n+1] = height of anchor box\n// n: output bounding box for each cell in the feature map\n// index: output bounding box index in the cell\n// i: `cx` in the paper\n// j: 'cy' in the paper\n// (cx, cy) is the offset from the left top corner of the feature map\n// (w, h) is the size of feature map (do normalization in the code)\nbox get_region_box(float *x, float *biases, int n, int index, int i, int j, int w, int h)\n{\n    box b;\n    // i <- cx, j <- cy\n    // index + 0: tx\n    // index + 1: ty\n    // index + 2: tw\n    // index + 3: th\n    // index + 4: to   // not used here\n    // index + 5, +6, ..., +(C+4)   // confidence of P(class c|Object), not used here\n    b.x = (i + logistic_activate(x[index + 0])) / w;    // bx = cx+sigmoid(tx)\n    b.y = (j + logistic_activate(x[index + 1])) / h;    // by = cy+sigmoid(ty)\n    b.w = exp(x[index + 2]) * biases[2*n]   / w;        // bw = exp(tw) * pw\n    b.h = exp(x[index + 3]) * biases[2*n+1] / h;        // bh = exp(th) * ph\n    // 注意这里都做了Normalization，将值化到[0, 1]，论文里面貌似没有提到\n    // 也就是说YOLO 用于detection层的bounding box大小和位置的输出参数都是相对值\n    return b;\n}\n```\n\n顺便说一下对bounding box的bp实现。具体代码如下：\n\n``` cpp\n// truth: ground truth\n// x: data pointer of feature map\n// biases: data pointer of anchor box data\n// n, index, i, j, w, h: same meaning with `get_region_box`\n// delta: data pointer of gradient\n// scale: just a weight, given by user\nfloat delta_region_box(box truth, float *x, float *biases,\n                       int n, int index, int i, int j, int w, int h,\n                       float *delta, float scale)\n{\n    box pred = get_region_box(x, biases, n, index, i, j, w, h);\n    // get iou of the bbox and truth\n    float iou = box_iou(pred, truth);\n    // ground truth of the parameters (tx, ty, tw, th)\n    float tx = (truth.x*w - i);\n    float ty = (truth.y*h - j);\n    float tw = log(truth.w*w / biases[2*n]);\n    float th = log(truth.h*h / biases[2*n + 1]);\n    // 这里是欧式距离损失的梯度回传\n    // 以tx为例。\n    // loss = 1/2*(bx^hat-bx)^2, 其中bx = cx + sigmoid(tx)\n    // d(loss)/d(tx) = -(bx^hat-bx) * d(bx)/d(tx)\n    // 注意，Darkent中的delta存储的是负梯度数值，所以下面的delta数组内数值实际是-d(loss)/d(tx)\n    // 也就是(bx^hat-bx) * d(bx)/d(tx)\n    // 前面的(bx^hat-bx)把cx约掉了（因为是同一个cell，偏移是一样的）\n    // 后面相当于是求sigmoid函数对输入自变量的梯度。\n    // 由于当初没有缓存 sigomid(tx)，所以作者又重新计算了一次 sigmoid(tx)，也就是下面的激活函数那里\n    delta[index + 0] = scale * (tx - logistic_activate(x[index + 0]))\n                             * logistic_gradient(logistic_activate(x[index + 0]));\n\n    delta[index + 1] = scale * (ty - logistic_activate(x[index + 1]))\n                             * logistic_gradient(logistic_activate(x[index + 1]));\n    // tw 相似，只不过这里的 loss = 1/2(tw^hat-tw)^2，而不是和上面一样使用bw^hat 和 bw\n    delta[index + 2] = scale * (tw - x[index + 2]);\n    delta[index + 3] = scale * (th - x[index + 3]);\n    return iou;\n}\n```\n\n接下来，我们看一下bp的计算。主要涉及到决定bounding box大小和位置的四个参数的回归，以及置信度$t_o$，以及$C$类分类概率。上面的代码中已经介绍了bounding box大小位置的四个参数的梯度计算。对于置信度$t_o$的计算，如下所示。\n\n``` cpp\n// 上面的代码遍历了所有的groundtruth，找出了与当前预测bounding box iou最大的那个\n// 首先，我们认为当前bounding box没有responsible for any groundtruth，\n// 那么，loss = 1/2*(0-sigmoid(to))^2\n// => d(loss)/d(to) = -(0-sigmoid(to)) * d(sigmoid)/d(to)\n// 由于之前的代码中已经将output取了sigmoid作用，所以就有了下面的代码\n// 其中，logistic_gradient(y) 是指dy/dx|(y=y0)的值。具体来说，logistic_gradient(y) = (1-y)*y\nl.delta[index + 4] = l.noobject_scale * ((0 - l.output[index + 4]) * logistic_gradient(l.output[index + 4]));\n// 如果best iou > thresh, 我们认为这个bounding box有了对应的groundtruth，把梯度直接设置为0即可\nif (best_iou > l.thresh) {\n    l.delta[index + 4] = 0;\n}\n```\n\n这里额外要说明的是，阅读代码可以发现，分类loss的计算方法和V1不同，不再使用MSELoss，而是使用了交叉熵损失函数。对应地，梯度计算的方法如下所示。不过这点在论文中貌似并没有体现。\n```\n// for each class\nfor(n = 0; n < classes; ++n){\n    // P_i = \\frac{exp^out_i}{sum of exp^out_j}\n    // SoftmaxLoss = -logP(class)\n    // ∂SoftmaxLoss/∂output = -(1(n==class)-P)\n    delta[index + n] = scale * (((n == class)?1 : 0) - output[index + n]);\n    if(n == class) *avg_cat += output[index + n];\n}\n```\n\n### 改进6：Fine-Gained Features\n这个trick是受Faster RCNN和SSD方法中使用多个不同feature map提高算法对不同分辨率目标物体的检测能力的启发，加入了一个pass-through层，直接将倒数第二层的$26\\times 26$大小的feature map加进来。\n\n在具体实现时，是将higher resolution（也就是$26\\times 26$）的feature map stacking在一起。比如，原大小为$26\\times 26 \\times 512$的feature map，因为我们要将其变为$13\\times 13$大小，所以，将在空间上相近的点移到后面的channel上去，这部分可以参考Darknet中`reorg_layer`的实现。\n\n使用这一扩展之后的feature map，提高了1%的性能提升。\n\n### 改进7：多尺度训练（Multi-Scale Training）\n在实际应用时，输入的图像大小有可能是变化的。我们也将这一点考虑进来。因为我们的网络是全卷积神经网络，只有conv和pooling层，没有全连接层，所以可以适应不同大小的图像输入。所以网络结构上是没问题的。\n\n具体来说，在训练的时候，我们每隔一定的epoch（例如10）就随机改变网络的输入图像大小。由于我们的网络最终降采样的比例是$32$，所以随机生成的图像大小为$32$的倍数，即$\\lbrace 320, 352, \\dots, 608\\rbrace$。\n\n在实际使用中，如果输入图像的分辨率较低，YOLO V2可以在不错的精度下达到很快的检测速度。这可以被用应在计算能力有限的场合（无GPU或者GPU很弱）或多路视频信号的实时处理。如果输入图像的分辨率较高，YOLO V2可以作为state of the art的检测器，并仍能获得不错的检测速度。对于目前流行的检测方法（Faster RCNN，SSD，YOLO）的精度和帧率之间的关系，见下图。可以看到，作者在30fps处画了一条竖线，这是算法能否达到实时处理的分水岭。Faster RCNN败下阵来，而YOLO V2的不同点代表了不同输入图像分辨率下算法的表现。对于详细数据，见图下的表格对比（VOC 2007上进行测试）。\n\n![不同检测方法的对比](/img/yolo2_different_methods_comparation.png)\n![不同检测方法的对比](/img/yolo2_different_methods_comparation_in_table.png)\n\n### 总结\n在Better这部分的末尾，作者给出了一个表格，指出了主要提升性能的措施。例外是网络结构上改为带Anchor box的全卷积网络结构（提升了recall，但对mAP基本无影响）和使用新的网络（计算量少了~33%）。\n![不同改进措施的影响](/img/yolo2_different_methods_improvement.png)\n\n## Faster\n这部分的改进为网络结构的变化。包括Faster RCNN在内的很多检测方法都使用VGG-16作为base network。VGG-16精度够高但是计算量较大（对于大小为$224\\times 224$的单幅输入图像，卷积层就需要30.69B次浮点运算）。在YOLO V1中，我们的网络结构较为简单，在精度上不如VGG-16（ImageNet测试，88.0% vs 90.0%）。\n\n在YOLO V2中，我们使用了一种新的网络结构Darknet-19（因为base network有19个卷积层）。和VGG相类似，我们的卷积核也都是$3\\times 3$大小，同时每次pooling操作后channel数double。另外，在NIN工作基础上，我们在网络最后使用了global average pooling层，同时使用$1\\times 1$大小的卷积核来做feature map的压缩（分辨率不变，channel减小，新的元素是原来相应位置不同channel的线性组合），同时使用了Batch Normalization技术。具体的网络结构见下表。Darknet-19计算量大大减小，同时精度超过了VGG-16。\n![Darknet-19的网络结构](/img/yolo2_dartnet_19_structure.png)\n\n在训练过程中，首先在ImageNet 1K上进行分类器的训练。使用数据增强技术（如随机裁剪、旋转、饱和度变化等）。和上面的讨论相对应，首先使用$224\\times 224$大小的图像进行训练，再使用$448\\times 448$的图像进行fine tuning，具体训练参数设置可以参见论文和对应的代码。这里不再多说。\n\n然后，我们对检测任务进行训练。对网络结构进行微调，去掉最后的卷积层（因为它是我们当初为了得到1K类的分类置信度加上去的），增加$3$个$3\\times 3$大小，channel数为$1024$的卷积层，并每个都跟着一个$1\\times 1$大小的卷积层，channel数由我们最终的检测任务需要的输出决定。对于VOC的检测任务来说，我们需要预测$5$个box，每个需要$5$个参数（相对位置，相对大小和置信度），同时$20$个类别的置信度，所以输出共$5\\times(5+20)=125$。从YOLO V2的`yolo_voc.cfg`[文件](https://github.com/pjreddie/darknet/blob/master/cfg/yolo.cfg)中，我们也可以看到如下的对应结构：\n\n```\n[convolutional]\nbatch_normalize=1\nsize=3\nstride=1\npad=1\nfilters=1024\nactivation=leaky\n\n[convolutional]\nsize=1\nstride=1\npad=1\nfilters=125\nactivation=linear\n```\n\n同时，加上上文提到的pass-through结构。\n\n## Stronger\n未完待续\n","slug":"yolo-paper","published":1,"updated":"2018-01-12T06:22:20.485Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjcu6vcuq002wqu465ouay5n8","content":"<p>YOLO(<strong>Y</strong>ou <strong>O</strong>nly <strong>L</strong>ook <strong>O</strong>nce)是一个流行的目标检测方法，和Faster RCNN等state of the art方法比起来，主打检测速度快。截止到目前为止（2017年2月初），YOLO已经发布了两个版本，在下文中分别称为<a href=\"https://arxiv.org/abs/1506.02640\" target=\"_blank\" rel=\"external\">YOLO V1</a>和<a href=\"https://arxiv.org/abs/1612.08242\" target=\"_blank\" rel=\"external\">YOLO V2</a>。YOLO V2的代码目前作为<a href=\"http://pjreddie.com/darknet/yolo/\" target=\"_blank\" rel=\"external\">Darknet</a>的一部分开源在<a href=\"\">GitHub</a>。在这篇博客中，记录了阅读YOLO两个版本论文中的重点内容，并着重总结V2版本的改进。</p>\n<p><img src=\"/img/yolo2_result.png\" alt=\"YOLO V2的检测效果示意\"><br><a id=\"more\"></a></p>\n<h2 id=\"YOLO-V1\"><a href=\"#YOLO-V1\" class=\"headerlink\" title=\"YOLO V1\"></a>YOLO V1</h2><p>这里不妨把YOLO V1论文<a href=\"https://arxiv.org/abs/1506.02640\" target=\"_blank\" rel=\"external\">“You Only Look Once: Unitied, Real-Time Object Detection”</a>的摘要部分意译如下：</p>\n<blockquote>\n<p>我们提出了一种新的物体检测方法YOLO。之前的目标检测方法大多把分类器重新调整用于检测（这里应该是在说以前的方法大多将检测问题看做分类问题，使用滑动窗提取特征并使用分类器进行分类）。我们将检测问题看做回归，分别给出bounding box的位置和对应的类别概率。对于给定输入图像，只需使用CNN网络计算一次，就同时给出bounding box位置和类别概率。由于整个pipeline都是同一个网络，所以很容易进行端到端的训练。<br>YOLO相当快。base model可以达到45fps，一个更小的model（Fast YOLO），可以达到155fps，同时mAP是其他可以实现实时检测方法的两倍。和目前的State of the art方法相比，YOLO的localization误差较大，但是对于背景有更低的误检（False Positives）。同时，YOLO能够学习到更加泛化的特征，例如艺术品中物体的检测等任务，表现很好。</p>\n</blockquote>\n<p>和以往较为常见的方法，如HoG+SVM的行人检测，DPM，RCNN系方法等不同，YOLO接受image作为输入，直接输出bounding box的位置和对应位置为某一类的概率。我们可以把它和目前的State of the art的Faster RCNN方法对比。Faster RCNN方法需要两个网络RPN和Fast RCNN，其中前者负责接受图像输入，并提出proposal。后续Fast RCNN再给出这些proposal是某一类的概率。也正是因为这种“直来直去”的方法，YOLO才能达到这么快的速度。也真是令人感叹，网络参数够多，数据够多，什么映射关系都能学出来。。。下图就是YOLO的检测系统示意图。在test阶段，经过单个CNN网络前向计算后，再经过非极大值抑制，就可以给出检测结果。<br><img src=\"/img/yolo1_detection_system.png\" alt=\"YOLO V1检测系统示意图\"></p>\n<h3 id=\"基本思路\"><a href=\"#基本思路\" class=\"headerlink\" title=\"基本思路\"></a>基本思路</h3><p><img src=\"/img/yolo1_basic_idea.png\" alt=\"基础思路示意图\"></p>\n<ul>\n<li>网格划分：将输入image划分为$S \\times S$个grid cell，如果image中某个object box的中心落在某个grid cell内部，那么这个cell就对检测该object负责（responsible for detection that object）。同时，每个grid cell同时预测$B$个bounding box的位置和一个置信度。这个置信度并不只是该bounding box是待检测目标的概率，而是该bounding box是待检测目标的概率乘上该bounding box和真实位置的IoU的积。通过乘上这个交并比，反映出该bounding box预测位置的精度。如下式所示：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\\text{confidence} = P(\\text{Object})\\times \\text{IoU}_{\\text{pred}}^{\\text{truth}}</script><ul>\n<li><p>网络输出：每个bounding box对应于5个输出，分别是$x,y,w,h$和上述提到的置信度。其中，$x,y$代表bounding box的中心离开其所在grid cell边界的偏移。$w,h$代表bounding box真实宽高相对于整幅图像的比例。$x,y,w,h$这几个参数都已经被bounded到了区间$[0,1]$上。除此以外，每个grid cell还产生$C$个条件概率，$P(\\text{Class}_i|\\text{Object})$。注意，我们不管$B$的大小，每个grid cell只产生一组这样的概率。在test的非极大值抑制阶段，对于每个bounding box，我们应该按照下式衡量该框是否应该予以保留。</p>\n<script type=\"math/tex; mode=display\">\\text{confidence}\\times P(\\text{Class}_i|\\text{Object}) = P(\\text{Class}_i)\\times \\text{IoU}_{\\text{pred}}^{\\text{truth}}</script></li>\n<li><p>实际参数：在PASCAL VOC进行测试时，使用$S = 7$, $B=2$。由于共有20类，故$C=20$。所以，我们的网络输出大小为$7\\times 7 \\times 30$</p>\n</li>\n</ul>\n<h3 id=\"网络模型结构\"><a href=\"#网络模型结构\" class=\"headerlink\" title=\"网络模型结构\"></a>网络模型结构</h3><p>Inspired by GoogLeNet，但是没有采取inception的结构，simply使用了$1\\times 1$的卷积核。base model共有24个卷积层，后面接2个全连接层，如下图所示。<br><img src=\"/img/yolo1_network_arch.png\" alt=\"YOLO的网络结构示意图\"></p>\n<p>另外，Fast YOLO使用了更小的网络结构（9个卷积层，且filter的数目也少了），其他部分完全一样。</p>\n<h3 id=\"训练\"><a href=\"#训练\" class=\"headerlink\" title=\"训练\"></a>训练</h3><p>同很多人一样，这里作者也是先在ImageNet上做了预训练。使用上图网络结构中的前20个卷积层，后面接一个average-pooling层和全连接层，在ImageNet 1000类数据集上训练了一周，达到了88%的top-5准确率。</p>\n<p>由于<a href=\"https://arxiv.org/abs/1504.06066\" target=\"_blank\" rel=\"external\">Ren的论文</a>提到给预训练的模型加上额外的卷积层和全连接层能够提升性能，所以我们加上了剩下的4个卷积层和2个全连接层，权值为随机初始化。同时，我们把网络输入的分辨率从$224\\times 224$提升到了$448 \\times 448$。</p>\n<p>在最后一层，我们使用了线性激活函数；其它层使用了leaky ReLU激活函数。如下所示：</p>\n<script type=\"math/tex; mode=display\">\nf(x)=\n\\begin{cases}\nx, &\\text{if}\\ x > 0 \\\\\\\\\n0.1x, &\\text{otherwise}\n\\end{cases}</script><p>很重要的问题就是定义很好的loss function，为此，作者提出了以下几点说明：</p>\n<ul>\n<li>loss的形式采用误差平方和的形式（真是把回归进行到底了。。。）</li>\n<li>由于很多的grid cell没有目标物体存在，所以给有目标存在的bounding box和没有目标存在的bounding box设置了不同的比例因子进行平衡。具体来说，<script type=\"math/tex; mode=display\">\\lambda_{\\text{coord}} = 5，\\lambda_{\\text{noobj}} = 0.5</script></li>\n<li>直接使用$w$和$h$，这样大的box的差异在loss中占得比重会和小的box不平衡，所以这里使用$\\sqrt{w}$和$\\sqrt{h}$。</li>\n<li>上文中已经提到，同一个grid cell会提出多个bounding box。在training阶段，我们只想让一个bounding box对应object。所以，我们计算每个bounding box和ground truth的IoU，以此为标准得到最好的那个bounding box，其他的认为no obj。</li>\n</ul>\n<p>loss函数的具体形式见下图（实在是不想打这一大串公式。。）。其中，$\\mathbb{1}_i$表示是否有目标出现在第$i$个grid cell。</p>\n<p>$\\mathbb{1}_{i,j}$表示第$i$个grid cell的第$j$个bounding box是否对某个目标负责。<br><img src=\"/img/yolo1_loss_fun.png\" alt=\"YOLO的损失函数定义\"></p>\n<p>在进行反向传播时，由于loss都是二次形式，所以导数形式都是统一的。下面是Darknet中<a href=\"https://github.com/pjreddie/darknet/blob/master/src/detection_layer.c\" target=\"_blank\" rel=\"external\">detection_layer.c</a>中训练部分的代码。在代码中，计算了cost（也就是loss）和delta（也就是反向的导数），</p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div><div class=\"line\">47</div><div class=\"line\">48</div><div class=\"line\">49</div><div class=\"line\">50</div><div class=\"line\">51</div><div class=\"line\">52</div><div class=\"line\">53</div><div class=\"line\">54</div><div class=\"line\">55</div><div class=\"line\">56</div><div class=\"line\">57</div><div class=\"line\">58</div><div class=\"line\">59</div><div class=\"line\">60</div><div class=\"line\">61</div><div class=\"line\">62</div><div class=\"line\">63</div><div class=\"line\">64</div><div class=\"line\">65</div><div class=\"line\">66</div><div class=\"line\">67</div><div class=\"line\">68</div><div class=\"line\">69</div><div class=\"line\">70</div><div class=\"line\">71</div><div class=\"line\">72</div><div class=\"line\">73</div><div class=\"line\">74</div><div class=\"line\">75</div><div class=\"line\">76</div><div class=\"line\">77</div><div class=\"line\">78</div><div class=\"line\">79</div><div class=\"line\">80</div><div class=\"line\">81</div><div class=\"line\">82</div><div class=\"line\">83</div><div class=\"line\">84</div><div class=\"line\">85</div><div class=\"line\">86</div><div class=\"line\">87</div><div class=\"line\">88</div><div class=\"line\">89</div><div class=\"line\">90</div><div class=\"line\">91</div><div class=\"line\">92</div><div class=\"line\">93</div><div class=\"line\">94</div><div class=\"line\">95</div><div class=\"line\">96</div><div class=\"line\">97</div><div class=\"line\">98</div><div class=\"line\">99</div><div class=\"line\">100</div><div class=\"line\">101</div><div class=\"line\">102</div><div class=\"line\">103</div><div class=\"line\">104</div><div class=\"line\">105</div><div class=\"line\">106</div><div class=\"line\">107</div><div class=\"line\">108</div><div class=\"line\">109</div><div class=\"line\">110</div><div class=\"line\">111</div><div class=\"line\">112</div><div class=\"line\">113</div><div class=\"line\">114</div><div class=\"line\">115</div><div class=\"line\">116</div><div class=\"line\">117</div><div class=\"line\">118</div><div class=\"line\">119</div><div class=\"line\">120</div><div class=\"line\">121</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">if</span>(state.train)&#123;</div><div class=\"line\">    <span class=\"keyword\">float</span> avg_iou = <span class=\"number\">0</span>;</div><div class=\"line\">    <span class=\"keyword\">float</span> avg_cat = <span class=\"number\">0</span>;</div><div class=\"line\">    <span class=\"keyword\">float</span> avg_allcat = <span class=\"number\">0</span>;</div><div class=\"line\">    <span class=\"keyword\">float</span> avg_obj = <span class=\"number\">0</span>;</div><div class=\"line\">    <span class=\"keyword\">float</span> avg_anyobj = <span class=\"number\">0</span>;</div><div class=\"line\">    <span class=\"keyword\">int</span> count = <span class=\"number\">0</span>;</div><div class=\"line\">    *(l.cost) = <span class=\"number\">0</span>;</div><div class=\"line\">    <span class=\"keyword\">int</span> size = l.inputs * l.batch;</div><div class=\"line\">    <span class=\"built_in\">memset</span>(l.delta, <span class=\"number\">0</span>, size * <span class=\"keyword\">sizeof</span>(<span class=\"keyword\">float</span>));</div><div class=\"line\">    <span class=\"keyword\">for</span> (b = <span class=\"number\">0</span>; b &lt; l.batch; ++b)&#123;</div><div class=\"line\">        <span class=\"keyword\">int</span> index = b*l.inputs;</div><div class=\"line\">        <span class=\"comment\">// for each grid cell</span></div><div class=\"line\">        <span class=\"keyword\">for</span> (i = <span class=\"number\">0</span>; i &lt; locations; ++i) &#123;   <span class=\"comment\">// locations = S * S = 49</span></div><div class=\"line\">            <span class=\"keyword\">int</span> truth_index = (b*locations + i)*(<span class=\"number\">1</span>+l.coords+l.classes);</div><div class=\"line\">            <span class=\"keyword\">int</span> is_obj = state.truth[truth_index];</div><div class=\"line\">            <span class=\"comment\">// for each bbox</span></div><div class=\"line\">            <span class=\"keyword\">for</span> (j = <span class=\"number\">0</span>; j &lt; l.n; ++j) &#123;     <span class=\"comment\">// l.n = B = 2</span></div><div class=\"line\">                <span class=\"keyword\">int</span> p_index = index + locations*l.classes + i*l.n + j;</div><div class=\"line\">                l.delta[p_index] = l.noobject_scale*(<span class=\"number\">0</span> - l.output[p_index]);</div><div class=\"line\">                <span class=\"comment\">// 因为no obj对应的bbox很多，而responsible的只有一个</span></div><div class=\"line\">                <span class=\"comment\">// 这里统一加上，如果一会判断该bbox responsible for object，再把它减去</span></div><div class=\"line\">                *(l.cost) += l.noobject_scale*<span class=\"built_in\">pow</span>(l.output[p_index], <span class=\"number\">2</span>);  </div><div class=\"line\">                avg_anyobj += l.output[p_index];</div><div class=\"line\">            &#125;</div><div class=\"line\"></div><div class=\"line\">            <span class=\"keyword\">int</span> best_index = <span class=\"number\">-1</span>;</div><div class=\"line\">            <span class=\"keyword\">float</span> best_iou = <span class=\"number\">0</span>;</div><div class=\"line\">            <span class=\"keyword\">float</span> best_rmse = <span class=\"number\">20</span>;</div><div class=\"line\">            <span class=\"comment\">// 该grid cell没有目标，直接返回</span></div><div class=\"line\">            <span class=\"keyword\">if</span> (!is_obj)&#123;</div><div class=\"line\">                <span class=\"keyword\">continue</span>;</div><div class=\"line\">            &#125;</div><div class=\"line\">            <span class=\"comment\">// 否则，找出responsible的bounding box，计算其他几项的loss</span></div><div class=\"line\">            <span class=\"keyword\">int</span> class_index = index + i*l.classes;</div><div class=\"line\">            <span class=\"keyword\">for</span>(j = <span class=\"number\">0</span>; j &lt; l.classes; ++j) &#123;</div><div class=\"line\">                l.delta[class_index+j] = l.class_scale * (state.truth[truth_index+<span class=\"number\">1</span>+j] - l.output[class_index+j]);</div><div class=\"line\">                *(l.cost) += l.class_scale * <span class=\"built_in\">pow</span>(state.truth[truth_index+<span class=\"number\">1</span>+j] - l.output[class_index+j], <span class=\"number\">2</span>);</div><div class=\"line\">                <span class=\"keyword\">if</span>(state.truth[truth_index + <span class=\"number\">1</span> + j]) avg_cat += l.output[class_index+j];</div><div class=\"line\">                avg_allcat += l.output[class_index+j];</div><div class=\"line\">            &#125;</div><div class=\"line\"></div><div class=\"line\">            box truth = float_to_box(state.truth + truth_index + <span class=\"number\">1</span> + l.classes);</div><div class=\"line\">            truth.x /= l.side;</div><div class=\"line\">            truth.y /= l.side;</div><div class=\"line\">            <span class=\"comment\">// 找到最好的IoU，对应的bbox是responsible的，记录其index</span></div><div class=\"line\">            <span class=\"keyword\">for</span>(j = <span class=\"number\">0</span>; j &lt; l.n; ++j)&#123;</div><div class=\"line\">                <span class=\"keyword\">int</span> box_index = index + locations*(l.classes + l.n) + (i*l.n + j) * l.coords;</div><div class=\"line\">                box out = float_to_box(l.output + box_index);</div><div class=\"line\">                out.x /= l.side;</div><div class=\"line\">                out.y /= l.side;</div><div class=\"line\"></div><div class=\"line\">                <span class=\"keyword\">if</span> (l.<span class=\"built_in\">sqrt</span>)&#123;</div><div class=\"line\">                    out.w = out.w*out.w;</div><div class=\"line\">                    out.h = out.h*out.h;</div><div class=\"line\">                &#125;</div><div class=\"line\"></div><div class=\"line\">                <span class=\"keyword\">float</span> iou  = box_iou(out, truth);</div><div class=\"line\">                <span class=\"comment\">//iou = 0;</span></div><div class=\"line\">                <span class=\"keyword\">float</span> rmse = box_rmse(out, truth);</div><div class=\"line\">                <span class=\"keyword\">if</span>(best_iou &gt; <span class=\"number\">0</span> || iou &gt; <span class=\"number\">0</span>)&#123;</div><div class=\"line\">                    <span class=\"keyword\">if</span>(iou &gt; best_iou)&#123;</div><div class=\"line\">                        best_iou = iou;</div><div class=\"line\">                        best_index = j;</div><div class=\"line\">                    &#125;</div><div class=\"line\">                &#125;<span class=\"keyword\">else</span>&#123;</div><div class=\"line\">                    <span class=\"keyword\">if</span>(rmse &lt; best_rmse)&#123;</div><div class=\"line\">                        best_rmse = rmse;</div><div class=\"line\">                        best_index = j;</div><div class=\"line\">                    &#125;</div><div class=\"line\">                &#125;</div><div class=\"line\">            &#125;</div><div class=\"line\"></div><div class=\"line\">            <span class=\"keyword\">if</span>(l.forced)&#123;</div><div class=\"line\">                <span class=\"keyword\">if</span>(truth.w*truth.h &lt; <span class=\"number\">.1</span>)&#123;</div><div class=\"line\">                    best_index = <span class=\"number\">1</span>;</div><div class=\"line\">                &#125;<span class=\"keyword\">else</span>&#123;</div><div class=\"line\">                    best_index = <span class=\"number\">0</span>;</div><div class=\"line\">                &#125;</div><div class=\"line\">            &#125;</div><div class=\"line\">            <span class=\"keyword\">if</span>(l.random &amp;&amp; *(state.net.seen) &lt; <span class=\"number\">64000</span>)&#123;</div><div class=\"line\">                best_index = rand()%l.n;</div><div class=\"line\">            &#125;</div><div class=\"line\"></div><div class=\"line\">            <span class=\"keyword\">int</span> box_index = index + locations*(l.classes + l.n) + (i*l.n + best_index) * l.coords;</div><div class=\"line\">            <span class=\"keyword\">int</span> tbox_index = truth_index + <span class=\"number\">1</span> + l.classes;</div><div class=\"line\"></div><div class=\"line\">            box out = float_to_box(l.output + box_index);</div><div class=\"line\">            out.x /= l.side;</div><div class=\"line\">            out.y /= l.side;</div><div class=\"line\">            <span class=\"keyword\">if</span> (l.<span class=\"built_in\">sqrt</span>) &#123;</div><div class=\"line\">                out.w = out.w*out.w;</div><div class=\"line\">                out.h = out.h*out.h;</div><div class=\"line\">            &#125;</div><div class=\"line\">            <span class=\"keyword\">float</span> iou  = box_iou(out, truth);</div><div class=\"line\"></div><div class=\"line\">            <span class=\"comment\">//printf(\"%d,\", best_index);</span></div><div class=\"line\">            <span class=\"keyword\">int</span> p_index = index + locations*l.classes + i*l.n + best_index;</div><div class=\"line\">            *(l.cost) -= l.noobject_scale * <span class=\"built_in\">pow</span>(l.output[p_index], <span class=\"number\">2</span>);  <span class=\"comment\">// 还记得我们曾经统一加过吗？这里需要减去了</span></div><div class=\"line\">            *(l.cost) += l.object_scale * <span class=\"built_in\">pow</span>(<span class=\"number\">1</span>-l.output[p_index], <span class=\"number\">2</span>);</div><div class=\"line\">            avg_obj += l.output[p_index];</div><div class=\"line\">            l.delta[p_index] = l.object_scale * (<span class=\"number\">1.</span>-l.output[p_index]);</div><div class=\"line\"></div><div class=\"line\">            <span class=\"keyword\">if</span>(l.rescore)&#123;</div><div class=\"line\">                l.delta[p_index] = l.object_scale * (iou - l.output[p_index]);</div><div class=\"line\">            &#125;</div><div class=\"line\"></div><div class=\"line\">            l.delta[box_index+<span class=\"number\">0</span>] = l.coord_scale*(state.truth[tbox_index + <span class=\"number\">0</span>] - l.output[box_index + <span class=\"number\">0</span>]);</div><div class=\"line\">            l.delta[box_index+<span class=\"number\">1</span>] = l.coord_scale*(state.truth[tbox_index + <span class=\"number\">1</span>] - l.output[box_index + <span class=\"number\">1</span>]);</div><div class=\"line\">            l.delta[box_index+<span class=\"number\">2</span>] = l.coord_scale*(state.truth[tbox_index + <span class=\"number\">2</span>] - l.output[box_index + <span class=\"number\">2</span>]);</div><div class=\"line\">            l.delta[box_index+<span class=\"number\">3</span>] = l.coord_scale*(state.truth[tbox_index + <span class=\"number\">3</span>] - l.output[box_index + <span class=\"number\">3</span>]);</div><div class=\"line\">            <span class=\"keyword\">if</span>(l.<span class=\"built_in\">sqrt</span>)&#123;</div><div class=\"line\">                l.delta[box_index+<span class=\"number\">2</span>] = l.coord_scale*(<span class=\"built_in\">sqrt</span>(state.truth[tbox_index + <span class=\"number\">2</span>]) - l.output[box_index + <span class=\"number\">2</span>]);</div><div class=\"line\">                l.delta[box_index+<span class=\"number\">3</span>] = l.coord_scale*(<span class=\"built_in\">sqrt</span>(state.truth[tbox_index + <span class=\"number\">3</span>]) - l.output[box_index + <span class=\"number\">3</span>]);</div><div class=\"line\">            &#125;</div><div class=\"line\"></div><div class=\"line\">            *(l.cost) += <span class=\"built_in\">pow</span>(<span class=\"number\">1</span>-iou, <span class=\"number\">2</span>);</div><div class=\"line\">            avg_iou += iou;</div><div class=\"line\">            ++count;</div><div class=\"line\">        &#125;</div><div class=\"line\">    &#125;</div></pre></td></tr></table></figure>\n<h2 id=\"YOLO-V2\"><a href=\"#YOLO-V2\" class=\"headerlink\" title=\"YOLO V2\"></a>YOLO V2</h2><p>YOLO V2是原作者在V1基础上做出改进后提出的。为了达到题目中所称的Better，Faster，Stronger的目标，主要改进点如下。当然，具体内容还是要深入论文。</p>\n<ul>\n<li>受到Faster RCNN方法的启发，引入了anchor。同时使用了K-Means方法，对anchor数量进行了讨论，在精度和速度之间做出折中；</li>\n<li>修改了网络结构，去掉了全连接层，改成了全卷积结构；</li>\n<li>引入了WordTree结构，将检测和分类问题做成了一个统一的框架，并充分利用ImageNet和COCO数据集的数据。</li>\n</ul>\n<p>下面，还是先把论文的摘要意译如下：</p>\n<blockquote>\n<p>我们引入了YOLO 9000模型，它是实时物体检测的State of the art的工作，能够检测超过9K类目标。首先，我们对以前的工作（YOLO V1）做出了若干改进，使其成为了一种在实时检测方法内在PASCAL VOC和COCO上State of the art的效果。通过一种新颖的多尺度训练犯法（multi-scale training method）， YOLO V2适用于不同大小尺寸输入的image，在精度和效率上达到了很好地trade-off。在67fps的速度下，VOC2007上达到了76.8mAP的精度。40fps时，达到了78.6mAP，已经超过了Faster RCNN with ResNet和SSD的精度，同时比它们更快！最后，我们提出了一种能够同时进行检测任务和分类任务的联合训练方法。使用这种方法，我们在COCO detection dataset和ImageNet classification dataset上同时训练模型。这种方法使得我们能够对那些没有label上detection data的数据做出detection的预测。我们使用ImageNet的detection任务做了验证。YOLO 9000在仅有200类中44类的detection data的情况下，仍然在ImageNet datection任务中取得了19.7mAP的成绩。对于不在COCO中的156类，YOLO9000成绩为16.0mAP。我们使得YOLO9000能够在保证实时的前提下对9K类目标进行检测。</p>\n</blockquote>\n<p>根据论文结构的安排，将从Better，Faster和Stronger三个方面对论文提出的各条改进措施进行介绍。</p>\n<h2 id=\"Better\"><a href=\"#Better\" class=\"headerlink\" title=\"Better\"></a>Better</h2><p>在YOLO V1的基础上，作者提出了不少的改进来进一步提升算法的性能（mAP），主要改进措施包括网络结构的改进（第1，3，5，6条）和Anchor Box的引进（第3，4，5条）以及训练方法（第2，7条）。</p>\n<h3 id=\"改进1：引入BN层（Batch-Normalization）\"><a href=\"#改进1：引入BN层（Batch-Normalization）\" class=\"headerlink\" title=\"改进1：引入BN层（Batch Normalization）\"></a>改进1：引入BN层（Batch Normalization）</h3><p>Batch Normalization能够加快模型收敛，并提供一定的正则化。作者在每个conv层都加上了了BN层，同时去掉了原来模型中的drop out部分，这带来了2%的性能提升。</p>\n<h3 id=\"改进2：高分辨率分类器（High-Resolution-Classifier）\"><a href=\"#改进2：高分辨率分类器（High-Resolution-Classifier）\" class=\"headerlink\" title=\"改进2：高分辨率分类器（High Resolution Classifier）\"></a>改进2：高分辨率分类器（High Resolution Classifier）</h3><p>YOLO V1首先在ImageNet上以$224\\times 224$大小图像作为输入进行训练，之后在检测任务中提升到$448\\times 448$。这里，作者在训练完224大小的分类网络后，首先调整网络大小为$448\\times 448$，然后在ImageNet上进行fine tuning（10个epoch）。也就是得到了一个高分辨率的cls。再把它用detection上训练。这样，能够提升4%。</p>\n<h3 id=\"改进3：引入Anchor-Box\"><a href=\"#改进3：引入Anchor-Box\" class=\"headerlink\" title=\"改进3：引入Anchor Box\"></a>改进3：引入Anchor Box</h3><p>YOLO V1中直接在CNN后面街上全连接层，直接回归bounding box的参数。这里引入了Faster RCNN中的anchor box概念，不再直接回归bounding box的参数，而是相对于anchor box的参数。</p>\n<p>作者去掉后面的fc层和最后一个max pooling层，以期得到更高分辨率的feature map。同时，shrink网络接受$416\\times 416$（而不是448）大小的输入image。这是因为我们想要最后的feature map大小是奇数，这样就能够得到一个center cell（比较大的目标，更有可能占据中间的位置）。由于YOLO conv-pooling的效应是将image downsamplig 32倍，所以最后feature map大小为$416/32 = 13$。</p>\n<p>与YOLO V1不同的是，我们不再对同一个grid cell下的bounding box统一产生一个数量为$C$的类别概率，而是对于每一个bounding box都产生对应的$C$类概率。和YOLO V1一样的是，我们仍然产生confidence，意义也完全一样。</p>\n<p>使用anchor后，我们的精度accuracy降低了，不过recall上来了。（这也较好理解。原来每个grid cell内部只有2个bounding box，造成recall不高。现在recall高上来了，accuracy会下降一些）。</p>\n<h3 id=\"改进4：Dimension-Cluster\"><a href=\"#改进4：Dimension-Cluster\" class=\"headerlink\" title=\"改进4：Dimension Cluster\"></a>改进4：Dimension Cluster</h3><p>在引入anchor box后，一个问题就是如何确定anchor的位置和大小？Faster RCNN中是手工选定的，每隔stride设定一个anchor，并根据不同的面积比例和长宽比例产生9个anchor box。在本文中，作者使用了聚类方法对如何选取anchor box做了探究。这点应该是论文中很有新意的地方。</p>\n<p>这里对作者使用的方法不再过多赘述，强调以下两点：</p>\n<ul>\n<li>作者使用的聚类方法是K-Means；</li>\n<li>相似性度量不用欧氏距离，而是用IoU，定义如下：<script type=\"math/tex; mode=display\">d(\\text{box}, \\text{centroid}) = 1-\\text{IoU}(\\text{box}, \\text{centroid})</script></li>\n</ul>\n<p>使用不同的$k$，聚类实验结果如下，作者折中采用了$k = 5$。而且经过实验，发现当取$k=9$时候，已经能够超过Faster RCNN采用的手工固定anchor box的方法。下图右侧图是在COCO和VOC数据集上$k=5$的聚类后结果。这些box可以作为anchor box使用。<br><img src=\"/img/yolo2_cluster_result.png\" alt=\"\"></p>\n<h3 id=\"改进5：直接位置预测（Direct-Location-Prediction）\"><a href=\"#改进5：直接位置预测（Direct-Location-Prediction）\" class=\"headerlink\" title=\"改进5：直接位置预测（Direct Location Prediction）\"></a>改进5：直接位置预测（Direct Location Prediction）</h3><p>我们仍然延续了YOLO V1中的思路，预测box相对于grid cell的位置。使用sigmoid函数作为激活函数，使得最终输出值落在$[0, 1]$这个区间上。</p>\n<p>在output的feature map上，对于每个cell（共计$13\\times 13$个），给出对应每个bounding box的输出$t_x$, $t_y$, $t_w$, $t_h$。每个cell共计$k=5$个bounding box。如何由这几个参数确定bounding box的真实位置呢？见下图。</p>\n<p><img src=\"/img/yolo2_bbox_location.png\" alt=\"确定bbox的位置\"></p>\n<p>设该grid cell距离图像左上角的offset是$(c_x, c_y)$，那么bounding box的位置和宽高计算如下。注意，box的位置是相对于grid cell的，而宽高是相对于anchor box的。<br><img src=\"/img/yolo2_bbox_param.png\" alt=\"bounding box参数的计算方法\"></p>\n<p>Darknet中的具体的实现代码如下（不停切换中英文输入实在是蛋疼，所以只有用我这蹩脚的英语来注释了。。。）：</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\">// get bounding box</span></div><div class=\"line\"><span class=\"comment\">// x: data pointer of feature map</span></div><div class=\"line\"><span class=\"comment\">// biases: data pointer of anchor box data</span></div><div class=\"line\"><span class=\"comment\">// biases[2*n] = width of anchor box</span></div><div class=\"line\"><span class=\"comment\">// biases[2*n+1] = height of anchor box</span></div><div class=\"line\"><span class=\"comment\">// n: output bounding box for each cell in the feature map</span></div><div class=\"line\"><span class=\"comment\">// index: output bounding box index in the cell</span></div><div class=\"line\"><span class=\"comment\">// i: `cx` in the paper</span></div><div class=\"line\"><span class=\"comment\">// j: 'cy' in the paper</span></div><div class=\"line\"><span class=\"comment\">// (cx, cy) is the offset from the left top corner of the feature map</span></div><div class=\"line\"><span class=\"comment\">// (w, h) is the size of feature map (do normalization in the code)</span></div><div class=\"line\"><span class=\"function\">box <span class=\"title\">get_region_box</span><span class=\"params\">(<span class=\"keyword\">float</span> *x, <span class=\"keyword\">float</span> *biases, <span class=\"keyword\">int</span> n, <span class=\"keyword\">int</span> index, <span class=\"keyword\">int</span> i, <span class=\"keyword\">int</span> j, <span class=\"keyword\">int</span> w, <span class=\"keyword\">int</span> h)</span></span></div><div class=\"line\">&#123;</div><div class=\"line\">    box b;</div><div class=\"line\">    <span class=\"comment\">// i &lt;- cx, j &lt;- cy</span></div><div class=\"line\">    <span class=\"comment\">// index + 0: tx</span></div><div class=\"line\">    <span class=\"comment\">// index + 1: ty</span></div><div class=\"line\">    <span class=\"comment\">// index + 2: tw</span></div><div class=\"line\">    <span class=\"comment\">// index + 3: th</span></div><div class=\"line\">    <span class=\"comment\">// index + 4: to   // not used here</span></div><div class=\"line\">    <span class=\"comment\">// index + 5, +6, ..., +(C+4)   // confidence of P(class c|Object), not used here</span></div><div class=\"line\">    b.x = (i + logistic_activate(x[index + <span class=\"number\">0</span>])) / w;    <span class=\"comment\">// bx = cx+sigmoid(tx)</span></div><div class=\"line\">    b.y = (j + logistic_activate(x[index + <span class=\"number\">1</span>])) / h;    <span class=\"comment\">// by = cy+sigmoid(ty)</span></div><div class=\"line\">    b.w = <span class=\"built_in\">exp</span>(x[index + <span class=\"number\">2</span>]) * biases[<span class=\"number\">2</span>*n]   / w;        <span class=\"comment\">// bw = exp(tw) * pw</span></div><div class=\"line\">    b.h = <span class=\"built_in\">exp</span>(x[index + <span class=\"number\">3</span>]) * biases[<span class=\"number\">2</span>*n+<span class=\"number\">1</span>] / h;        <span class=\"comment\">// bh = exp(th) * ph</span></div><div class=\"line\">    <span class=\"comment\">// 注意这里都做了Normalization，将值化到[0, 1]，论文里面貌似没有提到</span></div><div class=\"line\">    <span class=\"comment\">// 也就是说YOLO 用于detection层的bounding box大小和位置的输出参数都是相对值</span></div><div class=\"line\">    <span class=\"keyword\">return</span> b;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<p>顺便说一下对bounding box的bp实现。具体代码如下：</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\">// truth: ground truth</span></div><div class=\"line\"><span class=\"comment\">// x: data pointer of feature map</span></div><div class=\"line\"><span class=\"comment\">// biases: data pointer of anchor box data</span></div><div class=\"line\"><span class=\"comment\">// n, index, i, j, w, h: same meaning with `get_region_box`</span></div><div class=\"line\"><span class=\"comment\">// delta: data pointer of gradient</span></div><div class=\"line\"><span class=\"comment\">// scale: just a weight, given by user</span></div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">float</span> <span class=\"title\">delta_region_box</span><span class=\"params\">(box truth, <span class=\"keyword\">float</span> *x, <span class=\"keyword\">float</span> *biases,</span></span></div><div class=\"line\">                       <span class=\"keyword\">int</span> n, <span class=\"keyword\">int</span> index, <span class=\"keyword\">int</span> i, <span class=\"keyword\">int</span> j, <span class=\"keyword\">int</span> w, <span class=\"keyword\">int</span> h,</div><div class=\"line\">                       <span class=\"keyword\">float</span> *delta, <span class=\"keyword\">float</span> scale)</div><div class=\"line\">&#123;</div><div class=\"line\">    box pred = get_region_box(x, biases, n, index, i, j, w, h);</div><div class=\"line\">    <span class=\"comment\">// get iou of the bbox and truth</span></div><div class=\"line\">    <span class=\"keyword\">float</span> iou = box_iou(pred, truth);</div><div class=\"line\">    <span class=\"comment\">// ground truth of the parameters (tx, ty, tw, th)</span></div><div class=\"line\">    <span class=\"keyword\">float</span> tx = (truth.x*w - i);</div><div class=\"line\">    <span class=\"keyword\">float</span> ty = (truth.y*h - j);</div><div class=\"line\">    <span class=\"keyword\">float</span> tw = <span class=\"built_in\">log</span>(truth.w*w / biases[<span class=\"number\">2</span>*n]);</div><div class=\"line\">    <span class=\"keyword\">float</span> th = <span class=\"built_in\">log</span>(truth.h*h / biases[<span class=\"number\">2</span>*n + <span class=\"number\">1</span>]);</div><div class=\"line\">    <span class=\"comment\">// 这里是欧式距离损失的梯度回传</span></div><div class=\"line\">    <span class=\"comment\">// 以tx为例。</span></div><div class=\"line\">    <span class=\"comment\">// loss = 1/2*(bx^hat-bx)^2, 其中bx = cx + sigmoid(tx)</span></div><div class=\"line\">    <span class=\"comment\">// d(loss)/d(tx) = -(bx^hat-bx) * d(bx)/d(tx)</span></div><div class=\"line\">    <span class=\"comment\">// 注意，Darkent中的delta存储的是负梯度数值，所以下面的delta数组内数值实际是-d(loss)/d(tx)</span></div><div class=\"line\">    <span class=\"comment\">// 也就是(bx^hat-bx) * d(bx)/d(tx)</span></div><div class=\"line\">    <span class=\"comment\">// 前面的(bx^hat-bx)把cx约掉了（因为是同一个cell，偏移是一样的）</span></div><div class=\"line\">    <span class=\"comment\">// 后面相当于是求sigmoid函数对输入自变量的梯度。</span></div><div class=\"line\">    <span class=\"comment\">// 由于当初没有缓存 sigomid(tx)，所以作者又重新计算了一次 sigmoid(tx)，也就是下面的激活函数那里</span></div><div class=\"line\">    delta[index + <span class=\"number\">0</span>] = scale * (tx - logistic_activate(x[index + <span class=\"number\">0</span>]))</div><div class=\"line\">                             * logistic_gradient(logistic_activate(x[index + <span class=\"number\">0</span>]));</div><div class=\"line\"></div><div class=\"line\">    delta[index + <span class=\"number\">1</span>] = scale * (ty - logistic_activate(x[index + <span class=\"number\">1</span>]))</div><div class=\"line\">                             * logistic_gradient(logistic_activate(x[index + <span class=\"number\">1</span>]));</div><div class=\"line\">    <span class=\"comment\">// tw 相似，只不过这里的 loss = 1/2(tw^hat-tw)^2，而不是和上面一样使用bw^hat 和 bw</span></div><div class=\"line\">    delta[index + <span class=\"number\">2</span>] = scale * (tw - x[index + <span class=\"number\">2</span>]);</div><div class=\"line\">    delta[index + <span class=\"number\">3</span>] = scale * (th - x[index + <span class=\"number\">3</span>]);</div><div class=\"line\">    <span class=\"keyword\">return</span> iou;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<p>接下来，我们看一下bp的计算。主要涉及到决定bounding box大小和位置的四个参数的回归，以及置信度$t_o$，以及$C$类分类概率。上面的代码中已经介绍了bounding box大小位置的四个参数的梯度计算。对于置信度$t_o$的计算，如下所示。</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\">// 上面的代码遍历了所有的groundtruth，找出了与当前预测bounding box iou最大的那个</span></div><div class=\"line\"><span class=\"comment\">// 首先，我们认为当前bounding box没有responsible for any groundtruth，</span></div><div class=\"line\"><span class=\"comment\">// 那么，loss = 1/2*(0-sigmoid(to))^2</span></div><div class=\"line\"><span class=\"comment\">// =&gt; d(loss)/d(to) = -(0-sigmoid(to)) * d(sigmoid)/d(to)</span></div><div class=\"line\"><span class=\"comment\">// 由于之前的代码中已经将output取了sigmoid作用，所以就有了下面的代码</span></div><div class=\"line\"><span class=\"comment\">// 其中，logistic_gradient(y) 是指dy/dx|(y=y0)的值。具体来说，logistic_gradient(y) = (1-y)*y</span></div><div class=\"line\">l.delta[index + <span class=\"number\">4</span>] = l.noobject_scale * ((<span class=\"number\">0</span> - l.output[index + <span class=\"number\">4</span>]) * logistic_gradient(l.output[index + <span class=\"number\">4</span>]));</div><div class=\"line\"><span class=\"comment\">// 如果best iou &gt; thresh, 我们认为这个bounding box有了对应的groundtruth，把梯度直接设置为0即可</span></div><div class=\"line\"><span class=\"keyword\">if</span> (best_iou &gt; l.thresh) &#123;</div><div class=\"line\">    l.delta[index + <span class=\"number\">4</span>] = <span class=\"number\">0</span>;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<p>这里额外要说明的是，阅读代码可以发现，分类loss的计算方法和V1不同，不再使用MSELoss，而是使用了交叉熵损失函数。对应地，梯度计算的方法如下所示。不过这点在论文中貌似并没有体现。<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div></pre></td><td class=\"code\"><pre><div class=\"line\">// for each class</div><div class=\"line\">for(n = 0; n &lt; classes; ++n)&#123;</div><div class=\"line\">    // P_i = \\frac&#123;exp^out_i&#125;&#123;sum of exp^out_j&#125;</div><div class=\"line\">    // SoftmaxLoss = -logP(class)</div><div class=\"line\">    // ∂SoftmaxLoss/∂output = -(1(n==class)-P)</div><div class=\"line\">    delta[index + n] = scale * (((n == class)?1 : 0) - output[index + n]);</div><div class=\"line\">    if(n == class) *avg_cat += output[index + n];</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure></p>\n<h3 id=\"改进6：Fine-Gained-Features\"><a href=\"#改进6：Fine-Gained-Features\" class=\"headerlink\" title=\"改进6：Fine-Gained Features\"></a>改进6：Fine-Gained Features</h3><p>这个trick是受Faster RCNN和SSD方法中使用多个不同feature map提高算法对不同分辨率目标物体的检测能力的启发，加入了一个pass-through层，直接将倒数第二层的$26\\times 26$大小的feature map加进来。</p>\n<p>在具体实现时，是将higher resolution（也就是$26\\times 26$）的feature map stacking在一起。比如，原大小为$26\\times 26 \\times 512$的feature map，因为我们要将其变为$13\\times 13$大小，所以，将在空间上相近的点移到后面的channel上去，这部分可以参考Darknet中<code>reorg_layer</code>的实现。</p>\n<p>使用这一扩展之后的feature map，提高了1%的性能提升。</p>\n<h3 id=\"改进7：多尺度训练（Multi-Scale-Training）\"><a href=\"#改进7：多尺度训练（Multi-Scale-Training）\" class=\"headerlink\" title=\"改进7：多尺度训练（Multi-Scale Training）\"></a>改进7：多尺度训练（Multi-Scale Training）</h3><p>在实际应用时，输入的图像大小有可能是变化的。我们也将这一点考虑进来。因为我们的网络是全卷积神经网络，只有conv和pooling层，没有全连接层，所以可以适应不同大小的图像输入。所以网络结构上是没问题的。</p>\n<p>具体来说，在训练的时候，我们每隔一定的epoch（例如10）就随机改变网络的输入图像大小。由于我们的网络最终降采样的比例是$32$，所以随机生成的图像大小为$32$的倍数，即$\\lbrace 320, 352, \\dots, 608\\rbrace$。</p>\n<p>在实际使用中，如果输入图像的分辨率较低，YOLO V2可以在不错的精度下达到很快的检测速度。这可以被用应在计算能力有限的场合（无GPU或者GPU很弱）或多路视频信号的实时处理。如果输入图像的分辨率较高，YOLO V2可以作为state of the art的检测器，并仍能获得不错的检测速度。对于目前流行的检测方法（Faster RCNN，SSD，YOLO）的精度和帧率之间的关系，见下图。可以看到，作者在30fps处画了一条竖线，这是算法能否达到实时处理的分水岭。Faster RCNN败下阵来，而YOLO V2的不同点代表了不同输入图像分辨率下算法的表现。对于详细数据，见图下的表格对比（VOC 2007上进行测试）。</p>\n<p><img src=\"/img/yolo2_different_methods_comparation.png\" alt=\"不同检测方法的对比\"><br><img src=\"/img/yolo2_different_methods_comparation_in_table.png\" alt=\"不同检测方法的对比\"></p>\n<h3 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h3><p>在Better这部分的末尾，作者给出了一个表格，指出了主要提升性能的措施。例外是网络结构上改为带Anchor box的全卷积网络结构（提升了recall，但对mAP基本无影响）和使用新的网络（计算量少了~33%）。<br><img src=\"/img/yolo2_different_methods_improvement.png\" alt=\"不同改进措施的影响\"></p>\n<h2 id=\"Faster\"><a href=\"#Faster\" class=\"headerlink\" title=\"Faster\"></a>Faster</h2><p>这部分的改进为网络结构的变化。包括Faster RCNN在内的很多检测方法都使用VGG-16作为base network。VGG-16精度够高但是计算量较大（对于大小为$224\\times 224$的单幅输入图像，卷积层就需要30.69B次浮点运算）。在YOLO V1中，我们的网络结构较为简单，在精度上不如VGG-16（ImageNet测试，88.0% vs 90.0%）。</p>\n<p>在YOLO V2中，我们使用了一种新的网络结构Darknet-19（因为base network有19个卷积层）。和VGG相类似，我们的卷积核也都是$3\\times 3$大小，同时每次pooling操作后channel数double。另外，在NIN工作基础上，我们在网络最后使用了global average pooling层，同时使用$1\\times 1$大小的卷积核来做feature map的压缩（分辨率不变，channel减小，新的元素是原来相应位置不同channel的线性组合），同时使用了Batch Normalization技术。具体的网络结构见下表。Darknet-19计算量大大减小，同时精度超过了VGG-16。<br><img src=\"/img/yolo2_dartnet_19_structure.png\" alt=\"Darknet-19的网络结构\"></p>\n<p>在训练过程中，首先在ImageNet 1K上进行分类器的训练。使用数据增强技术（如随机裁剪、旋转、饱和度变化等）。和上面的讨论相对应，首先使用$224\\times 224$大小的图像进行训练，再使用$448\\times 448$的图像进行fine tuning，具体训练参数设置可以参见论文和对应的代码。这里不再多说。</p>\n<p>然后，我们对检测任务进行训练。对网络结构进行微调，去掉最后的卷积层（因为它是我们当初为了得到1K类的分类置信度加上去的），增加$3$个$3\\times 3$大小，channel数为$1024$的卷积层，并每个都跟着一个$1\\times 1$大小的卷积层，channel数由我们最终的检测任务需要的输出决定。对于VOC的检测任务来说，我们需要预测$5$个box，每个需要$5$个参数（相对位置，相对大小和置信度），同时$20$个类别的置信度，所以输出共$5\\times(5+20)=125$。从YOLO V2的<code>yolo_voc.cfg</code><a href=\"https://github.com/pjreddie/darknet/blob/master/cfg/yolo.cfg\" target=\"_blank\" rel=\"external\">文件</a>中，我们也可以看到如下的对应结构：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div></pre></td><td class=\"code\"><pre><div class=\"line\">[convolutional]</div><div class=\"line\">batch_normalize=1</div><div class=\"line\">size=3</div><div class=\"line\">stride=1</div><div class=\"line\">pad=1</div><div class=\"line\">filters=1024</div><div class=\"line\">activation=leaky</div><div class=\"line\"></div><div class=\"line\">[convolutional]</div><div class=\"line\">size=1</div><div class=\"line\">stride=1</div><div class=\"line\">pad=1</div><div class=\"line\">filters=125</div><div class=\"line\">activation=linear</div></pre></td></tr></table></figure>\n<p>同时，加上上文提到的pass-through结构。</p>\n<h2 id=\"Stronger\"><a href=\"#Stronger\" class=\"headerlink\" title=\"Stronger\"></a>Stronger</h2><p>未完待续</p>\n","excerpt":"<p>YOLO(<strong>Y</strong>ou <strong>O</strong>nly <strong>L</strong>ook <strong>O</strong>nce)是一个流行的目标检测方法，和Faster RCNN等state of the art方法比起来，主打检测速度快。截止到目前为止（2017年2月初），YOLO已经发布了两个版本，在下文中分别称为<a href=\"https://arxiv.org/abs/1506.02640\">YOLO V1</a>和<a href=\"https://arxiv.org/abs/1612.08242\">YOLO V2</a>。YOLO V2的代码目前作为<a href=\"http://pjreddie.com/darknet/yolo/\">Darknet</a>的一部分开源在<a href=\"\">GitHub</a>。在这篇博客中，记录了阅读YOLO两个版本论文中的重点内容，并着重总结V2版本的改进。</p>\n<p><img src=\"/img/yolo2_result.png\" alt=\"YOLO V2的检测效果示意\"><br>","more":"</p>\n<h2 id=\"YOLO-V1\"><a href=\"#YOLO-V1\" class=\"headerlink\" title=\"YOLO V1\"></a>YOLO V1</h2><p>这里不妨把YOLO V1论文<a href=\"https://arxiv.org/abs/1506.02640\">“You Only Look Once: Unitied, Real-Time Object Detection”</a>的摘要部分意译如下：</p>\n<blockquote>\n<p>我们提出了一种新的物体检测方法YOLO。之前的目标检测方法大多把分类器重新调整用于检测（这里应该是在说以前的方法大多将检测问题看做分类问题，使用滑动窗提取特征并使用分类器进行分类）。我们将检测问题看做回归，分别给出bounding box的位置和对应的类别概率。对于给定输入图像，只需使用CNN网络计算一次，就同时给出bounding box位置和类别概率。由于整个pipeline都是同一个网络，所以很容易进行端到端的训练。<br>YOLO相当快。base model可以达到45fps，一个更小的model（Fast YOLO），可以达到155fps，同时mAP是其他可以实现实时检测方法的两倍。和目前的State of the art方法相比，YOLO的localization误差较大，但是对于背景有更低的误检（False Positives）。同时，YOLO能够学习到更加泛化的特征，例如艺术品中物体的检测等任务，表现很好。</p>\n</blockquote>\n<p>和以往较为常见的方法，如HoG+SVM的行人检测，DPM，RCNN系方法等不同，YOLO接受image作为输入，直接输出bounding box的位置和对应位置为某一类的概率。我们可以把它和目前的State of the art的Faster RCNN方法对比。Faster RCNN方法需要两个网络RPN和Fast RCNN，其中前者负责接受图像输入，并提出proposal。后续Fast RCNN再给出这些proposal是某一类的概率。也正是因为这种“直来直去”的方法，YOLO才能达到这么快的速度。也真是令人感叹，网络参数够多，数据够多，什么映射关系都能学出来。。。下图就是YOLO的检测系统示意图。在test阶段，经过单个CNN网络前向计算后，再经过非极大值抑制，就可以给出检测结果。<br><img src=\"/img/yolo1_detection_system.png\" alt=\"YOLO V1检测系统示意图\"></p>\n<h3 id=\"基本思路\"><a href=\"#基本思路\" class=\"headerlink\" title=\"基本思路\"></a>基本思路</h3><p><img src=\"/img/yolo1_basic_idea.png\" alt=\"基础思路示意图\"></p>\n<ul>\n<li>网格划分：将输入image划分为$S \\times S$个grid cell，如果image中某个object box的中心落在某个grid cell内部，那么这个cell就对检测该object负责（responsible for detection that object）。同时，每个grid cell同时预测$B$个bounding box的位置和一个置信度。这个置信度并不只是该bounding box是待检测目标的概率，而是该bounding box是待检测目标的概率乘上该bounding box和真实位置的IoU的积。通过乘上这个交并比，反映出该bounding box预测位置的精度。如下式所示：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\\text{confidence} = P(\\text{Object})\\times \\text{IoU}_{\\text{pred}}^{\\text{truth}}</script><ul>\n<li><p>网络输出：每个bounding box对应于5个输出，分别是$x,y,w,h$和上述提到的置信度。其中，$x,y$代表bounding box的中心离开其所在grid cell边界的偏移。$w,h$代表bounding box真实宽高相对于整幅图像的比例。$x,y,w,h$这几个参数都已经被bounded到了区间$[0,1]$上。除此以外，每个grid cell还产生$C$个条件概率，$P(\\text{Class}_i|\\text{Object})$。注意，我们不管$B$的大小，每个grid cell只产生一组这样的概率。在test的非极大值抑制阶段，对于每个bounding box，我们应该按照下式衡量该框是否应该予以保留。</p>\n<script type=\"math/tex; mode=display\">\\text{confidence}\\times P(\\text{Class}_i|\\text{Object}) = P(\\text{Class}_i)\\times \\text{IoU}_{\\text{pred}}^{\\text{truth}}</script></li>\n<li><p>实际参数：在PASCAL VOC进行测试时，使用$S = 7$, $B=2$。由于共有20类，故$C=20$。所以，我们的网络输出大小为$7\\times 7 \\times 30$</p>\n</li>\n</ul>\n<h3 id=\"网络模型结构\"><a href=\"#网络模型结构\" class=\"headerlink\" title=\"网络模型结构\"></a>网络模型结构</h3><p>Inspired by GoogLeNet，但是没有采取inception的结构，simply使用了$1\\times 1$的卷积核。base model共有24个卷积层，后面接2个全连接层，如下图所示。<br><img src=\"/img/yolo1_network_arch.png\" alt=\"YOLO的网络结构示意图\"></p>\n<p>另外，Fast YOLO使用了更小的网络结构（9个卷积层，且filter的数目也少了），其他部分完全一样。</p>\n<h3 id=\"训练\"><a href=\"#训练\" class=\"headerlink\" title=\"训练\"></a>训练</h3><p>同很多人一样，这里作者也是先在ImageNet上做了预训练。使用上图网络结构中的前20个卷积层，后面接一个average-pooling层和全连接层，在ImageNet 1000类数据集上训练了一周，达到了88%的top-5准确率。</p>\n<p>由于<a href=\"https://arxiv.org/abs/1504.06066\">Ren的论文</a>提到给预训练的模型加上额外的卷积层和全连接层能够提升性能，所以我们加上了剩下的4个卷积层和2个全连接层，权值为随机初始化。同时，我们把网络输入的分辨率从$224\\times 224$提升到了$448 \\times 448$。</p>\n<p>在最后一层，我们使用了线性激活函数；其它层使用了leaky ReLU激活函数。如下所示：</p>\n<script type=\"math/tex; mode=display\">\nf(x)=\n\\begin{cases}\nx, &\\text{if}\\ x > 0 \\\\\\\\\n0.1x, &\\text{otherwise}\n\\end{cases}</script><p>很重要的问题就是定义很好的loss function，为此，作者提出了以下几点说明：</p>\n<ul>\n<li>loss的形式采用误差平方和的形式（真是把回归进行到底了。。。）</li>\n<li>由于很多的grid cell没有目标物体存在，所以给有目标存在的bounding box和没有目标存在的bounding box设置了不同的比例因子进行平衡。具体来说，<script type=\"math/tex; mode=display\">\\lambda_{\\text{coord}} = 5，\\lambda_{\\text{noobj}} = 0.5</script></li>\n<li>直接使用$w$和$h$，这样大的box的差异在loss中占得比重会和小的box不平衡，所以这里使用$\\sqrt{w}$和$\\sqrt{h}$。</li>\n<li>上文中已经提到，同一个grid cell会提出多个bounding box。在training阶段，我们只想让一个bounding box对应object。所以，我们计算每个bounding box和ground truth的IoU，以此为标准得到最好的那个bounding box，其他的认为no obj。</li>\n</ul>\n<p>loss函数的具体形式见下图（实在是不想打这一大串公式。。）。其中，$\\mathbb{1}_i$表示是否有目标出现在第$i$个grid cell。</p>\n<p>$\\mathbb{1}_{i,j}$表示第$i$个grid cell的第$j$个bounding box是否对某个目标负责。<br><img src=\"/img/yolo1_loss_fun.png\" alt=\"YOLO的损失函数定义\"></p>\n<p>在进行反向传播时，由于loss都是二次形式，所以导数形式都是统一的。下面是Darknet中<a href=\"https://github.com/pjreddie/darknet/blob/master/src/detection_layer.c\">detection_layer.c</a>中训练部分的代码。在代码中，计算了cost（也就是loss）和delta（也就是反向的导数），</p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div><div class=\"line\">47</div><div class=\"line\">48</div><div class=\"line\">49</div><div class=\"line\">50</div><div class=\"line\">51</div><div class=\"line\">52</div><div class=\"line\">53</div><div class=\"line\">54</div><div class=\"line\">55</div><div class=\"line\">56</div><div class=\"line\">57</div><div class=\"line\">58</div><div class=\"line\">59</div><div class=\"line\">60</div><div class=\"line\">61</div><div class=\"line\">62</div><div class=\"line\">63</div><div class=\"line\">64</div><div class=\"line\">65</div><div class=\"line\">66</div><div class=\"line\">67</div><div class=\"line\">68</div><div class=\"line\">69</div><div class=\"line\">70</div><div class=\"line\">71</div><div class=\"line\">72</div><div class=\"line\">73</div><div class=\"line\">74</div><div class=\"line\">75</div><div class=\"line\">76</div><div class=\"line\">77</div><div class=\"line\">78</div><div class=\"line\">79</div><div class=\"line\">80</div><div class=\"line\">81</div><div class=\"line\">82</div><div class=\"line\">83</div><div class=\"line\">84</div><div class=\"line\">85</div><div class=\"line\">86</div><div class=\"line\">87</div><div class=\"line\">88</div><div class=\"line\">89</div><div class=\"line\">90</div><div class=\"line\">91</div><div class=\"line\">92</div><div class=\"line\">93</div><div class=\"line\">94</div><div class=\"line\">95</div><div class=\"line\">96</div><div class=\"line\">97</div><div class=\"line\">98</div><div class=\"line\">99</div><div class=\"line\">100</div><div class=\"line\">101</div><div class=\"line\">102</div><div class=\"line\">103</div><div class=\"line\">104</div><div class=\"line\">105</div><div class=\"line\">106</div><div class=\"line\">107</div><div class=\"line\">108</div><div class=\"line\">109</div><div class=\"line\">110</div><div class=\"line\">111</div><div class=\"line\">112</div><div class=\"line\">113</div><div class=\"line\">114</div><div class=\"line\">115</div><div class=\"line\">116</div><div class=\"line\">117</div><div class=\"line\">118</div><div class=\"line\">119</div><div class=\"line\">120</div><div class=\"line\">121</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">if</span>(state.train)&#123;</div><div class=\"line\">    <span class=\"keyword\">float</span> avg_iou = <span class=\"number\">0</span>;</div><div class=\"line\">    <span class=\"keyword\">float</span> avg_cat = <span class=\"number\">0</span>;</div><div class=\"line\">    <span class=\"keyword\">float</span> avg_allcat = <span class=\"number\">0</span>;</div><div class=\"line\">    <span class=\"keyword\">float</span> avg_obj = <span class=\"number\">0</span>;</div><div class=\"line\">    <span class=\"keyword\">float</span> avg_anyobj = <span class=\"number\">0</span>;</div><div class=\"line\">    <span class=\"keyword\">int</span> count = <span class=\"number\">0</span>;</div><div class=\"line\">    *(l.cost) = <span class=\"number\">0</span>;</div><div class=\"line\">    <span class=\"keyword\">int</span> size = l.inputs * l.batch;</div><div class=\"line\">    <span class=\"built_in\">memset</span>(l.delta, <span class=\"number\">0</span>, size * <span class=\"keyword\">sizeof</span>(<span class=\"keyword\">float</span>));</div><div class=\"line\">    <span class=\"keyword\">for</span> (b = <span class=\"number\">0</span>; b &lt; l.batch; ++b)&#123;</div><div class=\"line\">        <span class=\"keyword\">int</span> index = b*l.inputs;</div><div class=\"line\">        <span class=\"comment\">// for each grid cell</span></div><div class=\"line\">        <span class=\"keyword\">for</span> (i = <span class=\"number\">0</span>; i &lt; locations; ++i) &#123;   <span class=\"comment\">// locations = S * S = 49</span></div><div class=\"line\">            <span class=\"keyword\">int</span> truth_index = (b*locations + i)*(<span class=\"number\">1</span>+l.coords+l.classes);</div><div class=\"line\">            <span class=\"keyword\">int</span> is_obj = state.truth[truth_index];</div><div class=\"line\">            <span class=\"comment\">// for each bbox</span></div><div class=\"line\">            <span class=\"keyword\">for</span> (j = <span class=\"number\">0</span>; j &lt; l.n; ++j) &#123;     <span class=\"comment\">// l.n = B = 2</span></div><div class=\"line\">                <span class=\"keyword\">int</span> p_index = index + locations*l.classes + i*l.n + j;</div><div class=\"line\">                l.delta[p_index] = l.noobject_scale*(<span class=\"number\">0</span> - l.output[p_index]);</div><div class=\"line\">                <span class=\"comment\">// 因为no obj对应的bbox很多，而responsible的只有一个</span></div><div class=\"line\">                <span class=\"comment\">// 这里统一加上，如果一会判断该bbox responsible for object，再把它减去</span></div><div class=\"line\">                *(l.cost) += l.noobject_scale*<span class=\"built_in\">pow</span>(l.output[p_index], <span class=\"number\">2</span>);  </div><div class=\"line\">                avg_anyobj += l.output[p_index];</div><div class=\"line\">            &#125;</div><div class=\"line\"></div><div class=\"line\">            <span class=\"keyword\">int</span> best_index = <span class=\"number\">-1</span>;</div><div class=\"line\">            <span class=\"keyword\">float</span> best_iou = <span class=\"number\">0</span>;</div><div class=\"line\">            <span class=\"keyword\">float</span> best_rmse = <span class=\"number\">20</span>;</div><div class=\"line\">            <span class=\"comment\">// 该grid cell没有目标，直接返回</span></div><div class=\"line\">            <span class=\"keyword\">if</span> (!is_obj)&#123;</div><div class=\"line\">                <span class=\"keyword\">continue</span>;</div><div class=\"line\">            &#125;</div><div class=\"line\">            <span class=\"comment\">// 否则，找出responsible的bounding box，计算其他几项的loss</span></div><div class=\"line\">            <span class=\"keyword\">int</span> class_index = index + i*l.classes;</div><div class=\"line\">            <span class=\"keyword\">for</span>(j = <span class=\"number\">0</span>; j &lt; l.classes; ++j) &#123;</div><div class=\"line\">                l.delta[class_index+j] = l.class_scale * (state.truth[truth_index+<span class=\"number\">1</span>+j] - l.output[class_index+j]);</div><div class=\"line\">                *(l.cost) += l.class_scale * <span class=\"built_in\">pow</span>(state.truth[truth_index+<span class=\"number\">1</span>+j] - l.output[class_index+j], <span class=\"number\">2</span>);</div><div class=\"line\">                <span class=\"keyword\">if</span>(state.truth[truth_index + <span class=\"number\">1</span> + j]) avg_cat += l.output[class_index+j];</div><div class=\"line\">                avg_allcat += l.output[class_index+j];</div><div class=\"line\">            &#125;</div><div class=\"line\"></div><div class=\"line\">            box truth = float_to_box(state.truth + truth_index + <span class=\"number\">1</span> + l.classes);</div><div class=\"line\">            truth.x /= l.side;</div><div class=\"line\">            truth.y /= l.side;</div><div class=\"line\">            <span class=\"comment\">// 找到最好的IoU，对应的bbox是responsible的，记录其index</span></div><div class=\"line\">            <span class=\"keyword\">for</span>(j = <span class=\"number\">0</span>; j &lt; l.n; ++j)&#123;</div><div class=\"line\">                <span class=\"keyword\">int</span> box_index = index + locations*(l.classes + l.n) + (i*l.n + j) * l.coords;</div><div class=\"line\">                box out = float_to_box(l.output + box_index);</div><div class=\"line\">                out.x /= l.side;</div><div class=\"line\">                out.y /= l.side;</div><div class=\"line\"></div><div class=\"line\">                <span class=\"keyword\">if</span> (l.<span class=\"built_in\">sqrt</span>)&#123;</div><div class=\"line\">                    out.w = out.w*out.w;</div><div class=\"line\">                    out.h = out.h*out.h;</div><div class=\"line\">                &#125;</div><div class=\"line\"></div><div class=\"line\">                <span class=\"keyword\">float</span> iou  = box_iou(out, truth);</div><div class=\"line\">                <span class=\"comment\">//iou = 0;</span></div><div class=\"line\">                <span class=\"keyword\">float</span> rmse = box_rmse(out, truth);</div><div class=\"line\">                <span class=\"keyword\">if</span>(best_iou &gt; <span class=\"number\">0</span> || iou &gt; <span class=\"number\">0</span>)&#123;</div><div class=\"line\">                    <span class=\"keyword\">if</span>(iou &gt; best_iou)&#123;</div><div class=\"line\">                        best_iou = iou;</div><div class=\"line\">                        best_index = j;</div><div class=\"line\">                    &#125;</div><div class=\"line\">                &#125;<span class=\"keyword\">else</span>&#123;</div><div class=\"line\">                    <span class=\"keyword\">if</span>(rmse &lt; best_rmse)&#123;</div><div class=\"line\">                        best_rmse = rmse;</div><div class=\"line\">                        best_index = j;</div><div class=\"line\">                    &#125;</div><div class=\"line\">                &#125;</div><div class=\"line\">            &#125;</div><div class=\"line\"></div><div class=\"line\">            <span class=\"keyword\">if</span>(l.forced)&#123;</div><div class=\"line\">                <span class=\"keyword\">if</span>(truth.w*truth.h &lt; <span class=\"number\">.1</span>)&#123;</div><div class=\"line\">                    best_index = <span class=\"number\">1</span>;</div><div class=\"line\">                &#125;<span class=\"keyword\">else</span>&#123;</div><div class=\"line\">                    best_index = <span class=\"number\">0</span>;</div><div class=\"line\">                &#125;</div><div class=\"line\">            &#125;</div><div class=\"line\">            <span class=\"keyword\">if</span>(l.random &amp;&amp; *(state.net.seen) &lt; <span class=\"number\">64000</span>)&#123;</div><div class=\"line\">                best_index = rand()%l.n;</div><div class=\"line\">            &#125;</div><div class=\"line\"></div><div class=\"line\">            <span class=\"keyword\">int</span> box_index = index + locations*(l.classes + l.n) + (i*l.n + best_index) * l.coords;</div><div class=\"line\">            <span class=\"keyword\">int</span> tbox_index = truth_index + <span class=\"number\">1</span> + l.classes;</div><div class=\"line\"></div><div class=\"line\">            box out = float_to_box(l.output + box_index);</div><div class=\"line\">            out.x /= l.side;</div><div class=\"line\">            out.y /= l.side;</div><div class=\"line\">            <span class=\"keyword\">if</span> (l.<span class=\"built_in\">sqrt</span>) &#123;</div><div class=\"line\">                out.w = out.w*out.w;</div><div class=\"line\">                out.h = out.h*out.h;</div><div class=\"line\">            &#125;</div><div class=\"line\">            <span class=\"keyword\">float</span> iou  = box_iou(out, truth);</div><div class=\"line\"></div><div class=\"line\">            <span class=\"comment\">//printf(\"%d,\", best_index);</span></div><div class=\"line\">            <span class=\"keyword\">int</span> p_index = index + locations*l.classes + i*l.n + best_index;</div><div class=\"line\">            *(l.cost) -= l.noobject_scale * <span class=\"built_in\">pow</span>(l.output[p_index], <span class=\"number\">2</span>);  <span class=\"comment\">// 还记得我们曾经统一加过吗？这里需要减去了</span></div><div class=\"line\">            *(l.cost) += l.object_scale * <span class=\"built_in\">pow</span>(<span class=\"number\">1</span>-l.output[p_index], <span class=\"number\">2</span>);</div><div class=\"line\">            avg_obj += l.output[p_index];</div><div class=\"line\">            l.delta[p_index] = l.object_scale * (<span class=\"number\">1.</span>-l.output[p_index]);</div><div class=\"line\"></div><div class=\"line\">            <span class=\"keyword\">if</span>(l.rescore)&#123;</div><div class=\"line\">                l.delta[p_index] = l.object_scale * (iou - l.output[p_index]);</div><div class=\"line\">            &#125;</div><div class=\"line\"></div><div class=\"line\">            l.delta[box_index+<span class=\"number\">0</span>] = l.coord_scale*(state.truth[tbox_index + <span class=\"number\">0</span>] - l.output[box_index + <span class=\"number\">0</span>]);</div><div class=\"line\">            l.delta[box_index+<span class=\"number\">1</span>] = l.coord_scale*(state.truth[tbox_index + <span class=\"number\">1</span>] - l.output[box_index + <span class=\"number\">1</span>]);</div><div class=\"line\">            l.delta[box_index+<span class=\"number\">2</span>] = l.coord_scale*(state.truth[tbox_index + <span class=\"number\">2</span>] - l.output[box_index + <span class=\"number\">2</span>]);</div><div class=\"line\">            l.delta[box_index+<span class=\"number\">3</span>] = l.coord_scale*(state.truth[tbox_index + <span class=\"number\">3</span>] - l.output[box_index + <span class=\"number\">3</span>]);</div><div class=\"line\">            <span class=\"keyword\">if</span>(l.<span class=\"built_in\">sqrt</span>)&#123;</div><div class=\"line\">                l.delta[box_index+<span class=\"number\">2</span>] = l.coord_scale*(<span class=\"built_in\">sqrt</span>(state.truth[tbox_index + <span class=\"number\">2</span>]) - l.output[box_index + <span class=\"number\">2</span>]);</div><div class=\"line\">                l.delta[box_index+<span class=\"number\">3</span>] = l.coord_scale*(<span class=\"built_in\">sqrt</span>(state.truth[tbox_index + <span class=\"number\">3</span>]) - l.output[box_index + <span class=\"number\">3</span>]);</div><div class=\"line\">            &#125;</div><div class=\"line\"></div><div class=\"line\">            *(l.cost) += <span class=\"built_in\">pow</span>(<span class=\"number\">1</span>-iou, <span class=\"number\">2</span>);</div><div class=\"line\">            avg_iou += iou;</div><div class=\"line\">            ++count;</div><div class=\"line\">        &#125;</div><div class=\"line\">    &#125;</div></pre></td></tr></table></figure>\n<h2 id=\"YOLO-V2\"><a href=\"#YOLO-V2\" class=\"headerlink\" title=\"YOLO V2\"></a>YOLO V2</h2><p>YOLO V2是原作者在V1基础上做出改进后提出的。为了达到题目中所称的Better，Faster，Stronger的目标，主要改进点如下。当然，具体内容还是要深入论文。</p>\n<ul>\n<li>受到Faster RCNN方法的启发，引入了anchor。同时使用了K-Means方法，对anchor数量进行了讨论，在精度和速度之间做出折中；</li>\n<li>修改了网络结构，去掉了全连接层，改成了全卷积结构；</li>\n<li>引入了WordTree结构，将检测和分类问题做成了一个统一的框架，并充分利用ImageNet和COCO数据集的数据。</li>\n</ul>\n<p>下面，还是先把论文的摘要意译如下：</p>\n<blockquote>\n<p>我们引入了YOLO 9000模型，它是实时物体检测的State of the art的工作，能够检测超过9K类目标。首先，我们对以前的工作（YOLO V1）做出了若干改进，使其成为了一种在实时检测方法内在PASCAL VOC和COCO上State of the art的效果。通过一种新颖的多尺度训练犯法（multi-scale training method）， YOLO V2适用于不同大小尺寸输入的image，在精度和效率上达到了很好地trade-off。在67fps的速度下，VOC2007上达到了76.8mAP的精度。40fps时，达到了78.6mAP，已经超过了Faster RCNN with ResNet和SSD的精度，同时比它们更快！最后，我们提出了一种能够同时进行检测任务和分类任务的联合训练方法。使用这种方法，我们在COCO detection dataset和ImageNet classification dataset上同时训练模型。这种方法使得我们能够对那些没有label上detection data的数据做出detection的预测。我们使用ImageNet的detection任务做了验证。YOLO 9000在仅有200类中44类的detection data的情况下，仍然在ImageNet datection任务中取得了19.7mAP的成绩。对于不在COCO中的156类，YOLO9000成绩为16.0mAP。我们使得YOLO9000能够在保证实时的前提下对9K类目标进行检测。</p>\n</blockquote>\n<p>根据论文结构的安排，将从Better，Faster和Stronger三个方面对论文提出的各条改进措施进行介绍。</p>\n<h2 id=\"Better\"><a href=\"#Better\" class=\"headerlink\" title=\"Better\"></a>Better</h2><p>在YOLO V1的基础上，作者提出了不少的改进来进一步提升算法的性能（mAP），主要改进措施包括网络结构的改进（第1，3，5，6条）和Anchor Box的引进（第3，4，5条）以及训练方法（第2，7条）。</p>\n<h3 id=\"改进1：引入BN层（Batch-Normalization）\"><a href=\"#改进1：引入BN层（Batch-Normalization）\" class=\"headerlink\" title=\"改进1：引入BN层（Batch Normalization）\"></a>改进1：引入BN层（Batch Normalization）</h3><p>Batch Normalization能够加快模型收敛，并提供一定的正则化。作者在每个conv层都加上了了BN层，同时去掉了原来模型中的drop out部分，这带来了2%的性能提升。</p>\n<h3 id=\"改进2：高分辨率分类器（High-Resolution-Classifier）\"><a href=\"#改进2：高分辨率分类器（High-Resolution-Classifier）\" class=\"headerlink\" title=\"改进2：高分辨率分类器（High Resolution Classifier）\"></a>改进2：高分辨率分类器（High Resolution Classifier）</h3><p>YOLO V1首先在ImageNet上以$224\\times 224$大小图像作为输入进行训练，之后在检测任务中提升到$448\\times 448$。这里，作者在训练完224大小的分类网络后，首先调整网络大小为$448\\times 448$，然后在ImageNet上进行fine tuning（10个epoch）。也就是得到了一个高分辨率的cls。再把它用detection上训练。这样，能够提升4%。</p>\n<h3 id=\"改进3：引入Anchor-Box\"><a href=\"#改进3：引入Anchor-Box\" class=\"headerlink\" title=\"改进3：引入Anchor Box\"></a>改进3：引入Anchor Box</h3><p>YOLO V1中直接在CNN后面街上全连接层，直接回归bounding box的参数。这里引入了Faster RCNN中的anchor box概念，不再直接回归bounding box的参数，而是相对于anchor box的参数。</p>\n<p>作者去掉后面的fc层和最后一个max pooling层，以期得到更高分辨率的feature map。同时，shrink网络接受$416\\times 416$（而不是448）大小的输入image。这是因为我们想要最后的feature map大小是奇数，这样就能够得到一个center cell（比较大的目标，更有可能占据中间的位置）。由于YOLO conv-pooling的效应是将image downsamplig 32倍，所以最后feature map大小为$416/32 = 13$。</p>\n<p>与YOLO V1不同的是，我们不再对同一个grid cell下的bounding box统一产生一个数量为$C$的类别概率，而是对于每一个bounding box都产生对应的$C$类概率。和YOLO V1一样的是，我们仍然产生confidence，意义也完全一样。</p>\n<p>使用anchor后，我们的精度accuracy降低了，不过recall上来了。（这也较好理解。原来每个grid cell内部只有2个bounding box，造成recall不高。现在recall高上来了，accuracy会下降一些）。</p>\n<h3 id=\"改进4：Dimension-Cluster\"><a href=\"#改进4：Dimension-Cluster\" class=\"headerlink\" title=\"改进4：Dimension Cluster\"></a>改进4：Dimension Cluster</h3><p>在引入anchor box后，一个问题就是如何确定anchor的位置和大小？Faster RCNN中是手工选定的，每隔stride设定一个anchor，并根据不同的面积比例和长宽比例产生9个anchor box。在本文中，作者使用了聚类方法对如何选取anchor box做了探究。这点应该是论文中很有新意的地方。</p>\n<p>这里对作者使用的方法不再过多赘述，强调以下两点：</p>\n<ul>\n<li>作者使用的聚类方法是K-Means；</li>\n<li>相似性度量不用欧氏距离，而是用IoU，定义如下：<script type=\"math/tex; mode=display\">d(\\text{box}, \\text{centroid}) = 1-\\text{IoU}(\\text{box}, \\text{centroid})</script></li>\n</ul>\n<p>使用不同的$k$，聚类实验结果如下，作者折中采用了$k = 5$。而且经过实验，发现当取$k=9$时候，已经能够超过Faster RCNN采用的手工固定anchor box的方法。下图右侧图是在COCO和VOC数据集上$k=5$的聚类后结果。这些box可以作为anchor box使用。<br><img src=\"/img/yolo2_cluster_result.png\" alt=\"\"></p>\n<h3 id=\"改进5：直接位置预测（Direct-Location-Prediction）\"><a href=\"#改进5：直接位置预测（Direct-Location-Prediction）\" class=\"headerlink\" title=\"改进5：直接位置预测（Direct Location Prediction）\"></a>改进5：直接位置预测（Direct Location Prediction）</h3><p>我们仍然延续了YOLO V1中的思路，预测box相对于grid cell的位置。使用sigmoid函数作为激活函数，使得最终输出值落在$[0, 1]$这个区间上。</p>\n<p>在output的feature map上，对于每个cell（共计$13\\times 13$个），给出对应每个bounding box的输出$t_x$, $t_y$, $t_w$, $t_h$。每个cell共计$k=5$个bounding box。如何由这几个参数确定bounding box的真实位置呢？见下图。</p>\n<p><img src=\"/img/yolo2_bbox_location.png\" alt=\"确定bbox的位置\"></p>\n<p>设该grid cell距离图像左上角的offset是$(c_x, c_y)$，那么bounding box的位置和宽高计算如下。注意，box的位置是相对于grid cell的，而宽高是相对于anchor box的。<br><img src=\"/img/yolo2_bbox_param.png\" alt=\"bounding box参数的计算方法\"></p>\n<p>Darknet中的具体的实现代码如下（不停切换中英文输入实在是蛋疼，所以只有用我这蹩脚的英语来注释了。。。）：</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\">// get bounding box</span></div><div class=\"line\"><span class=\"comment\">// x: data pointer of feature map</span></div><div class=\"line\"><span class=\"comment\">// biases: data pointer of anchor box data</span></div><div class=\"line\"><span class=\"comment\">// biases[2*n] = width of anchor box</span></div><div class=\"line\"><span class=\"comment\">// biases[2*n+1] = height of anchor box</span></div><div class=\"line\"><span class=\"comment\">// n: output bounding box for each cell in the feature map</span></div><div class=\"line\"><span class=\"comment\">// index: output bounding box index in the cell</span></div><div class=\"line\"><span class=\"comment\">// i: `cx` in the paper</span></div><div class=\"line\"><span class=\"comment\">// j: 'cy' in the paper</span></div><div class=\"line\"><span class=\"comment\">// (cx, cy) is the offset from the left top corner of the feature map</span></div><div class=\"line\"><span class=\"comment\">// (w, h) is the size of feature map (do normalization in the code)</span></div><div class=\"line\"><span class=\"function\">box <span class=\"title\">get_region_box</span><span class=\"params\">(<span class=\"keyword\">float</span> *x, <span class=\"keyword\">float</span> *biases, <span class=\"keyword\">int</span> n, <span class=\"keyword\">int</span> index, <span class=\"keyword\">int</span> i, <span class=\"keyword\">int</span> j, <span class=\"keyword\">int</span> w, <span class=\"keyword\">int</span> h)</span></div><div class=\"line\"></span>&#123;</div><div class=\"line\">    box b;</div><div class=\"line\">    <span class=\"comment\">// i &lt;- cx, j &lt;- cy</span></div><div class=\"line\">    <span class=\"comment\">// index + 0: tx</span></div><div class=\"line\">    <span class=\"comment\">// index + 1: ty</span></div><div class=\"line\">    <span class=\"comment\">// index + 2: tw</span></div><div class=\"line\">    <span class=\"comment\">// index + 3: th</span></div><div class=\"line\">    <span class=\"comment\">// index + 4: to   // not used here</span></div><div class=\"line\">    <span class=\"comment\">// index + 5, +6, ..., +(C+4)   // confidence of P(class c|Object), not used here</span></div><div class=\"line\">    b.x = (i + logistic_activate(x[index + <span class=\"number\">0</span>])) / w;    <span class=\"comment\">// bx = cx+sigmoid(tx)</span></div><div class=\"line\">    b.y = (j + logistic_activate(x[index + <span class=\"number\">1</span>])) / h;    <span class=\"comment\">// by = cy+sigmoid(ty)</span></div><div class=\"line\">    b.w = <span class=\"built_in\">exp</span>(x[index + <span class=\"number\">2</span>]) * biases[<span class=\"number\">2</span>*n]   / w;        <span class=\"comment\">// bw = exp(tw) * pw</span></div><div class=\"line\">    b.h = <span class=\"built_in\">exp</span>(x[index + <span class=\"number\">3</span>]) * biases[<span class=\"number\">2</span>*n+<span class=\"number\">1</span>] / h;        <span class=\"comment\">// bh = exp(th) * ph</span></div><div class=\"line\">    <span class=\"comment\">// 注意这里都做了Normalization，将值化到[0, 1]，论文里面貌似没有提到</span></div><div class=\"line\">    <span class=\"comment\">// 也就是说YOLO 用于detection层的bounding box大小和位置的输出参数都是相对值</span></div><div class=\"line\">    <span class=\"keyword\">return</span> b;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<p>顺便说一下对bounding box的bp实现。具体代码如下：</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\">// truth: ground truth</span></div><div class=\"line\"><span class=\"comment\">// x: data pointer of feature map</span></div><div class=\"line\"><span class=\"comment\">// biases: data pointer of anchor box data</span></div><div class=\"line\"><span class=\"comment\">// n, index, i, j, w, h: same meaning with `get_region_box`</span></div><div class=\"line\"><span class=\"comment\">// delta: data pointer of gradient</span></div><div class=\"line\"><span class=\"comment\">// scale: just a weight, given by user</span></div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">float</span> <span class=\"title\">delta_region_box</span><span class=\"params\">(box truth, <span class=\"keyword\">float</span> *x, <span class=\"keyword\">float</span> *biases,</div><div class=\"line\">                       <span class=\"keyword\">int</span> n, <span class=\"keyword\">int</span> index, <span class=\"keyword\">int</span> i, <span class=\"keyword\">int</span> j, <span class=\"keyword\">int</span> w, <span class=\"keyword\">int</span> h,</div><div class=\"line\">                       <span class=\"keyword\">float</span> *delta, <span class=\"keyword\">float</span> scale)</span></div><div class=\"line\"></span>&#123;</div><div class=\"line\">    box pred = get_region_box(x, biases, n, index, i, j, w, h);</div><div class=\"line\">    <span class=\"comment\">// get iou of the bbox and truth</span></div><div class=\"line\">    <span class=\"keyword\">float</span> iou = box_iou(pred, truth);</div><div class=\"line\">    <span class=\"comment\">// ground truth of the parameters (tx, ty, tw, th)</span></div><div class=\"line\">    <span class=\"keyword\">float</span> tx = (truth.x*w - i);</div><div class=\"line\">    <span class=\"keyword\">float</span> ty = (truth.y*h - j);</div><div class=\"line\">    <span class=\"keyword\">float</span> tw = <span class=\"built_in\">log</span>(truth.w*w / biases[<span class=\"number\">2</span>*n]);</div><div class=\"line\">    <span class=\"keyword\">float</span> th = <span class=\"built_in\">log</span>(truth.h*h / biases[<span class=\"number\">2</span>*n + <span class=\"number\">1</span>]);</div><div class=\"line\">    <span class=\"comment\">// 这里是欧式距离损失的梯度回传</span></div><div class=\"line\">    <span class=\"comment\">// 以tx为例。</span></div><div class=\"line\">    <span class=\"comment\">// loss = 1/2*(bx^hat-bx)^2, 其中bx = cx + sigmoid(tx)</span></div><div class=\"line\">    <span class=\"comment\">// d(loss)/d(tx) = -(bx^hat-bx) * d(bx)/d(tx)</span></div><div class=\"line\">    <span class=\"comment\">// 注意，Darkent中的delta存储的是负梯度数值，所以下面的delta数组内数值实际是-d(loss)/d(tx)</span></div><div class=\"line\">    <span class=\"comment\">// 也就是(bx^hat-bx) * d(bx)/d(tx)</span></div><div class=\"line\">    <span class=\"comment\">// 前面的(bx^hat-bx)把cx约掉了（因为是同一个cell，偏移是一样的）</span></div><div class=\"line\">    <span class=\"comment\">// 后面相当于是求sigmoid函数对输入自变量的梯度。</span></div><div class=\"line\">    <span class=\"comment\">// 由于当初没有缓存 sigomid(tx)，所以作者又重新计算了一次 sigmoid(tx)，也就是下面的激活函数那里</span></div><div class=\"line\">    delta[index + <span class=\"number\">0</span>] = scale * (tx - logistic_activate(x[index + <span class=\"number\">0</span>]))</div><div class=\"line\">                             * logistic_gradient(logistic_activate(x[index + <span class=\"number\">0</span>]));</div><div class=\"line\"></div><div class=\"line\">    delta[index + <span class=\"number\">1</span>] = scale * (ty - logistic_activate(x[index + <span class=\"number\">1</span>]))</div><div class=\"line\">                             * logistic_gradient(logistic_activate(x[index + <span class=\"number\">1</span>]));</div><div class=\"line\">    <span class=\"comment\">// tw 相似，只不过这里的 loss = 1/2(tw^hat-tw)^2，而不是和上面一样使用bw^hat 和 bw</span></div><div class=\"line\">    delta[index + <span class=\"number\">2</span>] = scale * (tw - x[index + <span class=\"number\">2</span>]);</div><div class=\"line\">    delta[index + <span class=\"number\">3</span>] = scale * (th - x[index + <span class=\"number\">3</span>]);</div><div class=\"line\">    <span class=\"keyword\">return</span> iou;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<p>接下来，我们看一下bp的计算。主要涉及到决定bounding box大小和位置的四个参数的回归，以及置信度$t_o$，以及$C$类分类概率。上面的代码中已经介绍了bounding box大小位置的四个参数的梯度计算。对于置信度$t_o$的计算，如下所示。</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\">// 上面的代码遍历了所有的groundtruth，找出了与当前预测bounding box iou最大的那个</span></div><div class=\"line\"><span class=\"comment\">// 首先，我们认为当前bounding box没有responsible for any groundtruth，</span></div><div class=\"line\"><span class=\"comment\">// 那么，loss = 1/2*(0-sigmoid(to))^2</span></div><div class=\"line\"><span class=\"comment\">// =&gt; d(loss)/d(to) = -(0-sigmoid(to)) * d(sigmoid)/d(to)</span></div><div class=\"line\"><span class=\"comment\">// 由于之前的代码中已经将output取了sigmoid作用，所以就有了下面的代码</span></div><div class=\"line\"><span class=\"comment\">// 其中，logistic_gradient(y) 是指dy/dx|(y=y0)的值。具体来说，logistic_gradient(y) = (1-y)*y</span></div><div class=\"line\">l.delta[index + <span class=\"number\">4</span>] = l.noobject_scale * ((<span class=\"number\">0</span> - l.output[index + <span class=\"number\">4</span>]) * logistic_gradient(l.output[index + <span class=\"number\">4</span>]));</div><div class=\"line\"><span class=\"comment\">// 如果best iou &gt; thresh, 我们认为这个bounding box有了对应的groundtruth，把梯度直接设置为0即可</span></div><div class=\"line\"><span class=\"keyword\">if</span> (best_iou &gt; l.thresh) &#123;</div><div class=\"line\">    l.delta[index + <span class=\"number\">4</span>] = <span class=\"number\">0</span>;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<p>这里额外要说明的是，阅读代码可以发现，分类loss的计算方法和V1不同，不再使用MSELoss，而是使用了交叉熵损失函数。对应地，梯度计算的方法如下所示。不过这点在论文中貌似并没有体现。<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div></pre></td><td class=\"code\"><pre><div class=\"line\">// for each class</div><div class=\"line\">for(n = 0; n &lt; classes; ++n)&#123;</div><div class=\"line\">    // P_i = \\frac&#123;exp^out_i&#125;&#123;sum of exp^out_j&#125;</div><div class=\"line\">    // SoftmaxLoss = -logP(class)</div><div class=\"line\">    // ∂SoftmaxLoss/∂output = -(1(n==class)-P)</div><div class=\"line\">    delta[index + n] = scale * (((n == class)?1 : 0) - output[index + n]);</div><div class=\"line\">    if(n == class) *avg_cat += output[index + n];</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure></p>\n<h3 id=\"改进6：Fine-Gained-Features\"><a href=\"#改进6：Fine-Gained-Features\" class=\"headerlink\" title=\"改进6：Fine-Gained Features\"></a>改进6：Fine-Gained Features</h3><p>这个trick是受Faster RCNN和SSD方法中使用多个不同feature map提高算法对不同分辨率目标物体的检测能力的启发，加入了一个pass-through层，直接将倒数第二层的$26\\times 26$大小的feature map加进来。</p>\n<p>在具体实现时，是将higher resolution（也就是$26\\times 26$）的feature map stacking在一起。比如，原大小为$26\\times 26 \\times 512$的feature map，因为我们要将其变为$13\\times 13$大小，所以，将在空间上相近的点移到后面的channel上去，这部分可以参考Darknet中<code>reorg_layer</code>的实现。</p>\n<p>使用这一扩展之后的feature map，提高了1%的性能提升。</p>\n<h3 id=\"改进7：多尺度训练（Multi-Scale-Training）\"><a href=\"#改进7：多尺度训练（Multi-Scale-Training）\" class=\"headerlink\" title=\"改进7：多尺度训练（Multi-Scale Training）\"></a>改进7：多尺度训练（Multi-Scale Training）</h3><p>在实际应用时，输入的图像大小有可能是变化的。我们也将这一点考虑进来。因为我们的网络是全卷积神经网络，只有conv和pooling层，没有全连接层，所以可以适应不同大小的图像输入。所以网络结构上是没问题的。</p>\n<p>具体来说，在训练的时候，我们每隔一定的epoch（例如10）就随机改变网络的输入图像大小。由于我们的网络最终降采样的比例是$32$，所以随机生成的图像大小为$32$的倍数，即$\\lbrace 320, 352, \\dots, 608\\rbrace$。</p>\n<p>在实际使用中，如果输入图像的分辨率较低，YOLO V2可以在不错的精度下达到很快的检测速度。这可以被用应在计算能力有限的场合（无GPU或者GPU很弱）或多路视频信号的实时处理。如果输入图像的分辨率较高，YOLO V2可以作为state of the art的检测器，并仍能获得不错的检测速度。对于目前流行的检测方法（Faster RCNN，SSD，YOLO）的精度和帧率之间的关系，见下图。可以看到，作者在30fps处画了一条竖线，这是算法能否达到实时处理的分水岭。Faster RCNN败下阵来，而YOLO V2的不同点代表了不同输入图像分辨率下算法的表现。对于详细数据，见图下的表格对比（VOC 2007上进行测试）。</p>\n<p><img src=\"/img/yolo2_different_methods_comparation.png\" alt=\"不同检测方法的对比\"><br><img src=\"/img/yolo2_different_methods_comparation_in_table.png\" alt=\"不同检测方法的对比\"></p>\n<h3 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h3><p>在Better这部分的末尾，作者给出了一个表格，指出了主要提升性能的措施。例外是网络结构上改为带Anchor box的全卷积网络结构（提升了recall，但对mAP基本无影响）和使用新的网络（计算量少了~33%）。<br><img src=\"/img/yolo2_different_methods_improvement.png\" alt=\"不同改进措施的影响\"></p>\n<h2 id=\"Faster\"><a href=\"#Faster\" class=\"headerlink\" title=\"Faster\"></a>Faster</h2><p>这部分的改进为网络结构的变化。包括Faster RCNN在内的很多检测方法都使用VGG-16作为base network。VGG-16精度够高但是计算量较大（对于大小为$224\\times 224$的单幅输入图像，卷积层就需要30.69B次浮点运算）。在YOLO V1中，我们的网络结构较为简单，在精度上不如VGG-16（ImageNet测试，88.0% vs 90.0%）。</p>\n<p>在YOLO V2中，我们使用了一种新的网络结构Darknet-19（因为base network有19个卷积层）。和VGG相类似，我们的卷积核也都是$3\\times 3$大小，同时每次pooling操作后channel数double。另外，在NIN工作基础上，我们在网络最后使用了global average pooling层，同时使用$1\\times 1$大小的卷积核来做feature map的压缩（分辨率不变，channel减小，新的元素是原来相应位置不同channel的线性组合），同时使用了Batch Normalization技术。具体的网络结构见下表。Darknet-19计算量大大减小，同时精度超过了VGG-16。<br><img src=\"/img/yolo2_dartnet_19_structure.png\" alt=\"Darknet-19的网络结构\"></p>\n<p>在训练过程中，首先在ImageNet 1K上进行分类器的训练。使用数据增强技术（如随机裁剪、旋转、饱和度变化等）。和上面的讨论相对应，首先使用$224\\times 224$大小的图像进行训练，再使用$448\\times 448$的图像进行fine tuning，具体训练参数设置可以参见论文和对应的代码。这里不再多说。</p>\n<p>然后，我们对检测任务进行训练。对网络结构进行微调，去掉最后的卷积层（因为它是我们当初为了得到1K类的分类置信度加上去的），增加$3$个$3\\times 3$大小，channel数为$1024$的卷积层，并每个都跟着一个$1\\times 1$大小的卷积层，channel数由我们最终的检测任务需要的输出决定。对于VOC的检测任务来说，我们需要预测$5$个box，每个需要$5$个参数（相对位置，相对大小和置信度），同时$20$个类别的置信度，所以输出共$5\\times(5+20)=125$。从YOLO V2的<code>yolo_voc.cfg</code><a href=\"https://github.com/pjreddie/darknet/blob/master/cfg/yolo.cfg\">文件</a>中，我们也可以看到如下的对应结构：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div></pre></td><td class=\"code\"><pre><div class=\"line\">[convolutional]</div><div class=\"line\">batch_normalize=1</div><div class=\"line\">size=3</div><div class=\"line\">stride=1</div><div class=\"line\">pad=1</div><div class=\"line\">filters=1024</div><div class=\"line\">activation=leaky</div><div class=\"line\"></div><div class=\"line\">[convolutional]</div><div class=\"line\">size=1</div><div class=\"line\">stride=1</div><div class=\"line\">pad=1</div><div class=\"line\">filters=125</div><div class=\"line\">activation=linear</div></pre></td></tr></table></figure>\n<p>同时，加上上文提到的pass-through结构。</p>\n<h2 id=\"Stronger\"><a href=\"#Stronger\" class=\"headerlink\" title=\"Stronger\"></a>Stronger</h2><p>未完待续</p>"}],"PostAsset":[],"PostCategory":[],"PostTag":[{"post_id":"cjcu6vcr50001qu46mldy0kmn","tag_id":"cjcu6vcre0004qu46fj3l75wa","_id":"cjcu6vcs0000dqu465d03x67i"},{"post_id":"cjcu6vcr50001qu46mldy0kmn","tag_id":"cjcu6vcru0008qu46g4ghtv04","_id":"cjcu6vcs2000fqu46fisz7kjs"},{"post_id":"cjcu6vcra0003qu46tagrhuz3","tag_id":"cjcu6vcrz000bqu46y0acurky","_id":"cjcu6vcs7000iqu46e535pkrh"},{"post_id":"cjcu6vcri0005qu46yyrwcoom","tag_id":"cjcu6vcru0008qu46g4ghtv04","_id":"cjcu6vcsb000mqu46x7idzl6u"},{"post_id":"cjcu6vcsl000pqu46aq2isouk","tag_id":"cjcu6vcre0004qu46fj3l75wa","_id":"cjcu6vcso000rqu465q5u6wo2"},{"post_id":"cjcu6vcrl0006qu465yeu34c2","tag_id":"cjcu6vcsa000kqu46bd4hhuxc","_id":"cjcu6vcsq000uqu46sdmfcqda"},{"post_id":"cjcu6vcrl0006qu465yeu34c2","tag_id":"cjcu6vcsk000oqu463z7y53ri","_id":"cjcu6vcsr000wqu46uls5snk6"},{"post_id":"cjcu6vcsq000vqu467j0mbntx","tag_id":"cjcu6vcre0004qu46fj3l75wa","_id":"cjcu6vcss000zqu4646zyl33c"},{"post_id":"cjcu6vcrp0007qu46luis53hl","tag_id":"cjcu6vcsa000kqu46bd4hhuxc","_id":"cjcu6vcsy0013qu46j9cp1e40"},{"post_id":"cjcu6vcrp0007qu46luis53hl","tag_id":"cjcu6vcsk000oqu463z7y53ri","_id":"cjcu6vct00015qu464kfqn7ln"},{"post_id":"cjcu6vcrv0009qu461gjqi831","tag_id":"cjcu6vcsa000kqu46bd4hhuxc","_id":"cjcu6vct5001bqu46481uat1q"},{"post_id":"cjcu6vcrv0009qu461gjqi831","tag_id":"cjcu6vcsk000oqu463z7y53ri","_id":"cjcu6vct7001dqu467z6s9siw"},{"post_id":"cjcu6vcrw000aqu46c2kjkmv8","tag_id":"cjcu6vcsa000kqu46bd4hhuxc","_id":"cjcu6vcte001jqu46ql0q76ss"},{"post_id":"cjcu6vcrw000aqu46c2kjkmv8","tag_id":"cjcu6vcsk000oqu463z7y53ri","_id":"cjcu6vcth001lqu46wgg326fs"},{"post_id":"cjcu6vcrz000cqu46zs9i694l","tag_id":"cjcu6vcsa000kqu46bd4hhuxc","_id":"cjcu6vcts001rqu468dnoy1sj"},{"post_id":"cjcu6vcrz000cqu46zs9i694l","tag_id":"cjcu6vcsk000oqu463z7y53ri","_id":"cjcu6vctt001tqu46rhril69w"},{"post_id":"cjcu6vctu001uqu46trk0ye4w","tag_id":"cjcu6vcre0004qu46fj3l75wa","_id":"cjcu6vctx001xqu460uxqbr2i"},{"post_id":"cjcu6vctw001wqu46q4y48ohf","tag_id":"cjcu6vcru0008qu46g4ghtv04","_id":"cjcu6vcu00020qu46r6n2xcll"},{"post_id":"cjcu6vcs0000equ46zflv0x6r","tag_id":"cjcu6vcsa000kqu46bd4hhuxc","_id":"cjcu6vcu10022qu46pm38qxe4"},{"post_id":"cjcu6vcs0000equ46zflv0x6r","tag_id":"cjcu6vcsk000oqu463z7y53ri","_id":"cjcu6vcu20025qu46i86p7m54"},{"post_id":"cjcu6vcs6000hqu46ix5rbdus","tag_id":"cjcu6vcsa000kqu46bd4hhuxc","_id":"cjcu6vcu50029qu46dyrleng3"},{"post_id":"cjcu6vcs6000hqu46ix5rbdus","tag_id":"cjcu6vcsk000oqu463z7y53ri","_id":"cjcu6vcu6002bqu46myx2qmmz"},{"post_id":"cjcu6vcs8000jqu46k5shk9as","tag_id":"cjcu6vcsa000kqu46bd4hhuxc","_id":"cjcu6vcug002hqu46fj9850d3"},{"post_id":"cjcu6vcs8000jqu46k5shk9as","tag_id":"cjcu6vcsk000oqu463z7y53ri","_id":"cjcu6vcui002jqu46qi92kyg0"},{"post_id":"cjcu6vcug002iqu46ylakbwvg","tag_id":"cjcu6vcre0004qu46fj3l75wa","_id":"cjcu6vcuj002mqu4692ia6yj0"},{"post_id":"cjcu6vcsb000lqu46mg8mu8jz","tag_id":"cjcu6vcsa000kqu46bd4hhuxc","_id":"cjcu6vcuk002oqu462n0vzqmw"},{"post_id":"cjcu6vcsb000lqu46mg8mu8jz","tag_id":"cjcu6vcsk000oqu463z7y53ri","_id":"cjcu6vcum002rqu46n4p6zodw"},{"post_id":"cjcu6vcsc000nqu46df3r9kvl","tag_id":"cjcu6vcuj002lqu46nap85mk7","_id":"cjcu6vcuo002tqu4641b31miw"},{"post_id":"cjcu6vcsn000qqu46kxddw44r","tag_id":"cjcu6vcul002qqu46ksye1twn","_id":"cjcu6vcur002xqu46b4zlxgr8"},{"post_id":"cjcu6vcso000squ465w2o0909","tag_id":"cjcu6vcul002qqu46ksye1twn","_id":"cjcu6vcut002zqu46urrg7ipu"},{"post_id":"cjcu6vcsr000xqu46edgi0ejw","tag_id":"cjcu6vcul002qqu46ksye1twn","_id":"cjcu6vcux0031qu46kijd0iw2"},{"post_id":"cjcu6vcss0010qu46djlehal0","tag_id":"cjcu6vcul002qqu46ksye1twn","_id":"cjcu6vcuy0033qu4620nepo0e"},{"post_id":"cjcu6vcsw0011qu4696wxtc7d","tag_id":"cjcu6vcul002qqu46ksye1twn","_id":"cjcu6vcv00035qu465p89t9v1"},{"post_id":"cjcu6vcsy0014qu46d0xac3tg","tag_id":"cjcu6vcul002qqu46ksye1twn","_id":"cjcu6vcv20037qu46zb6ayrlt"},{"post_id":"cjcu6vct00016qu46ixydzv8h","tag_id":"cjcu6vcul002qqu46ksye1twn","_id":"cjcu6vcv50039qu460ni1ek1l"},{"post_id":"cjcu6vct20018qu46pn80hrcg","tag_id":"cjcu6vcre0004qu46fj3l75wa","_id":"cjcu6vcv7003bqu46qgcem9y4"},{"post_id":"cjcu6vct20018qu46pn80hrcg","tag_id":"cjcu6vcv40038qu460xxq5w7f","_id":"cjcu6vcv8003cqu46borxodzm"},{"post_id":"cjcu6vct30019qu467r7zgtet","tag_id":"cjcu6vcv7003aqu4642snb70k","_id":"cjcu6vcv9003fqu462w1t0bio"},{"post_id":"cjcu6vct30019qu467r7zgtet","tag_id":"cjcu6vcv8003dqu46lv0t82yt","_id":"cjcu6vcv9003gqu46305hqlr4"},{"post_id":"cjcu6vct6001cqu4663b946a8","tag_id":"cjcu6vcul002qqu46ksye1twn","_id":"cjcu6vcv9003iqu466bhall0g"},{"post_id":"cjcu6vcta001gqu46nmp16g5g","tag_id":"cjcu6vcv9003hqu46tbwex56v","_id":"cjcu6vcvb003kqu464klgz1pz"},{"post_id":"cjcu6vctd001hqu46j2zpk0yh","tag_id":"cjcu6vcsk000oqu463z7y53ri","_id":"cjcu6vcvd003mqu46pjrvuykv"},{"post_id":"cjcu6vctd001hqu46j2zpk0yh","tag_id":"cjcu6vcv8003dqu46lv0t82yt","_id":"cjcu6vcve003nqu46qc523ceq"},{"post_id":"cjcu6vcte001kqu46xe2v3ocm","tag_id":"cjcu6vcv7003aqu4642snb70k","_id":"cjcu6vcvh003qqu46vi14dtd2"},{"post_id":"cjcu6vcte001kqu46xe2v3ocm","tag_id":"cjcu6vcve003oqu46dhlc2igs","_id":"cjcu6vcvi003rqu46sawg833o"},{"post_id":"cjcu6vcti001mqu46h5rzsoxg","tag_id":"cjcu6vcv8003dqu46lv0t82yt","_id":"cjcu6vcvj003tqu46ne9as54y"},{"post_id":"cjcu6vcti001mqu46h5rzsoxg","tag_id":"cjcu6vcsk000oqu463z7y53ri","_id":"cjcu6vcvj003uqu46snl8748u"},{"post_id":"cjcu6vctp001oqu46um6gj3ge","tag_id":"cjcu6vcsk000oqu463z7y53ri","_id":"cjcu6vcvl003xqu46siefle6f"},{"post_id":"cjcu6vctp001oqu46um6gj3ge","tag_id":"cjcu6vcv8003dqu46lv0t82yt","_id":"cjcu6vcvl003yqu46c6avslrz"},{"post_id":"cjcu6vctp001oqu46um6gj3ge","tag_id":"cjcu6vcvj003vqu463you7tl3","_id":"cjcu6vcvl0040qu463mth9k3r"},{"post_id":"cjcu6vctr001pqu46fs6cbv33","tag_id":"cjcu6vcuj002lqu46nap85mk7","_id":"cjcu6vcvn0042qu46m94bwk3k"},{"post_id":"cjcu6vctr001pqu46fs6cbv33","tag_id":"cjcu6vcvl003zqu469cod359e","_id":"cjcu6vcvn0043qu46eqa6gk8j"},{"post_id":"cjcu6vcts001squ46qckj35v8","tag_id":"cjcu6vcvm0041qu464gvz6wy3","_id":"cjcu6vcvo0045qu469ptekgk3"},{"post_id":"cjcu6vcty001yqu46hrp6t45r","tag_id":"cjcu6vcvj003vqu463you7tl3","_id":"cjcu6vcvp0047qu466g0xhyln"},{"post_id":"cjcu6vcu00021qu463t4rn4zu","tag_id":"cjcu6vcuj002lqu46nap85mk7","_id":"cjcu6vcvp0049qu46ol986oya"},{"post_id":"cjcu6vcu10023qu46b6oai86m","tag_id":"cjcu6vcvj003vqu463you7tl3","_id":"cjcu6vcvq004bqu46ju7kb97j"},{"post_id":"cjcu6vcu30026qu46zrc8yvpd","tag_id":"cjcu6vcuj002lqu46nap85mk7","_id":"cjcu6vcvr004dqu46wa8cgj01"},{"post_id":"cjcu6vcu30027qu460bvgzw5q","tag_id":"cjcu6vcv7003aqu4642snb70k","_id":"cjcu6vcvs004gqu46jkyvc0st"},{"post_id":"cjcu6vcu30027qu460bvgzw5q","tag_id":"cjcu6vcv8003dqu46lv0t82yt","_id":"cjcu6vcvs004hqu46kvw53q3a"},{"post_id":"cjcu6vcu5002aqu46r1varffd","tag_id":"cjcu6vcv7003aqu4642snb70k","_id":"cjcu6vcvt004kqu464t09o1rb"},{"post_id":"cjcu6vcu5002aqu46r1varffd","tag_id":"cjcu6vcv8003dqu46lv0t82yt","_id":"cjcu6vcvu004lqu46jhpy6y77"},{"post_id":"cjcu6vcu6002cqu46jq3vqhj8","tag_id":"cjcu6vcvt004jqu463mbk63vp","_id":"cjcu6vcvu004nqu46rakjlizt"},{"post_id":"cjcu6vcu8002equ469lvnbar6","tag_id":"cjcu6vcvu004mqu46tcbk7fo2","_id":"cjcu6vcw1004qqu4672b5af91"},{"post_id":"cjcu6vcu8002equ469lvnbar6","tag_id":"cjcu6vcvv004oqu46yiq1wlcd","_id":"cjcu6vcw1004rqu46cippmnqw"},{"post_id":"cjcu6vcuc002fqu46u6jthijo","tag_id":"cjcu6vcvt004jqu463mbk63vp","_id":"cjcu6vcw2004tqu46mynf89mf"},{"post_id":"cjcu6vcui002kqu4623ztmsf6","tag_id":"cjcu6vcre0004qu46fj3l75wa","_id":"cjcu6vcw2004vqu46wdbdo9w5"},{"post_id":"cjcu6vcui002kqu4623ztmsf6","tag_id":"cjcu6vcw1004squ46qu8uwfss","_id":"cjcu6vcw3004wqu46imbxxwp3"},{"post_id":"cjcu6vcuk002pqu46e7tdx1v8","tag_id":"cjcu6vcw2004uqu4662di8eg0","_id":"cjcu6vcw4004yqu46u9xfd3d5"},{"post_id":"cjcu6vcum002squ46wwfda2l0","tag_id":"cjcu6vcru0008qu46g4ghtv04","_id":"cjcu6vcw70050qu46xr0a4ooi"},{"post_id":"cjcu6vcum002squ46wwfda2l0","tag_id":"cjcu6vcv8003dqu46lv0t82yt","_id":"cjcu6vcw70051qu46fdpd5pei"},{"post_id":"cjcu6vcup002uqu46r43pyll7","tag_id":"cjcu6vcw5004zqu460w90ssm2","_id":"cjcu6vcwa0054qu46vlf36lzh"},{"post_id":"cjcu6vcup002uqu46r43pyll7","tag_id":"cjcu6vcv8003dqu46lv0t82yt","_id":"cjcu6vcwa0055qu46beekgwaq"},{"post_id":"cjcu6vcuq002wqu465ouay5n8","tag_id":"cjcu6vcv7003aqu4642snb70k","_id":"cjcu6vcwb0058qu46wi2gs5qx"},{"post_id":"cjcu6vcuq002wqu465ouay5n8","tag_id":"cjcu6vcw5004zqu460w90ssm2","_id":"cjcu6vcwb0059qu46whhsyzwq"},{"post_id":"cjcu6vcuq002wqu465ouay5n8","tag_id":"cjcu6vcv8003dqu46lv0t82yt","_id":"cjcu6vcwb005aqu46g98ddvra"}],"Tag":[{"name":"tool","_id":"cjcu6vcre0004qu46fj3l75wa"},{"name":"caffe","_id":"cjcu6vcru0008qu46g4ghtv04"},{"name":"cafe","_id":"cjcu6vcrz000bqu46y0acurky"},{"name":"cs131","_id":"cjcu6vcsa000kqu46bd4hhuxc"},{"name":"公开课","_id":"cjcu6vcsk000oqu463z7y53ri"},{"name":"python","_id":"cjcu6vcuj002lqu46nap85mk7"},{"name":"cpp","_id":"cjcu6vcul002qqu46ksye1twn"},{"name":"gsl","_id":"cjcu6vcv40038qu460xxq5w7f"},{"name":"paper","_id":"cjcu6vcv7003aqu4642snb70k"},{"name":"deep learning","_id":"cjcu6vcv8003dqu46lv0t82yt"},{"name":"扯淡","_id":"cjcu6vcv9003hqu46tbwex56v"},{"name":"quantize","_id":"cjcu6vcve003oqu46dhlc2igs"},{"name":"pytorch","_id":"cjcu6vcvj003vqu463you7tl3"},{"name":"qt","_id":"cjcu6vcvl003zqu469cod359e"},{"name":"ubuntu","_id":"cjcu6vcvm0041qu464gvz6wy3"},{"name":"reinforcement learning","_id":"cjcu6vcvt004jqu463mbk63vp"},{"name":"linux","_id":"cjcu6vcvu004mqu46tcbk7fo2"},{"name":"shell","_id":"cjcu6vcvv004oqu46yiq1wlcd"},{"name":"doxygen","_id":"cjcu6vcw1004squ46qu8uwfss"},{"name":"math","_id":"cjcu6vcw2004uqu4662di8eg0"},{"name":"yolo","_id":"cjcu6vcw5004zqu460w90ssm2"}]}}