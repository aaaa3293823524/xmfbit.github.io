---
title: Silver RL课程 - MDP
date: 2017-05-31 13:03:31
tags:
    - reinforcement learning
---
Silver在英国UCL讲授强化学习的slide总结。背景介绍部分略去不表，第一篇首先介绍强化学习中重要的数学基础-马尔科夫决策过程（MDP）。
![MDP](/img/silver_rl_mdp.png)
<!-- more -->

## 马尔科夫性质
不严谨的说法，马尔科夫性质是指未来与过去无关，只与当前的状态有关。我们说某个State是Markov的，等价于下面的等式成立：

$$P[S_{t+1}|S_t] = P[S_{t+1}|S_1, \dots, S_t]$$

定义状态转移概率（State Transition Probability）如下：
$$P_{ss^\prime} = P[S_{t+1}=s^\prime|S_t=s]$$

前后两个时刻的状态不同取值的状态转移概率可以写成一个矩阵的形式。矩阵中的任意元素$P_{i,j}$表示$t$时刻状态$i$在$t+1$时刻转移到状态$j$的概率。矩阵满足行和为$1$的约束。

下面，我们从马尔科夫性质展开，逐步地加入一些额外的参量，一步步引出强化学习中的马尔科夫决策过程。

### 马尔科夫过程
马尔科夫过程（或者叫做马尔科夫链）是指随机过程中的状态满足马尔科夫性质。我们可以使用二元组$(S, P)$来描述马氏过程。其中，
- $S$是一个有限状态集合。
- $P$是状态转移矩阵，定义如上。

### 马尔科夫奖赏过程
马尔科夫奖赏过程（不知道如何翻译，Markov reward process）在马氏过程基础上加上了状态转移过程中的奖赏reward。可以使用四元组$(S, P, R, \gamma)$来表示。其中，
- $R$代表奖励函数，$R\_s = E[R\_{t+1}|S\_t=s]$，是指当前状态为$s$时，下一步状态转移过程中的期望奖励。
- $\gamma$是折旧率（discount），$\gamma \in [0,1]$

定义回报（Return）为当前时刻往后得到的折旧总奖励，即：
$$G_t = R_{t+1}+\gamma R_{t+2}+... = \sum_{k=0}^{\infty}\gamma^kR_{t+k+1}$$

折旧率的引入，有以下几点考虑：
- 在有环存在的马氏过程中，避免了无穷大回报的出现。
- 未来的不确定性对当前的影响较小。
- 事实上的考虑，例如投资市场上，即时的奖励比迟滞的奖励能够有更多的利息。
- 人类行为倾向于即时奖励。
- 如果马氏过程是存在终止的，有的时候也可以使用$\gamma=1$，也就是不打折。

### 值函数
值函数（Value function）的意义是给出了状态$s$的长期回报，以期望的形式（条件期望），如下：
$$v(s) = E[G_t|S_t=s]$$

值函数可以分为两个部分，即时奖励$R_{t+1}$和后续状态的折旧值函数。如下所示：
![推导过程](/img/silver_mdp_value_function.png)

最后一步推导时，
